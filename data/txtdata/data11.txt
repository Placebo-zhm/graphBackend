FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Jiang, YR
   Chen, SY
   Fu, HB
   Gao, L
AF Jiang, Yue-Ren
   Chen, Shu-Yu
   Fu, Hongbo
   Gao, Lin
TI Identity-Aware and Shape-Aware Propagation of Face Editing in Videos
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Videos; Shape; Faces; Three-dimensional displays; Codes; Aerospace
   electronics; Semantics; Editing propagation; face editing; video editing
ID MANIPULATION; IMAGE
AB The development of deep generative models has inspired various facial image editing methods, but many of them are difficult to be directly applied to video editing due to various challenges ranging from imposing 3D constraints, preserving identity consistency, ensuring temporal coherence, etc. To address these challenges, we propose a new framework operating on the StyleGAN2 latent space for identity-aware and shape-aware edit propagation on face videos. In order to reduce the difficulties of maintaining the identity, keeping the original 3D motion, and avoiding shape distortions, we disentangle the StyleGAN2 latent vectors of human face video frames to decouple the appearance, shape, expression, and motion from identity. An edit encoding module is used to map a sequence of image frames to continuous latent codes with 3D parametric control and is trained in a self-supervised manner with identity loss and triple shape losses. Our model supports propagation of edits in various forms: I. direct appearance editing on a specific keyframe, II. implicit editing of face shape via a given reference image, and III. existing latent-based semantic edits. Experiments show that our method works well for various forms of videos in the wild and outperforms an animation-based approach and the recent deep generative techniques.
C1 [Jiang, Yue-Ren; Chen, Shu-Yu; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100045, Peoples R China.
   [Jiang, Yue-Ren; Gao, Lin] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; City University of Hong Kong
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100045, Peoples R China.; Gao, L (corresponding author), Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
EM jiangyueren19s@ict.ac.cn; chenshuyu@ict.ac.cn; hongbofu@cityu.edu.hk;
   gaolin@ict.ac.cn
OI FU, Hongbo/0000-0002-0284-726X
FU National Natural Science Foundation of China [62102403, 61872440];
   Beijing Municipal Natural Science Foundation for Distinguished Young
   Scholars [JQ21013]; China Postdoctoral Science Foundation [2022M713205];
   Open Research Projects of Zhejiang Lab [2021KE0AB06]; Youth Innovation
   Promotion Association CAS; Centre for Applied Computing and Interactive
   Media (ACIM) of School of Creative Media, CityU
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102403 and 61872440, in part by the
   Beijing Municipal Natural Science Foundation for Distinguished Young
   Scholars under Grant JQ21013, in part by China Postdoctoral Science
   Foundation under Grant 2022M713205, in part by the Open Research
   Projects of Zhejiang Lab under Grant 2021KE0AB06, in part by Youth
   Innovation Promotion Association CAS, and in part by the Centre for
   Applied Computing and Interactive Media (ACIM) of School of Creative
   Media, CityU.
CR Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2020, PROC CVPR IEEE, P8293, DOI 10.1109/CVPR42600.2020.00832
   Alaluf Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459805
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bonettini N, 2021, INT C PATT RECOG, P5012, DOI 10.1109/ICPR48806.2021.9412711
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chai YJ, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-020-0133-7
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen BJ, 2023, IEEE T VIS COMPUT GR, V29, P3617, DOI 10.1109/TVCG.2022.3166159
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Collins E, 2020, PROC CVPR IEEE, P5770, DOI 10.1109/CVPR42600.2020.00581
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Deng Y, 2020, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR42600.2020.00520
   Denton E, 2020, Arxiv, DOI arXiv:1906.06439
   Erik Harkonen, 2020, Advances in Neural Information Processing Systems, V33, P9841
   Ge WY, 2024, Arxiv, DOI arXiv:2110.03309
   Goetschalckx L, 2019, IEEE I CONF COMP VIS, P5743, DOI 10.1109/ICCV.2019.00584
   He Zhenliang, 2021, ICCV, P14408
   Hensel M, 2017, ADV NEUR IN, V30
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336
   Jiang K, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555377
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Kagaya M, 2011, IEEE T VIS COMPUT GR, V17, P74, DOI 10.1109/TVCG.2010.25
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kasten Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480546
   Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387
   Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769
   Li XY, 2022, IEEE T VIS COMPUT GR, V28, P2938, DOI 10.1109/TVCG.2021.3049419
   Ling JW, 2023, IEEE T VIS COMPUT GR, V29, P3630, DOI 10.1109/TVCG.2022.3166666
   Liu FL, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530056
   Ma LQ, 2012, COMPUT GRAPH-UK, V36, P1005, DOI 10.1016/j.cag.2012.08.001
   Mallikarjun BR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459765
   Paszke A, 2019, ADV NEUR IN, V32
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Roich D, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3544777
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen YJ, 2020, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR42600.2020.00926
   Shi M, 2023, IEEE T VIS COMPUT GR, V29, P2965, DOI 10.1109/TVCG.2022.3146000
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Siarohin A., 2019, ADV NEURAL INFORM PR, P7137
   Su WC, 2023, IEEE T VIS COMPUT GR, V29, P4074, DOI 10.1109/TVCG.2022.3178734
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tewari A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417803
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Texler O, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392453
   Nguyen T, 2021, PROC CVPR IEEE, P13300, DOI 10.1109/CVPR46437.2021.01310
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Tzaban R, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555382
   Voynov Andrey, 2022, arXiv
   Wang T.-C., 2019, ADV NEURAL INFORM PR, P5013
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Y., 2021, ICCV, P6721
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1135, DOI 10.1109/TVCG.2010.125
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Yao Xu, 2021, P ICCV, P13789
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou Peng, 2021, arXiv
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
   Zhuang JT, 2020, ADV NEUR IN, V33
NR 74
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3444
EP 3456
DI 10.1109/TVCG.2023.3235364
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700019
PM 37018564
DA 2024-11-06
ER

PT J
AU Choi, J
   Oh, HJ
   Lee, H
   Kim, S
   Kwon, SK
   Jeong, WK
AF Choi, JunYoung
   Oh, Hyun-Jic
   Lee, Hakjun
   Kim, Suyeon
   Kwon, Seok-Kyu
   Jeong, Won-Ki
TI MitoVis: A Unified Visual Analytics System for End-to-End Neuronal
   Mitochondria Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Morphology; Neurons; Microscopy; Dendrites
   (neurons); Deep learning; Axons; Biomedical and medical visualization;
   machine learning; task and requirements analysis; user interfaces;
   intelligence analysis
ID VISUALIZATION; FISSION
AB Neurons have a polarized structure, with dendrites and axons, and compartment-specific functions can be affected by the dwelling mitochondria. Recent studies have shown that the morphology of mitochondria is closely related to the functions of neurons and neurodegenerative diseases. However, the conventional mitochondria analysis workflow mainly relies on manual annotations and generic image-processing software. Moreover, even though there have been recent developments in automatic mitochondria analysis using deep learning, the application of existing methods in a daily analysis remains challenging because the performance of a pretrained deep learning model can vary depending on the target data, and there are always errors in inference time, requiring human proofreading. To address these issues, we introduce MitoVis, a novel visualization system for end-to-end data processing and an interactive analysis of the morphology of neuronal mitochondria. MitoVis introduces a novel active learning framework based on recent contrastive learning, which allows accurate fine-tuning of the neural network model. MitoVis also provides novel visual guides for interactive proofreading so that users can quickly identify and correct errors in the result with minimal effort. We demonstrate the usefulness and efficacy of the system via case studies conducted by neuroscientists. The results show that MitoVis achieved up to 13.3x faster total analysis time in the case study compared to the conventional manual analysis workflow.
C1 [Choi, JunYoung; Oh, Hyun-Jic; Lee, Hakjun; Jeong, Won-Ki] Korea Univ, Dept Comp Sci & Engn, Seoul 02841, South Korea.
   [Kim, Suyeon; Kwon, Seok-Kyu] Korea Inst Sci & Technol, Brain Sci Inst, Seoul 02792, South Korea.
C3 Korea University; Korea Institute of Science & Technology (KIST)
RP Jeong, WK (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul 02841, South Korea.
EM juny0603@gmail.com; hyunjic0127@korea.ac.kr; hjlee0413@korea.ac.kr;
   imas3104@kist.re.kr; skkwon@kist.re.kr; wkjeong@korea.ac.kr
RI choi, junyoung/T-4389-2019; Jeong, Won-Ki/F-8171-2011
OI , hjoh/0000-0002-4599-151X; Jeong, Won-Ki/0000-0002-9393-6451; Kwon,
   Seok-Kyu/0000-0002-7280-9867; Choi, JunYoung/0000-0002-4255-4402
FU Bio & Medical Technology Development Program of the National Research
   Foundation of Korea (NRF) - Ministry of Science and ICT (MSIT)
   [NRF-2019M3E5D2A01063819, NRF-2019M3E5D2A01063794]; Basic Science
   Research Program through the NRF - Ministry of Education
   [NRF-2021R1A6A1A13044830]; Korea Health Technology R&D Project through
   the Korea Health Industry Development Institute (KHIDI) - Ministry of
   Health Welfare [HI18C0316]; ICT Creative Consilience program of the
   Institute for Information & communications Technology Planning&
   Evaluation (IITP) - MSIT [IITP-2023-2020-0-01819]; Korea Institute of
   Science and Technology (KIST) Institutional Program, Republic of Korea
   [2E31511]; Korea University Grant
FX This work was supported in part by the Bio & Medical Technology
   Development Program of the National Research Foundation of Korea
   (NRF)funded by the Ministry of Science and ICT (MSIT) under Grants
   NRF-2019M3E5D2A01063819 and NRF-2019M3E5D2A01063794, in part by the
   Basic Science Research Program through the NRF funded by the Ministry of
   Education under Grant NRF-2021R1A6A1A13044830, in part by a grant from
   the Korea Health Technology R&D Project through the Korea Health
   Industry Development Institute (KHIDI) funded by the Ministry of Health
   & Welfare under Grant HI18C0316, in part by the ICT Creative Consilience
   program under Grant IITP-2023-2020-0-01819 of the Institute for
   Information & communications Technology Planning& Evaluation (IITP)
   funded by MSIT, in part by the Korea Institute of Science and Technology
   (KIST) Institutional Program, Republic of Korea under Grant 2E31511, and
   in part by a Korea University Grant.
CR Agus M, 2019, COMPUT GRAPH FORUM, V38, P427, DOI 10.1111/cgf.13700
   [Anonymous], 2021, R LANG ENV STAT COMP
   Baek SH, 2017, J NEUROSCI, V37, P5099, DOI 10.1523/JNEUROSCI.2385-16.2017
   Bäuerle A, 2020, COMPUT GRAPH FORUM, V39, P195, DOI 10.1111/cgf.13973
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Bernard J, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13406
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Beyer J, 2013, IEEE T VIS COMPUT GR, V19, P2868, DOI 10.1109/TVCG.2013.142
   Bido F. N., 2017, Sci. Rep., V7, P1
   Boorboor S, 2019, IEEE T VIS COMPUT GR, V25, P1018, DOI 10.1109/TVCG.2018.2864852
   Bria A, 2015, I S BIOMED IMAGING, P520, DOI 10.1109/ISBI.2015.7163925
   Bro R, 2014, ANAL METHODS-UK, V6, P2812, DOI 10.1039/c3ay41907j
   Budd S, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102062
   Cho S, 2021, I S BIOMED IMAGING, P761, DOI 10.1109/ISBI48211.2021.9434105
   Du H, 2010, P NATL ACAD SCI USA, V107, P18670, DOI 10.1073/pnas.1006586107
   Fecher C, 2019, NAT NEUROSCI, V22, P1731, DOI 10.1038/s41593-019-0479-z
   Feng LQ, 2015, ENEURO, V2, DOI 10.1523/ENEURO.0049-14.2014
   Fischer CA, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101601
   Fogo GM, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84528-8
   Ghahremani P, 2022, IEEE T VIS COMPUT GR, V28, P4951, DOI 10.1109/TVCG.2021.3109460
   Guo BH, 2020, NEUROBIOL AGING, V96, P223, DOI 10.1016/j.neurobiolaging.2020.09.011
   Hu HZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16271, DOI 10.1109/ICCV48922.2021.01598
   Jadhav S, 2019, IEEE T VIS COMPUT GR, V25, P2725, DOI 10.1109/TVCG.2018.2856744
   Jeong WK, 2010, IEEE COMPUT GRAPH, V30, P58, DOI 10.1109/MCG.2010.56
   Jun Young Choi, 2022, 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), P604, DOI 10.1109/VRW55335.2022.00153
   Karimov A, 2015, COMPUT GRAPH FORUM, V34, P91, DOI 10.1111/cgf.12621
   Khosla P., 2020, P ADV NEUR INF PROC, V33, P18661
   Kim GH, 2020, MOL BRAIN, V13, DOI 10.1186/s13041-020-00668-4
   Krueger R, 2020, IEEE T VIS COMPUT GR, V26, P227, DOI 10.1109/TVCG.2019.2934547
   Lamprecht MR, 2007, BIOTECHNIQUES, V42, P71, DOI 10.2144/000112257
   Lee A, 2018, CURR OPIN PHYSIOL, V3, P82, DOI 10.1016/j.cophys.2018.03.009
   Lekschas F, 2020, IEEE T VIS COMPUT GR, V26, P611, DOI 10.1109/TVCG.2019.2934555
   Lewis TL, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07416-2
   Lihavainen E, 2012, BIOINFORMATICS, V28, P1050, DOI 10.1093/bioinformatics/bts073
   Lin Yang, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10435, P399, DOI 10.1007/978-3-319-66179-7_46
   Liu YJ, 2020, MECH AGEING DEV, V186, DOI 10.1016/j.mad.2020.111212
   Magliaro C, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00036
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   Merrill RA, 2017, NEUROMETHODS, V123, P31, DOI 10.1007/978-1-4939-6890-9_2
   Microsoft HoloLens, About us
   Mohammed H, 2018, IEEE T VIS COMPUT GR, V24, P853, DOI 10.1109/TVCG.2017.2744278
   Ozdemir F, 2018, LECT NOTES COMPUT SC, V11045, P183, DOI 10.1007/978-3-030-00889-5_21
   Park C, 2021, I S BIOMED IMAGING, P1466, DOI 10.1109/ISBI48211.2021.9434102
   Paszke A, 2019, Arxiv, DOI arXiv:1912.01703
   Peng HC, 2014, NAT PROTOC, V9, P193, DOI 10.1038/nprot.2014.011
   Pfister H., 2014, Scientific Visualization, P221
   Popov V, 2005, J COMP NEUROL, V492, P50, DOI 10.1002/cne.20682
   Prisma, About us
   Ramonet D, 2013, CELL DEATH DIFFER, V20, P77, DOI 10.1038/cdd.2012.95
   Rangaraju V, 2019, J NEUROSCI, V39, P8200, DOI 10.1523/JNEUROSCI.1157-19.2019
   Rangaraju V, 2019, CELL, V176, P73, DOI 10.1016/j.cell.2018.12.013
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saalfeld S, 2009, BIOINFORMATICS, V25, P1984, DOI 10.1093/bioinformatics/btp266
   Schon EA, 2011, NEURON, V70, P1033, DOI 10.1016/j.neuron.2011.06.003
   Smailagic A, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P481, DOI 10.1109/ICMLA.2018.00078
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Thabtah F, 2020, INFORM SCIENCES, V513, P429, DOI 10.1016/j.ins.2019.11.004
   Toyama EQ, 2016, SCIENCE, V351, P275, DOI 10.1126/science.aab4138
   Varkuti BH, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaw8702
   Wan Y, 2012, IEEE PAC VIS SYMP, P201, DOI 10.1109/PacificVis.2012.6183592
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang XL, 2009, J NEUROSCI, V29, P9090, DOI 10.1523/JNEUROSCI.1357-09.2009
   Wiemerslage L, 2016, J NEUROSCI METH, V262, P56, DOI 10.1016/j.jneumeth.2016.01.008
   Wu YK, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118482
   Zahedi A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-34455-y
   Zeng HK, 2017, NAT REV NEUROSCI, V18, P530, DOI 10.1038/nrn.2017.85
   Zhou H, 2021, NEUROINFORMATICS, V19, P305, DOI 10.1007/s12021-020-09484-6
   Zhou ZW, 2017, PROC CVPR IEEE, P4761, DOI 10.1109/CVPR.2017.506
NR 68
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3457
EP 3473
DI 10.1109/TVCG.2022.3233548
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700077
DA 2024-11-06
ER

PT J
AU Yang, ZP
   Wen, YH
   Chen, SY
   Liu, X
   Gao, Y
   Liu, YJ
   Gao, L
   Fu, HB
AF Yang, Zhipeng
   Wen, Yu-Hui
   Chen, Shu-Yu
   Liu, Xiao
   Gao, Yuan
   Liu, Yong-Jin
   Gao, Lin
   Fu, Hongbo
TI Keyframe Control of Music-Driven 3D Dance Generation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Humanities; Animation; Three-dimensional displays; Deep learning;
   Probabilistic logic; Interpolation; Task analysis; 3D animation;
   choreography; generative flows; multi-modal; music-driven
ID NETWORK; CAPTURE; MOTION
AB For 3D animators, choreography with artificial intelligence has attracted more attention recently. However, most existing deep learning methods mainly rely on music for dance generation and lack sufficient control over generated dance motions. To address this issue, we introduce the idea of keyframe interpolation for music-driven dance generation and present a novel transition generation technique for choreography. Specifically, this technique synthesizes visually diverse and plausible dance motions by using normalizing flows to learn the probability distribution of dance motions conditioned on a piece of music and a sparse set of key poses. Thus, the generated dance motions respect both the input musical beats and the key poses. To achieve a robust transition of varying lengths between the key poses, we introduce a time embedding at each timestep as an additional condition. Extensive experiments show that our model generates more realistic, diverse, and beat-matching dance motions than the compared state-of-the-art methods, both qualitatively and quantitatively. Our experimental results demonstrate the superiority of the keyframe-based control for improving the diversity of the generated dance motions.
C1 [Yang, Zhipeng; Chen, Shu-Yu; Gao, Lin] Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Yang, Zhipeng; Chen, Shu-Yu; Gao, Lin] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Wen, Yu-Hui; Liu, Yong-Jin] Tsinghua Univ, CS Dept, BNRist, Beijing 100190, Peoples R China.
   [Liu, Xiao; Gao, Yuan] Tomorrow Adv Life Educ Grp, Beijing 100190, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Tsinghua University; City University of Hong Kong
RP Gao, L (corresponding author), Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.; Liu, YJ (corresponding author), Tsinghua Univ, CS Dept, BNRist, Beijing 100190, Peoples R China.
EM yangzhipeng19s@ict.ac.cn; wenyh1616@tsinghua.edu.cn;
   chenshuyu@ict.ac.cn; liuxiao15@tal.com; gaoyuan23@tal.com;
   liuyongjin@tsinghua.edu.cn; gaolin@ict.ac.cn; hongbofu@cityu.edu.hk
RI Wen, Yu-Hui/JCO-0775-2023
OI Wen, Yu-Hui/0000-0001-6195-9782; FU, Hongbo/0000-0002-0284-726X
FU National Key R&D Program of China [2020AAA0104500]; National Natural
   Science Foundation of China [62102403, 61725204, 62202257]; Beijing
   Natural Science Foundation [L222008]; Beijing Municipal Natural Science
   Foundation for Distinguished Young Scholars [JQ21013]; China
   Postdoctoral Science Foundation [2021M701891, 2022M713205]; Youth
   Innovation Promotion Association CAS
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020AAA0104500, in part by the National Natural Science
   Foundation of China under Grants 62102403, 61725204, and 62202257, in
   part by Beijing Natural Science Foundation under Grant L222008, in part
   by the Beijing Municipal Natural Science Foundation for Distinguished
   Young Scholars under Grant JQ21013, in part by China Postdoctoral
   Science Foundation under Grants 2021M701891 and 2022M713205, and in part
   by Youth Innovation Promotion Association CAS.
CR Aksan E, 2019, IEEE I CONF COMP VIS, P7143, DOI 10.1109/ICCV.2019.00724
   Alemi O., 2017, networks, V8
   Aristidou A, 2023, IEEE T VIS COMPUT GR, V29, P3519, DOI 10.1109/TVCG.2022.3163676
   Chai YJ, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-020-0133-7
   Chen K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459932
   Choi B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925970
   Ciccone L, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099570
   Ciccone L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322938
   Du XX, 2019, IEEE ROBOT AUTOM LET, V4, P1501, DOI 10.1109/LRA.2019.2895266
   Ferreira JP, 2021, COMPUT GRAPH-UK, V94, P11, DOI 10.1016/j.cag.2020.09.009
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Grassia F. S., 1998, Journal of Graphics Tools, V3, P29
   Harvey FG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392480
   Harvey FG, 2018, SA'18: SIGGRAPH ASIA 2018 TECHNICAL BRIEFS, DOI 10.1145/3283254.3283277
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Heusel M., 2017, P ADV NEUR INF PROC, P6629
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Holden Daniel, 2015, SIGGRAPH Asia 2015 Technical Briefs
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Izani M, 2004, IEEE INFOR VIS, P849, DOI 10.1109/IV.2004.1320239
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D.P., 2016, Advances in neural information processing systems, V29
   Kingma DP, 2018, ADV NEUR IN, V31
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lee H.-Y., 2019, P 33 C NEUR INF PROC, P3581
   Li RL, 2021, Arxiv, DOI arXiv:2101.08779
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lucic M, 2018, ADV NEUR IN, V31
   McFee B., 2015, P PYTH SCI C, P18, DOI 10.25080/Majora-7b98-3ed-003
   Moghaddam ER, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P253, DOI 10.1109/CW.2014.42
   Mou T.-Y., 2018, EURASIA J. Math., Sci. Technol. Educ
   Neagle R. J., 2004, AISB 2004 Convention: Motion, Emotion and Cognition. Symposium on Language, Speech and Gesture for Expressive Characters, P86
   Paszke A, 2019, ADV NEUR IN, V32
   Qiao YL, 2022, IEEE T VIS COMPUT GR, V28, P1906, DOI 10.1109/TVCG.2020.3028961
   Ren XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P46, DOI 10.1145/3394171.3413932
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Ruiz AH, 2019, IEEE I CONF COMP VIS, P7133, DOI 10.1109/ICCV.2019.00723
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Sun GF, 2021, IEEE T MULTIMEDIA, V23, P497, DOI 10.1109/TMM.2020.2981989
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   Valle-Pérez G, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480570
   Vaswani A., 2017, Advances in neural information processing systems
   Wen YH, 2021, PROC CVPR IEEE, P13607, DOI 10.1109/CVPR46437.2021.01340
   Xia SH, 2017, J COMPUT SCI TECH-CH, V32, P536, DOI 10.1007/s11390-017-1742-y
   Yalta N, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851872
   Ye ZJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P744, DOI 10.1145/3394171.3414005
   Zhang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201366
   Zhang XY, 2018, ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION, AND GAMES (MIG 2018), DOI 10.1145/3274247.3274502
   Zhuang WL, 2020, Arxiv, DOI arXiv:2006.05743
   Zhuang WL, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485664
NR 54
TC 1
Z9 1
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3474
EP 3486
DI 10.1109/TVCG.2023.3235538
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700020
PM 37021894
DA 2024-11-06
ER

PT J
AU Xiong, CY
   Lee-Robbins, E
   Zhang, ICY
   Gaba, A
   Franconeri, S
AF Xiong, Cindy
   Lee-Robbins, Elsie
   Zhang, Icy
   Gaba, Aimen
   Franconeri, Steven
TI Reasoning Affordances With Tables and Bar Charts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Cognition; Urban areas; Bars; Skin; Affordances;
   Weapons; tabular displays; empirical evaluation; reasoning
ID CONFIRMATION BIAS; INFORMATION; SCIENCE; VISUALIZATIONS; PERCEPTION;
   FREQUENCY; KNOWLEDGE; PEOPLE; FORMAT; IMPACT
AB A viewer's existing beliefs can prevent accurate reasoning with data visualizations. In particular, confirmation bias can cause people to overweigh information that confirms their beliefs, and dismiss information that disconfirms them. We tested whether confirmation bias exists when people reason with visualized data and whether certain visualization designs can elicit less biased reasoning strategies. We asked crowdworkers to solve reasoning problems that had the potential to evoke both poor reasoning strategies and confirmation bias. We created two scenarios, one in which we primed people with a belief before asking them to make a decision, and another in which people held pre-existing beliefs. The data was presented as either a table, a bar table, or a bar chart. To correctly solve the problem, participants should use a complex reasoning strategy to compare two ratios, each between two pairs of values. But participants could also be tempted to use simpler, superficial heuristics, shortcuts, or biased strategies to reason about the problem. Presenting the data in a table format helped participants reason with the correct ratio strategy while showing the data as a bar table or a bar chart led participants towards incorrect heuristics. Confirmation bias was not significantly present when beliefs were primed, but it was present when beliefs were pre-existing. Additionally, the table presentation format was more likely to afford the ratio reasoning strategy, and the use of ratio strategy was more likely to lead to the correct answer. These findings suggest that data presentation formats can affect affordances for reasoning.
C1 [Xiong, Cindy] UMass Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.
   [Lee-Robbins, Elsie] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Zhang, Icy] UCLA, Los Angeles, CA 90095 USA.
   [Gaba, Aimen] UMass Amherst, Amherst, MA 01003 USA.
   [Franconeri, Steven] Northwestern Univ, Evanston, IL 60208 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   University of Michigan System; University of Michigan; University of
   California System; University of California Los Angeles; University of
   Massachusetts System; University of Massachusetts Amherst; Northwestern
   University
RP Xiong, CY (corresponding author), UMass Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.
EM cindy.xiong@cs.umass.edu; elsielee@umich.edu; yunyi9847@g.ucla.edu;
   agaba@umass.edu; franconeri@northwestern.edu
RI Lee-Robbins, Elsie/IUP-2028-2023
OI Franconeri, Steven/0000-0001-5244-9764; Lee-Robbins,
   Elsie/0000-0002-4080-6506; Xiong Bearfield, Cindy/0000-0002-1451-4083
FU NSF [CHS-1901485]
FX This work was supported by NSF under Grant CHS-1901485.
CR Ajani K, 2022, IEEE T VIS COMPUT GR, V28, P3351, DOI 10.1109/TVCG.2021.3068337
   Andrade EB, 2011, ORGAN BEHAV HUM DEC, V116, P252, DOI 10.1016/j.obhdp.2011.07.002
   BADDELEY AD, 1978, ERGONOMICS, V21, P627, DOI 10.1080/00140137808931764
   Bartram L., 2021, arXiv
   Binder K, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01186
   Burns A, 2020, 2020 IEEE WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2020), P19, DOI 10.1109/BELIV51497.2020.00010
   Bylinskii Z, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/3126594.3126653
   Card SK., 1999, READINGS INFORM VISU
   CHAIKEN S, 1980, J PERS SOC PSYCHOL, V39, P752, DOI 10.1037/0022-3514.39.5.752
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P531, DOI 10.2307/2288400
   Cook MB, 2008, HUM FACTORS, V50, P745, DOI 10.1518/001872008X354183
   Dimara E, 2020, IEEE T VIS COMPUT GR, V26, P1413, DOI 10.1109/TVCG.2018.2872577
   Dimara E, 2019, IEEE T VIS COMPUT GR, V25, P850, DOI 10.1109/TVCG.2018.2865233
   Dimara E, 2017, IEEE T VIS COMPUT GR, V23, P471, DOI 10.1109/TVCG.2016.2598594
   Dragicevic P, 2018, IEEE T VIS COMPUT GR, V24, P781, DOI 10.1109/TVCG.2017.2744298
   Ellis A., 2015, P IEEE WORKSH VIS DE, P1
   Ellis Geoffrey, 2018, Cognitive Biases in Visualizations
   Evans JST, 2013, PERSPECT PSYCHOL SCI, V8, P223, DOI 10.1177/1745691612460685
   Feldman-Stewart D, 2000, MED DECIS MAKING, V20, P228, DOI 10.1177/0272989X0002000208
   Franconeri S. L., 2013, The nature and status of visual resources
   Franconeri SL, 2021, PSYCHOL SCI PUBL INT, V22, P110, DOI 10.1177/15291006211051956
   Franconeri SL, 2021, CURR DIR PSYCHOL SCI, V30, P367, DOI 10.1177/09637214211009512
   Franconeri SL, 2012, COGNITION, V122, P210, DOI 10.1016/j.cognition.2011.11.002
   Gaba Aimen, 2023, IEEE Trans Vis Comput Graph, V29, P1211, DOI 10.1109/TVCG.2022.3209456
   Garcia-Retamero R, 2009, AM J PUBLIC HEALTH, V99, P2196, DOI 10.2105/AJPH.2009.160234
   GIGERENZER G, 1995, PSYCHOL REV, V102, P684, DOI 10.1037/0033-295X.102.4.684
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Hawley ST, 2008, PATIENT EDUC COUNS, V73, P448, DOI 10.1016/j.pec.2008.07.023
   Hegarty M, 2010, J EXP PSYCHOL LEARN, V36, P37, DOI 10.1037/a0017683
   Heiser J, 2006, COGNITIVE SCI, V30, P581, DOI 10.1207/s15516709cog0000_70
   HIGGINS ET, 1985, J EXP PSYCHOL LEARN, V11, P59, DOI 10.1037/0278-7393.11.1.59
   Hink JK, 1996, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY - 40TH ANNUAL MEETING, VOLS 1 AND 2, P1155, DOI 10.1177/154193129604002302
   Kahan D.M., 2017, BEHAV PUBLIC POLICY, V1, P54, DOI [10.1017/bpp.2016.2, DOI 10.1017/BPP.2016.2]
   Kahneman D, 2011, Thinking, Fast and Slow
   Kappes A, 2020, NAT NEUROSCI, V23, P130, DOI 10.1038/s41593-019-0549-2
   Karduni A, 2021, IEEE T VIS COMPUT GR, V27, P978, DOI 10.1109/TVCG.2020.3029412
   Khan A, 2015, INT J HUM-COMPUT ST, V83, P94, DOI 10.1016/j.ijhcs.2015.07.001
   Killen CP, 2020, INT J PROJ MANAG, V38, P267, DOI 10.1016/j.ijproman.2020.04.002
   Kim Y.-S., 2019, P CHI C HUM FACT COM, P14
   Kim YS, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1375, DOI 10.1145/3025453.3025592
   Klayman J., 1995, PSYCHOL LEARN MOTIV, V32, P385, DOI [DOI 10.1016/S0079-7421(08)60315-1, 10.1016/s0079-7421(08)60315-1]
   LARKIN JH, 1987, COGNITIVE SCI, V11, P65, DOI 10.1016/S0364-0213(87)80026-5
   LI GH, 1994, AVIAT SPACE ENVIR MD, V65, P944
   Lovett A., 2022, What does the chart say? Grouping cues guide viewer comparisons and conclusions in visualizations
   Mahajan S, 2022, COMPUT GRAPH FORUM, V41, P477, DOI 10.1111/cgf.14556
   Mantri Prateek, 2023, IEEE Trans Vis Comput Graph, V29, P1005, DOI 10.1109/TVCG.2022.3209467
   Mendel R, 2011, PSYCHOL MED, V41, P2651, DOI 10.1017/S0033291711000808
   Meyer J, 1999, HUM FACTORS, V41, P570, DOI 10.1518/001872099779656707
   Micallef L, 2012, IEEE T VIS COMPUT GR, V18, P2536, DOI 10.1109/TVCG.2012.199
   Michal AL, 2017, COGN RES, V2, DOI 10.1186/s41235-017-0059-2
   Norman D., 2013, The Design of Everyday Things: Revised and Expanded Edition
   Nothelfer C, 2020, IEEE T VIS COMPUT GR, V26, P311, DOI 10.1109/TVCG.2019.2934801
   Nothelfer S., 2018, J. Vis., V18, P321
   O'Sullivan ED, 2018, J ROY COLL PHYS EDIN, V48, P225, DOI 10.4997/JRCPE.2018.306
   Ottley A, 2016, IEEE T VIS COMPUT GR, V22, P529, DOI 10.1109/TVCG.2015.2467758
   Padilla LM, 2018, COGN RES, V3, DOI 10.1186/s41235-018-0120-9
   Pandey AV, 2014, IEEE T VIS COMPUT GR, V20, P2211, DOI 10.1109/TVCG.2014.2346419
   Pariser Eli, 2011, The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think
   Patterson RE, 2014, COMPUT GRAPH-UK, V42, P42, DOI 10.1016/j.cag.2014.03.002
   Picon E., 2017, J. Vis., V17, P1284
   Saket B, 2019, IEEE T VIS COMPUT GR, V25, P2505, DOI 10.1109/TVCG.2018.2829750
   Schaubroeck J., 1991, HUM PERFORM, V4, P127
   Shah P, 2011, TOP COGN SCI, V3, P560, DOI 10.1111/j.1756-8765.2009.01066.x
   SNYDER M, 1979, J EXP SOC PSYCHOL, V15, P330, DOI 10.1016/0022-1031(79)90042-8
   Stasko J, 2008, INFORM VISUAL, V7, P118, DOI 10.1057/palgrave.ivs.9500180
   Sukumar R., 2018, Biases Visualizations, P161
   Szafir DA, 2016, J VISION, V16, DOI 10.1167/16.5.11
   Tal A, 2016, PUBLIC UNDERST SCI, V25, P117, DOI 10.1177/0963662514549688
   Talbot J, 2014, IEEE T VIS COMPUT GR, V20, P2152, DOI 10.1109/TVCG.2014.2346320
   Tingley D, 2014, J STAT SOFTW, V59
   Tsai J., 2011, P HUM FACT ERG SOC A, P385, DOI DOI 10.1177/1071181311551079
   TVERSKY A, 1991, Q J ECON, V106, P1039, DOI 10.2307/2937956
   Tversky B., 2014, Handbook of Human Centric Visualization, P3
   Valdez AC, 2018, IEEE T VIS COMPUT GR, V24, P584, DOI 10.1109/TVCG.2017.2744138
   Vallée-Tourangeau F, 2008, EUR J COGN PSYCHOL, V20, P177, DOI 10.1080/09541440601056588
   Wall E, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P111, DOI [10.1109/visual.2019.8933611, 10.1109/VISUAL.2019.8933611]
   WASON PC, 1968, Q J EXP PSYCHOL, V20, P273, DOI 10.1080/14640746808400161
   WASSERMAN EA, 1990, J EXP PSYCHOL LEARN, V16, P509, DOI 10.1037/0278-7393.16.3.509
   Winter L.-C., 2017, Doctoral Diss
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Wolfe JM, 1998, PSYCHOL SCI, V9, P33, DOI 10.1111/1467-9280.00006
   Wright W., 2006, Conference on Human Factors in Computing Systems. CHI2006, P801
   Xiong A., 2022, P CHI C HUM FACT COM, P1
   Xiong C., 2021, J. Vis., V21, P2095
   Xiong Cindy, 2023, IEEE Trans Vis Comput Graph, V29, P493, DOI 10.1109/TVCG.2022.3209405
   Xiong C, 2022, IEEE T VIS COMPUT GR, V28, P955, DOI 10.1109/TVCG.2021.3114823
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P853, DOI 10.1109/TVCG.2019.2934399
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P3051, DOI 10.1109/TVCG.2019.2917689
   YI YJ, 1990, J ADVERTISING, V19, P40, DOI 10.1080/00913367.1990.10673186
   Zacks J, 1999, MEM COGNITION, V27, P1073, DOI 10.3758/BF03201236
   Zacks JM, 2020, POL INS BEH BRAIN SC, V7, P52, DOI 10.1177/2372732219893712
NR 91
TC 2
Z9 3
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3487
EP 3502
DI 10.1109/TVCG.2022.3232959
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700025
PM 37015636
DA 2024-11-06
ER

PT J
AU Di Giacomo, E
   Didimo, W
   Liotta, G
   Montecchiani, F
   Tappini, A
AF Di Giacomo, Emilio
   Didimo, Walter
   Liotta, Giuseppe
   Montecchiani, Fabrizio
   Tappini, Alessandra
TI Comparative Study and Evaluation of Hybrid Visualizations of Graphs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Analytical models; Layout; Computational
   modeling; Social networking (online); Sparse matrices; Evaluation;
   hybrid visualizations; network visualization; user study
ID COMMUNITY; NETWORK; ALGORITHMS; CHORDLINK; NODETRIX; USER
AB Hybrid visualizations combine different metaphors into a single network layout, in order to help humans in finding the "right way" of displaying the different portions of the network, especially when it is globally sparse and locally dense. We investigate hybrid visualizations in two complementary directions: (i) On the one hand, we evaluate the effectiveness of different hybrid visualization models through a comparative user study; (ii) On the other hand, we estimate the usefulness of an interactive visualization that integrates all the considered hybrid models together. The results of our study provide some hints about the usefulness of the different hybrid visualizations for specific tasks of analysis and indicates that integrating different hybrid models into a single visualization may offer a valuable tool of analysis.
C1 [Di Giacomo, Emilio; Didimo, Walter; Liotta, Giuseppe; Montecchiani, Fabrizio; Tappini, Alessandra] Univ Perugia, Dept Engn, I-06123 Perugia, Italy.
C3 University of Perugia
RP Tappini, A (corresponding author), Univ Perugia, Dept Engn, I-06123 Perugia, Italy.
EM emilio.digiacomo@unipg.it; walter.didimo@unipg.it;
   giuseppe.liotta@unipg.it; fabrizio.montecchiani@unipg.it;
   alessandra.tappini@unipg.it
RI Tappini, Alessandra/HPE-6422-2023; Di Giacomo, Emilio/JPK-8948-2023;
   Liotta, Giuseppe/I-4638-2015
OI Di Giacomo, Emilio/0000-0002-9794-1928; Montecchiani,
   Fabrizio/0000-0002-0543-8912; Liotta, Giuseppe/0000-0002-2886-9694;
   Tappini, Alessandra/0000-0001-9192-2067
FU MIUR [20174LF3T8]
FX This work was supported in part by MIUR under Grant 20174LF3T8,
CR Abuthawabeh A., 2013, P 1 IEEE WORK C SOFT, P1
   Alper B., 2013, P SIGCHI C HUM FACT, P483, DOI [DOI 10.1145/2470654.24707243,4,5, 10.1145/2470654.2470724]
   Amar R, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P111, DOI 10.1109/INFVIS.2005.1532136
   Angelini P., 2017, J GRAPH ALGORITHMS A, V21, P731, DOI DOI 10.7155/JGAA.00437
   Angelini P., 2020, PLANAR GRAPHS COMMUN, P211, DOI [10.1007/978-981-15-6533-5_12, DOI 10.1007/978-981-15-6533-5_12]
   Angelini P, 2020, ALGORITHMS, V13, DOI 10.3390/a13080194
   Angelini P, 2018, LECT NOTES COMPUT SC, V11282, P67, DOI 10.1007/978-3-030-04414-5_5
   Angori L, 2022, IEEE T VIS COMPUT GR, V28, P1288, DOI 10.1109/TVCG.2020.3016055
   Angori L, 2019, LECT NOTES COMPUT SC, V11904, P276, DOI 10.1007/978-3-030-35802-0_22
   [Anonymous], 2015, NodeTrix Javascript
   Bach B, 2013, INT J SEMANT WEB INF, V9, P17, DOI 10.4018/ijswis.2013100102
   Bachmaier C, 2010, DISCRETE APPL MATH, V158, P159, DOI 10.1016/j.dam.2009.09.002
   Bar-Joseph Z., 2001, Bioinformatics, V17, P522
   Batagelj V, 2011, IEEE T VIS COMPUT GR, V17, P1587, DOI 10.1109/TVCG.2010.265
   Beck F, 2017, COMPUT GRAPH FORUM, V36, P133, DOI 10.1111/cgf.12791
   Bedi P, 2016, WIRES DATA MIN KNOWL, V6, P115, DOI 10.1002/widm.1178
   Besa JJ, 2019, LEIBNIZ INT PR INFOR, V144, DOI 10.4230/LIPIcs.ESA.2019.19
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Burch M, 2021, IEEE ACCESS, V9, P4173, DOI 10.1109/ACCESS.2020.3047616
   Christensen Johanne, 2014, Smart Graphics. 12th International Symposium (SG 2014). Proceedings: LNCS 8698, P174, DOI 10.1007/978-3-319-11650-1_17
   Conover W. J., 1998, Practical Nonparametric Statistics, V3rd
   Da Lozzo G., 2018, J. Graph Algorithms Appl., V22, P139, DOI [DOI 10.7155/JGAA.00461, /10.7155/jgaa.00461]
   Di Giacomo Emilio, 2018, Graph Drawing and Network Visualization. 25th International Symposium, GD 2017. Revised Selected Papers: LNCS 10692, P479, DOI 10.1007/978-3-319-73915-1_37
   Di Giacomo E., 2021, Graph Drawing and Network Visualization, P21
   Di Giacomo E, 2021, THEOR COMPUT SCI, V896, P19, DOI 10.1016/j.tcs.2021.09.044
   Di Giacomo E, 2019, ALGORITHMICA, V81, P3464, DOI 10.1007/s00453-019-00585-6
   Di Giacomo E, 2019, LECT NOTES COMPUT SC, V11355, P148, DOI 10.1007/978-3-030-10564-8_12
   Didimo Walter, 2008, Journal of Graph Algorithms and Applications, V12, P267, DOI 10.7155/jgaa.00167
   Didimo W, 2018, COMPUT GRAPH FORUM, V37, P288, DOI 10.1111/cgf.13266
   Didimo W, 2014, J VISUAL LANG COMPUT, V25, P433, DOI 10.1016/j.jvlc.2014.01.002
   Didimo W, 2014, INFORM SCIENCES, V260, P185, DOI 10.1016/j.ins.2013.09.048
   Dogrusoz U, 2009, INFORM SCIENCES, V179, P980, DOI 10.1016/j.ins.2008.11.017
   Duncan C. A., 2013, Handbook of Graph Drawing and Visualization, P223, DOI DOI 10.1201/B15385/HANDBOOK-GRAPH-DRAWING-VISUALIZATION-ROBERTO-TAMASSIA
   Flake GW, 2002, COMPUTER, V35, P66, DOI 10.1109/2.989932
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Ghoniem M., 2005, Information Visualization, V4, P114, DOI 10.1057/palgrave.ivs.9500092
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Hadlak S, 2011, IEEE T VIS COMPUT GR, V17, P2334, DOI 10.1109/TVCG.2011.213
   HAREL D, 1988, COMMUN ACM, V31, P514, DOI 10.1145/42411.42414
   Henry N, 2008, IEEE T VIS COMPUT GR, V14, P1317, DOI 10.1109/TVCG.2008.141
   Henry N, 2007, LECT NOTES COMPUT SC, V4663, P288
   Henry N, 2007, IEEE T VIS COMPUT GR, V13, P1302, DOI 10.1109/TVCG.2007.70582
   Kaufmann M., 2001, DRAWING GRAPHS METHO
   Keller R., 2006, Information Visualization, V5, P62, DOI 10.1057/palgrave.ivs.9500116
   Kindermann P., 2022, P 38 EUR WORKSH COMP, P1
   Lee B., 2006, P AVI WORKSH TIM ERR, P1, DOI [DOI 10.1145/1168149.1168168, 10.1145/1168149.1168168]
   Ley M., 2002, String Processing and Information Retrieval. 9th International Symposium, SPIRE 2002. Proceedings (Lecture Notes in Computer Science Vol.2476), P1
   Liotta G, 2022, LECT NOTES COMPUT SC, V13453, P383, DOI 10.1007/978-3-031-15914-5_28
   Liotta G, 2021, THEOR COMPUT SCI, V874, P59, DOI 10.1016/j.tcs.2021.05.012
   Liotta G, 2020, LECT NOTES COMPUT SC, V12011, P617, DOI 10.1007/978-3-030-38919-2_51
   Mahmoud H, 2014, LECT N BIOINFORMAT, V8452, P62, DOI 10.1007/978-3-319-09042-9_5
   Mangan S, 2003, P NATL ACAD SCI USA, V100, P11980, DOI 10.1073/pnas.2133841100
   Matuszewski C, 1999, LECT NOTES COMPUT SC, V1731, P217
   Melancon G., 2006, P AVI WORKSH TIM ERR, P1
   Muelder C, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P231
   Okoe M, 2019, IEEE T VIS COMPUT GR, V25, P2940, DOI 10.1109/TVCG.2018.2865940
   Okoe M, 2015, COMPUT GRAPH FORUM, V34, P451, DOI 10.1111/cgf.12657
   Omote H, 2007, ASIA-PACIFIC SYMPOSIUM ON VISUALISATION 2007, PROCEEDINGS, P85
   Onnela JP, 2004, EUR PHYS J B, V38, P353, DOI 10.1140/epjb/e2004-00128-7
   Porter MA., 2009, Notices of the American Mathematical Society, V56, P1082, DOI DOI 10.1103/PHYSREVE.69.066133
   Purchase Helen C., 2013, Graph Drawing. 20th International Symposium, GD 2012. Revised Selected Papers, P451, DOI 10.1007/978-3-642-36763-2_40
   Purchase H. C., 1997, ACM J. Exp. Algorithmics, V2
   Purchase H. C., 2012, Experimental Human-Computer Interaction - A Prac- tical Guide With Visual Examples
   Purchase HC, 1998, J VISUAL LANG COMPUT, V9, P647, DOI 10.1006/jvlc.1998.0093
   Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292
   Rufiange S, 2013, IEEE T VIS COMPUT GR, V19, P2556, DOI 10.1109/TVCG.2013.149
   Saket B, 2014, IEEE T VIS COMPUT GR, V20, P2231, DOI 10.1109/TVCG.2014.2346422
   Sindre G., 1993, Proceedings 1993 IEEE Symposium on Visual Languages (Cat. No.93TH0562-9), P287, DOI 10.1109/VL.1993.269613
   Stasko J., 2014, P 5 WORKSH TIM ERR N, P46, DOI [DOI 10.1145/2669557.2669579, 10.1145/2669557.26695792,9, DOI 10.1145/2669557.26695792,9]
   Stasko J., 2013, P POST IEEE INFOVIS
   Sugiyama K., 2002, GRAPH DRAWING APPL S
   Thode HC, 2002, Testing for normality, DOI DOI 10.1201/9780203910894
   van Dijk RE, 2014, ECOL LETT, V17, P1141, DOI 10.1111/ele.12320
   Wall E, 2019, IEEE T VIS COMPUT GR, V25, P491, DOI 10.1109/TVCG.2018.2865146
   Wu H, 2010, LECT NOTES COMPUT SC, V6215, P337
   Yang XS, 2017, IEEE T VIS COMPUT GR, V23, P181, DOI 10.1109/TVCG.2016.2598472
   Zhao SD, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P57, DOI 10.1109/INFVIS.2005.1532129
NR 77
TC 3
Z9 3
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3503
EP 3515
DI 10.1109/TVCG.2022.3233389
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700029
PM 37018276
OA hybrid
DA 2024-11-06
ER

PT J
AU Zhao, D
   Li, J
   Li, HY
   Xu, L
AF Zhao, Dong
   Li, Jia
   Li, Hongyu
   Xu, Long
TI Stripe Sensitive Convolution for Omnidirectional Image Dehazing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Convolution; Distortion; Feature extraction; Kernel; Estimation;
   Three-dimensional displays; Layout; Omnidirectional image dehazing;
   omnidirectional image depth estimation; stripe sensitive convolution;
   virtual reality
ID SALIENCY; VR
AB The haze in a scenario may affect the 360 photo/video quality and the immersive 360(degrees) virtual reality (VR) experience. The recent single image dehazing methods, to date, have been only focused on plane images. In this work, we propose a novel neural network pipeline for single omnidirectional image dehazing. To create the pipeline, we build the first hazy omnidirectional image dataset, which contains both synthetic and real-world samples. Then, we propose a new stripe sensitive convolution (SSConv) to handle the distortion problems due to the equirectangular projections. The SSConv calibrates distortion in two steps: 1) extracting features using different rectangular filters and, 2) learning to select the optimal features by a weighting of the feature stripes (a series of rows in the feature maps). Subsequently, using SSConv, we design an end-to-end network that jointly learns haze removal and depth estimation from a single omnidirectional image. The estimated depth map is leveraged as the intermediate representation and provides global context and geometric information to the dehazing module. Extensive experiments on challenging synthetic and real-world omnidirectional image datasets demonstrate the effectiveness of SSConv, and our network attains superior dehazing performance. The experiments on practical applications also demonstrate that our method can significantly improve the 3-D object detection and 3-D layout performances for hazy omnidirectional images.
C1 [Zhao, Dong; Li, Jia; Li, Hongyu] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Jia] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Xu, Long] Chinese Acad Sci, Natl Space Sci Ctr, Beijing 100190, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory; Chinese Academy of Sciences;
   National Space Science Center, CAS
RP Li, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Li, J (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM zhaodong@buaa.edu.cn; jiali@buaa.edu.cn; hongyuli@buaa.edu.cn;
   lxu@nao.cas.cn
RI Li, Jia/AAB-6431-2019; Li, Yanan/ABL-1507-2022; Xu, Long/AAH-9908-2019
OI Li, Jia/0000-0002-4346-8696; Xu, Long/0000-0002-9286-2876; Zhao,
   Dong/0000-0003-3754-1524
FU National Natural Science Foundation of China [62132002, 11790305]; Peng
   Cheng Laboratory Cloud Brain [PCL2021A13]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132002 and 11790305, and in part by
   Peng Cheng Laboratory Cloud Brain under Grant PCL2021A13.
CR Armeni I., 2017, arXiv
   Baslamisli AS, 2018, LECT NOTES COMPUT SC, V11210, P289, DOI 10.1007/978-3-030-01231-1_18
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bertel T, 2019, IEEE T VIS COMPUT GR, V25, P1828, DOI 10.1109/TVCG.2019.2898799
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Chen DW, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P92, DOI [10.1109/VR46266.2020.1581216087067, 10.1109/VR46266.2020.00-77]
   Chen HX, 2021, IEEE SIGNAL PROC LET, V28, P334, DOI 10.1109/LSP.2021.3050712
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen Z, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6742, DOI 10.1109/ICASSP.2018.8461630
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   de La Garanderie GP, 2018, LECT NOTES COMPUT SC, V11217, P812, DOI 10.1007/978-3-030-01261-8_48
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Du RF, 2021, IEEE COMPUT GRAPH, V41, P99, DOI 10.1109/MCG.2021.3080320
   gabarron, 2017, The Gabarron foundationunveils its unique VR experiences (vir-tual reality)
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu J., 2018, P 2018 IEEE CVF C CO, P7132
   Hu J, 2018, ADV NEUR IN, V31
   Huang KQ, 2006, COMPUT VIS IMAGE UND, V103, P52, DOI 10.1016/j.cviu.2006.02.007
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P519, DOI 10.1007/978-3-030-58545-7_30
   Kingma D.P., 2014, P INT C LEARNING REP
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lee Y, 2019, PROC CVPR IEEE, P9173, DOI 10.1109/CVPR.2019.00940
   Li D, 2021, IEEE T VIS COMPUT GR, V27, P2638, DOI 10.1109/TVCG.2021.3067762
   Li H, 2018, ARXIV180510180, DOI DOI 10.48550/ARXIV.1805.10180
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu HY, 2020, Arxiv, DOI arXiv:2008.10320
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Lo S-H., 2019, P 1 ACM INT C MULT A, P1, DOI DOI 10.1145/3338533.3366558
   Luo BC, 2018, IEEE T VIS COMPUT GR, V24, P1545, DOI 10.1109/TVCG.2018.2794071
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Marañes C, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P73, DOI [10.1109/VR46266.2020.00-79, 10.1109/VR46266.2020.1580727911717]
   Marrinan T, 2021, IEEE T VIS COMPUT GR, V27, P2587, DOI 10.1109/TVCG.2021.3067780
   Mirza M, 2014, Arxiv, DOI arXiv:1411.1784
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Park J., 2018, BRIT MACH VIS C 2018
   Pintore Giovanni, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P432, DOI 10.1007/978-3-030-58598-3_26
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Reed P., 2020, Virtual reality
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Su Y.-C., 2017, ADV NEURAL INF PROCE, P529
   Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wallgrün JO, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P83, DOI [10.1109/VR46266.2020.00-78, 10.1109/VR46266.2020.1581092881445]
   Wang LL, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P170, DOI 10.1109/VR50410.2021.00038
   Wang M, 2020, INT SYM MIX AUGMENT, P174, DOI [10.1109/1SMA1R50242.2020.00040, 10.1109/ISMAR50242.2020.00040]
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P666, DOI 10.1007/978-3-030-58517-4_39
   Won C, 2019, IEEE INT CONF ROBOT, P6073, DOI [10.1109/ICRA.2019.8793823, 10.1109/icra.2019.8793823]
   Xu Zheng., 2018, P BRIT MACH VIS C, V2, P5
   Young J, 2019, IEEE T VIS COMPUT GR, V25, P1908, DOI 10.1109/TVCG.2019.2898737
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P1975, DOI 10.1109/TCSVT.2019.2912145
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Y, 2020, IEEE IMAGE PROC, P3458, DOI 10.1109/ICIP40778.2020.9191158
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zioulis N, 2018, LECT NOTES COMPUT SC, V11210, P453, DOI 10.1007/978-3-030-01231-1_28
NR 72
TC 0
Z9 0
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3516
EP 3531
DI 10.1109/TVCG.2022.3233900
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700013
PM 37018251
DA 2024-11-06
ER

PT J
AU Sharma, M
   Bin Masood, T
   Thygesen, SS
   Linares, M
   Hotz, I
   Natarajan, V
AF Sharma, Mohit
   Bin Masood, Talha
   Thygesen, Signe Sidwall
   Linares, Mathieu
   Hotz, Ingrid
   Natarajan, Vijay
TI Continuous Scatterplot Operators for Bivariate Analysis and Study of
   Electronic Transitions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Orbits; Lenses; Pipelines; Isosurfaces; Space vehicles;
   Behavioral sciences; Bivariate field analysis; continuous scatterplot;
   fiber surface; control polygon; visual analysis; electronic transitions
ID FIBER SURFACES; HISTOGRAMS
AB Electronic transitions in molecules due to the absorption or emission of light is a complex quantum mechanical process. Their study plays an important role in the design of novel materials. A common yet challenging task in the study is to determine the nature of electronic transitions, namely which subgroups of the molecule are involved in the transition by donating or accepting electrons, followed by an investigation of the variation in the donor-acceptor behavior for different transitions or conformations of the molecules. In this article, we present a novel approach for the analysis of a bivariate field and show its applicability to the study of electronic transitions. This approach is based on two novel operators, the continuous scatterplot (CSP) lens operator and the CSP peel operator, that enable effective visual analysis of bivariate fields. Both operators can be applied independently or together to facilitate analysis. The operators motivate the design of control polygon inputs to extract fiber surfaces of interest in the spatial domain. The CSPs are annotated with a quantitative measure to further support the visual analysis. We study different molecular systems and demonstrate how the CSP peel and CSP lens operators help identify and study donor and acceptor characteristics in molecular systems.
C1 [Sharma, Mohit; Natarajan, Vijay] Indian Inst Sci, Dept Comp Sci & Automat, Bengaluru 560012, Karnataka, India.
   [Bin Masood, Talha; Thygesen, Signe Sidwall; Linares, Mathieu; Hotz, Ingrid] Linkoping Univ, Dept Sci & Technol ITN, S-58183 Linkoping, Sweden.
   [Hotz, Ingrid] Indian Inst Sci, Bengaluru 560012, Karnataka, India.
C3 Indian Institute of Science (IISC) - Bangalore; Linkoping University;
   Indian Institute of Science (IISC) - Bangalore
RP Sharma, M (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bengaluru 560012, Karnataka, India.
EM mohitsharma@iisc.ac.in; talha.bin.masood@liu.se;
   signe.sidwall.thygesen@liu.se; mathieu.linares@liu.se;
   ingrid.hotz@liu.se; vijayn@iisc.ac.in
RI ; linares, mathieu/C-5791-2008
OI Bin Masood, Talha/0000-0001-5352-1086; Natarajan,
   Vijay/0000-0002-7956-1470; Hotz, Ingrid/0000-0001-7285-0483; Sharma,
   Mohit/0000-0002-9153-7465; Sidwall Thygesen, Signe/0000-0001-9156-647X;
   linares, mathieu/0000-0002-9720-5429
FU Indo-Swedish joint network project [DST/INT/SWD/VR/P-02/2019]; VR
   [2018-07085, 2018-05973]; MoE Govt. of India; Swarnajayanti Fellowship
   from DST India [DST/SJF/ETA-02/2015-16]; Mindtree Chair research grant;
   SeRC (Swedish e-Science Research Center); Swedish Research Council (VR)
   [2019-05487]; Swedish Research Council [2018-07085] Funding Source:
   Swedish Research Council
FX This work was supported in part by an Indo-Swedish joint network project
   under Grant DST/INT/SWD/VR/P-02/2019 and in part by VR under Grant
   2018-07085, in part by the MoE Govt. of India, a Swarnajayanti
   Fellowship from DST India under Grant DST/SJF/ETA-02/2015-16, in part by
   a Mindtree Chair research grant, in part by the SeRC (Swedish e-Science
   Research Center), in part by the Swedish Research Council (VR) under
   Grant 2019-05487.SST is associated to the Wallenberg AI, Autonomous
   Systems and Software Program (WASP). The computations were enabled by
   resources provided by the Swedish National Infrastructure for Computing
   (SNIC) with NSC partially funded by the VR under Grant 2018-05973.
CR AURENHAMMER F, 1987, SIAM J COMPUT, V16, P78, DOI 10.1137/0216006
   Ayachit U., 2015, The Paraview Guide: A Parallel Visualization Application
   Bachthaler S, 2008, IEEE T VIS COMPUT GR, V14, P1428, DOI 10.1109/TVCG.2008.119
   Blecha C, 2019, IEEE PAC VIS SYMP, P189, DOI 10.1109/PacificVis.2019.00030
   Carr H, 2015, COMPUT GRAPH FORUM, V34, P241, DOI 10.1111/cgf.12636
   Carr H, 2006, IEEE T VIS COMPUT GR, V12, P1259, DOI 10.1109/TVCG.2006.168
   Edelsbrunner H, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P242, DOI 10.1145/1377676.1377720
   Frisch M. J., Gaussian
   García G, 2013, PHYS CHEM CHEM PHYS, V15, P20210, DOI 10.1039/c3cp53740d
   Guido CA, 2013, J CHEM THEORY COMPUT, V9, P3118, DOI 10.1021/ct400337e
   Haranczyk M, 2008, J CHEM THEORY COMPUT, V4, P689, DOI 10.1021/ct800043a
   Huet L, 2020, J CHEM THEORY COMPUT, V16, P4543, DOI 10.1021/acs.jctc.0c00296
   Humphrey W, 1996, J MOL GRAPH MODEL, V14, P33, DOI 10.1016/0263-7855(96)00018-5
   Kim H. W., 2019, inMolecular Spectroscopy: A. Quantum Chemistry Approach, V1
   Klacansky P, 2017, IEEE T VIS COMPUT GR, V23, P1782, DOI 10.1109/TVCG.2016.2570215
   Le Bahers T, 2011, J CHEM THEORY COMPUT, V7, P2498, DOI 10.1021/ct200308m
   Lehmann DJ, 2010, IEEE T VIS COMPUT GR, V16, P1291, DOI 10.1109/TVCG.2010.146
   Martin RL, 2003, J CHEM PHYS, V118, P4775, DOI 10.1063/1.1558471
   Bin Masood T, 2021, COMPUT GRAPH FORUM, V40, P287, DOI 10.1111/cgf.14307
   Morgan F., 2000, Geometric Measure Theory: A Beginners Guide
   Mulliken RS, 1932, PHYS REV, V41, P49, DOI 10.1103/PhysRev.41.49
   Nagaraj S, 2011, IEEE T VIS COMPUT GR, V17, P182, DOI 10.1109/TVCG.2010.64
   Raith F, 2019, IEEE T VIS COMPUT GR, V25, P1122, DOI 10.1109/TVCG.2018.2864846
   Sarikaya A, 2018, IEEE T VIS COMPUT GR, V24, P402, DOI 10.1109/TVCG.2017.2744184
   Scheidegger CE, 2008, IEEE T VIS COMPUT GR, V14, P1659, DOI 10.1109/TVCG.2008.160
   Sharma M, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P96, DOI 10.1109/VIS49827.2021.9623300
   Stone D. J., 2011, inGPU Computing Gems Emerald Edition, ser. Applications of GPUComputing Series, P5
   Thygesen SS, 2022, COMPUT GRAPH FORUM, V41, P333, DOI 10.1111/cgf.14544
   Tierny J, 2018, IEEE T VIS COMPUT GR, V24, P832, DOI 10.1109/TVCG.2017.2743938
   Tierny J, 2017, IEEE T VIS COMPUT GR, V23, P960, DOI 10.1109/TVCG.2016.2599017
NR 30
TC 4
Z9 4
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3532
EP 3544
DI 10.1109/TVCG.2023.3237768
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700002
PM 37021886
OA Green Submitted
DA 2024-11-06
ER

PT J
AU McGuffin, MJ
   Servera, R
   Forest, M
AF McGuffin, Michael J.
   Servera, Ryan
   Forest, Marie
TI Path Tracing in 2D, 3D, and Physicalized Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Layout; Task analysis; Headphones; Data
   visualization; Mice; Visualization; 3D printing; augmented reality; data
   physicalization; graph visualization; path finding; path following;
   tangible
ID VISUALIZATION; EXPLORATION; INTERFACES; DISPLAYS; STEREO
AB It is common to advise against using 3D to visualize abstract data such as networks, however Ware and Mitchell's 2008 study showed that path tracing in a network is less error prone in 3D than in 2D. It is unclear, however, if 3D retains its advantage when the 2D presentation of a network is improved using edge-routing, and when simple interaction techniques for exploring the network are available. We address this with two studies of path tracing under new conditions. The first study was preregistered, involved 34 users, and compared 2D and 3D layouts that the user could rotate and move in virtual reality with a handheld controller. Error rates were lower in 3D than in 2D, despite the use of edge-routing in 2D and the use of mouse-driven interactive highlighting of edges. The second study involved 12 users and investigated data physicalization, comparing 3D layouts in virtual reality versus physical 3D printouts of networks augmented with a Microsoft HoloLens headset. No difference was found in error rate, but users performed a variety of actions with their fingers in the physical condition which can inform new interaction techniques.
C1 [McGuffin, Michael J.; Servera, Ryan; Forest, Marie] Ecole Technol Super, Montreal, PQ H3C 1K3, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada
RP McGuffin, MJ (corresponding author), Ecole Technol Super, Montreal, PQ H3C 1K3, Canada.
EM michael.mcguffin@etsmtl.ca; servera.ryan@gmail.com;
   marie.forest@etsmtl.ca
OI McGuffin, Michael/0000-0001-7782-5754
FU Microsoft Research; NSERC
FX This work was supported by Microsoft Research and NSERC.
CR Alper B, 2011, IEEE T VIS COMPUT GR, V17, P2325, DOI 10.1109/TVCG.2011.234
   Bach B, 2018, IEEE T VIS COMPUT GR, V24, P457, DOI 10.1109/TVCG.2017.2745941
   Belcher D, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P84, DOI 10.1109/ISMAR.2003.1240691
   Berard F, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4396, DOI 10.1145/3025453.3025806
   Brath R, 2014, 2014 IEEE VIS INTERNATIONAL WORKSHOP ON 3DVIS (3DVIS), P25, DOI 10.1109/3DVis.2014.7160096
   Burch M, 2021, IEEE ACCESS, V9, P4173, DOI 10.1109/ACCESS.2020.3047616
   Cattan E., 2015, ACM INT C INT TABL S, P215
   Collins C, 2007, IEEE T VIS COMPUT GR, V13, P1192, DOI 10.1109/TVCG.2007.70521
   Cordeil M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376613
   Cordeil M, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P71, DOI 10.1145/3126594.3126613
   Cordeil M, 2017, IEEE PAC VIS SYMP, P46, DOI 10.1109/PACIFICVIS.2017.8031578
   Cordeil M, 2017, IEEE T VIS COMPUT GR, V23, P441, DOI 10.1109/TVCG.2016.2599107
   Crundall D, 2008, PERCEPT PSYCHOPHYS, V70, P422, DOI 10.3758/PP.70.3.422
   Danyluk K, 2022, IEEE T VIS COMPUT GR, V28, P1930, DOI 10.1109/TVCG.2020.3023336
   dataphys, 2021, Data physicalization wiki
   Dhand N. K., 2014, Statulator: An online statistical cal- culator. Sample size calculator for comparing two paired means
   docs.microsoft, About us
   Dragicevic P, 2016, HUM-COMPUT INT-SPRIN, P291, DOI 10.1007/978-3-319-26633-6_13
   Drogemuller A., 2021, P CHI C HUM FACT COM, P1
   Drogemuller A, 2020, J COMPUT LANG, V56, DOI 10.1016/j.cola.2019.100937
   Dwyer T, 2010, LECT NOTES COMPUT SC, V5849, P147, DOI 10.1007/978-3-642-11805-0_15
   Ens Barrett, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3446866
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Gansner ER, 2004, LECT NOTES COMPUT SC, V3383, P239
   Gillet A, 2005, STRUCTURE, V13, P483, DOI 10.1016/j.str.2005.01.009
   github, About us
   Greffard N, 2014, 2014 IEEE VIS INTERNATIONAL WORKSHOP ON 3DVIS (3DVIS), P19, DOI 10.1109/3DVis.2014.7160095
   Greffard N, 2012, LECT NOTES COMPUT SC, V7034, P215
   GUIARD Y, 1987, J MOTOR BEHAV, V19, P486
   Guillon M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2777, DOI 10.1145/2702123.2702375
   Hayatpur D., 2020, P 33 ANN ACM S US IN, P818, DOI DOI 10.1145/3379337.3415878
   Irani P., 2003, ACM Transactions on Computer-Human Interaction, V10, P1, DOI 10.1145/606658.606659
   James R., 2020, P GRAPH INT, P1
   Jansen Y., 2013, P SIGCHI C HUMAN FAC, DOI [10.1145/2470654, DOI 10.1145/2470654.24813591,2,3,6,11]
   Jansen Y, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3227, DOI 10.1145/2702123.2702180
   Kamakura N., 2022, Postures and Movement Patterns of the Human Hand: A Framework for Understanding Hand Activity for Clinicians and Engi- neers
   Kaufmann M., 2001, DRAWING GRAPHS METHO
   Kim K, 2018, IEEE T VIS COMPUT GR, V24, P2947, DOI 10.1109/TVCG.2018.2868591
   Kraus M, 2022, COMPUT GRAPH FORUM, V41, P201, DOI 10.1111/cgf.14430
   Kwon OH, 2016, IEEE T VIS COMPUT GR, V22, P1802, DOI 10.1109/TVCG.2016.2520921
   Lee B., 2006, P AVI WORKSH TIM ERR, P1, DOI [DOI 10.1145/1168149.1168168, 10.1145/1168149.1168168]
   Lee B, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501859
   Marriott K, 2018, LECT NOTES COMPUT SC, V11190, P25, DOI 10.1007/978-3-030-01388-2_2
   McGuffin M. J., 2022, Preregistration
   McIntire JP, 2014, 2014 IEEE VIS INTERNATIONAL WORKSHOP ON 3DVIS (3DVIS), P1, DOI 10.1109/3DVis.2014.7160093
   McIntire JP, 2014, DISPLAYS, V35, P18, DOI 10.1016/j.displa.2013.10.004
   Munzner T., 2014, Visualization Analysis and Design, P117
   Munzner T, 2008, LECT NOTES COMPUT SC, V4950, P134, DOI 10.1007/978-3-540-70956-5_6
   Ramcharitar Adrian., 2018, Proceedings of the 44th Graphics Interface Conference, P123
   Satriadi KA, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517715
   Schmidt S., 2010, Proc. ACM Conference on Interactive Tabletops and Surfaces (ITS), P113, DOI DOI 10.1145/1936652.1936673
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Shneiderman B, 2003, IEEE COMPUT GRAPH, V23, P12, DOI 10.1109/MCG.2003.1242376
   Smiley Jim, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3488546
   SOLLENBERGER RL, 1993, HUM FACTORS, V35, P483, DOI 10.1177/001872089303500306
   Taher F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3237, DOI 10.1145/2702123.2702604
   Taher F, 2017, IEEE T VIS COMPUT GR, V23, P451, DOI 10.1109/TVCG.2016.2598498
   Tamassia R., 1999, GRAPH DRAWING ALGORI
   Thompson J, 2018, AVI'18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, DOI 10.1145/3206505.3206519
   van Beurden Maurice H. P. H., 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P176, DOI 10.1109/QOMEX.2010.5516268
   von Landesberger T, 2011, COMPUT GRAPH FORUM, V30, P1719, DOI 10.1111/j.1467-8659.2011.01898.x
   Walny J, 2018, IEEE T VIS COMPUT GR, V24, P770, DOI 10.1109/TVCG.2017.2745958
   Ware C., 2001, Information Design Journal, V10, P258, DOI 10.1075/idj.10.3.07war
   Ware C, 1996, ACM T GRAPHIC, V15, P121, DOI 10.1145/234972.234975
   Ware C, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279640.1279642
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wong P. C., 1997, Scientific Visualization: Overviews, Methodologies, and Techniques
   Yoghourdjian V, 2018, VIS INFORM, V2, P264, DOI 10.1016/j.visinf.2018.12.006
   Zou BC, 2022, VISION RES, V198, DOI 10.1016/j.visres.2022.108061
NR 70
TC 2
Z9 2
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3564
EP 3577
DI 10.1109/TVCG.2023.3238989
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700027
PM 37021998
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Martin-Gomez, A
   Li, HW
   Song, TY
   Yang, S
   Wang, GZ
   Ding, H
   Navab, N
   Zhao, Z
   Armand, M
AF Martin-Gomez, Alejandro
   Li, Haowei
   Song, Tianyu
   Yang, Sheng
   Wang, Guangzhi
   Ding, Hui
   Navab, Nassir
   Zhao, Zhe
   Armand, Mehran
TI STTAR: Surgical Tool Tracking Using Off-the-Shelf Augmented Reality
   Head-Mounted Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; computer-assisted medical procedures; navigation;
   tracking
AB The use of Augmented Reality (AR) for navigation purposes has shown beneficial in assisting physicians during the performance of surgical procedures. These applications commonly require knowing the pose of surgical tools and patients to provide visual information that surgeons can use during the performance of the task. Existing medical-grade tracking systems use infrared cameras placed inside the Operating Room (OR) to identify retro-reflective markers attached to objects of interest and compute their pose. Some commercially available AR Head-Mounted Displays (HMDs) use similar cameras for self-localization, hand tracking, and estimating the objects' depth. This work presents a framework that uses the built-in cameras of AR HMDs to enable accurate tracking of retro-reflective markers without the need to integrate any additional electronics into the HMD. The proposed framework can simultaneously track multiple tools without having previous knowledge of their geometry and only requires establishing a local network between the headset and a workstation. Our results show that the tracking and detection of the markers can be achieved with an accuracy of 0.09 +/- 0.06 mm on lateral translation, 0.42 +/- 0.32 mm on longitudinal translation and 0.80 +/- 0.39(degrees) for rotations around the vertical axis. Furthermore, to showcase the relevance of the proposed framework, we evaluate the system's performance in the context of surgical procedures. This use case was designed to replicate the scenarios of k-wire insertions in orthopedic procedures. For evaluation, seven surgeons were provided with visual navigation and asked to perform 24 injections using the proposed framework. A second study with ten participants served to investigate the capabilities of the framework in the context of more general scenarios. Results from these studies provided comparable accuracy to those reported in the literature for AR-based navigation procedures.
C1 [Martin-Gomez, Alejandro; Armand, Mehran] Johns Hopkins Univ, Whiting Sch Engn, Lab Computat Sensing & Robot, Baltimore, MD 21218 USA.
   [Armand, Mehran] Johns Hopkins Univ, Sch Med, Dept Orthopaed Surg, Baltimore, MD 21205 USA.
   [Li, Haowei; Yang, Sheng; Wang, Guangzhi; Ding, Hui] Tsinghua Univ, Dept Biomed Engn, Beijing 100190, Peoples R China.
   [Song, Tianyu; Zhao, Zhe] Tsinghua Univ, Beijing Tsinghua Changgung Hosp, Sch Clin Med, Dept Orthopaed, Beijing 100190, Peoples R China.
   [Song, Tianyu; Navab, Nassir; Zhao, Zhe] Tech Univ Munich, Chair Comp Aided Med Procedures & Augmented Real, Dept Informat, D-80333 Munich, Germany.
C3 Johns Hopkins University; Johns Hopkins University; Tsinghua University;
   Tsinghua University; Technical University of Munich
RP Martin-Gomez, A (corresponding author), Johns Hopkins Univ, Whiting Sch Engn, Lab Computat Sensing & Robot, Baltimore, MD 21218 USA.
EM alejandro.martin@jhu.edu; lihaowei19991202@gmail.com;
   tianyu.song@tum.de; yangs19@mails.tsinghua.edu.cn;
   wgz-dea@tsinghua.edu.cn; dinghui@tsinghua.edu.cn; nassir.navab@tum.de;
   zhaozhao_02@163.com; mehran.armand@jhuapl.edu
RI Gil-Ley, Alejandro/J-5851-2012; Wang, Guangzhi/AAN-8789-2020
OI Yang, Sheng/0000-0001-9901-7512; Song, Tianyu/0000-0002-8428-9651;
   Martin-Gomez, Alejandro/0000-0001-9341-3477; haowei,
   li/0000-0001-6558-1369; Wang, Guangzhi/0000-0002-4677-1041; Armand,
   Mehran/0000-0003-1028-8303
FU National Institutes of Health [R01-EB023939, R01-EB016703,
   R01-AR080315]; National Natural Science Foundation of China [U20A20389];
   Tsinghua University Initiative Scientific Research Program
   [20197010009]; Laboratory Innovation Fund of Tsinghua University
FX This work was supported by the National Institutes of Health under
   Grants R01-EB023939, R01-EB016703, and R01-AR080315, in part by the
   National Natural Science Foundation of China under Grant U20A20389,in
   part by the Tsinghua University Initiative Scientific Research Program
   under Grant 20197010009, and in part by the Laboratory Innovation Fund
   of Tsinghua University.
CR Andress S, 2018, J MED IMAGING, V5, DOI 10.1117/1.JMI.5.2.021209
   Campisi C. A., 2020, Augmented Reality Education, P111
   Casari FA, 2021, CURR REV MUSCULOSKE, V14, P192, DOI 10.1007/s12178-021-09699-3
   Desh G., 2018, J. Neurointerventional Surg., V10, P1187
   Fida B, 2018, UPDATES SURG, V70, P389, DOI 10.1007/s13304-018-0567-8
   Fotouhi J, 2021, IEEE T MED IMAGING, V40, P765, DOI 10.1109/TMI.2020.3037013
   Fotouhi J, 2019, INT J COMPUT ASS RAD, V14, P913, DOI 10.1007/s11548-019-01943-z
   Fotouhi J, 2018, J MED IMAGING, V5, DOI 10.1117/1.JMI.5.2.021205
   Gibby JT, 2019, INT J COMPUT ASS RAD, V14, P525, DOI 10.1007/s11548-018-1814-7
   Gsaxner C, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489863
   Jud L, 2020, BMC MUSCULOSKEL DIS, V21, DOI 10.1186/s12891-020-3110-2
   Kamphuis C, 2014, PERSPECT MED EDUC, V3, P300, DOI 10.1007/s40037-013-0107-7
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Kunz C., 2020, Curz Directions Riomeil. Fing, V6
   Lahini M.S., 2019, P INT C INT COMPN, P716
   Liebmann F, 2019, INT J COMPUT ASS RAD, V14, P1157, DOI 10.1007/s11548-019-01973-7
   Liu H, 2018, ANN BIOMED ENG, V46, P1595, DOI 10.1007/s10439-018-2055-1
   Martin-Gomez A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P735, DOI [10.1109/vr.2019.8798135, 10.1109/VR.2019.8798135]
   Mehrfard A, 2021, COMP M BIO BIO E-IV, V9, P233, DOI 10.1080/21681163.2020.1835559
   Molina CA, 2019, J NEUROSURG-SPINE, V31, P139, DOI 10.3171/2018.12.SPINE181142
   Müller F, 2020, SPINE J, V20, P621, DOI 10.1016/j.spinee.2019.10.012
   Ortega G, 2008, ARCH ORTHOP TRAUM SU, V128, P1123, DOI 10.1007/s00402-007-0500-y
   Peters TM., 2018, Mixed and augmented reality in medicine
   Rahman R, 2020, SURG INNOV, V27, P88, DOI 10.1177/1553350619871787
   Schweighofer G, 2006, IEEE T PATTERN ANAL, V28, P2024, DOI 10.1109/TPAMI.2006.252
   Spirig JM, 2021, EUR SPINE J, V30, P3731, DOI 10.1007/s00586-021-06950-w
   Ungureanu D, 2020, Arxiv, DOI [arXiv:2008.11239, DOI 10.48550/ARXIV.2008.11239, 10.48550/ARXIV.2008.11239]
   Viehöfer AF, 2020, BMC MUSCULOSKEL DIS, V21, DOI 10.1186/s12891-020-03373-4
   Yoon JW, 2017, INT J MED ROBOT COMP, V13, DOI 10.1002/rcs.1770
NR 29
TC 13
Z9 13
U1 12
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3578
EP 3593
DI 10.1109/TVCG.2023.3238309
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700040
PM 37021885
OA hybrid, Green Accepted, Green Submitted
DA 2024-11-06
ER

PT J
AU Jin, ZH
   Wang, XB
   Cheng, FR
   Sun, CH
   Liu, Q
   Qu, HM
AF Jin, Zhihua
   Wang, Xingbo
   Cheng, Furui
   Sun, Chunhui
   Liu, Qun
   Qu, Huamin
TI ShortcutLens: A Visual Analytics Approach for Exploring Shortcuts in
   Natural Language Understanding Dataset
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Benchmark testing; Task analysis; Natural language processing;
   Cognition; Guidelines; Predictive models; Computational modeling;
   Natural language understanding; shortcut; visual analytics
AB Benchmark datasets play an important role in evaluating Natural Language Understanding (NLU) models. However, shortcuts-unwanted biases in the benchmark datasets-can damage the effectiveness of benchmark datasets in revealing models' real capabilities. Since shortcuts vary in coverage, productivity, and semantic meaning, it is challenging for NLU experts to systematically understand and avoid them when creating benchmark datasets. In this paper, we develop a visual analytics system, ShortcutLens, to help NLU experts explore shortcuts in NLU benchmark datasets. The system allows users to conduct multi-level exploration of shortcuts. Specifically, Statistics View helps users grasp the statistics such as coverage and productivity of shortcuts in the benchmark dataset. Template View employs hierarchical and interpretable templates to summarize different types of shortcuts. Instance View allows users to check the corresponding instances covered by the shortcuts. We conduct case studies and expert interviews to evaluate the effectiveness and usability of the system. The results demonstrate that ShortcutLens supports users in gaining a better understanding of benchmark dataset issues through shortcuts, inspiring them to create challenging and pertinent benchmark datasets.
C1 [Jin, Zhihua; Wang, Xingbo; Cheng, Furui; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Cheng, Furui] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.
   [Sun, Chunhui] Peking Univ, Beijing 100871, Peoples R China.
   [Liu, Qun] Huawei, Noahs Ark Lab, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; Swiss Federal Institutes
   of Technology Domain; ETH Zurich; Peking University; Huawei Technologies
RP Jin, ZH (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM zjinak@cse.ust.hk; xwangeg@cse.ust.hk; furui.cheng@inf.ethz.ch;
   sch@pku.edu.cn; qun.liu@huawei.com; huamin@cse.ust.hk
RI Wang, Xingbo/JHS-6567-2023
OI Cheng, Furui/0000-0003-2329-6126; Wang, Xingbo/0000-0001-5693-1128
FU Hong Kong Theme-based Research Scheme [T41-709/17N]
FX This work was supported in part by the Hong Kong Theme-based Research
   Scheme under Grant T41-709/17N.
CR Alemzadeh S, 2020, COMPUT GRAPH FORUM, V39, P63, DOI 10.1111/cgf.13662
   Bender E. M., 2018, T ASSOC COMPUT LING, V6, P587, DOI [DOI 10.1162/TACL_A_00041, 10.1162/tacla00041, DOI 10.1162/TACLA00041]
   Bors C, 2019, IEEE COMPUT GRAPH, V39, P61, DOI 10.1109/MCG.2019.2941856
   Bowman SR, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P4843
   Branco R, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P1504
   Chen CJ, 2021, IEEE T VIS COMPUT GR, V27, P3335, DOI 10.1109/TVCG.2020.2973258
   Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
   Clements D.H., 1992, handbook of research on mathematics teaching and learning, P420
   Cui YM, 2021, IEEE-ACM T AUDIO SPE, V29, P3504, DOI 10.1109/TASLP.2021.3124365
   DeRose JF, 2021, IEEE T VIS COMPUT GR, V27, P1160, DOI 10.1109/TVCG.2020.3028976
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ethayarajh K, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4846
   EVERITT B, 1980, QUAL QUANT, V14, P75, DOI 10.1007/BF00154794
   Gebru T., 2021, Commun. ACM, V64, P92
   Geirhos R, 2020, NAT MACH INTELL, V2, P665, DOI 10.1038/s42256-020-00257-z
   Guan J, 2020, T ASSOC COMPUT LING, V8, P93, DOI 10.1162/tacl_a_00302
   Gururangan S, 2018, P 2018 C N AM CHAPT, V2, P107, DOI DOI 10.18653/V1/N18-2017
   Jin D, 2020, AAAI CONF ARTIF INTE, V34, P8018
   Jin Y., IEEE Trans. Vis. Comput. Graph., DOI [10.1109/TVCG.2022.3148107.33, DOI 10.1109/TVCG.2022.3148107.33]
   Jwa H, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194062
   Kaliyar RK, 2021, MULTIMED TOOLS APPL, V80, P11765, DOI 10.1007/s11042-020-10183-2
   Kandel S, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P547, DOI 10.1145/2254556.2254659
   Kiela D, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P4110
   Lai YX, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P989
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Laughlin B, 2019, IEEE SYM VIS CYB SEC, DOI 10.1109/vizsec48167.2019.9161563
   Le Bras R, 2020, PR MACH LEARN RES, V119
   Lee Kimin, 2018, P INT C LEARN REPR
   Liu ALS, 2022, Arxiv, DOI arXiv:2201.05955
   Liu SS, 2019, IEEE T VIS COMPUT GR, V25, P651, DOI 10.1109/TVCG.2018.2865230
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   McCoy RT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3428
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   Ming Y, 2020, IEEE T VIS COMPUT GR, V26, P238, DOI 10.1109/TVCG.2019.2934267
   Ming Y, 2017, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2017.8585721
   Mirzaee R, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P4582
   Mishra S, 2020, Arxiv, DOI arXiv:2005.00816
   Niven T, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4658
   Paullada A, 2021, PATTERNS, V2, DOI 10.1016/j.patter.2021.100336
   Poliak A, 2018, P 7 JOINT C LEX COMP, P180, DOI [10.18653/v1/S18-2023, DOI 10.18653/V1/S18-2023]
   Rogers A., 2021, QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension
   Sakaguchi K, 2020, AAAI CONF ARTIF INTE, V34, P8732
   Schick T, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6943
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Sun C, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P380
   Tenney I, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P107
   Wang A., 2019, P INT C LEARN REPR, P1
   Wang A., 2019, P ADV NEUR INF PROC, P3275
   Wang B., 2021, P NEUR INF PROC SYST, P13
   Wang XB, 2022, IEEE T VIS COMPUT GR, V28, P802, DOI 10.1109/TVCG.2021.3114794
   Warstadt A, 2019, T ASSOC COMPUT LING, V7, P625, DOI 10.1162/tacl_a_00290
   Wu K., 2020, P CHI C HUM FACT COM, P1
   Xu H, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P2324
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Zang Y., 2020, P 58 ANN M ASS COMPU, P6066, DOI [10.18653/v1/2020.aclmain.540, DOI 10.18653/V1/2020.ACLMAIN.540]
   Zeng GY, 2021, ACL-IJCNLP 2021: THE JOINT CONFERENCE OF THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING: PROCEEDINGS OF THE SYSTEM DEMONSTRATIONS, P363
   Zhang ZY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1441
   Zhao M., 2022, FINDINGS ASS COMPUTA, P675, DOI 10
NR 58
TC 5
Z9 5
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3594
EP 3608
DI 10.1109/TVCG.2023.3236380
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700068
PM 37018729
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Pisha, L
   Yadegari, S
AF Pisha, Louis
   Yadegari, Shahrokh
TI Specular Path Generation and Near-Reflective Diffraction in Interactive
   Acoustical Simulations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Diffraction; Reflection; Computational modeling; Solid modeling;
   Acoustics; Real-time systems; Graphics processing units; graph and tree
   search strategies; neural nets; parallel algorithms; raytracing; virtual
   reality
ID SOUND-PROPAGATION; EDGE; MODEL
AB Most systems for simulating sound propagation in a virtual environment for interactive applications use ray- or path-based models of sound. With these models, the "early" (low-order) specular reflection paths play a key role in defining the "sound" of the environment. However, the wave nature of sound, and the fact that smooth objects are approximated by triangle meshes, pose challenges for creating realistic approximations of the reflection results. Existing methods which produce accurate results are too slow to be used in most interactive applications with dynamic scenes. This paper presents a method for reflections modeling called spatially sampled near-reflective diffraction (SSNRD), based on an existing approximate diffraction model, Volumetric Diffraction and Transmission (VDaT). The SSNRD model addresses the challenges mentioned above, produces results accurate to within 1-2 dB on average compared to edge diffraction, and is fast enough to generate thousands of paths in a few milliseconds in large scenes. This method encompasses scene geometry processing, path trajectory generation, spatial sampling for diffraction modeling, and a small deep neural network (DNN) to produce the final response of each path. All steps of the method are GPU-accelerated, and NVIDIA RTX real-time ray tracing hardware is used for spatial computing tasks beyond just traditional ray tracing.
C1 [Pisha, Louis; Yadegari, Shahrokh] Univ Calif San Diego, Sonic Arts Res & Dev Qualcomm Inst, San Diego, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Pisha, L (corresponding author), Univ Calif San Diego, Sonic Arts Res & Dev Qualcomm Inst, San Diego, CA 92093 USA.
EM lpisha@ucsd.edu; sdy@ucsd.edu
OI Pisha, Louis A/0000-0001-5307-4263; Yadegari,
   Shahrokh/0000-0001-7617-2325
CR Agafonkin Volodymyr., 2019, Fast icosphere mesh
   ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   Anderson R.J., 1991, P 23 ANN ACM S THEOR, P370, DOI DOI 10.1145/103418.103458
   Antani L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077348
   BIOT MA, 1957, J ACOUST SOC AM, V29, P381, DOI 10.1121/1.1908899
   Blender community, 2020, BMesh design document
   Botteldooren D, 1995, J ACOUST SOC AM, V98, P3302, DOI 10.1121/1.413817
   Calamia P.T., 2007, EURASIP J. Appl. Signal Process, V2007, P186
   Cao C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925873
   Chaitanya CRA, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392459
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   Embrechts JJ, 2000, J ACOUST SOC AM, V107, P2068, DOI 10.1121/1.428489
   Evangelou I., 2021, Journal of Computer Graphics Techniques, V10, P25
   Google, 2016, Resonance audio-fundamental concepts
   GOURAUD H, 1971, IEEE T COMPUT, VC 20, P623, DOI 10.1109/T-C.1971.223313
   Hrádek J, 2003, COMPUT GEOSCI-UK, V29, P741, DOI 10.1016/S0098-3004(03)00037-2
   Ioffe Sergey, 2015, P MACHINE LEARNING R, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kharya P., 2020, Nvidia blogs: Tensorfloat-32 accelerates ai training hpc upto 20x
   KOUYOUMJIAN RG, 1974, P IEEE, V62, P1448, DOI 10.1109/PROC.1974.9651
   McDowell S, 2019, PyTorch C API
   Microsoft Game Dev, 2022, Project acoustics overview-what is projectacoustics?
   Morrical N, 2022, IEEE T VIS COMPUT GR, V28, P2852, DOI 10.1109/TVCG.2020.3042930
   Morrical N, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P256, DOI 10.1109/visual.2019.8933539
   Morrical S., 2021, Tracing Gems II, P625
   Morton GM, 1966, COMPUTER ORIENTED GE
   Murphy D, 2007, IEEE SIGNAL PROC MAG, V24, P55, DOI 10.1109/MSP.2007.323264
   NVIDIA Corporation., 2022, CUDA zone
   NVIDIA Corporation, 2018, NVIDIA TURING GPU Architecture
   NVIDIA Corporation, 2022, NVIDIA OptiX ray tracing engine
   NVIDIA Corporation, 2022, NVIDIA OptiX 7.4 programming guide: Accelera-tion structures
   NVIDIA Corporation, 2018, VRWorks-audio
   NVIDIA Corporation, 2022, NVIDIA RTX platform
   NVIDIA Corporation, 2017, VRWorks audio SDK in-depth
   Pisha L, 2020, J ACOUST SOC AM, V148, P1922, DOI 10.1121/10.0002115
   Pohl A., 2013, Ph.D. dissertation
   Raghuvanshi N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201339
   Rosen M, 2020, COMPUT GRAPH FORUM, V39, P39, DOI 10.1111/cgf.14099
   Saarelma J, 2018, 2018 AES INTERNATIONAL CONFERENCE ON SPATIAL REPRODUCTION - AESTHETICS AND SCIENCE
   Savioja L, 2015, J ACOUST SOC AM, V138, P708, DOI 10.1121/1.4926438
   Schissler C, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459751
   Schissler C, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2943779
   Schissler C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601216
   Schissler D., 2011, P AUD ENG SOC C 41 I, P1
   Schubert S, 2019, IEEE INT VEH SYM, P653, DOI [10.1109/IVS.2019.8813862, 10.1109/ivs.2019.8813862]
   Stephenson UM, 2010, ACTA ACUST UNITED AC, V96, P516, DOI 10.3813/AAA.918304
   Summers JE, 2013, J ACOUST SOC AM, V133, P3673, DOI 10.1121/1.4802650
   Svensson UP, 1999, J ACOUST SOC AM, V106, P2331, DOI 10.1121/1.428071
   TROPF H, 1981, ANGEW INFORM, P71
   Tsingos N, 2001, COMP GRAPH, P545, DOI 10.1145/383259.383323
   Wald I, 2022, OWL: A node graph "wrapper" library for OptiX 7
   Wald Ingo., 2020, The elephant on RTX - first light, or: Ray tracing Disneys MoanaIsland using RTX, OptiX, and OWL
   Wilkie A, 2014, COMPUT GRAPH FORUM, V33, P123, DOI 10.1111/cgf.12419
   Zellmann S, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P96, DOI 10.1109/VIS47514.2020.00026
NR 53
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3609
EP 3621
DI 10.1109/TVCG.2023.3238662
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700060
PM 37022015
DA 2024-11-06
ER

PT J
AU Le, TNH
   Yao, SY
   Wu, CT
   Lee, TY
AF Le, Thi-Ngoc-Hanh
   Yao, Sheng-Yi
   Wu, Chun-Te
   Lee, Tong-Yee
TI Regenerating Arbitrary Video Sequences With Distillation Path-Finding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; Feature extraction; Correlation; Shape; Video sequences;
   Prediction algorithms; Measurement; sequencing; RSFNet; distillation;
   SDPF
AB If the video has long been mentioned as a widespread visualization form, the animation sequence in the video is mentioned as storytelling for people. Producing an animation requires intensive human labor from skilled professional artists to obtain plausible animation in both content and motion direction, incredibly for animations with complex content, multiple moving objects, and dense movement. This article presents an interactive framework to generate new sequences according to the users' preference on the starting frame. The critical contrast of our approach versus prior work and existing commercial applications is that novel sequences with arbitrary starting frame are produced by our system with a consistent degree in both content and motion direction. To achieve this effectively, we first learn the feature correlation on the frameset of the given video through a proposed network called RSFNet. Then, we develop a novel path-finding algorithm, SDPF, which formulates the knowledge of motion directions of the source video to estimate the smooth and plausible sequences. The extensive experiments show that our framework can produce new animations on the cartoon and natural scenes and advance prior works and commercial applications to enable users to obtain more predictable results.
C1 [Le, Thi-Ngoc-Hanh; Yao, Sheng-Yi; Wu, Chun-Te; Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Lee, TY (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM ngochanh.le1987@gmail.com; nd8081018@gs.ncku.edu.tw;
   oojimda3838@hotmail.com; tonylee@mail.ncku.edu.tw
OI Le, Thi Ngoc Hanh/0000-0001-9667-9780; Yao, Sheng-Yi/0000-0002-7233-9615
FU National Science and Technology Council, Taiwan [111-2221-E-006-112-MY3,
   110-2221-E-006-135-MY3]
FX This work was supported in part by the National Science and Technology
   Council under Grants 111-2221-E-006-112-MY3 and 110-2221-E-006-135-MY3,
   Taiwan.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Chen Y, 2020, IEEE T CIRC SYST VID, V30, P3282, DOI 10.1109/TCSVT.2019.2931589
   de Juan C., 2004, P ACM SIGGRAPH EUR S, P267, DOI DOI 10.1145/1028523.1028559
   Eng E., 1996, Linux J, V1996, P2
   Fried O, 2017, COMPUT GRAPH FORUM, V36, P183, DOI 10.1111/cgf.13284
   Gammerman A., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P148
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I., 2016, Deep Learn., V1
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Holden Daniel, 2015, SIGGRAPH Asia 2015 Technical Briefs
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jiao Y, 2021, IEEE IMAGE PROC, P2558, DOI 10.1109/ICIP42928.2021.9506523
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Khan A, 2022, Arxiv, DOI arXiv:2112.01641
   Kingma D.P., 2014, P INT C LEARNING REP
   Mahasseni B, 2017, PROC CVPR IEEE, P2077, DOI 10.1109/CVPR.2017.224
   Morace CC, 2022, MULTIMED TOOLS APPL, V81, P23687, DOI 10.1007/s11042-022-12251-1
   Nwankpa C., 2018, 2nd International Conference on Computational Sciences and Technology, P124, DOI DOI 10.48550/ARXIV.1811.03378
   Osadchy M, 2007, J MACH LEARN RES, V8, P1197
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Schodl A., 2002, P 2002 ACM SIGGRAPHE, P121, DOI DOI 10.1145/545261.545281
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Xu C, 2023, IEEE T VIS COMPUT GR, V29, P4001, DOI 10.1109/TVCG.2022.3174656
   Yang Y, 2010, IEEE T CIRC SYST VID, V20, P1745, DOI 10.1109/TCSVT.2010.2087452
   Yu J, 2014, INFORM SCIENCES, V281, P674, DOI 10.1016/j.ins.2014.01.025
   Yu J, 2012, IEEE T SYST MAN CY B, V42, P1413, DOI 10.1109/TSMCB.2012.2192108
   Yu J, 2011, MULTIMEDIA SYST, V17, P409, DOI 10.1007/s00530-010-0225-6
   Zhang J, 2018, IEEE T IMAGE PROCESS, V27, P2420, DOI 10.1109/TIP.2018.2804218
   Zhang L, 2012, IEEE T VIS COMPUT GR, V18, P1156, DOI 10.1109/TVCG.2011.111
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang S.-W., 2019, P SIGGRAPH AS, P1
NR 33
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3622
EP 3635
DI 10.1109/TVCG.2023.3237739
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700083
PM 37021849
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Palacios-Ibáñez, A
   Pirault, S
   Ochando-Martí, F
   Contero, M
   Camba, JD
AF Palacios-Ibanez, Almudena
   Pirault, Simon
   Ochando-Marti, Francesc
   Contero, Manuel
   Camba, Jorge D.
TI An Examination of the Relationship Between Visualization Media and
   Consumer Product Evaluation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Media; Visualization; Telephone sets; Prototypes; Electronic mail; Solid
   modeling; Rendering (computer graphics); Artificial; augmented; and
   virtual realities; consumer products; perception and psychophysics;
   virtual reality
ID GENDER-DIFFERENCES; VIRTUAL-REALITY; ONLINE; PERCEPTION; DESIGN; USER;
   REPRESENTATIONS; CUSTOMERS; RESPONSES; HAPTICS
AB Virtual product presentations that rely on static images and text are often insufficient to communicate all the information that is necessary to accurately evaluate a product. Technologies such as Virtual Reality (VR) or Augmented Reality (AR) have enabled more sophisticated representation methods, but certain product characteristics are difficult to assess and may result in perceptual differences when a product is evaluated in different visual media. In this article, we report two case studies in which a group of participants evaluated three designs of two product typologies (i.e., a desktop telephone and a coffee maker) as presented in three different visual media (i.e., photorealistic renderings, AR, and VR for the first case study; and photographs, a non-immersive virtual environment, and AR for the second case study) using eight semantic scales. An inferential statistical method using Aligned Rank Transform (ART) proceedings was applied to determine perceptual differences between groups. Our results show that in both cases product attributes in Jordan's physio-pleasure category are the most affected by the presentation media. The socio-pleasure category was also affected for the case of the coffee makers. The level of immersion afforded by the medium significantly affects product evaluation.
C1 [Palacios-Ibanez, Almudena; Contero, Manuel] Univ Politecn Valencia, HUMAN Tech, Valencia 46022, Spain.
   [Pirault, Simon; Ochando-Marti, Francesc] Univ Politecn Valencia, Valencia 46022, Spain.
   [Camba, Jorge D.] Purdue Univ, Sch Engn Technol, W Lafayette, IN 47907 USA.
C3 Universitat Politecnica de Valencia; Universitat Politecnica de
   Valencia; Purdue University System; Purdue University
RP Palacios-Ibáñez, A (corresponding author), Univ Politecn Valencia, HUMAN Tech, Valencia 46022, Spain.
EM alpaib@upv.es; sipi@etsii.upv.es; fraocmar@etsii.upv.es;
   mcontero@upv.es; jdorribo@purdue.edu
RI Palacios-Ibáñez, Almudena/AFX-0246-2022; Contero, Manuel/F-4276-2010
OI Palacios Ibanez, Almudena/0000-0002-1115-0720; Dorribo Camba,
   Jorge/0000-0001-5384-3253; Contero, Manuel/0000-0002-6081-9988
FU Spanish Ministry of Education and Vocational Training under a FPU
   fellowship [FPU19/03878]
FX This research work was supported in part by the Spanish Ministry of
   Education and Vocational Training under a FPU fellowship under Grant
   FPU19/03878.
CR Achiche S., 2014, P ASME 2014 INT MECH, V11, P1, DOI [10.111 5/IMECE2014-40443, DOI 10.1115/IMECE2014-40443]
   Agost M. J., 2020, P 24 INT C PROJ MAN, P1827
   Agost M.-J., 2021, The Use of New Presentation Technologies in Electronic Sales Environments and Their Influence On Product Perception
   Agost MJ, 2021, IN SY AP IN WE HC, V12765, P3, DOI 10.1007/978-3-030-78321-1_1
   Arrighi PA, 2019, J INTELL MANUF, V30, P743, DOI 10.1007/s10845-016-1276-0
   Artacho-Ramírez MA, 2008, INT J IND ERGONOM, V38, P942, DOI 10.1016/j.ergon.2008.02.020
   Arvanitis TN, 2009, PERS UBIQUIT COMPUT, V13, P243, DOI 10.1007/s00779-007-0187-7
   Banerjee NT, 2022, IEEE T VIS COMPUT GR, V28, P4787, DOI 10.1109/TVCG.2021.3105606
   Beck M, 2018, J RETAIL CONSUM SERV, V40, P279, DOI 10.1016/j.jretconser.2016.08.006
   Berni A., 2020, P SCO C, P1607, DOI [10.1017/dsd.2020.296, DOI 10.1017/DSD.2020.296]
   Berni A, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9071064
   Bimber O., 2005, Spatial augmented reality: merging real and virtual worlds
   Bordegoni M, 2011, INNOVATION IN PRODUCT DESIGN: FROM CAD TO VIRTUAL PROTOTYPING, P117, DOI 10.1007/978-0-85729-775-4_7
   Cecil J, 2007, INT J ADV MANUF TECH, V31, P846, DOI 10.1007/s00170-005-0267-7
   Cho JS, 2004, INFORM MANAGE-AMSTER, V41, P827, DOI 10.1016/j.im.2003.08.013
   Chu CH, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10144723
   Coutts E. R., 2019, P DESIGN SOC INT C E, V1, P1313, DOI [10.1017/dsi.2019.137, DOI 10.1017/DSI.2019.137]
   Desmet P., 2011, P C INT UZ KANS PRAK, P8
   Desmet P., 2001, Des. J., V4, P32, DOI [10.2752/146069201789378496, DOI 10.2752/146069201789378496, 10.2752/146069201789378496arXiv:https://doi.org/10.2752/146069201789378496]
   Desmet P., 2002, DESIGNING EMOTIONS
   Dittmar H, 2004, SEX ROLES, V50, P423, DOI 10.1023/B:SERS.0000018896.35251.c7
   Donald N., 2004, Why We Love (or Hate) Everyday Things
   Engelbrektsson P., 2000, P INT PROD DEV MAN C, P29
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Felip F., 2021, P INT C PROJ MAN EG, P980
   Felip F, 2020, VIRTUAL REAL-LONDON, V24, P439, DOI 10.1007/s10055-019-00406-9
   Forbes T., 2018, DS 91 P NORD DES
   Galán J, 2021, INT J INTERACT MULTI, V6, P196, DOI 10.9781/ijimai.2021.01.001
   Galán J, 2021, J COMPUT DES ENG, V8, P330, DOI 10.1093/jcde/qwaa081
   Greengard S., 2022, Where augmented reality is going and why you should care
   Grohmann B, 2007, J RETAILING, V83, P237, DOI 10.1016/j.jretai.2006.09.001
   Hagtvedt H, 2017, J CONSUM RES, V44, P396, DOI 10.1093/jcr/ucx039
   Hannah R, 2012, J ENG DESIGN, V23, P443, DOI 10.1080/09544828.2011.615302
   Hasan B, 2010, COMPUT HUM BEHAV, V26, P597, DOI 10.1016/j.chb.2009.12.012
   Heineken E, 2007, HUM FACTORS, V49, P136, DOI 10.1518/001872007779598028
   Higgins J. J., 1990, P 34 ANN ACM S US IN, P754, DOI DOI 10.4148/2475-7772.1443
   Hoermann M, 2015, LECT NOTES COMPUT SC, V9179, P470, DOI 10.1007/978-3-319-21067-4_48
   Hsu CC, 2017, DISPLAYS, V50, P21, DOI 10.1016/j.displa.2017.09.002
   Hsu SH, 2000, INT J IND ERGONOM, V25, P375, DOI 10.1016/S0169-8141(99)00026-8
   Jeong SW, 2009, INTERNET RES, V19, P105, DOI 10.1108/10662240910927858
   Jiang ZJ, 2007, MIS QUART, V31, P475
   Jordan P.W., 2002, Designing pleasurable products: An introduction to the new human factors
   Kelly JW, 2023, IEEE T VIS COMPUT GR, V29, P4978, DOI 10.1109/TVCG.2022.3196606
   Kim TH, 2021, FASH TEXT, V8, DOI 10.1186/s40691-021-00261-w
   Kinzinger A, 2022, INFORM SYST J, V32, P1034, DOI 10.1111/isj.12382
   Li SM, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9040337
   Lin XL, 2019, INFORM SYST FRONT, V21, P1187, DOI 10.1007/s10796-018-9831-1
   Liu X, 2022, J ENG DESIGN, V33, P412, DOI 10.1080/09544828.2022.2078660
   Mansouri H, 2004, COMMUN STAT-THEOR M, V33, P2217, DOI 10.1081/STA-200026599
   Mata MP, 2017, RES ENG DES, V28, P357, DOI 10.1007/s00163-016-0244-1
   Meta, 2021, Meta Press Release
   Ortiz Nicolas J. C., 2013, P 5 INT ASS SOC RES, P5546
   Osgood C. E., 1957, MEASUREMENT MEANING
   Ozer M, 1999, J PROD INNOVAT MANAG, V16, P77, DOI 10.1016/S0737-6782(98)00037-X
   Ozok AA, 2009, INT J HUM-COMPUT INT, V25, P243, DOI 10.1080/10447310802546724
   Palacios-Ibáñez A, 2023, J MECH DESIGN, V145, DOI 10.1115/1.4055952
   Palacios-Ibáñez A, 2023, COMPUT IND, V144, DOI 10.1016/j.compind.2022.103780
   Park H, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app112311510
   Qu QX, 2019, INT J IND ERGONOM, V72, P281, DOI 10.1016/j.ergon.2019.06.006
   Ray S, 2017, INT CONF ENG DES, P563
   Reid TN, 2013, J MECH DESIGN, V135, DOI 10.1115/1.4024724
   Roy R, 2009, CIRP J MANUF SCI TEC, V1, P172, DOI 10.1016/j.cirpj.2008.10.007
   SCHOORMANS JPL, 1995, J PROD INNOVAT MANAG, V12, P153, DOI 10.1111/1540-5885.1220153
   Singh G, 2020, IEEE T VIS COMPUT GR, V26, P1385, DOI 10.1109/TVCG.2018.2869729
   Singh Jitender, 2022, Advances in Mechanical Engineering and Material Science: Select Proceedings of ICAMEMS-2022. Lecture Notes in Mechanical Engineering, P149, DOI 10.1007/978-981-19-0676-3_12
   Söderman M, 2005, J ENG DESIGN, V16, P311, DOI 10.1080/09544820500128967
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Tesch A, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2673, DOI 10.1145/3394171.3413980
   Tiainen T, 2014, VIRTUAL PHYS PROTOTY, V9, P169, DOI 10.1080/17452759.2014.934573
   Tiger L., 1992, PURSUIT PLEASURE
   Van Slyke C, 2002, COMMUN ACM, V45, P82, DOI 10.1145/545151.545155
   Virzi R. A., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P236, DOI 10.1145/238386.238516
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Ye J, 2007, LECT NOTES COMPUT SC, V4553, P1190
   Yoo J, 2014, J BUS RES, V67, P2464, DOI 10.1016/j.jbusres.2014.03.006
NR 75
TC 5
Z9 5
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3636
EP 3649
DI 10.1109/TVCG.2023.3238428
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700034
PM 37022017
OA hybrid
DA 2024-11-06
ER

PT J
AU Zhong, FH
   Xue, ML
   Zhang, J
   Zhang, F
   Ban, R
   Deussen, O
   Wang, YH
AF Zhong, Fahai
   Xue, Mingliang
   Zhang, Jian
   Zhang, Fan
   Ban, Rui
   Deussen, Oliver
   Wang, Yunhai
TI Force-Directed Graph Layouts Revisited: A New Force Based on the
   T-Distribution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Computational modeling; Force; Stress; Springs; Scalability;
   Graphics processing units; Graph layout; force directed placement;
   student's t-distribution; FFT
ID DRAWING LARGE GRAPHS; STRESS MODEL; ALGORITHM
AB In this article, we propose the t-FDP model, a force-directed placement method based on a novel bounded short-range force (t-force) defined by Student's t-distribution. Our formulation is flexible, exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. Using such forces in force-directed graph layouts yields better neighborhood preservation than current methods, while maintaining low stress errors. Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU, enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time for complex graphs. We demonstrate the quality of our approach by numerical evaluation against state-of-the-art approaches and extensions for interactive exploration.
C1 [Zhong, Fahai; Xue, Mingliang; Wang, Yunhai] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Zhang, Jian] Chinese Acad Sci, Comp Network Informat Ctr, Beijing 100045, Peoples R China.
   [Zhang, Fan] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 265600, Peoples R China.
   [Ban, Rui] CITC, Intelligent Network Design Inst, Beijing 100022, Peoples R China.
   [Deussen, Oliver] Univ Konstanz, Comp & Informat Sci, D-78464 Constance, Germany.
C3 Shandong University; Chinese Academy of Sciences; Computer Network
   Information Center, CAS; Shandong Technology & Business University;
   University of Konstanz
RP Wang, YH (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM zhongfahai@gmail.com; xml95007@gmail.com; zhangjian@sccas.cn;
   zhangfan@sdtbu.edu.cn; banrui1@chinaunicom.cn;
   Oliver.Deussen@uni-konstanz.de; cloudseawang@gmail.com
RI Deussen, Oliver/HKF-2004-2023; Zhang, Fan/GLT-6231-2022
OI Zhang, Fan/0000-0002-0343-3499; Zhong, Fahai/0000-0002-0971-0821; Zhang,
   Jian/0000-0003-1348-8124; Xue, Mingliang/0000-0001-8842-1667
FU National Key Research& Development Plan of China [2019YFB1704201];
   Shandong Provincial Natural Science Foundation [ZR2022JQ32]; NSFC
   [62132017, 62141217]; Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation) under Germany's Excellence Strategy [EXC 2117 -
   422037984]
FX This work was supported in part by the National Key Research&
   Development Plan of China under Grant 2019YFB1704201, in part by the
   Shandong Provincial Natural Science Foundation under Grant
   ZR2022JQ32,and in part by the NSFC under Grants 62132017 and 62141217,
   as well as by the Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) under Germany's Excellence Strategy - EXC 2117 - 422037984.
CR Amid E., 2019, arXiv
   BARNES J, 1986, NATURE, V324, P446, DOI 10.1038/324446a0
   Bastian M, 2009, P INT AAAI C WEBL SO, V3, P361, DOI 10.13140/2.1.1341.1520
   Biedl T, 1998, COMP GEOM-THEOR APPL, V9, P159, DOI 10.1016/S0925-7721(97)00026-6
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Brandes U, 2007, LECT NOTES COMPUT SC, V4372, P42
   Chan DM, 2018, INT SYM COMP ARCHIT, P330, DOI [10.1109/CAHPC.2018.8645912, 10.1109/SBAC-PAD.2018.00060]
   Chen LS, 2009, J AM STAT ASSOC, V104, P209, DOI 10.1198/jasa.2009.0111
   Chen Y, 2019, J VISUAL-JAPAN, V22, P625, DOI 10.1007/s12650-019-00551-y
   Chimani M., 2013, Handbook of Graph Drawing and Visualization, P543, DOI [10.1201/b153854, DOI 10.1201/B153854]
   Davis TA, 2011, ACM T MATH SOFTWARE, V38, DOI 10.1145/2049662.2049663
   Douglas Carroll J., 1998, Multidimensional Scaling
   Dwyer T, 2009, COMPUT GRAPH FORUM, V28, P991, DOI 10.1111/j.1467-8659.2009.01449.x
   Ellson J, 2002, LECT NOTES COMPUT SC, V2265, P483
   EPPERSON JF, 1987, AM MATH MON, V94, P329, DOI 10.2307/2323093
   FRUCHTERMAN TMJ, 1991, SOFTWARE PRACT EXPER, V21, P1129, DOI 10.1002/spe.4380211102
   Gansner ER, 2013, IEEE T VIS COMPUT GR, V19, P927, DOI 10.1109/TVCG.2012.299
   Gove R, 2019, COMPUT GRAPH FORUM, V38, P739, DOI 10.1111/cgf.13724
   Greengard L. F., 1987, PhD dissertation, DOI [10.5555/913529, DOI 10.5555/913529]
   Hachul S, 2004, LECT NOTES COMPUT SC, V3383, P285
   Hu YF, 2009, IEEE PAC VIS SYMP, P129, DOI 10.1109/PACIFICVIS.2009.4906847
   Hu Yifan, 2005, Mathematica J., V10, P37
   Jacomy M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098679
   KAMADA T, 1989, INFORM PROCESS LETT, V31, P7, DOI 10.1016/0020-0190(89)90102-6
   Khoury M, 2012, COMPUT GRAPH FORUM, V31, P975, DOI 10.1111/j.1467-8659.2012.03090.x
   Kobak D, 2021, NAT BIOTECHNOL, V39, DOI 10.1038/s41587-020-00809-z
   Kobourov S. G., 2013, Force-Directed Drawing Algorithms
   Koren Y, 2009, LECT NOTES COMPUT SC, V5417, P193, DOI 10.1007/978-3-642-00219-9_19
   Kruiger JF, 2017, COMPUT GRAPH FORUM, V36, P283, DOI 10.1111/cgf.13187
   Kumar P., 2009, P 2009 ACM S APPL CO, P1800
   Lam S. K., 2015, P 2 WORKSH LLYM COMP, P1
   Lee B., 2006, P AVI WORKSH TIM ERR, P1, DOI [DOI 10.1145/1168149.1168168, 10.1145/1168149.1168168]
   Leskovec J, 2014, SNAP Datasets: Stanford large network dataset collection, DOI DOI 10.18637/JSS.V024.I04
   Linderman GC, 2019, NAT METHODS, V16, P243, DOI 10.1038/s41592-018-0308-4
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   Noack Andreas, 2007, Journal of Graph Algorithms and Applications, V11, P453, DOI 10.7155/jgaa.00154
   Pearson K., 1895, PHILOS T R SOC A, V186, P343, DOI [10.1098/rsta.1895.0010, DOI 10.1098/RSTA.1895.0010]
   Peixoto Tiago P, 2017, Figshare
   Pezzotti N, 2020, IEEE T VIS COMPUT GR, V26, P1172, DOI 10.1109/TVCG.2019.2934307
   Purchase HC, 2002, J VISUAL LANG COMPUT, V13, P501, DOI 10.1006/S1045-926X(02)00016-2
   Schneider Johannes J., 2006, Stochastic Optimization
   Tang J, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P287, DOI 10.1145/2872427.2883041
   Tutte W.T., 1963, Proc. Lond. Math. Soc, Vs3-13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Walshaw C., 2001, Graph Drawing. 8th International Symposium, GD 2000. Proceedings (Lecture Notes in Computer Science Vol.1984), P171
   Wang Y, 2018, IEEE T VIS COMPUT GR, V24, P489, DOI 10.1109/TVCG.2017.2745919
   Zhu MF, 2021, IEEE T VIS COMPUT GR, V27, P1666, DOI 10.1109/TVCG.2020.3030447
NR 48
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3650
EP 3663
DI 10.1109/TVCG.2023.3238821
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700017
PM 37021999
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Ma, ZH
   Li, CZ
   Liu, XT
   Wu, HS
   Wen, ZK
AF Ma, Ziheng
   Li, Chengze
   Liu, Xueting
   Wu, Huisi
   Wen, Zhenkun
TI Separating Shading and Reflectance From Cartoon Illustrations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cartoon; deep learning; layer decomposition; shading extraction; shading
   removal
ID INTRINSIC IMAGE DECOMPOSITION
AB Shading plays an important role in cartoon drawings to present the 3D lighting and depth information in a 2D image to improve the visual information and pleasantness. But it also introduces apparent challenges in analyzing and processing the cartoon drawings for different computer graphics and vision applications, such as segmentation, depth estimation, and relighting. Extensive research has been made in removing or separating the shading information to facilitate these applications. Unfortunately, the existing researches only focused on natural images, which are natively different from cartoons since the shading in natural images is physically correct and can be modeled based on physical priors. However, shading in cartoons is manually created by artists, which may be imprecise, abstract, and stylized. This makes it extremely difficult to model the shading in cartoon drawings. Without modeling the shading prior, in the paper, we propose a learning-based solution to separate the shading from the original colors using a two-branch system consisting of two subnetworks. To the best of our knowledge, our method is the first attempt in separating shading information from cartoon drawings. Our method significantly outperforms the methods tailored for natural images. Extensive evaluations have been performed with convincing results in all cases.
C1 [Ma, Ziheng; Liu, Xueting; Wu, Huisi; Wen, Zhenkun] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Li, Chengze] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong 999077, Peoples R China.
C3 Shenzhen University; Saint Francis University Hong Kong
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
EM 2070276193@email.szu.edu.cn; ljsabc@gmail.com; xtliu@szu.edu.cn;
   hswu@szu.edu.cn; wenzk@szu.edu.cn
RI Liu, Xueting/AAG-9648-2019; Li, Chengze/AAU-7168-2021
OI Li, Chengze/0000-0002-1519-750X; Ma, Ziheng/0000-0002-2628-553X; Wu,
   Huisi/0000-0002-0399-9089
FU National Natural Science Foundation of China [61973221, 62002232,
   62273241]; Natural Science Foundation of Guangdong Province, China
   [2019A1515011165]; Major Project of the New Generation of Artificial
   Intelligence [2018AAA0102900]; Research Grants Council of the Hong Kong
   Special Administrative Region, China [UGC/FDS11/E02/21]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61973221, 62002232 and 62273241, inpart
   by the Natural Science Foundation of Guangdong Province, China under
   Grant 2019A1515011165, in part by the Major Project of the New
   Generation of Artificial Intelligence under Grant 2018AAA0102900, and in
   part by the Research Grants Council of the Hong Kong Special
   Administrative Region, China under Grant UGC/FDS11/E02/21.
CR Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Bi S, 2018, Arxiv, DOI arXiv:1807.11226
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Janner M, 2018, Arxiv, DOI arXiv:1711.03678
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li CL, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3091108
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331
   Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rother M., 2011, ADV NEURAL INFORM PR, P765
   Selinger P., 2003, Potrace: a polygon-based tracing algorithm
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P425, DOI 10.1109/TSMCB.2012.2208744
   Shen L, 2008, PROC CVPR IEEE, P2479
   Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738
   STERNBERG SR, 1983, COMPUTER, V16, P22, DOI 10.1109/MC.1983.1654163
   Wang ZJ, 2019, IEEE INT CONF COMP V, P4310, DOI 10.1109/ICCVW.2019.00531
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 29
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3664
EP 3679
DI 10.1109/TVCG.2023.3239364
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700039
PM 37021997
DA 2024-11-06
ER

PT J
AU Seo, MY
   Kang, KE
   Kang, HY
AF Seo, Min Yeong
   Kang, Kyung Eun
   Kang, Hyeong Yeop
TI VR Blowing: A Physically Plausible Interaction Method for Blowing Air in
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Microphones; Fluid flow measurement; Navigation; Usability; Force;
   Tracking; Mouth; Blowing control; human-computer interaction; immersion;
   virtual reality
AB This article introduces an interaction method allowing virtual reality (VR) users to interact with virtual objects by blowing air. The proposed method allows users to interact with virtual objects in a physically plausible way by recognizing the intensity of the wind generated by the user's actual wind blowing activity in the physical world. This is expected to provide immersed VR experience since it enables users to interact with virtual objects in the same way they do in the real world. Three experiments were carried out to develop and improve this method. In the first experiment, we collected the user's blowing data and used it to model a formula to estimate the speed of the wind from the sound waves obtained through a microphone. In the second experiment, we investigated how much gain can be applied to the formula obtained in the first experiment. The aim is to reduce the lung capacity required to generate wind without compromising physical plausibility. In the third experiment, the advantages and disadvantages of the proposed method compared to the controller-based method were investigated in two scenarios of blowing a ball and a pinwheel. According to the experimental results and participant interview, participants felt a stronger sense of presence and found the VR experience more fun with the proposed blowing interaction method.
C1 [Seo, Min Yeong; Kang, Hyeong Yeop] Kyung Hee Univ, Dept Software Convergence, Yongin 130701, South Korea.
   [Kang, Kyung Eun] Kyung Hee Univ, Dept Elect & Software Convergence, Yongin 130701, South Korea.
C3 Kyung Hee University; Kyung Hee University
RP Kang, HY (corresponding author), Kyung Hee Univ, Dept Software Convergence, Yongin 130701, South Korea.
EM sminy98@khu.ac.kr; kyungh22@khu.ac.kr; siamiz@khu.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kang, HyeongYeop/0000-0001-5292-4342
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [NRF-2020R1F1A1076528]
FX This work was supported in part by the Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education under Grant NRF-2020R1F1A1076528.
CR Abkarian M, 2020, P NATL ACAD SCI USA, V117, P25237, DOI 10.1073/pnas.2012156117
   Arora R, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P463, DOI 10.1145/3332165.3347942
   Arroyo-Palacios Jorge, 2009, 2009 International IEEE Consumer Electronics Society's Games Innovations Conference (ICE-GIC 2009), P154, DOI 10.1109/ICEGIC.2009.5293588
   Arroyo-Palacios J, 2016, IEEE T AFFECT COMPUT, V7, P326, DOI 10.1109/TAFFC.2015.2472013
   Blum J, 2020, APPL PSYCHOPHYS BIOF, V45, P153, DOI 10.1007/s10484-020-09468-x
   Bouzbib E., 2021, P 32 C FRANC INT HOM, P1
   Brooke J., 1986, SYSTEM USABILITY SCA, V43, P1
   Bruder G., 2014, P 20 ACM S VIRT REAL, P177
   Chakraborty BK, 2018, IET COMPUT VIS, V12, P3, DOI 10.1049/iet-cvi.2017.0052
   Chen YQ, 2020, IEEE ACCESS, V8, P115486, DOI 10.1109/ACCESS.2020.3004349
   Cherni H., 2021, International Journal of Virtual Reality, V21, P1, DOI [DOI 10.20870/IJVR.2021.21.1.3046, 10.20870/ijvr.2021.21.1.3046]
   Craig CL, 2003, MED SCI SPORT EXER, V35, P1381, DOI 10.1249/01.MSS.0000078924.61453.FB
   Davies C., 1996, Computer Graphics, V30, P25, DOI 10.1145/240806.240808
   Delrieu T, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P266, DOI [10.1109/VR46266.2020.00-58, 10.1109/VR46266.2020.1581332505844]
   Desnoyers-Stewart E. R., 2019, P EXT ABSTR CHI C HU, P1
   Fahy F.J., 1989, SOUND INTENSITY
   Gupta A. E., 2019, Advanced ComputationalIntelligence Techniques for Virtual Reality in Healthcare, V875
   Höll M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P175, DOI 10.1109/VR.2018.8448284
   Hoppe M, 2018, 17TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2018), P7, DOI 10.1145/3282894.3282898
   Hoshikawa Y, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P464, DOI 10.1109/VR51125.2022.00066
   Huang K. N., 1999, Proceedings of the First Joint BMES/EMBS Conference. 1999 IEEE Engineering in Medicine and Biology 21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering Society (Cat. No.99CH37015), DOI 10.1109/IEMBS.1999.803946
   Inazawa M, 2019, SIGGRAPH ASIA 2019 EMERGING TECHNOLOGIES, P5, DOI 10.1145/3355049.3360523
   Ioannou C, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300388
   Jain Dhruv., 2016, Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, P1563
   Jovanov E, 2001, Biomed Sci Instrum, V37, P493
   Kim JS, 2019, IEEE INT C INT ROBOT, P3235, DOI [10.1109/iros40897.2019.8968088, 10.1109/IROS40897.2019.8968088]
   Kim M, 2020, INT J HUM-COMPUT INT, V36, P685, DOI 10.1080/10447318.2019.1680920
   Kim S.-G., 2018, P CHI C HUM FACT COM, P1
   Kleinbaum L. L., 2013, AppliedRegression Analysisand OtherMultivariable Methods, P33
   Korsi MJL, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P91, DOI 10.1145/2967934.2968110
   Lampe OD, 2011, IEEE PAC VIS SYMP, P171, DOI 10.1109/PACIFICVIS.2011.5742387
   Li Y.X., 2019, VRIH, V56, P84, DOI DOI 10.3724/SP.J.2096-5796.2018.0006
   Lin W., 2017, Human-Computer Interaction. User Interface Design, V10271, P584, DOI [10.1007/978-3-319-58071-5_44, DOI 10.1007/978-3-319-58071-5_44]
   Llorens M., 2016, J. Neuroengineering Rehabil., V13, P1
   Marill KA, 2004, ACAD EMERG MED, V11, P87, DOI 10.1197/S1069-6563(03)00600-6
   Marti-Puig P, 2014, FRONT ARTIF INTEL AP, V269, P257, DOI 10.3233/978-1-61499-452-7-257
   Mhetre MR, 2017, ENG SCI TECHNOL, V20, P332, DOI 10.1016/j.jestch.2016.06.012
   Mine M.R., 1995, Virtual environment interaction techniques
   Montali N, 2018, Definition of different calibration methods for digital memsmicrophones
   Moon HS, 2023, INT J HUM-COMPUT INT, V39, P2840, DOI 10.1080/10447318.2022.2087000
   Norman Donald A., 2010, Interactions, V17, P6, DOI [DOI 10.1145/1744161.1744163, 10.1145/1744161.1744163]
   Oo Jin Heng, 2020, BDIOT 2020: Proceedings of the 2020 4th International Conference on Big Data and Internet of Things, P33, DOI 10.1145/3421537.3421552
   Patibanda R, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P19, DOI 10.1145/3116595.3116621
   Poupyrev I., 1998, Computer Graphics Forum, V17, pC41, DOI 10.1111/1467-8659.00252
   Prpa M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376183
   Qudsi H, 2013, PR SOUTH BIOMED ENG, P23, DOI 10.1109/SBEC.2013.20
   Re Guido Maria, 2014, Virtual, Augmented and Mixed Reality. Designing and Developing Virtual and Augmented Environments. 6th International Conference, VAMR 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8525, P93, DOI 10.1007/978-3-319-07458-0_10
   Ritter KA III, 2022, VIRTUAL REAL-LONDON, V26, P571, DOI 10.1007/s10055-021-00502-9
   Rockstroh C, 2021, VIRTUAL REAL-LONDON, V25, P539, DOI 10.1007/s10055-020-00471-5
   Roo JS, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1459, DOI 10.1145/3025453.3025743
   Ruddle RA, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1970378.1970384
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Sampson H, 2018, INT CONF IMAG VIS
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Schäfer A, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10060715
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Slater M, 1998, HUM FACTORS, V40, P469, DOI 10.1518/001872098779591368
   SLATER M, 1994, ARTIFICIAL LIFE AND VIRTUAL REALITY, P125
   Slater M, 2000, PRESENCE-TELEOP VIRT, V9, P413, DOI 10.1162/105474600566925
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Smith N. F., 1972, Phys. Teacher, V10, P451, DOI [10.1119/1.2352317, DOI 10.1119/1.2352317]
   Smus B., 2013, WEB AUDIO API ADV SO
   Sra X., 2018, P CHI C HUM FACT COM, P1
   Steed A, 2016, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2016.7504689
   Stepanova ER, 2020, PROCEEDINGS OF THE 2020 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2020), P641, DOI 10.1145/3357236.3395532
   Suma EA, 2010, IEEE T VIS COMPUT GR, V16, P690, DOI 10.1109/TVCG.2009.93
   Tatzgern M., 2022, P CHI C HUM FACT COM, P1
   Tennent P, 2011, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2011)
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Verschoor M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P183, DOI 10.1109/VR.2018.8447555
   Voigt-Antons J.N, 2020, P 12 INT C QUAL MULT, P1
   Waltemate T., 2015, P 21 ACM S VIRT REAL, P139, DOI DOI 10.1145/2821592.2821607
   Weerdmeester J, 2017, EXTENDED ABSTRACTS PUBLICATION OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY'17 EXTENDED ABSTRACTS), P453, DOI 10.1145/31308593131299
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Wolf K., 2020, P CHI C HUM FACT COM, P1
   Xu C, 2015, INDOOR AIR, V25, P198, DOI 10.1111/ina.12135
   Yan C. Yu, 2018, P CHI C HUM FACT COM, P1
   Yu D. A., 2018, Front.ICT, V5
   Yu Y, 2014, SIMUL MODEL PRACT TH, V45, P62, DOI 10.1016/j.simpat.2014.04.001
   Zenner A, 2020, IEEE T VIS COMPUT GR, V26, P2104, DOI 10.1109/TVCG.2020.2973476
   Zielasko D., 2015, P 3 ACM S SPAT US IN, P20
   Zielasko D, 2017, IEEE SYMP 3D USER, P40, DOI 10.1109/3DUI.2017.7893316
   Zielasko D, 2017, P IEEE VIRT REAL ANN, P319, DOI 10.1109/VR.2017.7892305
NR 84
TC 2
Z9 2
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3680
EP 3692
DI 10.1109/TVCG.2023.3238478
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700095
PM 37022016
DA 2024-11-06
ER

PT J
AU Shen, LX
   Tai, ZW
   Shen, EY
   Wang, JM
AF Shen, Leixian
   Tai, Zhiwei
   Shen, Enya
   Wang, Jianmin
TI Graph Exploration With Embedding-Guided Layouts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph embedding; graph exploration; graph layout
ID OF-THE-ART; VISUALIZATION; ALGORITHM
AB Node-link diagrams are widely used to visualize graphs. Most graph layout algorithms only use graph topology for aesthetic goals (e.g., minimize node occlusions and edge crossings) or use node attributes for exploration goals (e.g., preserve visible communities). Existing hybrid methods that bind the two perspectives still suffer from various generation restrictions (e.g., limited input types and required manual adjustments and prior knowledge of graphs) and the imbalance between aesthetic and exploration goals. In this article, we propose a flexible embedding-based graph exploration pipeline to enjoy the best of both graph topology and node attributes. First, we leverage embedding algorithms for attributed graphs to encode the two perspectives into latent space. Then, we present an embedding-driven graph layout algorithm, GEGraph, which can achieve aesthetic layouts with better community preservation to support an easy interpretation of the graph structure. Next, graph explorations are extended based on the generated graph layout and insights extracted from the embedding vectors. Illustrated with examples, we build a layout-preserving aggregation method with Focus+Context interaction and a related nodes searching approach with multiple proximity strategies. Finally, we conduct quantitative and qualitative evaluations, a user study, and two case studies to validate our approach.
C1 [Shen, Leixian; Tai, Zhiwei; Shen, Enya; Wang, Jianmin] Tsinghua Univ, Beijing 100190, Peoples R China.
C3 Tsinghua University
RP Shen, EY (corresponding author), Tsinghua Univ, Beijing 100190, Peoples R China.
EM slx20@mails.tsinghua.edu.cn; tzw20@mails.tsinghua.edu.cn;
   shenenya@tsinghua.edu.cn; jimwang@tsinghua.edu.cn
OI Shen, Leixian/0000-0003-1084-4912
FU National Natural Science Foundation of China [71690231]; Beijing Key
   Laboratory of Industrial Bigdata System and Application
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 71690231 and in part by the Beijing Key
   Laboratory of Industrial Bigdata System and Application.
CR Abello J, 2006, IEEE T VIS COMPUT GR, V12, P669, DOI 10.1109/TVCG.2006.120
   Ahmed NK, 2022, IEEE T KNOWL DATA EN, V34, P2401, DOI 10.1109/TKDE.2020.3006475
   [Anonymous], 2004, J. Graph Algorithms Appl., DOI 10.7155/jgaa.00089
   Archambault D, 2008, IEEE T VIS COMPUT GR, V14, P900, DOI 10.1109/TVCG.2008.34
   Bannister Michael J., 2013, Graph Drawing. 20th International Symposium, GD 2012. Revised Selected Papers, P414, DOI 10.1007/978-3-642-36763-2_37
   Barsky A, 2008, IEEE T VIS COMPUT GR, V14, P1253, DOI 10.1109/TVCG.2008.117
   Bezerianos A, 2010, COMPUT GRAPH FORUM, V29, P863, DOI 10.1111/j.1467-8659.2009.01687.x
   Bharadwaj A, 2022, ACM T COMPUT-HUM INT, V29, DOI 10.1145/3479196
   Börner K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0039464
   Brandes U, 2007, LECT NOTES COMPUT SC, V4372, P42
   Büschel W, 2019, IEEE COMPUT GRAPH, V39, P29, DOI 10.1109/MCG.2019.2897927
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Chen W, 2019, IEEE T VIS COMPUT GR, V25, P555, DOI 10.1109/TVCG.2018.2865139
   Chen Y, 2019, J VISUAL-JAPAN, V22, P625, DOI 10.1007/s12650-019-00551-y
   Cohen J. D., 1997, ACM Transactions on Computer-Human Interaction, V4, P197, DOI 10.1145/264645.264657
   Craven M, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P509
   Dong YX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P135, DOI 10.1145/3097983.3098036
   Dork S., 2011, P C VIS DAT AN, V17
   Dunne C, 2015, IBM J RES DEV, V59, DOI 10.1147/JRD.2015.2411412
   Fischer MT, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P81, DOI 10.1109/VIS49827.2021.9623305
   FRUCHTERMAN TMJ, 1991, SOFTWARE PRACT EXPER, V21, P1129, DOI 10.1002/spe.4380211102
   Gajer P., 2001, Graph Drawing. 8th International Symposium, GD 2000. Proceedings (Lecture Notes in Computer Science Vol.1984), P211
   Gao HC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3364
   Gehlenborg N, 2010, NAT METHODS, V7, pS56, DOI [10.1038/nmeth.1436, 10.1038/NMETH.1436]
   Ghani S, 2013, IEEE T VIS COMPUT GR, V19, P2032, DOI 10.1109/TVCG.2013.223
   Gibson H, 2017, Arxiv, DOI arXiv:1712.05644
   Gibson H, 2016, APPL SOFT COMPUT, V42, P80, DOI 10.1016/j.asoc.2016.01.036
   Gibson H, 2013, INFORM VISUAL, V12, P324, DOI 10.1177/1473871612455749
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Guo DS, 2009, IEEE T VIS COMPUT GR, V15, P1041, DOI 10.1109/TVCG.2009.143
   Hachul S, 2004, LECT NOTES COMPUT SC, V3383, P285
   Haleem H, 2019, IEEE COMPUT GRAPH, V39, P40, DOI 10.1109/MCG.2018.2881501
   Henderson K., 2012, P ACM SIGKDD INT C K, P1231, DOI 10.1145/2339530.2339723
   Herman I, 2000, IEEE T VIS COMPUT GR, V6, P24, DOI 10.1109/2945.841119
   Horak T, 2021, IEEE T VIS COMPUT GR, V27, P1644, DOI 10.1109/TVCG.2020.3030371
   Itoh T, 2015, IEEE COMPUT GRAPH, V35, P30, DOI 10.1109/MCG.2015.115
   Jusufi Ilir, 2013, 2013 17th International Conference on Information Visualisation, P19, DOI 10.1109/IV.2013.3
   KAMADA T, 1989, INFORM PROCESS LETT, V31, P7, DOI 10.1016/0020-0190(89)90102-6
   Kerren A., 2014, Lecture Notes in Computer Science, V8380
   KNUTH DE, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P41
   Kruiger JF, 2017, COMPUT GRAPH FORUM, V36, P283, DOI 10.1111/cgf.13187
   Krzywinski M, 2009, GENOME RES, V19, P1639, DOI 10.1101/gr.092759.109
   Kwon OH, 2020, IEEE T VIS COMPUT GR, V26, P665, DOI 10.1109/TVCG.2019.2934396
   Kwon OH, 2018, IEEE T VIS COMPUT GR, V24, P478, DOI 10.1109/TVCG.2017.2743858
   Lee C., 2006, P AVI WROKSH TIM ERR, P1
   Lehtinen M., 2012, Systems Reference Library, P46
   Leow T., 2019, P INT C LEARN REPR
   Leskovec J., 2012, ADV NEURAL INFORM PR, V25
   Liao LZ, 2018, IEEE T KNOWL DATA EN, V30, P2257, DOI 10.1109/TKDE.2018.2819980
   Lu Q., 2003, ICML 03, P496
   Martins J. F., 2017, P 19 EG VGTC C VIS, P10
   Meyer M, 2009, IEEE T VIS COMPUT GR, V15, P897, DOI 10.1109/TVCG.2009.167
   Neuweger Heiko, 2009, BMC Syst Biol, V3, P82, DOI 10.1186/1752-0509-3-82
   Noack A, 2004, LECT NOTES COMPUT SC, V2912, P425
   Noack A., 2005, SoftVis '05: Proceedings of the 2005 ACM symposium on Software visualization, P155
   Nobre C, 2019, COMPUT GRAPH FORUM, V38, P807, DOI 10.1111/cgf.13728
   Partl C, 2014, IEEE T VIS COMPUT GR, V20, P1883, DOI 10.1109/TVCG.2014.2346752
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Pretorius AJ, 2008, COMPUT GRAPH FORUM, V27, P967, DOI 10.1111/j.1467-8659.2008.01231.x
   Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061
   Robertson S, 2004, J DOC, V60, P503, DOI 10.1108/00220410410560582
   Shen L, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3104146
   Shi L, 2014, IEEE PAC VIS SYMP, P89, DOI 10.1109/PacificVis.2014.44
   Shneiderman B, 2006, IEEE T VIS COMPUT GR, V12, P733, DOI 10.1109/TVCG.2006.166
   Spanurattana S., 2011, 2011 IEEE International Conference on Data Mining Workshops, P833, DOI 10.1109/ICDMW.2011.175
   Spritzer A. S., 2008, P WORK C ADV VIS INT, P271
   Spritzer AS, 2012, IEEE T VIS COMPUT GR, V18, P822, DOI 10.1109/TVCG.2011.106
   Srinivasan A, 2018, IEEE T VIS COMPUT GR, V24, P511, DOI 10.1109/TVCG.2017.2745219
   Suh A, 2020, IEEE T VIS COMPUT GR, V26, P697, DOI 10.1109/TVCG.2019.2934802
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   van den Elzen S, 2014, IEEE T VIS COMPUT GR, V20, P2310, DOI 10.1109/TVCG.2014.2346441
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Landesberger T, 2011, COMPUT GRAPH FORUM, V30, P1719, DOI 10.1111/j.1467-8659.2011.01898.x
   Walshaw C., 2001, Graph Drawing. 8th International Symposium, GD 2000. Proceedings (Lecture Notes in Computer Science Vol.1984), P171
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P676, DOI 10.1109/TVCG.2019.2934798
   Wang Y, 2016, IEEE T VIS COMPUT GR, V22, P359, DOI 10.1109/TVCG.2015.2467691
   Wang YH, 2020, IEEE T VIS COMPUT GR, V26, P687, DOI 10.1109/TVCG.2019.2934805
   Wattenberg M., 2006, Conference on Human Factors in Computing Systems. CHI2006, P811, DOI 10.1145/1124772.1124891
   Wongsuphasawat K, 2018, IEEE T VIS COMPUT GR, V24, P1, DOI 10.1109/TVCG.2017.2744878
   Wu M., 2006, P AS PAC S INF VIS, P77
   Wu YH, 2016, IEEE T VIS COMPUT GR, V22, P260, DOI [10.1109/TVCG.2015.2468151, 10.1109/TVCG.2015.2465151]
   Wu YX, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P223
   Xue ML, 2023, IEEE T VIS COMPUT GR, V29, P4256, DOI 10.1109/TVCG.2022.3187425
   Yang C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2111
   Yang H, 2018, IEEE DATA MINING, P1476, DOI [10.1109/ICDM.2018.00207, 10.1109/ICDM.2018.8626170]
   Yang J, 2014, P IEEE, V102, P1892, DOI 10.1109/JPROC.2014.2364018
   Zhang DK, 2018, LECT NOTES ARTIF INT, V10938, P196, DOI 10.1007/978-3-319-93037-4_16
   Zhang DK, 2016, IEEE DATA MINING, P609, DOI [10.1109/ICDM.2016.0072, 10.1109/ICDM.2016.139]
   Zheng JX, 2019, IEEE T VIS COMPUT GR, V25, P2738, DOI 10.1109/TVCG.2018.2859997
   Zhou ZG, 2021, IEEE T VIS COMPUT GR, V27, P1709, DOI 10.1109/TVCG.2020.3030440
   Zhu DY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2827, DOI 10.1145/3219819.3220052
   Zhu MF, 2021, IEEE T VIS COMPUT GR, V27, P1666, DOI 10.1109/TVCG.2020.3030447
NR 92
TC 1
Z9 1
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3693
EP 3708
DI 10.1109/TVCG.2023.3238909
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700041
PM 37022062
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhong, CL
   Sang, XZ
   Yan, BB
   Li, H
   Chen, D
   Qin, XJ
   Chen, S
   Ye, XQ
AF Zhong, Chongli
   Sang, Xinzhu
   Yan, Binbin
   Li, Hui
   Chen, Duo
   Qin, Xiujuan
   Chen, Shuo
   Ye, Xiaoqian
TI Real-Time High-Quality Computer-Generated Hologram Using Complex-Valued
   Convolutional Neural Network
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neural networks; Computer architecture; Three-dimensional displays; Deep
   learning; Image reconstruction; Holography; Convolution; neural models;
   virtual and augmented reality
ID SPATIAL LIGHT-MODULATOR; ANGULAR SPECTRUM METHOD; 3-DIMENSIONAL DISPLAY;
   PHASE; PROPAGATION; IMAGE
AB Holographic displays are ideal display technologies for virtual and augmented reality because all visual cues are provided. However, real-time high-quality holographic displays are difficult to achieve because the generation of high-quality computer-generated hologram (CGH) is inefficient in existing algorithms. Here, complex-valued convolutional neural network (CCNN) is proposed for phase-only CGH generation. The CCNN-CGH architecture is effective with a simple network structure based on the character design of complex amplitude. A holographic display prototype is set up for optical reconstruction. Experiments verify that state-of-the-art performance is achieved in terms of quality and generation speed in existing end-to-end neural holography methods using the ideal wave propagation model. The generation speed is three times faster than HoloNet and one-sixth faster than Holo-encoder, and the Peak Signal to Noise Ratio (PSNR) is increased by 3 dB and 9 dB, respectively. Real-time high-quality CGHs are generated in 1920 x 1072 and 3840 x 2160 resolutions for dynamic holographic displays.
C1 [Zhong, Chongli; Sang, Xinzhu; Yan, Binbin; Chen, Duo; Qin, Xiujuan; Chen, Shuo; Ye, Xiaoqian] Beijing Univ Posts & Telecommun, State Key Lab Informat Photon & Opt Commun, Beijing 100876, Peoples R China.
   [Li, Hui] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Tsinghua University
RP Sang, XZ; Yan, BB (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Informat Photon & Opt Commun, Beijing 100876, Peoples R China.
EM zclda@bupt.edu.cn; xzsang@bupt.edu.cn; yanbinbin@bupt.edu.cn;
   huilijtsd@163.com; chenduo15@aliyun.com; 18801175281@163.com;
   shuochen365@bupt.edu.cn; xiaoqianye@bupt.edu.cn
RI z, cl/KZU-4124-2024
OI Zhong, Chongli/0000-0001-8917-7512
FU National Natural Science Foundation of China [62075016, 62175017]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62075016, 62175017.
CR Amano H, 2020, OPT EXPRESS, V28, P5692, DOI 10.1364/OE.387072
   Blinder D, 2021, IEEE T IMAGE PROCESS, V30, P9418, DOI 10.1109/TIP.2021.3125495
   Chakravarthula P, 2021, IEEE T VIS COMPUT GR, V27, P4194, DOI 10.1109/TVCG.2021.3106433
   Chakravarthula P, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417846
   Chakravarthula P, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356539
   Chang C, 2020, OPTICA, V7, P1563, DOI 10.1364/OPTICA.406004
   Chen C, 2021, OPT EXPRESS, V29, P15089, DOI 10.1364/OE.425077
   Choi H.-S., 2018, P INT C LEARN REPR, P63
   Choi Suyeon, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530734
   Choi S, 2021, OPTICA, V8, P143, DOI 10.1364/OPTICA.410622
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   Eybposh MH, 2020, OPT EXPRESS, V28, P26636, DOI 10.1364/OE.399624
   GERCHBERG RW, 1972, OPTIK, V35, P237
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZH, 2021, APPL OPTICS, V60, pA145, DOI 10.1364/AO.404934
   Horisaki R, 2021, APPL OPTICS, V60, pA323, DOI 10.1364/AO.404151
   Horisaki R, 2018, APPL OPTICS, V57, P3859, DOI 10.1364/AO.57.003859
   Huang HK, 2018, OPT EXPRESS, V26, P17578, DOI 10.1364/OE.26.017578
   Ishii Y, 2022, APPL PHYS B-LASERS O, V128, DOI 10.1007/s00340-022-07753-7
   Khan A, 2021, COMPLEXITY, V2021, DOI 10.1155/2021/6662161
   Kim S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459776
   Krajancich B, 2020, IEEE T VIS COMPUT GR, V26, P1871, DOI 10.1109/TVCG.2020.2973443
   Li G, 2016, OPT LETT, V41, P2486, DOI 10.1364/OL.41.002486
   Li H, 2020, OPT ENG, V59, DOI 10.1117/1.OE.59.10.102408
   Lin X, 2018, SCIENCE, V361, P1004, DOI 10.1126/science.aat8084
   Liu KX, 2022, APPL PHYS LETT, V120, DOI 10.1063/5.0080797
   Ma XL, 2018, OPT ENG, V57, DOI 10.1117/1.OE.57.9.095105
   Macedo MCF, 2023, IEEE T VIS COMPUT GR, V29, P1590, DOI 10.1109/TVCG.2021.3117866
   Maimone A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073624
   Matsushima K, 2009, OPT EXPRESS, V17, P19662, DOI 10.1364/OE.17.019662
   Pan YJ, 2016, IEEE T IND INFORM, V12, P1599, DOI 10.1109/TII.2015.2496304
   Peng YF, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417802
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi L, 2021, NATURE, V591, P234, DOI 10.1038/s41586-020-03152-0
   Slinger C, 2005, COMPUTER, V38, P46, DOI 10.1109/MC.2005.260
   Su P, 2016, J DISP TECHNOL, V12, P1688, DOI 10.1109/JDT.2016.2553440
   Su YF, 2018, OPT COMMUN, V428, P216, DOI 10.1016/j.optcom.2018.07.061
   Su YF, 2018, J OPT SOC AM A, V35, P1477, DOI 10.1364/JOSAA.35.001477
   Sui XM, 2021, OPT EXPRESS, V29, P2597, DOI 10.1364/OE.414299
   Sun F., 2021, Research On Single Fiber Imaging Technology Under Fiber Deformation, P336
   Trabelsi C, 2018, Arxiv, DOI arXiv:1705.09792
   Tsang PWM, 2018, PHOTONICS RES, V6, P837, DOI 10.1364/PRJ.6.000837
   Wang F, 2021, OPT EXPRESS, V29, P35442, DOI 10.1364/OE.435966
   Wu JC, 2021, OPT LETT, V46, P2908, DOI 10.1364/OL.425485
   Xiao JS, 2022, J OPT SOC AM A, V39, pA15, DOI 10.1364/JOSAA.440464
   Yang R, 2008, IEEE T VIS COMPUT GR, V14, P84, DOI 10.1109/70410
   Zhang H, 2016, APPL OPTICS, V55, pA154, DOI 10.1364/AO.55.00A154
   Zhang WH, 2020, OPT EXPRESS, V28, P39916, DOI 10.1364/OE.413636
   Zhang WH, 2020, OPT LETT, V45, P1543, DOI 10.1364/OL.385553
   Zhong C., 2022, CCNN-CGH
   Zhou TK, 2021, NAT PHOTONICS, V15, P367, DOI 10.1038/s41566-021-00796-w
NR 51
TC 3
Z9 3
U1 12
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3709
EP 3718
DI 10.1109/TVCG.2023.3239670
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700006
PM 37022034
DA 2024-11-06
ER

PT J
AU Hu, JB
   Wang, SF
   He, Y
   Luo, ZX
   Lei, N
   Liu, LG
AF Hu, Jiangbei
   Wang, Shengfa
   He, Ying
   Luo, Zhongxuan
   Lei, Na
   Liu, Ligang
TI A Parametric Design Method for Engraving Patterns on Thin Shells
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Optimization; Shape; Topology; Design methodology; Computational
   modeling; Solid modeling; Three-dimensional displays; Parametric design;
   pattern engraving; structural optimization; thin shells
ID FINITE-ELEMENT-ANALYSIS; TOPOLOGY OPTIMIZATION; SURFACES
AB Designing thin-shell structures that are diverse, lightweight, and physically viable is a challenging task for traditional heuristic methods. To address this challenge, we present a novel parametric design framework for engraving regular, irregular, and customized patterns on thin-shell structures. Our method optimizes pattern parameters such as size and orientation, to ensure structural stiffness while minimizing material consumption. Our method is unique in that it works directly with shapes and patterns represented by functions, and can engrave patterns through simple function operations. By eliminating the need for remeshing in traditional FEM methods, our method is more computationally efficient in optimizing mechanical properties and can significantly increase the diversity of shell structure design. Quantitative evaluation confirms the convergence of the proposed method. We conduct experiments on regular, irregular, and customized patterns and present 3D printed results to demonstrate the effectiveness of our approach.
C1 [Hu, Jiangbei; Wang, Shengfa; Lei, Na] Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116024, Peoples R China.
   [Hu, Jiangbei; Lei, Na] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116024, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Luo, Zhongxuan] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.
   [Luo, Zhongxuan] Guilin Univ Elect Technol, Inst Artificial Intelligence, Guilin 541004, Guangxi, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology;
   Nanyang Technological University; Dalian University of Technology;
   Guilin University of Electronic Technology; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Wang, SF (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116024, Peoples R China.; He, Y (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM jiangbei.hu@ntu.edu.sg; sfwang@dlut.edu.cn; yhe@ntu.edu.sg;
   zxluo@dlut.edu.cn; nalei@dlut.edu.cn; lgliu@ustc.edu.cn
RI Liu, Ligang/IZQ-5817-2023; He, Ying/A-3708-2011; liu, na/HKF-7392-2023;
   Hu, Jiangbei/KXJ-1687-2024
OI Hu, Jiangbei/0000-0002-6774-6267; Lei, Na/0000-0003-3361-0756
FU National Key R&D Program of China [2020YFB1709402, 2021YFA1003003];
   Academic Research Fund of the Ministry of Education of Singapore
   [MOE-T2EP20220-0005, RG20/20]
FX This work was supported by the National Key R&D Program of China under
   Grants 2020YFB1709402 and 2021YFA1003003 and in part by the Academic
   Research Fund under Grant MOE-T2EP20220-0005 & RG20/20 of the Ministry
   of Education of Singapore.
CR Adriaenssens S., 2014, Shell Struc-tures for Architecture: Form Finding and Optimization
   Ahsan AMMN, 2018, PROCEDIA MANUF, V26, P900, DOI 10.1016/j.promfg.2018.07.117
   Allaire G, 1997, NUMER MATH, V76, P27, DOI 10.1007/s002110050253
   [Anonymous], 2015, Comput. Vis. Media
   Bächer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601157
   Bendse MP, 1989, Struct. Optim, V1, P192, DOI [DOI 10.1007/BF01650949, 10.1007/BF01650949]
   Bermano AH, 2017, COMPUT GRAPH FORUM, V36, P509, DOI 10.1111/cgf.13146
   Bickel B, 2018, COMPUT GRAPH FORUM, V37, P325, DOI 10.1111/cgf.13327
   Bucalem ML, 1997, ARCH COMPUT METHOD E, V4, P3, DOI 10.1007/BF02818930
   Chai SM, 2018, GRAPH MODELS, V97, P80, DOI 10.1016/j.gmod.2018.04.002
   Chen WK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925911
   Cheng L, 2019, COMPUT METHOD APPL M, V344, P334, DOI 10.1016/j.cma.2018.10.010
   Cirak F, 2002, COMPUT AIDED DESIGN, V34, P137, DOI 10.1016/S0010-4485(01)00061-6
   Cohen E., 2001, Geometric Modeling WithSplines: An Introduction
   Dumas J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766984
   Farshad M., 2013, Design and Analysis of Shell Structures, V16
   Hollister SJ, 2005, NAT MATER, V4, P518, DOI 10.1038/nmat1421
   Hu JB, 2022, IEEE T VIS COMPUT GR, V28, P2615, DOI 10.1109/TVCG.2020.3037697
   Jawad Maan., 2012, Theory and design of plate and shell structures
   Jiang CG, 2014, COMPUT GRAPH FORUM, V33, P185, DOI 10.1111/cgf.12444
   Kuipers T, 2019, COMPUT AIDED DESIGN, V114, P37, DOI 10.1016/j.cad.2019.05.003
   Le T. N. H., 2021, P SIGGRAPH AS POST, P1
   Lévy B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778856
   Liu C, 2018, STRUCT MULTIDISCIP O, V58, P2455, DOI 10.1007/s00158-018-2114-0
   Liu JK, 2018, STRUCT MULTIDISCIP O, V57, P2457, DOI 10.1007/s00158-018-1994-3
   Liu X., 2020, Comput.-Aided Des., V127, P1
   Lu L, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601168
   Martínez J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322989
   Martínez J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818101
   Melaragno M., 2012, An introduction to shell structures: the art and science of vaulting
   Nguyen TH, 2010, STRUCT MULTIDISCIP O, V41, P525, DOI 10.1007/s00158-009-0443-8
   Panetta J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323040
   Peng CH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201306
   Pietroni N, 2015, COMPUT GRAPH FORUM, V34, P627, DOI 10.1111/cgf.12590
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Rao C, 2019, COMPUT AIDED GEOM D, V71, P130, DOI 10.1016/j.cagd.2019.04.018
   Schroeder C, 2005, COMPUT AIDED DESIGN, V37, P339, DOI 10.1016/j.cad.2004.03.008
   Schumacher C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275085
   Schumacher C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201278
   Schumacher C, 2016, COMPUT GRAPH FORUM, V35, P101, DOI 10.1111/cgf.12967
   Skouras M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461979
   SKYUM S, 1991, INFORM PROCESS LETT, V37, P121, DOI 10.1016/0020-0190(91)90030-L
   Stadlbauer P, 2020, COMPUT GRAPH FORUM, V39, P277, DOI 10.1111/cgf.13929
   Svanberg K, 2001, SIAM J OPTIMIZ, V12, P555
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Vaxman Amir, 2022, Zenodo, DOI 10.5281/ZENODO.3338174
   Wang MY, 2003, COMPUT METHOD APPL M, V192, P227, DOI 10.1016/S0045-7825(02)00559-5
   Wang WM, 2018, IEEE T VIS COMPUT GR, V24, P2787, DOI 10.1109/TVCG.2017.2764462
   Wang WM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508382
   Wu J, 2018, IEEE T VIS COMPUT GR, V24, P1127, DOI 10.1109/TVCG.2017.2655523
   Wu ZY, 2005, J GLOBAL OPTIM, V31, P45, DOI 10.1007/s10898-004-0569-6
   XIE YM, 1993, COMPUT STRUCT, V49, P885, DOI 10.1016/0045-7949(93)90035-C
   Yang JR, 2019, COMPUT AIDED DESIGN, V114, P191, DOI 10.1016/j.cad.2019.05.028
   Zehnder J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925888
   Zhang JY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661255
   Zhang WS, 2016, STRUCT MULTIDISCIP O, V53, P1243, DOI 10.1007/s00158-015-1372-3
   Zhang XL, 2015, COMPUT AIDED GEOM D, V35-36, P149, DOI 10.1016/j.cagd.2015.03.012
   Zhang XT, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P243, DOI 10.1145/3126594.3126600
NR 58
TC 1
Z9 1
U1 3
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3719
EP 3730
DI 10.1109/TVCG.2023.3240503
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700058
PM 37022859
DA 2024-11-06
ER

PT J
AU Zhao, ZM
   Xie, W
   Zuo, BH
   Wang, YG
AF Zhao, Zimeng
   Xie, Wei
   Zuo, Binghui
   Wang, Yangang
TI Skeleton Extraction for Articulated Objects With the Spherical
   Unwrapping Profiles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Skeleton; Three-dimensional displays; Feature extraction; Heating
   systems; Topology; Point cloud compression; Task analysis; Skeleton
   embedding; spherical unwrapping; surface-to-image representation
ID HUMAN POSE ESTIMATION; ALGORITHM
AB Embedding unified skeletons into unregistered scans is fundamental to finding correspondences, depicting motions, and capturing underlying structures among the articulated objects in the same category. Some existing approaches rely on laborious registration to adapt a predefined LBS model to each input, while others require the input to be set to a canonical pose, e.g., T-pose or A-pose. However, their effectiveness is always influenced by the water-tightness, face topology, and vertex density of the input mesh. At the core of our approach lies a novel unwrapping method, named SUPPLE (Spherical UnwraPping ProfiLEs), which maps a surface into image planes independent of mesh topologies. Based on this lower-dimensional representation, a learning-based framework is further designed to localize and connect skeletal joints with fully convolutional architectures. Experiments demonstrate that our framework yields reliable skeleton extractions across a broad range of articulated categories, from raw scans to online CADs.
C1 [Zhao, Zimeng; Xie, Wei; Zuo, Binghui; Wang, Yangang] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China
RP Wang, YG (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
EM zzmpassion@gmail.com; xiewei.xw@outlook.com; z_binghui@163.com;
   yangangwang@seu.edu.cn
RI Wang, Yangang/IVH-8352-2023
OI Zhao, Zimeng/0000-0001-6570-0620; Xie, Wei/0000-0001-7632-2987; Wang,
   Yangang/0000-0002-1325-9252
FU National Natural Science Foundation of China [62076061]; Natural Science
   Foundation of Jiangsu Province [BK20220127]; Young Elite Scientists
   Sponsorship Program by CAST [YES20200025]; Program of Southeast
   University [2242021R41083]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076061, in part by the Natural Science
   Foundation of Jiangsu Province under Grant BK20220127, by the "Young
   Elite Scientists Sponsorship Program by CAST" under Grant YES20200025
   and by the "Zhishan Young Scholar" Program of Southeast University under
   Grant 2242021R41083.
CR Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392462
   Akenine-Mo T., 2018, Real-time rendering
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], UTAH 3D ANIMATION RE
   [Anonymous], TURBOSQUID
   [Anonymous], THINGIVERSE
   Antotsiou D, 2019, LECT NOTES COMPUT SC, V11134, P287, DOI 10.1007/978-3-030-11024-6_19
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   Baran I, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239523
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Biggs Benjamin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P195, DOI 10.1007/978-3-030-58621-8_12
   Le BH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925959
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Bridson Robert., 2007, SIGGRAPH sketches, V10, DOI DOI 10.1145/1278780.1278807
   Cao ZJ, 2017, INT CONF 3D VISION, P566, DOI 10.1109/3DV.2017.00070
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chao YW, 2021, PROC CVPR IEEE, P9040, DOI 10.1109/CVPR46437.2021.00893
   Chen Cailing, 2018, Wuhan University Journal of Natural Sciences, V23, P201, DOI 10.1007/s11859-018-1311-4
   Chen P., 2021, P IEEE CVF INT C COM, P12929
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Choi H, 2020, EUR C COMP VIS, P769
   Choutas Vasileios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P20, DOI 10.1007/978-3-030-58607-2_2
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   de Aguiar E, 2008, COMPUT GRAPH FORUM, V27, P389, DOI 10.1111/j.1467-8659.2008.01136.x
   Deng B., 2020, P COMP VIS ECCV 2020, P612, DOI [10.1007/978-3-030-58571-636, DOI 10.1007/978-3-030-58571-636]
   Ding YK, 2022, PROC CVPR IEEE, P8575, DOI 10.1109/CVPR52688.2022.00839
   Dong ZJ, 2022, PROC CVPR IEEE, P20438, DOI 10.1109/CVPR52688.2022.01982
   Garau N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11657, DOI 10.1109/ICCV48922.2021.01147
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Ge LH, 2018, LECT NOTES COMPUT SC, V11217, P489, DOI 10.1007/978-3-030-01261-8_29
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Ge LH, 2018, PROC CVPR IEEE, P8417, DOI 10.1109/CVPR.2018.00878
   Ge L, 2016, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2016.391
   Georgakis Georgios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P768, DOI 10.1007/978-3-030-58520-4_45
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415
   Hasler N., 2010, P ACM SIGGRAPH S INT, P23, DOI DOI 10.1145/1730804.1730809
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang BZ, 2022, IEEE T IMAGE PROCESS, V31, P4679, DOI 10.1109/TIP.2022.3187294
   Huang BZ, 2021, INT CONF 3D VISION, P710, DOI 10.1109/3DV53792.2021.00080
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Jiang B., 2022, P IEEE CVF C COMP VI, P5605
   Junjie Cao, 2010, Proceedings of the Shape Modeling International (SMI 2010), P187, DOI 10.1109/SMI.2010.25
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Karunratanakul K, 2021, INT CONF 3D VISION, P11, DOI 10.1109/3DV53792.2021.00012
   Karunratanakul K, 2020, INT CONF 3D VISION, P333, DOI 10.1109/3DV50981.2020.00043
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kingma D.P., 2014, P INT C LEARNING REP
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Kulon D., 2019, P BRIT MACH VIS C, V30, P1
   Kulon D, 2020, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR42600.2020.00504
   Le BH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601161
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Lin C, 2021, PROC CVPR IEEE, P4275, DOI 10.1109/CVPR46437.2021.00426
   Loop C.T., 1987, Smooth Subdivision Surfaces Based on Triangles
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma QL, 2021, PROC CVPR IEEE, P16077, DOI 10.1109/CVPR46437.2021.01582
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Maas A.L., 2013, P 30 INT C MACH LEAR, P28
   Magnenat-Thalmann Nadia., 1988, P GRAPH INT 88, P26, DOI [10.20380/GI1988.04, DOI 10.20380/GI1988.04]
   Malik J, 2020, PROC CVPR IEEE, P7111, DOI 10.1109/CVPR42600.2020.00714
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mihajlovic M, 2021, PROC CVPR IEEE, P10456, DOI 10.1109/CVPR46437.2021.01032
   Mildenhall B., 2020, P ECCV
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Moon G., 2020, EUR C COMP VIS, P440
   Moon G, 2018, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2018.00533
   Morreale L, 2022, PROC CVPR IEEE, P19311, DOI 10.1109/CVPR52688.2022.01873
   Kipf TN, 2017, Arxiv, DOI [arXiv:1609.02907, 10.48550/arXiv.1609.02907]
   Ni S., 2020, P ACM SIGGRAPH 2020
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Nt H, 2019, Arxiv, DOI arXiv:1905.09550
   Oono K., 2019, arXiv
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Palmer D, 2022, PROC CVPR IEEE, P18644, DOI 10.1109/CVPR52688.2022.01811
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Patel P, 2021, PROC CVPR IEEE, P13463, DOI 10.1109/CVPR46437.2021.01326
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Peng C, 2016, COMPUT GRAPH-UK, V59, P107, DOI 10.1016/j.cag.2016.06.001
   Pickup D, 2016, INT J COMPUT VISION, V120, P169, DOI 10.1007/s11263-016-0903-8
   Pishchulin L, 2017, PATTERN RECOGN, V67, P276, DOI 10.1016/j.patcog.2017.02.018
   Poranne R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130845
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin HX, 2020, COMPUT GRAPH FORUM, V39, P363, DOI 10.1111/cgf.14151
   Rabinovich M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983621
   Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054
   Reynolds D. A., 2009, ENCY BIOMETRICS
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito S, 2021, PROC CVPR IEEE, P2885, DOI 10.1109/CVPR46437.2021.00291
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Sawhney R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132705
   Schaefer S., 2007, P S GEOM PROC, P153, DOI DOI 10.2312/SGP/SGP07/153-162
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shi RX, 2021, PROC CVPR IEEE, P43, DOI 10.1109/CVPR46437.2021.00011
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tiwari Garvita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P1, DOI 10.1007/978-3-030-58580-8_1
   Vaswani A., 2017, Advances in neural information processing systems
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang SF, 2021, PROC CVPR IEEE, P7635, DOI 10.1109/CVPR46437.2021.00755
   Wang YG, 2020, IEEE T IMAGE PROCESS, V29, P2977, DOI 10.1109/TIP.2019.2955280
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiong F, 2019, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2019.00088
   Xiu YL, 2022, PROC CVPR IEEE, P13286, DOI 10.1109/CVPR52688.2022.01294
   Xu Z, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392379
   Xu Z, 2019, INT CONF 3D VISION, P298, DOI 10.1109/3DV.2019.00041
   Yang YP, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P41, DOI 10.1109/3DV.2014.47
   Yang ZQ, 2020, PROC CVPR IEEE, P5305, DOI 10.1109/CVPR42600.2020.00535
   Yao PF, 2019, Arxiv, DOI arXiv:1903.10153
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   You Y., 2020, P IEEE CVF C COMP VI, p13 647
   You Y, 2022, PROC CVPR IEEE, P17021, DOI 10.1109/CVPR52688.2022.01653
   Yu ZX, 2020, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR42600.2020.00306
   Yuan SX, 2017, PROC CVPR IEEE, P2605, DOI 10.1109/CVPR.2017.279
   Yuan Y., 2021, P IEEE CVF C COMP VI
   Zhang BW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11334, DOI 10.1109/ICCV48922.2021.01116
   Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582
   Zhang JK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459756
   ZHANG KZ, 1989, SIAM J COMPUT, V18, P1245, DOI 10.1137/0218082
   Zhang T., 2020, IEEECVF C COMPUTER V, P7376
   Zhang ZH, 2020, IEEE T VIS COMPUT GR, V26, P1851, DOI 10.1109/TVCG.2020.2973076
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
   Zhao ZY, 2020, INT CONF ACOUST SPEE, P2478, DOI [10.1109/icassp40776.2020.9053321, 10.1109/ICASSP40776.2020.9053321]
   Zhao ZM, 2022, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR52688.2022.00169
   Zhao ZM, 2021, INT CONF 3D VISION, P899, DOI 10.1109/3DV53792.2021.00098
   Zhao ZM, 2020, PROC SPIE, V11550, DOI 10.1117/12.2572825
   Zhong CL, 2022, Arxiv, DOI arXiv:2206.01724
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
   Zuffi S, 2019, IEEE I CONF COMP VIS, P5358, DOI 10.1109/ICCV.2019.00546
   Zuffi S, 2018, PROC CVPR IEEE, P3955, DOI 10.1109/CVPR.2018.00416
   Zuffi S, 2017, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2017.586
NR 151
TC 0
Z9 0
U1 6
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3731
EP 3748
DI 10.1109/TVCG.2023.3239370
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700011
PM 37022000
DA 2024-11-06
ER

PT J
AU Lyu, Y
   Lu, HX
   Lee, MK
   Schmitt, G
   Lim, BY
AF Lyu, Yan
   Lu, Hangxin
   Lee, Min Kyung
   Schmitt, Gerhard
   Lim, Brian Y.
TI IF-City: Intelligible Fair City Planning to Measure, Explain and
   Mitigate Inequality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Resource management; Visualization; Urban planning; Indexes; Data
   visualization; Buildings; Sociology; Fairness; intelligibility;
   explainable artificial intelligence; resource allocation; urban planning
ID LAND-USE; URBAN; ACCESSIBILITY; EQUITY; NETWORKS; DESIGN
AB With the increasing pervasiveness of Artificial Intelligence (AI), many visual analytics tools have been proposed to examine fairness, but they mostly focus on data scientist users. Instead, tackling fairness must be inclusive and involve domain experts with specialized tools and workflows. Thus, domain-specific visualizations are needed for algorithmic fairness. Furthermore, while much work on AI fairness has focused on predictive decisions, less has been done for fair allocation and planning, which require human expertise and iterative design to integrate myriad constraints. We propose the Intelligible Fair Allocation (IF-Alloc) Framework that leverages explanations of causal attribution (Why), contrastive (Why Not) and counterfactual reasoning (What If, How To) to aid domain experts to assess and alleviate unfairness in allocation problems. We apply the framework to fair urban planning for designing cities that provide equal access to amenities and benefits for diverse resident types. Specifically, we propose an interactive visual tool, Intelligible Fair City Planner (IF-City), to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations (IF-Plan). We demonstrate and evaluate the usage and usefulness of IF-City on a real neighborhood in New York City, US, with practicing urban planners from multiple countries, and discuss generalizing our findings, application, and framework to other use cases and applications of fair allocation.
C1 [Lyu, Yan] Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.
   [Lu, Hangxin; Schmitt, Gerhard] Singapore ETH Ctr, Future Cities Lab, Singapore 138602, Singapore.
   [Lu, Hangxin; Schmitt, Gerhard] Swiss Fed Inst Technol, Dept Architecture, CH-8092 Zurich, Switzerland.
   [Lee, Min Kyung] Univ Texas Austin, Sch Informat, Austin, TX 78712 USA.
   [Lim, Brian Y.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Southeast University - China; Swiss Federal Institutes of Technology
   Domain; ETH Zurich; University of Texas System; University of Texas
   Austin; National University of Singapore
RP Lim, BY (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM lvyanly@gmail.com; luhangxin@gmail.com; minkyung.lee@austin.utexas.edu;
   schmitt@arch.ethz.ch; brianlim@comp.nus.edu.sg
OI LYU, Yan/0000-0001-9959-9217; Lim, Brian/0000-0002-0543-2414; Lee, Min
   Kyung/0000-0002-2696-6546
FU National Key Research and Development Program of China [2019YFB2102200];
   Singapore Ministry of Education (MOE) Academic Research Fund Tier 2
   [T2EP20121-0040]; Natural Science Foundation of China [62102082];
   Jiangsu Natural Science Foundation of China [BK20210203]; National
   Science Foundation [CNS-1952085, IIS-1939606, DGE-2125858]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2019YFB2102200; in part by the
   Singapore Ministry of Education (MOE) Academic Research Fund Tier 2
   under Grant T2EP20121-0040; in part by the Natural Science Foundation of
   China under Grant 62102082; in part by the Jiangsu Natural Science
   Foundation of China under Grant BK20210203; and in part by National
   Science Foundation under Grants CNS-1952085, IIS-1939606, and
   DGE-2125858.
CR Abdul A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376615
   Abdul A, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174156
   Ahn Y, 2020, IEEE T VIS COMPUT GR, V26, P1086, DOI 10.1109/TVCG.2019.2934262
   Andrienko N, 2011, IEEE T VIS COMPUT GR, V17, P205, DOI 10.1109/TVCG.2010.44
   [Anonymous], 2011, TRANSP RES BOARD 90
   ANSELIN L, 1995, GEOGR ANAL, V27, P93, DOI 10.1111/j.1538-4632.1995.tb00338.x
   ATKINSON AB, 1970, J ECON THEORY, V2, P244, DOI 10.1016/0022-0531(70)90039-6
   Bao J., 2012, P 20 INT C ADV GEOGR, P199, DOI [DOI 10.1145/2424321.2424348, 10.1145/2424321.2424348]
   Baruah SK, 1996, ALGORITHMICA, V15, P600, DOI 10.1007/BF01940883
   Beutel A, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P453, DOI 10.1145/3306618.3314234
   Bird S., 2020, Microsoft Tech. Rep. MSR-TR-2020-32
   Buolamwini J., 2018, P 1 C FAIRN ACC TRAN, P77, DOI DOI 10.2147/OTT.S126905
   Cabrera AA, 2019, IEEE CONF VIS ANAL, P46, DOI [10.1109/VAST47406.2019.8986948, 10.1109/vast47406.2019.8986948]
   Castro J, 2009, COMPUT OPER RES, V36, P1726, DOI 10.1016/j.cor.2008.04.004
   Cheng FR, 2021, IEEE T VIS COMPUT GR, V27, P1438, DOI 10.1109/TVCG.2020.3030342
   Cheng P, 2021, FORESTS, V12, DOI 10.3390/f12070890
   Corbett-Davies S, 2018, Arxiv, DOI arXiv:1808.00023
   Das T., 1982, EMPIR ECON, V7, P23, DOI DOI 10.1007/BF02506823
   Datta A, 2016, P IEEE S SECUR PRIV, P598, DOI 10.1109/SP.2016.42
   Deng ZK, 2020, IEEE T VIS COMPUT GR, V26, P800, DOI 10.1109/TVCG.2019.2934670
   Dhurandhar A, 2018, ADV NEUR IN, V31
   Doraiswamy H, 2014, IEEE T VIS COMPUT GR, V20, P2634, DOI 10.1109/TVCG.2014.2346449
   Dwork C., 2012, P 3 INNOVATIONS THEO, P214, DOI DOI 10.1145/2090236.2090255
   Fieseler C, 2019, J BUS ETHICS, V156, P987, DOI 10.1007/s10551-017-3607-2
   Foth N, 2013, J TRANSP GEOGR, V29, P1, DOI 10.1016/j.jtrangeo.2012.12.008
   Frick M., 2003, P 3 SWISS TRANSP RES
   Ghodsi An, 2011, Computer Communication Review, V41, P507, DOI 10.1145/2018584.2018586
   Gomez O, 2020, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2020, P531, DOI 10.1145/3377325.3377536
   Goodall J.R., 2005, CHI 05, P1403
   Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009
   Hardt M, 2016, ADV NEUR IN, V29
   Henry N, 2007, IEEE T VIS COMPUT GR, V13, P1302, DOI 10.1109/TVCG.2007.70582
   Hohman F, 2020, IEEE T VIS COMPUT GR, V26, P1096, DOI 10.1109/TVCG.2019.2934659
   Hohman F, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300809
   Holten D, 2009, COMPUT GRAPH FORUM, V28, P983, DOI 10.1111/j.1467-8659.2009.01450.x
   Hoover EM, 1936, REV ECON STATISTICS, V18, P162, DOI 10.2307/1927875
   Hurter C, 2012, COMPUT GRAPH FORUM, V31, P865, DOI 10.1111/j.1467-8659.2012.03079.x
   Jacono M., 2008, Rep. No. 4
   Jaggi M., 2013, ICML 13, V28
   Jain R., 1984, A Quantitative Measure of Fairness and Discrimination
   Joe-Wong C, 2013, IEEE ACM T NETWORK, V21, P1785, DOI 10.1109/TNET.2012.2233213
   Bellamy RKE, 2018, Arxiv, DOI [arXiv:1810.01943, 10.48550/arXiv.1810.01943, DOI 10.48550/ARXIV.1810.01943]
   Kahng M, 2018, IEEE T VIS COMPUT GR, V24, P88, DOI 10.1109/TVCG.2017.2744718
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Krause J, 2017, IEEE CONF VIS ANAL, P162, DOI 10.1109/VAST.2017.8585720
   Krause J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5686, DOI 10.1145/2858036.2858529
   Kumar A., 2021, P INT C AUT PLAN SCH
   Law M., 2015, Getting to know ArcGIS
   Law PM, 2017, IEEE T VIS COMPUT GR, V23, P231, DOI 10.1109/TVCG.2016.2599378
   Lee MK, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1603, DOI 10.1145/2702123.2702548
   Levin G, 2010, EUROMICRO, P3, DOI 10.1109/ECRTS.2010.34
   Liao QV, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376590
   Lim BY, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P13
   Lim BY, 2009, UBICOMP'09: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P195
   Liu DY, 2017, IEEE T VIS COMPUT GR, V23, P1, DOI 10.1109/TVCG.2016.2598432
   Liu J, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13031587
   Liu MH, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10070439
   Lorenz MO, 1905, J AM STAT ASSOC, V9, P209
   Lu YS, 2022, IEEE T MOBILE COMPUT, V21, P663, DOI 10.1109/TMC.2020.3008315
   Lundberg SM, 2017, ADV NEUR IN, V30
   Lyu Y, 2020, IEEE T VIS COMPUT GR, V26, P811, DOI 10.1109/TVCG.2019.2934657
   Mehrabi N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457607
   Miller T, 2021, KNOWL ENG REV, V36, DOI 10.1017/S0269888921000102
   Miller T, 2019, ARTIF INTELL, V267, P1, DOI 10.1016/j.artint.2018.07.007
   Min Kyung Lee, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359283
   Ming Y, 2019, IEEE T VIS COMPUT GR, V25, P342, DOI 10.1109/TVCG.2018.2864812
   Miranda F, 2017, IEEE T VIS COMPUT GR, V23, P791, DOI 10.1109/TVCG.2016.2598585
   Mueller J, 2018, CITIES, V72, P181, DOI 10.1016/j.cities.2017.08.018
   Neutens T, 2010, ENVIRON PLANN A, V42, P1613, DOI 10.1068/a4230
   New York City Department of city planning (DCP), 2005, land use code
   Nyelele C, 2020, URBAN FOR URBAN GREE, V53, DOI 10.1016/j.ufug.2020.126723
   Olah C., 2017, DISTILL, DOI [10.23915/distill.00007, DOI 10.23915/DISTILL.00007]
   Páez A, 2012, J TRANSP GEOGR, V25, P141, DOI 10.1016/j.jtrangeo.2012.03.016
   Pandey A, 2021, AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P822, DOI 10.1145/3461702.3462561
   Popolin Neto M, 2021, IEEE T VIS COMPUT GR, V27, P1427, DOI 10.1109/TVCG.2020.3030354
   Radunovic B, 2007, IEEE ACM T NETWORK, V15, P1073, DOI 10.1109/TNET.2007.896231
   Raman N, 2021, PROCEEDINGS OF THE THIRTIETH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2021, P363
   Rambonilaza M, 2007, LANDSCAPE URBAN PLAN, V83, P318, DOI 10.1016/j.landurbplan.2007.05.013
   Renyi A., 1961, P 4 BERKELEY S MATH, V1
   Reyes M, 2014, LANDSCAPE URBAN PLAN, V125, P38, DOI 10.1016/j.landurbplan.2014.02.002
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Robertson S., 2021, P CHI C HUM FACT COM
   Roth A.E., 1988, The Shapley Value: Essays in Honor of Lloyd S. Shapley, DOI DOI 10.1017/CBO9780511528446
   Santos B, 2008, TRANSPORT RES REC, P35, DOI 10.3141/2089-05
   SHORROCKS AF, 1980, ECONOMETRICA, V48, P613, DOI 10.2307/1913126
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, 10.48550/arXiv.1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Speicher T, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2239, DOI 10.1145/3219819.3220046
   Spinner T, 2020, IEEE T VIS COMPUT GR, V26, P1064, DOI 10.1109/TVCG.2019.2934629
   Sühr T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P3082, DOI 10.1145/3292500.3330793
   Talen E, 1998, J AM PLANN ASSOC, V64, P22, DOI 10.1080/01944369808975954
   Uhde A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376656
   Verma S, 2018, 2018 IEEE/ACM INTERNATIONAL WORKSHOP ON SOFTWARE FAIRNESS (FAIRWARE 2018), P1, DOI [10.1145/3194770.3194776, 10.23919/FAIRWARE.2018.8452913]
   Wachter S., 2017, SSRN Electronic Journal, V31, DOI DOI 10.2139/SSRN.3063289
   Waddell P, 2002, J AM PLANN ASSOC, V68, P297, DOI 10.1080/01944360208976274
   Wang DD, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI [10.1145/3290605.3300831, 10.1109/icocn.2019.8934212]
   Wang QW, 2021, IEEE T VIS COMPUT GR, V27, P1470, DOI 10.1109/TVCG.2020.3030471
   Wang ZJ, 2021, IEEE T VIS COMPUT GR, V27, P1396, DOI 10.1109/TVCG.2020.3030418
   Weng D, 2021, IEEE T VIS COMPUT GR, V27, P817, DOI 10.1109/TVCG.2020.3030458
   Weng D, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173821
   Wexler J, 2020, IEEE T VIS COMPUT GR, V26, P56, DOI 10.1109/TVCG.2019.2934619
   Xu C., 2020, Google Research
   Xu YF, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4199
   Xu Z, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P905, DOI 10.1145/3219819.3219824
   Yan JN, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376447
   Yoo CY, 2009, JOURNALISM MASS COMM, V86, P401, DOI 10.1177/107769900908600209
   Yu H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6575
   Zeng W, 2019, COMPUT GRAPH FORUM, V38, P581, DOI 10.1111/cgf.13712
   Zeng W, 2018, IEEE COMPUT GRAPH, V38, P38, DOI 10.1109/MCG.2018.053491730
   Zhang Y., 2017, CityMatrix - An Urban Decision Support System Augmented by Artificial Intelligence
   Zhu Y, 2014, TRANSPORT RES REC, P168, DOI 10.3141/2429-18
NR 110
TC 2
Z9 2
U1 12
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3749
EP 3766
DI 10.1109/TVCG.2023.3239909
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700038
PM 37022033
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wu, HY
   Sun, XX
   Tu, HW
   Zhang, XL
AF Wu, Huiyue
   Sun, Xiaoxuan
   Tu, Huawei
   Zhang, Xiaolong
TI ClockRay: A Wrist-Rotation Based Technique for Occluded-Target Selection
   in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Three-dimensional displays; Wrist; Visualization;
   Input devices; Virtual reality; Usability; object selection; RayCasting;
   disambiguation; wrist rotation; 3D data visualization
ID 3D SELECTION; DESIGN
AB Target selection is one of essential operation made available by interaction techniques in virtual reality (VR) environments. However, effectively positioning or selecting occluded objects is under-investigated in VR, especially in the context of high-density or a high-dimensional data visualization with VR. In this paper, we propose ClockRay, an occluded-object selection technique that can maximize the intrinsic human wrist rotation skills through the integration of emerging ray selection techniques in VR environments. We describe the design space of the ClockRay technique and then evaluate its performance in a series of user studies. Drawing on the experimental results, we discuss the benefits of ClockRay compared to two popular ray selection techniques - RayCursor and RayCasting. Our findings can inform the design of VR-based interactive visualization systems for high-density data.
C1 [Wu, Huiyue] Sun Yat Sen Univ, Sch Journalism & Commun, Guangzhou 510275, Guangdong, Peoples R China.
   [Wu, Huiyue] Guangdong Key Lab Big Data Anal & Simulat Publ Opi, Guangzhou 510275, Guangdong, Peoples R China.
   [Sun, Xiaoxuan] Sun Yat Sen Univ, Guangzhou 510275, Guangdong, Peoples R China.
   [Tu, Huawei] La Trobe Univ, Melbourne, Vic 3086, Australia.
   [Zhang, Xiaolong] Penn State Univ, University Pk, PA 16802 USA.
C3 Sun Yat Sen University; Sun Yat Sen University; La Trobe University;
   Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park
RP Wu, HY (corresponding author), Sun Yat Sen Univ, Sch Journalism & Commun, Guangzhou 510275, Guangdong, Peoples R China.; Wu, HY (corresponding author), Guangdong Key Lab Big Data Anal & Simulat Publ Opi, Guangzhou 510275, Guangdong, Peoples R China.
EM wuhuiyue@mail.sysu.edu.cn; quantalion@163.com; h.tu@latrobe.edu.au;
   lzhang@ist.psu.edu
RI Sun, Xiaoxuan/HOH-1184-2023; Zhang, Xiaolong/HMD-9038-2023
OI Wu, Huiyue/0000-0001-7027-518X
FU National Natural Science Foundation of China [62272500, 61772564];
   Guangdong Basic and Applied Basic Research Foundation [2021A1515011990];
   Guangdong Key Laboratory for Big Data Analysis and Simulation of Public
   Opinion [2017B030301003]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62272500 and 61772564, in part by the Guangdong Basic
   and Applied Basic Research Foundation under Grant 2021A1515011990, and
   in part by the Guangdong Key Laboratory for Big Data Analysis and
   Simulation of Public Opinion under Grant 2017B030301003.
CR Bacim F, 2013, INT J HUM-COMPUT ST, V71, P785, DOI 10.1016/j.ijhcs.2013.03.003
   Balogun MB, 2019, AFRICON, DOI [10.1109/africon46755.2019.9133906, 10.1145/3290605.3300331]
   Bowman D. A., 2002, Virtual Reality, V6, P122, DOI 10.1007/s100550200013
   Bowman DA, 2001, P IEEE VIRT REAL ANN, P149, DOI 10.1109/VR.2001.913781
   Bowman DA, 2007, COMPUTER, V40, P36, DOI 10.1109/MC.2007.257
   Cashion J, 2012, IEEE T VIS COMPUT GR, V18, P634, DOI 10.1109/TVCG.2012.40
   Casiez G., 2012, P SIGCHI C HUM FACT, P2527, DOI [10.1145/2207676.22086391,2,5, DOI 10.1145/2207676.22086391,2,5]
   Crossan A., 2008, P C HUM COMP INT MOB, P435
   de Haan Gerwin., 2005, Proceedings of the 11th Eurographics conference on Virtual Environments EGVE'05, P201, DOI [10.2312/EGVE/IPT_EGVE2005/201-209, DOI 10.2312/EGVE/IPT_EGVE2005/201-209]
   Debarba HG, 2013, LECT NOTES COMPUT SC, V8119, P388
   Donalek C, 2014, IEEE INT CONF BIG DA, P609, DOI 10.1109/BigData.2014.7004282
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Feiner A.O.S., 2003, P UIST, P81
   Grandjean E., 1969, Fitting the Task to the Man: An Ergonomic Approach
   Grossman Tovi, 2006, P 19 ANN ACM S US IN, P3, DOI [10.1145/1166253.1166257, DOI 10.1145/1166253.1166257]
   Guo AH, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI'16), P17, DOI 10.1145/2935334.2935345
   Haque F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3653, DOI 10.1145/2702123.2702133
   LaViola Jr J. J., 2017, 3D user interfaces: theory and practice
   Lilija K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300676
   Lu YQ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P35, DOI [10.1109/VR46266.2020.1581165829725, 10.1109/VR46266.2020.00-83]
   Lyu F, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-2999-y
   Man-Systems Integration Standards, 2015, NASA-STD-3000
   Mendes D, 2017, COMPUT GRAPH-UK, V67, P95, DOI 10.1016/j.cag.2017.06.003
   Mendes D, 2017, IEEE SYMP 3D USER, P237, DOI 10.1109/3DUI.2017.7893359
   Millais P, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188537
   Ni T, 2011, INT J HUM-COMPUT ST, V69, P551, DOI 10.1016/j.ijhcs.2011.05.001
   Oakley I, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P40
   Poupyrev M., 1996, P 9 ANN ACM S USER I, P79, DOI DOI 10.1145/237091.237102
   Rahman M, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1943
   Ramos G., 2004, Proceedings of the CHI '04 Conference on Human Factors in Computing Systems, P487
   Rekimoto J., 1996, Proc. UIST, P167
   Ro H, 2017, IEEE SYS MAN CYBERN, P2873, DOI 10.1109/SMC.2017.8123063
   Shi K, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1295
   Vanacken L, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P115
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Yeo HS, 2019, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI'19), DOI 10.1145/3338286.3340130
   Yu DF, 2020, IEEE T VIS COMPUT GR, V26, P3402, DOI [10.1109/TVCG.2020.3023606, 10.1109/TCVG.2020.3023606]
   Yu LY, 2012, IEEE T VIS COMPUT GR, V18, P2245, DOI 10.1109/TVCG.2012.217
NR 38
TC 0
Z9 0
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3767
EP 3778
DI 10.1109/TVCG.2023.3239951
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700024
PM 37022075
DA 2024-11-06
ER

PT J
AU Shao, ZK
   Sun, SR
   Zhao, YH
   Wang, SY
   Wei, ZY
   Gui, T
   Turkay, C
   Chen, SM
AF Shao, Zekai
   Sun, Shuran
   Zhao, Yuheng
   Wang, Siyuan
   Wei, Zhongyu
   Gui, Tao
   Turkay, Cagatay
   Chen, Siming
TI Visual Explanation for Open-Domain Question Answering With BERT
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical models; Transformers; Task analysis; Data models; Visual
   analytics; Bit error rate; Semantics; Explainable machine learning;
   open-domain question answering; visual analytics
ID ANALYTICS
AB Open-domain question answering (OpenQA) is an essential but challenging task in natural language processing that aims to answer questions in natural language formats on the basis of large-scale unstructured passages. Recent research has taken the performance of benchmark datasets to new heights, especially when these datasets are combined with techniques for machine reading comprehension based on Transformer models. However, as identified through our ongoing collaboration with domain experts and our review of literature, three key challenges limit their further improvement: (i) complex data with multiple long texts, (ii) complex model architecture with multiple modules, and (iii) semantically complex decision process. In this paper, we present VEQA, a visual analytics system that helps experts understand the decision reasons of OpenQA and provides insights into model improvement. The system summarizes the data flow within and between modules in the OpenQA model as the decision process takes place at the summary, instance and candidate levels. Specifically, it guides users through a summary visualization of dataset and module response to explore individual instances with a ranking visualization that incorporates context. Furthermore, VEQA supports fine-grained exploration of the decision flow within a single module through a comparative tree visualization. We demonstrate the effectiveness of VEQA in promoting interpretability and providing insights into model enhancement through a case study and expert evaluation.
C1 [Shao, Zekai; Sun, Shuran; Zhao, Yuheng; Wang, Siyuan; Wei, Zhongyu; Gui, Tao; Chen, Siming] Fudan Univ, Shanghai 200437, Peoples R China.
   [Turkay, Cagatay] Univ Warwick, Coventry CV4 7AL, England.
C3 Fudan University; University of Warwick
RP Chen, SM (corresponding author), Fudan Univ, Shanghai 200437, Peoples R China.
EM zkshao19@fudan.edu.cn; srsun20@fudan.edu.cn; yuhengzhao@fudan.edu.cn;
   wangsy18@fudan.edu.cn; zywei@fudan.edu.cn; tgui@fudan.edu.cn;
   cagatay.turkay@warwick.ac.uk; simingchen3@gmail.com
RI Turkay, Cagatay/AAA-3810-2020; Chen, Siming/AAK-1874-2020
OI Zhao, Yuheng/0000-0003-1573-8772; Wei, Zhongyu/0000-0003-3789-8507;
   Turkay, Cagatay/0000-0001-6788-251X; Shao, Zekai/0000-0003-2014-5293;
   Sun, Shuran/0000-0003-2297-5602; Wang, Siyuan/0000-0001-7357-2913
FU National Natural Science Foundation of China [62202105]; Shanghai
   Municipal Science and Technology Major [2018SHZDZX01, 2021SHZDZX0103];
   General Program [21ZR1403300]; Sailing Program [21YF1402900]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 62202105, in part by Shanghai Municipal Science and
   Technology Major Project under Grants 2018SHZDZX01, and 2021SHZDZX0103,
   in part by General Program under Grant 21ZR1403300,in part by Sailing
   Program under Grant 21YF1402900, and in part by ZJLab.
CR Abadi M., 2015, Tensorflow: Large-scale machine learning on heterogeneous distributed systems, DOI DOI 10.48550/ARXIV.1603.04467
   Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   Agrawal A, 2017, INT J COMPUT VISION, V123, P4, DOI 10.1007/s11263-016-0966-6
   Alicioglu G, 2022, COMPUT GRAPH-UK, V102, P502, DOI 10.1016/j.cag.2021.09.002
   Asai A, 2020, Arxiv, DOI arXiv:1911.10470
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Caballero M., 2021, Int. J. Artif. Intell. Appl., V12, P1
   Cheng FR, 2022, IEEE T VIS COMPUT GR, V28, P378, DOI 10.1109/TVCG.2021.3114836
   Cheng FR, 2021, IEEE T VIS COMPUT GR, V27, P1438, DOI 10.1109/TVCG.2020.3030342
   Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
   Chung JY, 2017, Arxiv, DOI arXiv:1609.01704
   Clark K, 2019, BLACKBOXNLP WORKSHOP ON ANALYZING AND INTERPRETING NEURAL NETWORKS FOR NLP AT ACL 2019, P276, DOI 10.18653/v1/w19-4828
   Collins Christopher, 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P91, DOI 10.1109/VAST.2009.5333443
   Denil M., 2014, arXiv
   DeRose JF, 2021, IEEE T VIS COMPUT GR, V27, P1160, DOI 10.1109/TVCG.2020.3028976
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dhamdhere K, 2018, Arxiv, DOI arXiv:1805.12233
   Drozdov A, 2019, Arxiv, DOI arXiv:1904.02142
   Falconer J., 2011, Google: Our new search strategy is to compute answers, not links
   Fujiwara T., 2022, J. Data Sci. Statist. Visualisation, V2, P1
   Fujiwara T, 2022, IEEE T VIS COMPUT GR, V28, P758, DOI 10.1109/TVCG.2021.3114807
   Gratzl S, 2013, IEEE T VIS COMPUT GR, V19, P2277, DOI 10.1109/TVCG.2013.173
   Gutmann M., 2010, JMLR W CP, P297
   Hao YR, 2021, Arxiv, DOI arXiv:2004.11207
   Herman I, 2000, IEEE T VIS COMPUT GR, V6, P24, DOI 10.1109/2945.841119
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P498, DOI 10.1037/h0070888
   Hou YJ, 2022, IEEE T VIS COMPUT GR, V28, P1030, DOI 10.1109/TVCG.2021.3114777
   Jain S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P3543
   Jaunet T, 2022, IEEE T VIS COMPUT GR, V28, P976, DOI 10.1109/TVCG.2021.3114683
   Joshua Robinson, 2020, arXiv
   Kahng M, 2019, IEEE T VIS COMPUT GR, V25, P310, DOI 10.1109/TVCG.2018.2864500
   Karpukhin V, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6769
   Khattab O, 2021, T ASSOC COMPUT LING, V9, P929, DOI 10.1162/tacl_a_00405
   Kitaev N, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3499
   Kobayashi G, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7057
   Kovaleva O., 1908, arXiv
   Kwiatkowski T, 2019, T ASSOC COMPUT LING, V7, P453, DOI 10.1162/tacl_a_00276/1923288
   Lal V, 2021, EACL 2021: THE 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: PROCEEDINGS OF THE SYSTEM DEMONSTRATIONS, P135
   Lee G, 2019, AAAI CONF ARTIF INTE, P9861
   Lee J, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3661
   Lee K, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6086
   Lewis M, 2019, Arxiv, DOI arXiv:1910.13461
   Li J, 2016, Visualizing and Understanding Neural Models in NLP Association for Computational Linguistics, DOI DOI 10.18653/V1/N16-1082
   Li R, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P220
   Li Z, 2022, IEEE T VIS COMPUT GR, V28, P4980, DOI 10.1109/TVCG.2022.3184186
   Liu MC, 2018, IEEE T VIS COMPUT GR, V24, P77, DOI 10.1109/TVCG.2017.2744938
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Liu SS, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P36, DOI 10.13697/j.cnki.32-1449/tu.2018.01.013
   Liu Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P188
   Lundberg SM, 2017, ADV NEUR IN, V30
   Mao YN, 2021, Arxiv, DOI arXiv:2009.08553
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Ming Y, 2017, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2017.8585721
   Ming Y, 2019, IEEE T VIS COMPUT GR, V25, P342, DOI 10.1109/TVCG.2018.2864812
   Nie YX, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2553
   Ono JP, 2021, IEEE T VIS COMPUT GR, V27, P390, DOI 10.1109/TVCG.2020.3030361
   OPENAI, 2022, ChatGPT: Optimizing Language Models for Dialogue
   Pascual D, 2021, Arxiv, DOI arXiv:2004.05916
   Qu YQ, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5835
   Ramnath S, 2020, Arxiv, DOI arXiv:2010.08983
   Ren RY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P2825
   Ren RY, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P2173
   Rücklé A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P19, DOI 10.18653/v1/P17-4004
   Sachan D. S., 2021, P ASS COMP LING 11 I, P6648, DOI 10.18653/v1/2021.acl-long.519
   Schulz HJ, 2011, IEEE T VIS COMPUT GR, V17, P393, DOI 10.1109/TVCG.2010.79
   Shen YK, 2019, Arxiv, DOI arXiv:1810.09536
   Shrikumar A, 2018, Arxiv, DOI arXiv:1807.09946
   Silva S, 2011, COMPUT GRAPH-UK, V35, P320, DOI 10.1016/j.cag.2010.11.015
   Skrlj B, 2020, Arxiv, DOI arXiv:2005.05716
   Spinner T, 2020, IEEE T VIS COMPUT GR, V26, P1064, DOI 10.1109/TVCG.2019.2934629
   Strobelt H, 2019, IEEE T VIS COMPUT GR, V25, P353, DOI 10.1109/TVCG.2018.2865044
   Sun HT, 2019, Arxiv, DOI arXiv:1904.09537
   Sun HT, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4231
   Sun K, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P4126
   Sundararajan M, 2017, Arxiv, DOI [arXiv:1703.01365, 10.48550/ARXIV.1703.01365]
   Tenney I, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P107
   van Aken B, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1823, DOI 10.1145/3357384.3358028
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Vig J, 2019, PROCEEDINGS OF THE 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, (ACL 2019), P37
   Wang JH, 2023, IEEE T VIS COMPUT GR, V29, P5033, DOI 10.1109/TVCG.2022.3201101
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P288, DOI 10.1109/TVCG.2018.2864504
   Wang SH, 2018, AAAI CONF ARTIF INTE, P5981
   Wang XB, 2022, IEEE T VIS COMPUT GR, V28, P802, DOI 10.1109/TVCG.2021.3114794
   Wang ZJ, 2021, Arxiv, DOI arXiv:2103.14625
   Wang ZJ, 2021, IEEE T VIS COMPUT GR, V27, P1396, DOI 10.1109/TVCG.2020.3030418
   Wiegreffe S, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P11
   Yang W, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P72
   Yasunaga M, 2021, Arxiv, DOI arXiv:2104.06378
   Ye X, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P5496
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Zhang D, 2021, AAAI CONF ARTIF INTE, V35, P14328
   Zhu FB, 2021, Arxiv, DOI arXiv:2101.00774
NR 94
TC 1
Z9 1
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3779
EP 3797
DI 10.1109/TVCG.2023.3243676
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700093
PM 37027746
DA 2024-11-06
ER

PT J
AU Zeng, W
   Chen, X
   Hou, YH
   Shao, LD
   Chu, Z
   Chang, R
AF Zeng, Wei
   Chen, Xi
   Hou, Yihan
   Shao, Lingdan
   Chu, Zhe
   Chang, Remco
TI Semi-Automatic Layout Adaptation for Responsive Multiple-View
   Visualization Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Layout; Mobile handsets; Bars;
   Stacking; Design methodology; Multiple-view visualization; responsive
   design; layout adaptation; mobile devices
ID PATTERNS
AB Multiple-view (MV) visualizations have become ubiquitous for visual communication and exploratory data visualization. However, most existing MV visualizations are designed for the desktop, which can be unsuitable for the continuously evolving displays of varying screen sizes. In this article, we present a two-stage adaptation framework that supports the automated retargeting and semi-automated tailoring of a desktop MV visualization for rendering on devices with displays of varying sizes. First, we cast layout retargeting as an optimization problem and propose a simulated annealing technique that can automatically preserve the layout of multiple views. Second, we enable fine-tuning for the visual appearance of each view, using a rule-based auto configuration method complemented with an interactive interface for chart-oriented encoding adjustment. To demonstrate the feasibility and expressivity of our proposed approach, we present a gallery of MV visualizations that have been adapted from the desktop to small displays. We also report the result of a user study comparing visualizations generated using our approach with those by existing methods. The outcome indicates that the participants generally prefer visualizations generated using our approach and find them to be easier to use.
C1 [Zeng, Wei; Hou, Yihan] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou 511400, Peoples R China.
   [Zeng, Wei; Hou, Yihan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Chen, Xi; Shao, Lingdan; Chu, Zhe] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing 100045, Peoples R China.
   [Chen, Xi; Shao, Lingdan; Chu, Zhe] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Chang, Remco] Tufts Univ, Medford, MA 02155 USA.
C3 Hong Kong University of Science & Technology (Guangzhou); Hong Kong
   University of Science & Technology; Chinese Academy of Sciences;
   Shenzhen Institute of Advanced Technology, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Tufts
   University
RP Zeng, W (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Guangzhou 511400, Peoples R China.
EM weizeng@hkust-gz.edu.cn; xi.chen2@siat.ac.cn;
   yhou073@connect.hkust-gz.edu.cn; ld.shao@siat.ac.cn; zhe.chu@siat.ac.cn;
   remco@cs.tufts.edu
OI Hou, Yihan/0000-0002-1459-8766; Zeng, Wei/0000-0002-5600-8824; Chang,
   Remco/0000-0002-6484-6430
FU National Natural Science Foundation of China [62172398]; National
   Science Foundation [OAC-1940175, OAC-1939945, OAC-2118201, IIS-1452977]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172398 and in part by the National
   Science Foundation under Grants OAC-1940175, OAC-1939945, OAC-2118201,
   and IIS-1452977.
CR Agrawala M, 2001, COMP GRAPH, P241, DOI 10.1145/383259.383286
   Andrews K., 2017, EUROVIS 2017 EUR ASS, DOI DOI 10.2312/EURP.20171182
   Andrews K., 2018, P MOB VIS WORKSH CHI
   Badam SK, 2021, INFORM VISUAL, V20, P229, DOI 10.1177/14738716211038614
   Blascheck T, 2019, IEEE T VIS COMPUT GR, V25, P630, DOI 10.1109/TVCG.2018.2865142
   Brehmer M, 2020, IEEE T VIS COMPUT GR, V26, P364, DOI 10.1109/TVCG.2019.2934397
   Brehmer M, 2019, IEEE T VIS COMPUT GR, V25, P619, DOI 10.1109/TVCG.2018.2865234
   Chen R, 2022, IEEE T VIS COMPUT GR, V28, P4127, DOI 10.1109/TVCG.2021.3076222
   Chen X, 2021, IEEE T VIS COMPUT GR, V27, P1514, DOI 10.1109/TVCG.2020.3030338
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chung HY, 2015, IEEE CONF VIS ANAL, P33, DOI 10.1109/VAST.2015.7347628
   Drucker S.M., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '13, P2301
   Eisensten Jacob., 2001, IUI 01 P 6 INT C INT, P69
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   esri, ArcGIS Dashboards
   Filonik Daniel, 2013, Persuasive Technology. 8th International Conference, PERSUASIVE 2013. Proceedings, P51, DOI 10.1007/978-3-642-37157-8_8
   Gajos K., 2004, P INT C INT US INT, P93
   Gleicher M, 2018, IEEE T VIS COMPUT GR, V24, P413, DOI 10.1109/TVCG.2017.2744199
   Heer J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1303
   Hinderman B., 2015, Building Resp.nsive Data Visualization for the Web
   Hoffswell J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376777
   Horak S. K., 2018, P CHI C HUM FACT COM, P1
   Horak T, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300846
   Jacobs C, 2003, ACM T GRAPHIC, V22, P838, DOI 10.1145/882262.882353
   Javed W, 2012, IEEE PAC VIS SYMP, P1, DOI 10.1109/PacificVis.2012.6183556
   Jo J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2660, DOI 10.1145/3025453.3025752
   Johns Hopkins Coronavirus Resource Center, COVID-19 dashboard
   Kay M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5092, DOI 10.1145/2858036.2858558
   Kieffer S, 2016, IEEE T VIS COMPUT GR, V22, P349, DOI 10.1109/TVCG.2015.2467451
   Kim H, 2022, IEEE T VIS COMPUT GR, V28, P129, DOI 10.1109/TVCG.2021.3114782
   Kim H, 2021, COMPUT GRAPH FORUM, V40, P459, DOI 10.1111/cgf.14321
   Kulkarni Chinmay., 2011, CHI, V2011, P1573, DOI DOI 10.1145/1979742.1979810
   Langner R, 2019, IEEE T VIS COMPUT GR, V25, P608, DOI 10.1109/TVCG.2018.2865235
   Langner R, 2018, IEEE T VIS COMPUT GR, V24, P626, DOI 10.1109/TVCG.2017.2744019
   Lu M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376263
   Ma RX, 2021, IEEE T VIS COMPUT GR, V27, P3717, DOI 10.1109/TVCG.2020.2980227
   Matkovic K, 2008, IEEE INT CONF INF VI, P215, DOI 10.1109/IV.2008.87
   microsoft, Power BI
   North C., 2000, Proceedings of the the working conference on Advanced visual interfaces (AVI) 2000, P128, DOI DOI 10.1145/345513.345282
   O'Donovan P, 2014, IEEE T VIS COMPUT GR, V20, P1200, DOI 10.1109/TVCG.2014.48
   Park S, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173758
   Qian CY, 2021, IEEE T VIS COMPUT GR, V27, P443, DOI 10.1109/TVCG.2020.3030448
   Qu ZN, 2018, IEEE T VIS COMPUT GR, V24, P468, DOI 10.1109/TVCG.2017.2744198
   Roberts JC, 2007, CMV 2007: FIFTH INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P61, DOI 10.1109/CMV.2007.20
   Roberts JC, 2014, IEEE COMPUT GRAPH, V34, P26, DOI 10.1109/MCG.2014.82
   Sadana R, 2016, COMPUT GRAPH FORUM, V35, P261, DOI 10.1111/cgf.12902
   Sadana R, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, AVI 2014, P265, DOI 10.1145/2598153.2598163
   Sarikaya A, 2019, IEEE T VIS COMPUT GR, V25, P682, DOI 10.1109/TVCG.2018.2864903
   Shao LD, 2021, J VISUAL-JAPAN, V24, P1237, DOI 10.1007/s12650-021-00781-z
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Spotfire, about us
   tableau, ABOUT US
   Wang Baldonado M. Q., 2000, P WORK C ADV VIS INT, P110, DOI [DOI 10.1145/345513.345271, 10/cqq2cj]
   Wang SD, 2023, IEEE T VIS COMPUT GR, V29, P1610, DOI 10.1109/TVCG.2021.3126478
   Weaver C, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P159, DOI 10.1109/INFVIS.2004.12
   Weaver C, 2010, IEEE T VIS COMPUT GR, V16, P192, DOI 10.1109/TVCG.2009.94
   Wong B, 2011, NAT METHODS, V8, P5, DOI 10.1038/nmeth0111-5
   Wu AY, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445179
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Wu AY, 2021, IEEE T VIS COMPUT GR, V27, P464, DOI 10.1109/TVCG.2020.3030423
   Wu YC, 2013, IEEE T VIS COMPUT GR, V19, P278, DOI 10.1109/TVCG.2012.114
   Yigitbasioglu Ogan M., 2012, International Journal of Accounting Information Systems, V13, P41, DOI 10.1016/j.accinf.2011.08.002
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
NR 64
TC 0
Z9 1
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3798
EP 3812
DI 10.1109/TVCG.2023.3240356
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700054
PM 37022242
DA 2024-11-06
ER

PT J
AU Feng, YCJ
   Wang, XB
   Pan, B
   Wong, KK
   Ren, Y
   Liu, S
   Yan, ZH
   Ma, YX
   Qu, HM
   Chen, W
AF Feng, Yingchaojie
   Wang, Xingbo
   Pan, Bo
   Wong, Kam Kwai
   Ren, Yi
   Liu, Shi
   Yan, Zihan
   Ma, Yuxin
   Qu, Huamin
   Chen, Wei
TI XNLI: Explaining and Diagnosing NLI-Based Visual Data Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Task analysis; Encoding; Data
   analysis; Motion pictures; Prototypes; Natural language interface;
   visual data analysis; explainability
ID MULTIMODAL INTERACTION; NATURAL-LANGUAGE; VISUALIZATION; EXPLORATION;
   FRAMEWORK
AB Natural language interfaces (NLIs) enable users to flexibly specify analytical intentions in data visualization. However, diagnosing the visualization results without understanding the underlying generation process is challenging. Our research explores how to provide explanations for NLIs to help users locate the problems and further revise the queries. We present XNLI, an explainable NLI system for visual data analysis. The system introduces a Provenance Generator to reveal the detailed process of visual transformations, a suite of interactive widgets to support error adjustments, and a Hint Generator to provide query revision hints based on the analysis of user queries and interactions. Two usage scenarios of XNLI and a user study verify the effectiveness and usability of the system. Results suggest that XNLI can significantly enhance task accuracy without interrupting the NLI-based analysis process.
C1 [Feng, Yingchaojie; Pan, Bo; Ren, Yi; Liu, Shi; Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wang, Xingbo; Wong, Kam Kwai] Hong Kong Univ Sci & Technol, Hong Kong 999077, Peoples R China.
   [Yan, Zihan] MIT, Media Lab, Cambridge, MA 02139 USA.
   [Ma, Yuxin] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen 518055, Guangdong, Peoples R China.
   [Chen, Wei] Zhejiang Univ, Lab Art & Archaeol Image, Minist Educ, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Hong Kong University of Science & Technology;
   Massachusetts Institute of Technology (MIT); Southern University of
   Science & Technology; Zhejiang University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM fycj@zju.edu.cn; xwangeg@cse.ust.hk; bopan@zju.edu.cn;
   kkwongar@cse.ust.hk; rymaster@zju.edu.cn; zju_ls@zju.edu.cn;
   yzihan@media.mit.edu; mayx@sustech.edu.cn; huamin@cse.ust.hk;
   chenvis@zju.edu.cn
RI yan, zihan/KVY-5472-2024; Wang, Xingbo/JHS-6567-2023; Ma,
   Yuxin/AAJ-4486-2020; Zhang, Huanhuan/LDG-2528-2024; Ouyang,
   Fan/GQI-3203-2022; Chen, Wei/AAR-9817-2020
OI Pan, Bo/0009-0009-4561-2469; Feng, Yingchaojie/0000-0002-1418-4635;
   Chen, Wei/0000-0002-8365-4741; WONG, Kam Kwai/0000-0002-2813-1972; Wang,
   Xingbo/0000-0001-5693-1128
FU National Natural Science Foundation of China [62132017]; Fundamental
   Research Funds for the Central Universities [226-2022-00235]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62132017, and in part by the Fundamental Research
   Funds for the Central Universities under Grant 226-2022-00235.
CR Berant J, 2019, PROC INT CONF DATA, P1570, DOI 10.1109/ICDE.2019.00144
   Bors C, 2019, IEEE COMPUT GRAPH, V39, P61, DOI 10.1109/MCG.2019.2941856
   Brown TB, 2020, ADV NEUR IN, V33
   Card SK., 1999, READINGS INFORM VISU
   Chen R, 2022, IEEE T VIS COMPUT GR, V28, P4127, DOI 10.1109/TVCG.2021.3076222
   Chen Z., 2022, P ACM SIGCHI C HUM F, P15
   Chen Zhutian, 2023, IEEE Trans Vis Comput Graph, V29, P918, DOI 10.1109/TVCG.2022.3209497
   Cox K., 2001, International Journal of Speech Technology, V4, P297, DOI 10.1023/A:1011368926479
   Dhamdhere K, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P493, DOI 10.1145/3025171.3025227
   Fast B., 2018, P ACM SIGCHI C HUM F, P1
   Fu SW, 2020, Arxiv, DOI arXiv:2005.03257
   Gaba Aimen, 2023, IEEE Trans Vis Comput Graph, V29, P1211, DOI 10.1109/TVCG.2022.3209456
   Gao T, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P489, DOI 10.1145/2807442.2807478
   Heer J, 2008, IEEE T VIS COMPUT GR, V14, P1189, DOI 10.1109/TVCG.2008.137
   Hoque E, 2018, IEEE T VIS COMPUT GR, V24, P309, DOI 10.1109/TVCG.2017.2744684
   Huang Jieying, 2023, IEEE Trans Vis Comput Graph, V29, P1200, DOI 10.1109/TVCG.2022.3209453
   Islam MR, 2024, ANN OPER RES, V339, P1569, DOI 10.1007/s10479-021-04465-7
   Jiang E, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501870
   Kasica S, 2021, IEEE T VIS COMPUT GR, V27, P957, DOI 10.1109/TVCG.2020.3030462
   Khan M, 2017, PROC VLDB ENDOW, V10, P661, DOI 10.14778/3055330.3055333
   Kumar A., 2016, P 17 ANN M SPECIAL I, P304
   Levenshtein V.I., 1966, Soviet Physics Doklady
   Leventidis A, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P2303, DOI 10.1145/3318464.3389767
   Liu C, 2021, IEEE PAC VIS SYMP, P11, DOI 10.1109/PacificVis52677.2021.00010
   Luo YY, 2021, Arxiv, DOI arXiv:2112.12926
   Luo YY, 2021, INT CONF MANAGE DATA, P1235, DOI 10.1145/3448016.3457261
   Luo YY, 2022, IEEE T VIS COMPUT GR, V28, P217, DOI 10.1109/TVCG.2021.3114848
   Mitra R, 2022, Arxiv, DOI arXiv:2207.00189
   Narechania A, 2021, IUI '21 - 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P597, DOI 10.1145/3397481.3450667
   Narechania A, 2021, IEEE T VIS COMPUT GR, V27, P369, DOI 10.1109/TVCG.2020.3030378
   powerbi.microsoft.com, Microsoft power BI
   Pu S., 2021, P ACM CHI C HUM FACT, P1
   Ragan ED, 2016, IEEE T VIS COMPUT GR, V22, P31, DOI 10.1109/TVCG.2015.2467551
   Saktheeswaran A, 2020, IEEE T VIS COMPUT GR, V26, P2168, DOI 10.1109/TVCG.2020.2970512
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Setlur Vidya, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P966, DOI 10.1145/3379337.3415813
   Setlur V, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P166, DOI [10.1109/VIS49827.2021.00041, 10.1109/VIS49827.2021.9623324]
   Setlur V, 2019, PROCEEDINGS OF IUI 2019, P40, DOI 10.1145/3301275.3302270
   Setlur V, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P365, DOI 10.1145/2984511.2984588
   Shen L., 2021, arXiv
   Shrinivasan YB, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1237
   Srinivasan Arjun, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P864, DOI 10.1145/3472749.3474792
   Srinivasan A, 2020, IEEE COMPUT GRAPH, V40, P96, DOI 10.1109/MCG.2020.2986902
   Srinivasan A, 2021, IEEE T VIS COMPUT GR, V27, P3519, DOI 10.1109/TVCG.2020.2978050
   Srinivasan A, 2019, PROCEEDINGS OF IUI 2019, P661, DOI 10.1145/3301275.3302292
   Srinivasan A, 2018, IEEE T VIS COMPUT GR, V24, P511, DOI 10.1109/TVCG.2017.2745219
   Srinivasan Arjun, 2017, P EUROVIS, P55, DOI [10.2312/ eurovisshort.20171133, DOI 10.2312/EUROVISSHORT.20171133]
   Srinivasan B., 2020, P ACM SIGCHI C HUM F, P1
   Srinivasan N., 2021, P ACM SIGCHI C HUM F, P1
   Tory M, 2019, IEEE CONF VIS ANAL, P93, DOI [10.1109/VAST47406.2019.8986918, 10.1109/vast47406.2019.8986918]
   Wambsganss T, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376732
   Wang XB, 2022, Arxiv, DOI arXiv:2201.04868
   Wang XB, 2022, IEEE T VIS COMPUT GR, V28, P4609, DOI 10.1109/TVCG.2021.3097709
   Wang Y, 2023, IEEE T VIS COMPUT GR, V29, P1222, DOI 10.1109/TVCG.2022.3209357
   Wu Jiang, 2023, IEEE Trans Vis Comput Graph, V29, P940, DOI 10.1109/TVCG.2022.3209452
   Wu TS, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517582
   Xia M, 2022, Arxiv, DOI arXiv:2204.07741
   Xiong K, 2023, IEEE T VIS COMPUT GR, V29, P2950, DOI 10.1109/TVCG.2022.3144975
   Yu BW, 2020, IEEE T VIS COMPUT GR, V26, P1, DOI 10.1109/TVCG.2019.2934668
NR 59
TC 10
Z9 12
U1 11
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3813
EP 3827
DI 10.1109/TVCG.2023.3240003
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700084
PM 37610216
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Batch, A
   Ji, YP
   Fan, MM
   Zhao, J
   Elmqvist, N
AF Batch, Andrea
   Ji, Yipeng
   Fan, Mingming
   Zhao, Jian
   Elmqvist, Niklas
TI uxSense: Supporting User Experience Analysis with Visualization and
   Computer Vision
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Annotations; Data visualization; Feature extraction; Usability;
   Measurement; Gaze tracking; Visualization; visual analytics; evaluation;
   video analytics; machine learning; deep learning; computer vision
AB Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose uxSense, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.
C1 [Batch, Andrea; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Ji, Yipeng; Zhao, Jian] Univ Waterloo, Waterloo, ON N2L 3G1, Canada.
   [Fan, Mingming] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou 511453, Peoples R China.
   [Fan, Mingming] Hong Kong Univ Sci & Technol, Hong Kong 511453, Peoples R China.
C3 University System of Maryland; University of Maryland College Park;
   University of Waterloo; Hong Kong University of Science & Technology
   (Guangzhou); Hong Kong University of Science & Technology
RP Batch, A (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM ajulca@umd.edu; y43ji@edu.uwaterloo.ca; mingmingfan@ust.hk;
   jianzhao@edu.uwaterloo.ca; elm@umd.edu
RI Fan, Mingming/LMM-9437-2024
OI Fan, Mingming/0000-0002-0356-4712; Elmqvist, Niklas/0000-0001-5805-5301;
   Zhao, Jian/0000-0001-5008-4319
FU U.S. National Science Foundation [IIS-1908605]; Natural Sciences and
   Engineering Research Council of Canada
FX This work was supported in part by U.S. National Science Foundation
   under Grant IIS-1908605, and in part by the Natural Sciences and
   Engineering Research Council of Canada through the Discovery Grant
   program. Any opinions, findings, and conclusions or recommendations
   expressed here are those of the authors and do not necessarily reflect
   the views of the funding agencies.
CR Ahmed AAE, 2007, IEEE T DEPEND SECURE, V4, P165, DOI 10.1109/TDSC.2007.70207
   Truong A, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P497, DOI 10.1145/2984511.2984569
   Arriaga O, 2017, Arxiv, DOI arXiv:1710.07557
   Batch A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P1, DOI 10.1109/AIVR.2018.00009
   Batch N., 2019, P MACH LEARN US INT, P1
   Blascheck T, 2017, COMPUT GRAPH FORUM, V36, P260, DOI 10.1111/cgf.13079
   Blascheck T, 2016, IEEE T VIS COMPUT GR, V22, P61, DOI 10.1109/TVCG.2015.2467871
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chandrasegaran S, 2017, INT J HUM-COMPUT ST, V100, P66, DOI 10.1016/j.ijhcs.2016.12.007
   Franco RYD, 2019, INFORMATION, V10, DOI 10.3390/info10120366
   Dou WW, 2009, IEEE COMPUT GRAPH, V29, P52, DOI 10.1109/MCG.2009.49
   Fan MM, 2020, ACM T INTERACT INTEL, V10, DOI 10.1145/3385732
   Fan MM, 2020, J USABILITY STUD, V15, P85
   Fan MM, 2019, ACM T COMPUT-HUM INT, V26, DOI 10.1145/3325281
   Fan MM, 2020, IEEE T VIS COMPUT GR, V26, P343, DOI 10.1109/TVCG.2019.2934797
   Fan Q. Zhao, 2021, P ACM C HUM FACT COM, DOI [10.1145/3411764.344568065M.Bostock,V.Ogievetsky,andJ, DOI 10.1145/3411764.344568065M.BOSTOCK,V.OGIEVETSKY,ANDJ]
   Felix C, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P153, DOI 10.1145/3242587.3242596
   Gotz D, 2008, IEEE S VIS ANAL, P123, DOI 10.1109/VAST.2008.4677365
   Halter G, 2019, COMPUT GRAPH FORUM, V38, P119, DOI 10.1111/cgf.13676
   Harezlak K, 2014, C HUM SYST INTERACT, P95, DOI 10.1109/HSI.2014.6860455
   Heer J, 2008, IEEE T VIS COMPUT GR, V14, P1189, DOI 10.1109/TVCG.2008.137
   Higuch K, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P6536, DOI 10.1145/3025453.3025821
   Holtzblatt H., 2014, Contextual Design: Evolved
   Kerdvibulvech C, 2007, I C COMP GRAPH IM VI, P419, DOI 10.1109/CGIV.2007.88
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Kurzhals K, 2016, IEEE T MULTIMEDIA, V18, P2149, DOI 10.1109/TMM.2016.2614184
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Leake M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376519
   Lipford H. R., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P187, DOI 10.1109/VAST.2010.5653598
   Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493
   Marqués G, 2015, PROCEEDINGS IEEE/IFIP 13TH INTERNATIONAL CONFERENCE ON EMBEDDED AND UBIQUITOUS COMPUTING 2015, P132, DOI 10.1109/EUC.2015.25
   Matteson DS, 2014, J AM STAT ASSOC, V109, P334, DOI 10.1080/01621459.2013.849605
   Meng HY, 2016, IEEE T CYBERNETICS, V46, P916, DOI 10.1109/TCYB.2015.2418092
   Mingming Fan, 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3512943
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Munim I., 2017, P IEEE INT C EL INF, P1, DOI [10.1109/EICT.2017.827522752R.Y.da, DOI 10.1109/EICT.2017.827522752R.Y.DA]
   Nunnally D., 2016, UX Research: Practical Techniques for De-signing Better Products, P62
   Pavel A, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P517, DOI 10.1145/2984511.2984552
   Pavel C., 2014, P ACM S US INT SOFTW, P573, DOI [DOI 10.1145/2642918.2647400, 10.1145/2642918.264740012A, DOI 10.1145/2642918.264740012A]
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Ramakant, 2015, IEEE INT ADV COMPUT, P639, DOI 10.1109/IADCC.2015.7154785
   Robinson C., 2006, P WORKSH VIS AN SPAT, P1
   Sauro J, 2012, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, P1
   Shi H, 2014, I C VIRTUAL REALITY, P350, DOI 10.1109/ICVRV.2014.6
   Silva N, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204546
   Sloetjes H, 2008, SIXTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, LREC 2008, P816
   Soure EJ, 2022, IEEE T VIS COMPUT GR, V28, P643, DOI 10.1109/TVCG.2021.3114822
   Staiano J., 2012, P DES INT SYST C NEW, DOI DOI 10.1145/2317956.2318068
   Suja P, 2015, INT CONF CONTEMP, P348, DOI 10.1109/IC3.2015.7346705
   Tan S., 2014, P ACM C INT ENT, P1, DOI [10.1145/2677758.267776551K.M., DOI 10.1145/2677758.267776551K.M]
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Tullis B Albert T., 2013, Measuring the User Experience: Collecting, Analyzing, and Presenting Usability Metrics
   van de Wolfshaar J, 2015, 2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P188, DOI 10.1109/SSCI.2015.37
   WEISER M, 1991, SCI AM, V265, P94, DOI 10.1038/scientificamerican0991-94
   Zhao J, 2018, IEEE T VIS COMPUT GR, V24, P340, DOI 10.1109/TVCG.2017.2745279
   Zhao J, 2017, IEEE T VIS COMPUT GR, V23, P261, DOI 10.1109/TVCG.2016.2598543
NR 57
TC 2
Z9 2
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3841
EP 3856
DI 10.1109/TVCG.2023.3241581
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700050
PM 37022364
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Song, XH
   Liu, C
   Zheng, YY
   Feng, ZL
   Li, LC
   Zhou, K
   Yu, X
AF Song, Xinhui
   Liu, Chen
   Zheng, Youyi
   Feng, Zunlei
   Li, Lincheng
   Zhou, Kun
   Yu, Xin
TI HairStyle Editing via Parametric Controllable Strokes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hair; Image color analysis; Shape; Stroke (medical condition); Rendering
   (computer graphics); Faces; Pipelines; Hairstyle editing; hairstyle
   transfer; parameterized hair strokes; stroke-controllable
ID IMAGE SYNTHESIS; CAPTURE
AB In this work, we propose a stroke-based hairstyle editing network, dubbed HairstyleNet, allowing users to conveniently change the hairstyles of an image in an interactive fashion. Different from previous works, we simplify the hairstyle editing process where users can manipulate local or entire hairstyles by adjusting the parameterized hair regions. Our HairstyleNet consists of two stages: a stroke parameterization stage and a stroke-to-hair generation stage. In the stroke parameterization stage, we first introduce parametric strokes to approximate the hair wisps, where the stroke shape is controlled by a quadratic B & eacute;zier curve and a thickness parameter. Since rendering strokes with thickness to an image is not differentiable, we opt to leverage a neural renderer to construct the mapping from stroke parameters to a stroke image. Thus, the stroke parameters can be directly estimated from hair regions in a differentiable way, enabling us to flexibly edit the hairstyles of input images. In the stroke-to-hair generation stage, we design a hairstyle refinement network that first encodes coarsely composed images of hair strokes, face, and background into latent representations and then generates high-fidelity face images with desirable new hairstyles from the latent codes. Extensive experiments demonstrate that our HairstyleNet achieves state-of-the-art performance and allows flexible hairstyle manipulation.
C1 [Song, Xinhui; Li, Lincheng] Fuxi AI Lab Netease Inc, Hangzhou 310052, Zhejiang, Peoples R China.
   [Zheng, Youyi; Feng, Zunlei; Zhou, Kun] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liu, Chen; Yu, Xin] Univ Queensland, Brisbane, Qld 4072, Australia.
C3 Zhejiang University; University of Queensland
RP Li, LC (corresponding author), Fuxi AI Lab Netease Inc, Hangzhou 310052, Zhejiang, Peoples R China.
EM songxinhui@corp.netease.com; yenanliu36@gmail.com;
   youyizheng@zju.edu.cn; zunleifeng@zju.edu.cn;
   lilincheng@corp.netease.com; kunzhou@acm.org; xin.yu@uts.edu.au
RI Hu, Xin/JEP-4891-2023; Zhou, Kun/G-1249-2010; li, lincheng/AAO-4355-2020
OI Feng, Zunlei/0000-0001-8640-8434; Yu, Xin/0000-0002-0269-5649; LIU,
   CHEN/0000-0003-3159-0034
FU ARC-Discovery [DP220100800]; ARC-DECRA [DE230100477]
FX This work was supported in part by the ARC-Discovery under Grant
   DP220100800 and in part by the ARC-DECRA under Grant DE230100477.
CR Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2020, PROC CVPR IEEE, P8293, DOI 10.1109/CVPR42600.2020.00832
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Anokhin I, 2020, PROC CVPR IEEE, P7485, DOI 10.1109/CVPR42600.2020.00751
   Bin H, 2017, Arxiv, DOI arXiv:1707.00737
   Brock A, 2019, Arxiv, DOI arXiv:1809.11096
   Chai ML, 2020, Arxiv, DOI arXiv:2004.13297
   Chai ML, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818112
   Chai ML, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461990
   Chai ML, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185612
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Chen S-Y, 2021, arXiv
   Chen YC, 2020, PROC CVPR IEEE, P5273, DOI 10.1109/CVPR42600.2020.00532
   Choi Y, 2020, PROC CVPR IEEE, P8185, DOI 10.1109/CVPR42600.2020.00821
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dong H., 2018, arXiv
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Guan SY, 2020, Arxiv, DOI arXiv:2007.01758
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hu LW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661254
   Hu LW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766931
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI arXiv:1710.10196
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D.P., 2014, P INT C LEARNING REP
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2016, ADV NEUR IN, V29
   Liwen Hu, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601194
   Luo LJ, 2013, PROC CVPR IEEE, P265, DOI 10.1109/CVPR.2013.41
   Luo LJ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462026
   Luo LJ, 2012, PROC CVPR IEEE, P1490, DOI 10.1109/CVPR.2012.6247838
   Olszewski K, 2020, PROC CVPR IEEE, P7444, DOI 10.1109/CVPR42600.2020.00747
   Paris S, 2004, ACM T GRAPHIC, V23, P712, DOI 10.1145/1015706.1015784
   Perarnau G, 2016, Arxiv, DOI [arXiv:1611.06355, DOI 10.48550/ARXIV.1611.06355]
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Romero A., 2020, arXiv
   Saha R, 2021, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR46437.2021.00202
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shen YJ, 2020, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR42600.2020.00926
   Shiri F, 2019, INT J COMPUT VISION, V127, P863, DOI 10.1007/s11263-019-01169-1
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Tan ZT, 2020, Arxiv, DOI arXiv:2010.16417
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Vasu S., 2018, P EUR C COMP VIS WOR, P114
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wei L., 2018, P EUR C COMP VIS, P99
   Wei TY, 2022, PROC CVPR IEEE, P18051, DOI 10.1109/CVPR52688.2022.01754
   Wu YQ, 2022, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR52688.2022.00419
   Xiao CF, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480502
   Yang Y, 2019, Arxiv, DOI arXiv:1911.11394
   Yin WD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1627, DOI 10.1145/3123266.3123423
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2148, DOI 10.1109/TPAMI.2019.2914039
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275039
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou SC, 2017, Arxiv, DOI arXiv:1705.04932
   Zhou Y, 2018, LECT NOTES COMPUT SC, V11215, P249, DOI 10.1007/978-3-030-01252-6_15
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Peihao, 2021, arXiv
   Zou Z., 2020, arXiv
NR 76
TC 3
Z9 3
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3857
EP 3870
DI 10.1109/TVCG.2023.3241894
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700067
PM 37022457
DA 2024-11-06
ER

PT J
AU Goudé, I
   Bruckert, A
   Olivier, AH
   Pettré, J
   Cozot, R
   Bouatouch, K
   Christie, M
   Hoyet, L
AF Goude, Ific
   Bruckert, Alexandre
   Olivier, Anne-Helene
   Pettre, Julien
   Cozot, Remi
   Bouatouch, Kadi
   Christie, Marc
   Hoyet, Ludovic
TI Real-Time Multi-Map Saliency-Driven Gaze Behavior for Non-Conversational
   Characters
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Behavioral sciences; Animation; Visualization; Solid modeling; Real-time
   systems; Biological system modeling; Head; dataset; eye-tracking data;
   gaze behavior; neural networks; simulation
ID EYE-MOVEMENTS; ATTENTION; MODEL
AB Gaze behavior of virtual characters in video games and virtual reality experiences is a key factor of realism and immersion. Indeed, gaze plays many roles when interacting with the environment; not only does it indicate what characters are looking at, but it also plays an important role in verbal and non-verbal behaviors and in making virtual characters alive. Automated computing of gaze behaviors is however a challenging problem, and to date none of the existing methods are capable of producing close-to-real results in an interactive context. We therefore propose a novel method that leverages recent advances in several distinct areas related to visual saliency, attention mechanisms, saccadic behavior modelling, and head-gaze animation techniques. Our approach articulates these advances to converge on a multi-map saliency-driven model which offers real-time realistic gaze behaviors for non-conversational characters, together with additional user-control over customizable features to compose a wide variety of results. We first evaluate the benefits of our approach through an objective evaluation that confronts our gaze simulation with ground truth data using an eye-tracking dataset specifically acquired for this purpose. We then rely on subjective evaluation to measure the level of realism of gaze animations generated by our method, in comparison with gaze animations captured from real actors. Our results show that our method generates gaze behaviors that cannot be distinguished from captured gaze animations. Overall, we believe that these results will open the way for more natural and intuitive design of realistic and coherent gaze animations for real-time applications.
C1 [Goude, Ific; Olivier, Anne-Helene; Pettre, Julien; Bouatouch, Kadi; Christie, Marc; Hoyet, Ludovic] Univ Rennes, Inria, CNRS, IRISA, F-35000 Rennes, France.
   [Bruckert, Alexandre] Nantes Univ, CNRS, IRCCyN, UMR 6004,Eecole Cent Nantes,LS2N, F-44000 Nantes, France.
   [Cozot, Remi] Littoral Opal Coast Univ, F-59140 Dunkerque, France.
C3 Universite de Rennes; Centre National de la Recherche Scientifique
   (CNRS); Inria; Nantes Universite; Ecole Centrale de Nantes; Centre
   National de la Recherche Scientifique (CNRS)
RP Bruckert, A (corresponding author), Nantes Univ, CNRS, IRCCyN, UMR 6004,Eecole Cent Nantes,LS2N, F-44000 Nantes, France.
EM ific.goude@irisa.fr; alexandre.bruckert@univ-nantes.fr;
   anne-helene.olivier@univ-rennes2.fr; julien.pettre@inria.fr;
   remi.cozot@univ-littoral.fr; kadi.bouatouch@irisa.fr;
   marc.christie@irisa.fr; ludovic.hoyet@inria.fr
RI Bruckert, Alexandre/AAW-6393-2021; Hoyet, Ludovic/IWU-9100-2023; Pettre,
   Julien/KZT-8249-2024; Olivier, Anne-Hélène/AAH-7378-2020
OI Pettre, Julien/0000-0003-1812-1436; Hoyet, Ludovic/0000-0002-7373-6049;
   Bruckert, Alexandre/0000-0003-2623-4975
CR Abid M, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901782
   Agil U, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1806
   Andrist S., 2012, Proceedings of the International Conference on Human Factors in Computing, CHI '12, P705, DOI DOI 10.1145/2207676.2207777
   Arpa Sami, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P168, DOI 10.1007/978-3-642-25090-3_15
   Bannier K, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204560
   Berton F, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P322, DOI [10.1109/VR46266.2020.1581264804299, 10.1109/VR46266.2020.00-52]
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   BOTZEL K, 1993, BRAIN, V116, P337, DOI 10.1093/brain/116.2.337
   Brockmole JR, 2005, PSYCHON B REV, V12, P1061, DOI 10.3758/BF03206444
   Bruckert A, 2021, NEUROCOMPUTING, V453, P693, DOI 10.1016/j.neucom.2020.06.131
   Bulbul A., 2010, P 7 S APPL PERC GRAP, P81
   Courty N., 2003, Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429), pIII
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   Djilali YAD, 2021, IEEE INT CONF COMP V, P3743, DOI 10.1109/ICCVW54120.2021.00418
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   Gillies M., 2001, Tech. Rep. TR522
   Gillies MFP, 2002, J VISUAL COMP ANIMAT, V13, P287, DOI 10.1002/vis.296
   Gu E, 2007, LECT NOTES ARTIF INT, V4840, P277
   Imaoka Y, 2020, FRONT PSYCHIATRY, V11, DOI 10.3389/fpsyt.2020.572938
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Jarodzka H., 2010, P S EYE TRACK RES AP, P211, DOI DOI 10.1145/1743666.1743718
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T., 2012, Tech. Rep. TR-2012-001
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Khullar SC, 2001, AUTON AGENT MULTI-AG, V4, P9, DOI 10.1023/A:1010010528443
   Klein A., 2019, P 12 ACM SIGGRAPH C, P1
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kümmerer M, 2018, LECT NOTES COMPUT SC, V11220, P798, DOI 10.1007/978-3-030-01270-0_47
   Le Meur O, 2017, EUR SIGNAL PR CONF, P1892, DOI 10.23919/EUSIPCO.2017.8081538
   Le Meur O, 2015, VISION RES, V116, P152, DOI 10.1016/j.visres.2014.12.026
   Lee SP, 2002, ACM T GRAPHIC, V21, P637
   Loth S, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P263, DOI 10.1145/3267851.3267852
   Manor BR, 2003, J NEUROSCI METH, V128, P85, DOI 10.1016/S0165-0270(03)00151-1
   Mills M, 2011, J VISION, V11, DOI 10.1167/11.8.17
   Normoyle Aline., 2013, Proceedings of motion on games, P141, DOI [10.1145/2522628.2522630, DOI 10.1145/2522628.2522630]
   Oyekoya Oyewole., 2009, Proceedings of the 16th acm symposium on virtual reality software and technology, P199
   Pelachaud C, 2003, LECT NOTES ARTIF INT, V2792, P93
   Peters C, 2003, COMP ANIM CONF PROC, P111, DOI 10.1109/CASA.2003.1199311
   Peters C., 2003, P ACM SIGGRAPH C 200, P1
   Peters C, 2011, COGN TECHNOL, P293, DOI 10.1007/978-3-642-15184-2_16
   Peters C, 2010, COMPUT GRAPH-UK, V34, P677, DOI 10.1016/j.cag.2010.09.007
   Picot A, 2007, LECT NOTES ARTIF INT, V4722, P272
   Ruhland Kerstin., 2014, Eurographics state-of-the-art report, P69
   Salvucci Dario D, 2000, P 2000 S EYE TRACK R, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Samuel AG, 2003, PSYCHON B REV, V10, P897, DOI 10.3758/BF03196550
   Satogata R, 2020, ACMIEEE INT CONF HUM, P433, DOI 10.1145/3371382.3378248
   Sidenmark L, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3361218
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Stellmach S., 2010, P S EYE TRACK RES AP, P109, DOI [DOI 10.1145/1743666.1743693, DOI 10.1145/1743666]
   Steptoe W, 2010, COMPUT ANIMAT VIRT W, V21, P161, DOI 10.1002/cav.354
   Takashima K., 2008, Proceedings of Graphics Interface 2008, P169, DOI DOI 10.1145/1375714.1375744
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
NR 56
TC 5
Z9 5
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3871
EP 3883
DI 10.1109/TVCG.2023.3244679
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700044
PM 37022858
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wang, ZY
   Wang, YY
   Yan, SQ
   Zhu, ZZ
   Zhang, KJ
   Wei, HK
AF Wang, Ziyao
   Wang, Yiye
   Yan, Shiqi
   Zhu, Zhongzheng
   Zhang, Kanjian
   Wei, Haikun
TI Redirected Walking on Omnidirectional Treadmill
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Device integration; haptic feedback; locomotion interfaces;
   omnidirectional treadmill; redirected walking
ID VIRTUAL ENVIRONMENTS; CONTROLLER
AB Redirected walking (RDW) and omnidirectional treadmill (ODT) are two effective solutions to the natural locomotion interface in virtual reality. ODT fully compresses the physical space and can be used as the integration carrier of all kinds of devices. However, the user experience varies in different directions of ODT, and the premise of interaction between users and integrated devices is a good match between virtual and real objects. RDW technology uses visual cues to guide the user's location in physical space. Based on this principle, combining RDW technology with ODT to guide the user's walking direction through visual cues can effectively improve user experience on ODT and make full use of various devices integrated on ODT. This paper explores the novel prospects of combining RDW technology with ODT and formally puts forward the concept of O-RDW (ODT-based RDW). Two baseline algorithms, i.e., OS2MD (ODT-based steer to multi-direction), and OS2MT (ODT-based steer to multi-target), are proposed to combine the merits of both RDW and ODT. With the help of the simulation environment, this paper quantitatively analyzes the applicable scenarios of the two algorithms and the influence of several main factors on the performance. Based on the conclusions of the simulation experiments, the two O-RDW algorithms are successfully applied in the practical application case of multi-target haptic feedback. Combined with the user study, the practicability and effectiveness of O-RDW technology in practical use are further verified.
C1 [Wang, Ziyao; Wang, Yiye; Yan, Shiqi; Zhu, Zhongzheng; Zhang, Kanjian; Wei, Haikun] Southeast Univ, Nanjing 211189, Jiangning, Peoples R China.
C3 Southeast University - China
RP Wei, HK (corresponding author), Southeast Univ, Nanjing 211189, Jiangning, Peoples R China.
EM zy_wang@seu.edu.cn; wangyiye@seu.edu.cn; sqyan@seu.edu.cn;
   zzzhu@seu.edu.cn; kjzhang@seu.edu.cn; hkwei@seu.edu.cn
OI Wei, Haikun/0000-0002-6667-3166
FU National Natural Science Foundation of China [61773118, 61973083];
   Science and Technology Project of State Grid Corporation of China
   [SGTJDK00DYJS2000148]; Key Laboratory of Measurement and Control of
   Complex Systems of Engineering, Ministry of Education, Nanjing, China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61773118 and 61973083, in part by
   Science and Technology Project of State Grid Corporation of
   China(Intelligent operation and maintenance technology of distributed
   photovoltaic system SGTJDK00DYJS2000148), the Key Laboratory of
   Measurement and Control of Complex Systems of Engineering, Ministry of
   Education, Nanjing 210096, China.
CR Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Ang YY, 2018, ADV INTELL SYST, V696, P367, DOI 10.1007/978-981-10-7386-1_32
   Azmandian M, 2016, 2016 IEEE 2ND WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P9, DOI 10.1109/WEVR.2016.7859537
   Bachmann ER, 2019, IEEE T VIS COMPUT GR, V25, P2022, DOI 10.1109/TVCG.2019.2898764
   Bölling L, 2019, IEEE T VIS COMPUT GR, V25, P2032, DOI 10.1109/TVCG.2019.2899228
   BRIDSON R., 2007, SIGGRAPH SKETCHES, V22, DOI DOI 10.1145/1278780.1278807
   Cakmak T., 2014, ACM SIGGRAPH 2014 Emerging Technologies, P1, DOI DOI 10.1145/2614066.2614105
   Chen ZY, 2021, INT SYM MIX AUGMENT, P184, DOI 10.1109/ISMAR52148.2021.00033
   Cheng LP, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P3718, DOI 10.1145/3025453.3025753
   Cheng LP, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P417
   Cinelli M, 2012, EXP BRAIN RES, V219, P175, DOI 10.1007/s00221-012-3077-9
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Dong TY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P146, DOI [10.1109/VR46266.2020.00-71, 10.1109/VR46266.2020.1581490806361]
   Freitag S, 2016, IEEE T VIS COMPUT GR, V22, P1462, DOI 10.1109/TVCG.2016.2518298
   Hodgson E, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043604
   Hodgson E, 2013, IEEE T VIS COMPUT GR, V19, P634, DOI 10.1109/TVCG.2013.28
   Hosoi J, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 EMERGING TECHNOLOGIES, DOI 10.1145/3476122.3484848
   Huang JY, 2003, IEEE T MULTIMEDIA, V5, P39, DOI 10.1109/TMM.2003.808822
   Iwata H, 2000, ROBOTICS RESEARCH, P275
   Iwata H, 1999, P IEEE VIRT REAL ANN, P286, DOI 10.1109/VR.1999.756964
   KatVr, 2015, Kat walk mini walking tutorials
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kim SW, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-68362-y
   Kohli L., 2005, Proceedings of the 2005 international conference on Augmented tele-existence, P253
   Langbehn E., 2018, Redirected Walking in Vir- tual Reality, DOI [10.1007/978-3319-08234-9253-1.1-11, DOI 10.1007/978-3319-08234-9253-1.1-11]
   Langbehn E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201335
   LaViola JosephJ., 2001, Proceedings Symposium on Interactive 3D Graphics, P9
   Lee DY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P63, DOI [10.1109/vr.2019.8798121, 10.1109/VR.2019.8798121]
   Lee H, 2016, INT CONF UBIQ ROBOT, P889, DOI 10.1109/URAI.2016.7734002
   Lohse AL, 2019, 2019 IEEE 5TH WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), DOI 10.1109/wevr.2019.8809587
   Matsukura H, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P127, DOI 10.1109/VR.2012.6180915
   Metsis V, 2017, 2017 INTERNATIONAL SYMPOSIUM ON WEARABLE ROBOTICS AND REHABILITATION (WEROB), P33
   Moon T., 2004, 11 ACM S VIRTUAL REA, P122, DOI [10.1145/1077534.1077558, DOI 10.1145/1077534.1077558]
   Nescher T, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P111, DOI 10.1109/3DUI.2014.6798851
   Nilsson NC, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3180658
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Ogiwara Y, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P567, DOI [10.1109/VRW50115.2020.00135, 10.1109/VRW50115.2020.0-141]
   Pyo SH, 2018, IEEE INT CONF ROBOT, P760
   Razzaque S., 2005, Redirected Walking
   Razzaque S., 2001, P 22 ANN C EUR ASS C, P289, DOI DOI 10.2312/EGS.20011036
   Rietzler M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376821
   Rietzler M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5669, DOI 10.1145/3025453.3026009
   Schmitz P, 2018, IEEE T VIS COMPUT GR, V24, P1623, DOI 10.1109/TVCG.2018.2793671
   Souman JL, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043607
   Steinicke F., 2013, Human Walking in Virtual Environments, V2
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Strauss RR, 2020, IEEE T VIS COMPUT GR, V26, P1955, DOI 10.1109/TVCG.2020.2973060
   Suma EA, 2012, IEEE T VIS COMPUT GR, V18, P555, DOI 10.1109/TVCG.2012.47
   Sun Q, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201294
   Suzuki M., 2019, P VIRT REAL INT C VR, P39, DOI DOI 10.20870/IJVR.2019..0.2920
   Swapp D, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P71, DOI 10.1109/3DUI.2010.5444717
   Thomas J., 2020, P 26 ACM S VIRT REAL, P1
   Wang ZY, 2023, IEEE T VIS COMPUT GR, V29, P5538, DOI 10.1109/TVCG.2022.3216211
   Wang ZY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P382, DOI [10.1109/VR46266.2020.1580802970259, 10.1109/VR46266.2020.00-46, 10.1109/ICEDME50972.2020.00092]
   Whitmire E, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173660
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
   Williams NL, 2021, IEEE T VIS COMPUT GR, V27, P4267, DOI 10.1109/TVCG.2021.3106432
   Williams NL, 2021, IEEE T VIS COMPUT GR, V27, P2535, DOI 10.1109/TVCG.2021.3067781
   Wilson PT, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P243, DOI 10.1145/3013971.3014010
   Zenner A, 2021, IEEE T VIS COMPUT GR, V27, P2627, DOI 10.1109/TVCG.2021.3067777
   Zenner A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P549, DOI [10.1109/VRW50115.2020.0-150, 10.1109/VRW50115.2020.00126]
   Zenner A, 2017, IEEE T VIS COMPUT GR, V23, P1312, DOI 10.1109/TVCG.2017.2656978
   Zmuda MA, 2013, IEEE T VIS COMPUT GR, V19, P1872, DOI 10.1109/TVCG.2013.88
NR 63
TC 0
Z9 0
U1 6
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3884
EP 3901
DI 10.1109/TVCG.2023.3244359
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700012
PM 37027618
DA 2024-11-06
ER

PT J
AU Wang, Y
   Bâce, M
   Bulling, A
AF Wang, Yao
   Bace, Mihai
   Bulling, Andreas
TI Scanpath Prediction on Information Visualisations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Data visualization; Predictive models; Computational
   modeling; Hidden Markov models; Gaze tracking; Task analysis; Gaze
   behaviour analysis; MASSVIS; scanpath prediction; visual attention;
   visual saliency
ID EYE FIXATION; METRICS; MODEL
AB We propose Unified Model of Saliency and Scanpaths (UMSS)- a model that learns to predict multi-duration saliency and scanpaths (i.e. sequences of eye fixations) on information visualisations. Although scanpaths provide rich information about the importance of different visualisation elements during the visual exploration process, prior work has been limited to predicting aggregated attention statistics, such as visual saliency. We present in-depth analyses of gaze behaviour for different information visualisation elements (e.g. Title, Label, Data) on the popular MASSVIS dataset. We show that while, overall, gaze patterns are surprisingly consistent across visualisations and viewers, there are also structural differences in gaze dynamics for different elements. Informed by our analyses, UMSS first predicts multi-duration element-level saliency maps, then probabilistically samples scanpaths from them. Extensive experiments on MASSVIS show that our method consistently outperforms state-of-the-art methods with respect to several, widely used scanpath and saliency evaluation metrics. Our method achieves a relative improvement in sequence score of 11.5% for scanpath prediction, and a relative improvement in Pearson correlation coefficient of up to 23.6% for saliency prediction. These results are auspicious and point towards richer user models and simulations of visual attention on visualisations without the need for any eye tracking equipment.
C1 [Wang, Yao; Bace, Mihai; Bulling, Andreas] Univ Stuttgart, Inst Visualisat & Interact Syst, Stuttgart, Germany.
C3 University of Stuttgart
RP Wang, Y (corresponding author), Univ Stuttgart, Inst Visualisat & Interact Syst, Stuttgart, Germany.
EM yao.wang@vis.uni-stuttgart.de; Bace@vis.uni-stuttgart.de;
   andreas.bulling@vis.uni-stuttgart.de
OI Wang, Yao/0000-0002-3633-8623
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [251654672-TRR 161]; Swiss National Science Foundation (SNSF) Early
   Postdoc. Mobility Fellowship [199991]; European Research Council
   [801708]; European Research Council (ERC) [801708] Funding Source:
   European Research Council (ERC)
FX The work of Yao Wang was funded by the Deutsche Forschungsgemeinschaft
   (DFG, German Research Foundation) under Grant 251654672-TRR 161. The
   work of Mihai Bace was funded by the Swiss National Science Foundation
   (SNSF) Early Postdoc. Mobility Fellowship under Grant 199991. The work
   of Andreas Bulling was funded by the European Research Council under
   Grant 801708.
CR Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275
   Assens M, 2019, LECT NOTES COMPUT SC, V11133, P406, DOI 10.1007/978-3-030-11021-5_25
   Bao WT, 2020, NEUROCOMPUTING, V404, P154, DOI 10.1016/j.neucom.2020.03.060
   Beck F, 2017, MATH VIS, P113, DOI 10.1007/978-3-319-47024-5_7
   Behrisch M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13446
   Berndt D. J., 1994, P KDD WORKSH SEATTL, P359, DOI DOI 10.5555/3000850.3000887
   Boccignone G., 2010, 2010 2nd European Workshop on Visual Information Processing (EUVIP 2010), P29, DOI 10.1109/EUVIP.2010.5699099
   Boccignone V., 2019, P INT S FORM METH, P131, DOI [10.1007/978-3-030-54994-7_10, DOI 10.1007/978-3-030-54994-7_10]
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Brockmann D, 2000, NEUROCOMPUTING, V32, P643, DOI 10.1016/S0925-2312(00)00227-7
   Burch L., 2017, EyeTracking and Visualization: Foundations, Techniques, and Applications
   Bylinskii Z, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/3126594.3126653
   Bylinskii Z, 2017, MATH VIS, P235, DOI 10.1007/978-3-319-47024-5_14
   Chen M., 2021, P IEEE CVF C COMP VI
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, LECT NOTES COMPUT SC, V9914, P302, DOI 10.1007/978-3-319-48881-3_21
   Coutrot A, 2018, BEHAV RES METHODS, V50, P362, DOI 10.3758/s13428-017-0876-8
   Cristino F, 2010, BEHAV RES METHODS, V42, P692, DOI 10.3758/BRM.42.3.692
   Droste R, 2020, Arxiv, DOI arXiv:2003.05477
   Faggi L, 2020, Arxiv, DOI arXiv:2006.11035
   Feit AM, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1118, DOI 10.1145/3025453.3025599
   Feit L., 2020, P ACM S EYE TRACK RE, P1
   Fosco C., 2020, P 33 ANN ACM S US IN, P249, DOI DOI 10.1145/3379337.3415825
   Fosco C, 2020, PROC CVPR IEEE, P4472, DOI 10.1109/CVPR42600.2020.00453
   Gupta P, 2018, IEEE WINT CONF APPL, P1529, DOI 10.1109/WACV.2018.00171
   Harezlak K, 2014, PROCEDIA COMPUT SCI, V35, P1073, DOI 10.1016/j.procs.2014.08.194
   Hu ZM, 2021, IEEE T VIS COMPUT GR, V27, P2681, DOI 10.1109/TVCG.2021.3067779
   Hu ZM, 2020, IEEE T VIS COMPUT GR, V26, P1902, DOI 10.1109/TVCG.2020.2973473
   Huettig F, 2005, COGNITION, V96, pB23, DOI 10.1016/j.cognition.2004.10.003
   Islam M, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101837
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jarodzka H., 2010, P S EYE TRACK RES AP, P211, DOI DOI 10.1145/1743666.1743718
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Komogortsev OV, 2010, IEEE T BIO-MED ENG, V57, P2635, DOI 10.1109/TBME.2010.2057429
   Kümmerer M, 2022, J VISION, V22, DOI 10.1167/jov.22.5.7
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Kummerer M., 2017, Journal of Vision, V17, P1147, DOI [10.1167/17.10.1147, DOI 10.1167/17.10.1147]
   Kurzhals Kuno, 2014, P 5 WORKSH TIM ERR N, P61
   Leiva L. A., 2020, P 22 INT C HUM COMP, P1
   Li WS, 2019, IEEE ACCESS, V7, P14488, DOI 10.1109/ACCESS.2019.2894275
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   Matzen L, 2020, Arxiv, DOI arXiv:2009.14465
   Matzen LE, 2018, IEEE T VIS COMPUT GR, V24, P563, DOI 10.1109/TVCG.2017.2743939
   Matzen LE, 2017, LECT NOTES ARTIF INT, V10284, P176, DOI 10.1007/978-3-319-58628-1_15
   Micallef L, 2017, IEEE T VIS COMPUT GR, V23, P1588, DOI 10.1109/TVCG.2017.2674978
   Muller M., 2007, Information Retrieval for Music and Motion, P69, DOI 10.1007/978-3-540-74048-34
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Nguyen T.H.D., 2015, P WORKSH EYE TRACK V, P23
   O'Donovan P, 2014, IEEE T VIS COMPUT GR, V20, P1200, DOI 10.1109/TVCG.2014.48
   Pan SW, 2020, CHAOS, V30, DOI 10.1063/5.0010886
   Pang XF, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982422
   Pirinen A, 2018, PROC CVPR IEEE, P6945, DOI 10.1109/CVPR.2018.00726
   Salverda AP., 2017, RES METHODS PSYCHOLI, V9, P89
   Scholl BJ, 2007, MIND LANG, V22, P563, DOI 10.1111/j.1468-0017.2007.00321.x
   Shen CY, 2014, LECT NOTES COMPUT SC, V8695, P33, DOI 10.1007/978-3-319-10584-0_3
   Shojaeizadeh M, 2016, LECT NOTES COMPUT SC, V9737, P465, DOI 10.1007/978-3-319-40250-5_44
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Staub A, 2013, PSYCHON B REV, V20, P1304, DOI 10.3758/s13423-013-0444-x
   Sun WJ, 2021, IEEE T PATTERN ANAL, V43, P2101, DOI 10.1109/TPAMI.2019.2956930
   Verma A, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902643
   Wang M., 2022, P S EYE TRACK RES AP, P1
   Wang W, 2011, PROC CVPR IEEE, P441, DOI 10.1109/CVPR.2011.5995423
   Wang YX, 2017, COGN PROCESS, V18, P87, DOI 10.1007/s10339-016-0781-6
   Wloka C, 2018, PROC CVPR IEEE, P3184, DOI 10.1109/CVPR.2018.00336
   Xia C, 2019, IEEE T IMAGE PROCESS, V28, P3502, DOI 10.1109/TIP.2019.2897966
   Xu PM, 2015, Arxiv, DOI arXiv:1504.06755
   Xu PM, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3299, DOI 10.1145/2858036.2858479
   Yang ZB, 2020, PROC CVPR IEEE, P190, DOI [10.1109/CVPR42600.2020.00027, 10.1109/cvpr42600.2020.00027]
   Zanca D, 2018, Arxiv, DOI arXiv:1802.02534
   Zanca D, 2020, IEEE T PATTERN ANAL, V42, P2983, DOI 10.1109/TPAMI.2019.2920636
   Zhang XC, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174198
NR 72
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3902
EP 3914
DI 10.1109/TVCG.2023.3242293
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700071
PM 37022407
OA Green Accepted, Green Submitted
DA 2024-11-06
ER

PT J
AU Wei, YT
   Wang, ZY
   Wang, ZW
   Dai, Y
   Ou, GC
   Gao, H
   Yang, HT
   Wang, Y
   Cao, CC
   Weng, LX
   Lu, JY
   Zhu, RC
   Chen, W
AF Wei, Yating
   Wang, Zhiyong
   Wang, Zhongwei
   Dai, Yong
   Ou, Gongchang
   Gao, Han
   Yang, Haitao
   Wang, Yue
   Cao, Caleb Chen
   Weng, Luoxuan
   Lu, Jiaying
   Zhu, Rongchen
   Chen, Wei
TI Visual Diagnostics of Parallel Performance in Training Large-Scale DNN
   Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Data visualization; Computational modeling; Solid modeling;
   Parallel processing; Performance evaluation; Data models; Deep neural
   network; model training; parallel performance; visual analysis
ID VISUALIZATION; FLOW
AB Diagnosing the cluster-based performance of large-scale deep neural network (DNN) models during training is essential for improving training efficiency and reducing resource consumption. However, it remains challenging due to the incomprehensibility of the parallelization strategy and the sheer volume of complex data generated in the training processes. Prior works visually analyze performance profiles and timeline traces to identify anomalies from the perspective of individual devices in the cluster, which is not amenable for studying the root cause of anomalies. In this article, we present a visual analytics approach that empowers analysts to visually explore the parallel training process of a DNN model and interactively diagnose the root cause of a performance issue. A set of design requirements is gathered through discussions with domain experts. We propose an enhanced execution flow of model operators for illustrating parallelization strategies within the computational graph layout. We design and implement an enhanced Marey's graph representation, which introduces the concept of time-span and a banded visual metaphor to convey training dynamics and help experts identify inefficient training processes. We also propose a visual aggregation technique to improve visualization efficiency. We evaluate our approach using case studies, a user study and expert interviews on two large-scale models run in a cluster, namely, the PanGu-alpha 13B model (40 layers), and the Resnet model (50 layers).
C1 [Wei, Yating; Wang, Zhiyong; Wang, Zhongwei; Dai, Yong; Weng, Luoxuan; Lu, Jiaying; Zhu, Rongchen; Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Ou, Gongchang; Gao, Han; Yang, Haitao; Wang, Yue; Cao, Caleb Chen] Huawei Technol Co Ltd, Distributed Data Lab, Shenzhen 518129, Peoples R China.
C3 Zhejiang University; Huawei Technologies
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM weiyating@zju.edu.cn; zerowangzy@zju.edu.cn; wzw09@zju.edu.cn;
   daiyong@zju.edu.cn; ougongchang@huawei.com; gaohan19@huawei.com;
   yanghaitao1@huawei.com; wangyue53@huawei.com; caochen.hkust@gmail.com;
   lukeweng@zju.edu.cn; 3180103570@zju.edu.cn; zrccrz@zju.edu.cn;
   chenvis@zju.edu.cn
RI Zhu, Rongchen/AAZ-2501-2020
OI Lu, Jiaying/0009-0008-3578-346X; Weng, Luoxuan/0000-0003-4350-9141
FU National Natural Science Foundation of China [62132017]; Fundamental
   Research Funds for the Central Universities [226-2022-00235]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62132017 and in part by the Fundamental
   Research Funds for the Central Universities under Grant 226-2022-00235.
CR Abadi M., 2015, Tensorflow: Large-scale machine learning on heterogeneous distributed systems, DOI DOI 10.48550/ARXIV.1603.04467
   Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Alsallakh B, 2018, IEEE T VIS COMPUT GR, V24, P152, DOI 10.1109/TVCG.2017.2744683
   Amodei D, 2016, PR MACH LEARN RES, V48
   Bach B., 2014, P EUR C VIS, DOI [10.2312/eurovisstar.2014117125, DOI 10.2312/EUROVISSTAR.2014117125]
   Dean Jeffrey, 2012, Advances in neural information processing systems
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Fujiwara T, 2017, IEEE CONF VIS ANAL, P59, DOI 10.1109/VAST.2017.8585646
   Gang Xian, 2021, 2021 7th International Conference on Computer and Communications (ICCC), P1436, DOI 10.1109/ICCC54389.2021.9674694
   Gantt HenryL., 1919, Organizing for Work
   Gotz D, 2014, IEEE T VIS COMPUT GR, V20, P1783, DOI 10.1109/TVCG.2014.2346682
   Gu T., 2018, IEEE Trans. Computat. Social Syst., V5, P1
   Guo H., 2018, P EUR S PAR GRAPH VI, P91
   Harlap A, 2018, Arxiv, DOI arXiv:1806.03377
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henry N, 2007, IEEE T VIS COMPUT GR, V13, P1302, DOI 10.1109/TVCG.2007.70582
   Huang YP, 2019, ADV NEUR IN, V32
   Jia Zhihao, 2019, Proceedings of Machine Learning and Systems, V1, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu DY, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3200489
   Liu MC, 2018, IEEE T VIS COMPUT GR, V24, P77, DOI 10.1109/TVCG.2017.2744938
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Luz S, 2007, IEEE INT CONF INF VI, P197
   Milash B., 1996, P C COMP HUM FACT CO, P392
   Monroe M, 2013, IEEE T VIS COMPUT GR, V19, P2227, DOI 10.1109/TVCG.2013.200
   Narayanan D, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P1, DOI 10.1145/3341301.3359646
   Palomo C, 2016, IEEE T VIS COMPUT GR, V22, P170, DOI 10.1109/TVCG.2015.2467592
   Shazeer N., 2018, ADV NEURAL INF PROCE, P1
   Keskar NS, 2017, Arxiv, DOI [arXiv:1712.07628, 10.48550/arXiv.1712.07628]
   Shoeybi M, 2020, Arxiv, DOI arXiv:1909.08053
   SUGIYAMA K, 1981, IEEE T SYST MAN CYB, V11, P109, DOI 10.1109/TSMC.1981.4308636
   Tufte E. R., 1985, TLS-TIMES LIT SUPPL, V7
   Tzeng FY, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P383
   Vaswani A, 2017, ADV NEUR IN, V30
   WAUGH SJ, 1995, J OPT SOC AM A, V12, P2305, DOI 10.1364/JOSAA.12.002305
   Wongsuphasawat K, 2018, IEEE T VIS COMPUT GR, V24, P1, DOI 10.1109/TVCG.2017.2744878
   Wongsuphasawat K, 2012, IEEE T VIS COMPUT GR, V18, P2659, DOI 10.1109/TVCG.2012.225
   Xu PP, 2017, IEEE T VIS COMPUT GR, V23, P291, DOI 10.1109/TVCG.2016.2598664
   Yamazaki M, 2019, Arxiv, DOI arXiv:1903.12650
   Yang SH, 2020, Arxiv, DOI arXiv:2002.07526
   Zeng W., 2021, arXiv
NR 41
TC 2
Z9 2
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3915
EP 3929
DI 10.1109/TVCG.2023.3243228
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700059
PM 37022820
DA 2024-11-06
ER

PT J
AU de Silva, A
   Zhao, MA
   Stewart, D
   Khan, FH
   Dusek, G
   Davis, J
   Pang, A
AF de Silva, Akila
   Zhao, Mona
   Stewart, Donald
   Khan, Fahim Hasan
   Dusek, Gregory
   Davis, James
   Pang, Alex
TI RipViz: Finding Rip Currents by Learning Pathline Behavior
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Videos; Behavioral sciences; Visualization; Optical flow; Noise
   measurement; Bathymetry; Task analysis; anomaly detection; Flow
   visualization; 2D unsteady flow fields; pathlines; LSTM autoencoders
AB We present a hybrid machine learning and flow analysis feature detection method, RipViz, to extract rip currents from stationary videos. Rip currents are dangerous strong currents that can drag beachgoers out to sea. Most people are either unaware of them or do not know what they look like. In some instances, even trained personnel such as lifeguards have difficulty identifying them. RipViz produces a simple, easy to understand visualization of rip location overlaid on the source video. With RipViz, we first obtain an unsteady 2D vector field from the stationary video using optical flow. Movement at each pixel is analyzed over time. At each seed point, sequences of short pathlines, rather a single long pathline, are traced across the frames of the video to better capture the quasi-periodic flow behavior of wave activity. Because of the motion on the beach, the surf zone, and the surrounding areas, these pathlines may still appear very cluttered and incomprehensible. Furthermore, lay audiences are not familiar with pathlines and may not know how to interpret them. To address this, we treat rip currents as a flow anomaly in an otherwise normal flow. To learn about the normal flow behavior, we train an LSTM autoencoder with pathline sequences from normal ocean, foreground, and background movements. During test time, we use the trained LSTM autoencoder to detect anomalous pathlines (i.e., those in the rip zone). The origination points of such anomalous pathlines, over the course of the video, are then presented as points within the rip zone. RipViz is fully automated and does not require user input. Feedback from domain expert suggests that RipViz has the potential for wider use.
C1 [de Silva, Akila; Zhao, Mona; Stewart, Donald; Khan, Fahim Hasan; Davis, James; Pang, Alex] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Dusek, Gregory] NOAA, Silver Spring, MD USA.
C3 University of California System; University of California Santa Cruz;
   National Oceanic Atmospheric Admin (NOAA) - USA
RP de Silva, A; Pang, A (corresponding author), Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
EM audesilv@ucsc.edu; yzhao172@ucsc.edu; dolstewa@ucsc.edu;
   fkhan4@ucsc.edu; gregory.dusek@noaa.gov; davis@cs.ucsc.edu;
   pang@soe.ucsc.edu
OI Davis, James/0000-0002-1413-2616; de Silva, Akila/0000-0002-7553-7270;
   Khan, Fahim Hasan/0000-0003-3130-6259; Pang, Alex/0000-0003-0720-7162;
   Dusek, Gregory/0000-0003-0289-1356
FU Southeast Coastal Ocean Observing Regional Association (SECOORA); NOAA
   [NA20NOS0120220]
FX This report was prepared in part as a result of work sponsored by the
   Southeast Coastal Ocean Observing Regional Association (SECOORA) with
   NOAA financial assistance under Grant NA20NOS0120220.
CR Ba J.L., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Berenjkoub M, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P261, DOI 10.1109/VIS47514.2020.00059
   BOWEN AJ, 1969, J GEOPHYS RES, V74, P5467, DOI 10.1029/JC074i023p05467
   Bradski G. a. A. K., 2000, Dr. Dobbs J, V120, P122
   Bürger K, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P71
   Carmo BS, 2004, LECT NOTES COMPUT SC, V3216, P451
   Castelle B, 2016, EARTH-SCI REV, V163, P1, DOI 10.1016/j.earscirev.2016.09.008
   Chen J., 2019, Biodiesel Production: Technologies, Challenges, and Future Prospects, P229
   Chen Jenny J, 2011, Interfaces, V5, P1, DOI 10.1145/1357054.1357127
   Chollet F., 2015, KERAS
   de Silva A, 2021, COAST ENG, V166, DOI 10.1016/j.coastaleng.2021.103859
   Ester M., 1996, P KDD, P226
   Günther T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073684
   Haller MC, 2014, J WATERW PORT COAST, V140, P115, DOI 10.1061/(ASCE)WW.1943-5460.0000229
   Han J, 2020, IEEE T VIS COMPUT GR, V26, P1732, DOI 10.1109/TVCG.2018.2880207
   Holman R, 2013, ANNU REV MAR SCI, V5, P95, DOI 10.1146/annurev-marine-121211-172408
   Kim B, 2019, COMPUT GRAPH FORUM, V38, P285, DOI 10.1111/cgf.13689
   Klein AHDF, 2003, J COASTAL RES, P107
   Leatherman J., 2011, Rip Currents: Beach Safety, Phys-ical Oceanography, and Wave Modeling, V1st, DOI [10.1201/b10916.21, DOI 10.1201/B10916.21]
   Leatherman S., Techniques for detecting and measur-ing rip currents
   Liu L., 2020, Int. J. Comput. Vis, V3, P1, DOI [10.35840/2631-5033/1814, DOI 10.35840/2631-5033/1814]
   Lodha SK, 2000, IEEE VISUAL, P343, DOI 10.1109/VISUAL.2000.885714
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674
   Lushine J.B., 1991, National Weather Digest, V16, P13
   Ma J, 2013, PROC SPIE, V8654, DOI 10.1117/12.2001887
   MacMahan J., 2011, J. Coastal Res., V27, P3, DOI [10.2112/JCOAS-TRES-D-11-00024.1, DOI 10.2112/JCOAS-TRES-D-11-00024.1]
   Marchesin S, 2010, IEEE T VIS COMPUT GR, V16, P1578, DOI 10.1109/TVCG.2010.212
   Maryan C, 2019, APPL SOFT COMPUT, V78, P84, DOI 10.1016/j.asoc.2019.02.017
   McGill S.P., 2022, Shore Beach, V90, P50, DOI [10.34237/1009015, DOI 10.34237/1009015]
   Mori I, 2022, IEEE ACCESS, V10, P6483, DOI 10.1109/ACCESS.2022.3140340
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Paolacci G, 2010, JUDGM DECIS MAK, V5, P411
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Philip A., 2016, P EUR IEEE VGTC C VI, P19
   Rashid A.H., 2020, INT C NEURAL INF PRO, P172, DOI [10.1007/978-3-030-63823-8_21, DOI 10.1007/978-3-030-63823-821.36A.H]
   Rashid AH, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533849
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Ribeiro M, 2018, PATTERN RECOGN LETT, V105, P13, DOI 10.1016/j.patrec.2017.07.016
   Sane S, 2020, COMPUT GRAPH FORUM, V39, P785, DOI 10.1111/cgf.14036
   Shadden SC, 2005, PHYSICA D, V212, P271, DOI 10.1016/j.physd.2005.10.007
   Tao J, 2013, IEEE T VIS COMPUT GR, V19, P393, DOI 10.1109/TVCG.2012.143
   Theisel H., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P141
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yu HF, 2012, IEEE T VIS COMPUT GR, V18, P1353, DOI 10.1109/TVCG.2011.155
   Zhang SF, 2018, IEEE SYS MAN CYBERN, P415, DOI 10.1109/SMC.2018.00080
NR 45
TC 1
Z9 1
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3930
EP 3944
DI 10.1109/TVCG.2023.3243834
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700005
PM 37022897
OA Green Submitted, hybrid
DA 2024-11-06
ER

PT J
AU Vohra, SK
   Harth, P
   Isoe, Y
   Bahl, A
   Fotowat, H
   Engert, F
   Hege, HC
   Baum, D
AF Vohra, Sumit Kumar
   Harth, Philipp
   Isoe, Yasuko
   Bahl, Armin
   Fotowat, Haleh
   Engert, Florian
   Hege, Hans-Christian
   Baum, Daniel
TI A Visual Interface for Exploring Hypotheses About Neural Circuits
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neurons; Neural circuits; Visualization; Imaging; Image reconstruction;
   Behavioral sciences; Data visualization; Human-centered computing;
   scientific visualization; visualization systems and tools; web-based
   interaction
ID BRAIN; VISUALIZATION
AB One of the fundamental problems in neurobiological research is to understand how neural circuits generate behaviors in response to sensory stimuli. Elucidating such neural circuits requires anatomical and functional information about the neurons that are active during the processing of the sensory information and generation of the respective response, as well as an identification of the connections between these neurons. With modern imaging techniques, both morphological properties of individual neurons as well as functional information related to sensory processing, information integration and behavior can be obtained. Given the resulting information, neurobiologists are faced with the task of identifying the anatomical structures down to individual neurons that are linked to the studied behavior and the processing of the respective sensory stimuli. Here, we present a novel interactive tool that assists neurobiologists in the aforementioned tasks by allowing them to extract hypothetical neural circuits constrained by anatomical and functional data. Our approach is based on two types of structural data: brain regions that are anatomically or functionally defined, and morphologies of individual neurons. Both types of structural data are interlinked and augmented with additional information. The presented tool allows the expert user to identify neurons using Boolean queries. The interactive formulation of these queries is supported by linked views, using, among other things, two novel 2D abstractions of neural circuits. The approach was validated in two case studies investigating the neural basis of vision-based behavioral responses in zebrafish larvae. Despite this particular application, we believe that the presented tool will be of general interest for exploring hypotheses about neural circuits in other species, genera and taxa.
C1 [Vohra, Sumit Kumar; Harth, Philipp; Hege, Hans-Christian; Baum, Daniel] Zuse Inst Berlin ZIB, Dept Visual & Data Ctr Comp, D-14195 Berlin, Germany.
   [Isoe, Yasuko; Fotowat, Haleh; Engert, Florian] Harvard Univ, Dept Mol & Cellular Biol, Cambridge, MA 02138 USA.
   [Bahl, Armin] Univ Konstanz, Ctr Adv Study Collect Behav, D-78464 Constance, Germany.
C3 Zuse Institute Berlin; Harvard University; University of Konstanz
RP Vohra, SK (corresponding author), Zuse Inst Berlin ZIB, Dept Visual & Data Ctr Comp, D-14195 Berlin, Germany.
EM vohra@zib.de; harth@zib.de; yasuko_isoe@fas.harvard.edu;
   armin.bahl@uni-konstanz.de; haleh.fotowat@wyss.harvard.edu;
   florian@mcb.harvard.edu; hege@zib.de; baum@zib.de
OI Hege, Hans-Christian/0000-0002-6574-0988; Vohra, Sumit
   Kumar/0000-0003-2443-7461; Isoe, Yasuko/0000-0002-1391-7921; Baum,
   Daniel/0000-0003-1550-7245; Bahl, Armin/0000-0001-7591-5860; Harth,
   Philipp/0000-0001-8831-3893
FU National Institute of Neurological Disorders and Stroke (NINDS) of the
   National Institutes of Health (NIH) [1U19NS104653]; Deutsche
   Forschungsgemeinschaft (DFG); SPP 2041 Computational Connectomics
   [347210657]; NIH [U19NS104653, 1R01NS124017]; National Science
   Foundation [IIS-1912293]; Simons Foundation [SCGB 542973]; Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) through
   Germany's Excellence Strategy [EXC 2117 422037984]; Emmy Noether Program
   [429442687]; Zukunftskolleg Konstanz
FX This work was supported in part by the National Institute of
   Neurological Disorders and Stroke (NINDS) of the National Institutes of
   Health (NIH) under Grant 1U19NS104653, and in part by the Deutsche
   Forschungsgemeinschaft (DFG), SPP 2041 Computational Connectomics, under
   Grant 347210657. The work of Florian Engert received funding from the
   NIH under Grants U19NS104653 and 1R01NS124017, in part by the National
   Science Foundation under Grant IIS-1912293, and in part by the Simons
   Foundation under Grant SCGB 542973. The work of Armin Bahl was supported
   in part by the Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) through Germany's Excellence Strategy under Grant EXC 2117
   422037984, in part by the Emmy Noether Program under Grant 429442687,
   and in part by the Zukunftskolleg Konstanz.
CR Abouzied A, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P207
   Al-Awami AK, 2014, IEEE T VIS COMPUT GR, V20, P2369, DOI 10.1109/TVCG.2014.2346312
   Allen Institute, 2019, Documentation: Allen mouse brain connectivity atlas
   [Anonymous], 2021, React complex tree
   [Anonymous], 2016, Neuroglancer: WebGL-based viewer for volumetric data
   [Anonymous], 2022, FishExplorer: A multiscale, explorative atlas of the larval zebrafishh's brain
   [Anonymous], 2022, The larval standard brain
   [Anonymous], 2020, ZFIN-Anatomy atlases and resources
   [Anonymous], 2010, Gunicorn-Python WSGI HTTP Server for UNIX
   [Anonymous], 2013, BabylonJs: The WebGL-based rendering engine
   Bahl A, 2020, NAT NEUROSCI, V23, P94, DOI 10.1038/s41593-019-0534-9
   Beyer J, 2022, COMPUT GRAPH FORUM, V41, P573, DOI 10.1111/cgf.14574
   Beyer J, 2013, IEEE T VIS COMPUT GR, V19, P2868, DOI 10.1109/TVCG.2013.142
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Brandt R, 2005, J COMP NEUROL, V492, P1, DOI 10.1002/cne.20644
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Bruckner S, 2009, IEEE T VIS COMPUT GR, V15, P1497, DOI 10.1109/TVCG.2009.121
   Cantarelli M, 2018, PHILOS T R SOC B, V373, DOI 10.1098/rstb.2017.0380
   Chen WF, 2018, CHINESE J ELECTRON, V27, P889, DOI 10.1049/cje.2018.04.007
   Claudi F, 2021, ELIFE, V10, DOI 10.7554/eLife.65751
   Dercksen V. J., 2012, P EUR WORKSH VIS COM, P17, DOI [10.2312/VCBM/VCBM12/017-024, DOI 10.2312/VCBM/VCBM12/017-024]
   Dunn TW, 2016, NEURON, V89, P613, DOI 10.1016/j.neuron.2015.12.021
   Fornito A., 2016, FUNDAMENTALS BRAIN N
   Fotowat H, 2022, bioRxiv, DOI [10.1101/2022.06.23.497366, 10.1101/2022.06.23.497366, DOI 10.1101/2022.06.23.497366]
   Friedrich RW, 2021, ANNU REV NEUROSCI, V44, P275, DOI 10.1146/annurev-neuro-110220-013050
   Ganglberger F., 2018, P EUR WORKSH VIS COM, P77
   Ganglberger F, 2022, COMPUT GRAPH-UK, V105, P12, DOI 10.1016/j.cag.2022.04.014
   Ganglberger F, 2019, COMPUT GRAPH-UK, V82, P304, DOI 10.1016/j.cag.2019.05.032
   Gerhard Stephan, 2011, Front Neuroinform, V5, P3, DOI 10.3389/fninf.2011.00003
   Gleeson P, 2019, NEURON, V103, P395, DOI 10.1016/j.neuron.2019.05.019
   Göbel W, 2007, PHYSIOLOGY, V22, P358, DOI 10.1152/physiol.00032.2007
   Helmbrecht TO, 2018, NEURON, V100, P1429, DOI 10.1016/j.neuron.2018.10.021
   Hildebrand DGC, 2017, NATURE, V545, P345, DOI 10.1038/nature22356
   Kunst M, 2019, NEURON, V103, P21, DOI 10.1016/j.neuron.2019.04.034
   Kuss A, 2010, COMPUT GRAPH FORUM, V29, P1003, DOI 10.1111/j.1467-8659.2009.01703.x
   KuSS A., 2008, P EUR WORKSH VIS COM, P177
   Leventidis A, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P2303, DOI 10.1145/3318464.3389767
   Likert R., 1932, ARCH PSYCHOL, V22, P55
   Lin CY, 2011, IEEE PAC VIS SYMP, P35, DOI 10.1109/PACIFICVIS.2011.5742370
   Margulies DS, 2013, NEUROIMAGE, V80, P445, DOI 10.1016/j.neuroimage.2013.04.111
   Matejek B, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-15027-7
   Maye A, 2006, INT J NEUROSCI, V116, P431, DOI 10.1080/00207450500505860
   Milyaev N, 2012, BIOINFORMATICS, V28, P411, DOI 10.1093/bioinformatics/btr677
   Nobre C, 2019, COMPUT GRAPH FORUM, V38, P807, DOI 10.1111/cgf.13728
   Ohyama T, 2015, NATURE, V520, P633, DOI 10.1038/nature14297
   Pastor L, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104652
   Pfister H., 2014, Scientific Visualization, P221
   Plaza SM, 2022, FRONT NEUROINFORM, V16, DOI 10.3389/fninf.2022.896292
   Randlett O, 2015, NAT METHODS, V12, P1039, DOI 10.1038/NMETH.3581
   Saalfeld S, 2009, BIOINFORMATICS, V25, P1984, DOI 10.1093/bioinformatics/btp266
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Scheibel W, 2020, IVAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 3: IVAPP, P273, DOI 10.5220/0009153902730280
   Schirner M, 2022, NEUROIMAGE, V251, DOI 10.1016/j.neuroimage.2022.118973
   Schulz HJ, 2011, IEEE T VIS COMPUT GR, V17, P393, DOI 10.1109/TVCG.2010.79
   Schwanke S, 2019, NEUROINFORMATICS, V17, P163, DOI 10.1007/s12021-018-9389-6
   Sorger J, 2013, 2013 IEEE SYMPOSIUM ON BIOLOGICAL DATA VISUALIZATION (BIOVIS), P73, DOI 10.1109/BioVis.2013.6664349
   Swoboda N, 2017, COMPUT GRAPH FORUM, V36, P160, DOI 10.1111/cgf.12792
   Swoboda N., 2014, P EUR WORKSH VIX COM, P107
   Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450
   Vehlow C, 2017, COMPUT GRAPH FORUM, V36, P201, DOI 10.1111/cgf.12872
   Yao YY, 2016, NEURON, V89, P598, DOI 10.1016/j.neuron.2015.12.036
NR 61
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3945
EP 3958
DI 10.1109/TVCG.2023.3243668
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700087
PM 37022819
DA 2024-11-06
ER

PT J
AU Leventhal, S
   Gyulassy, A
   Heimann, M
   Pascucci, V
AF Leventhal, Samuel
   Gyulassy, Attila
   Heimann, Mark
   Pascucci, Valerio
TI Exploring Classification of Topological Priors With Machine Learning for
   Feature Extraction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image segmentation; Machine learning; Semantics; Task analysis;
   Labeling; Training; Topology; Computational topology; feature detection;
   graph learning; graph neural networks; machine learning; Morse-Smale
   complex; scientific visualization; segmentation; topological data
   analysis
ID SEGMENTATION; PERSISTENCE; MODEL; SHAPE
AB In many scientific endeavors, increasingly abstract representations of data allow for new interpretive methodologies and conceptualization of phenomena. For example, moving from raw imaged pixels to segmented and reconstructed objects allows researchers new insights and means to direct their studies toward relevant areas. Thus, the development of new and improved methods for segmentation remains an active area of research. With advances in machine learning and neural networks, scientists have been focused on employing deep neural networks such as U-Net to obtain pixel-level segmentations, namely, defining associations between pixels and corresponding/referent objects and gathering those objects afterward. Topological analysis, such as the use of the Morse-Smale complex to encode regions of uniform gradient flow behavior, offers an alternative approach: first, create geometric priors, and then apply machine learning to classify. This approach is empirically motivated since phenomena of interest often appear as subsets of topological priors in many applications. Using topological elements not only reduces the learning space but also introduces the ability to use learnable geometries and connectivity to aid the classification of the segmentation target. In this article, we describe an approach to creating learnable topological elements, explore the application of ML techniques to classification tasks in a number of areas, and demonstrate this approach as a viable alternative to pixel-level classification, with similar accuracy, improved execution time, and requiring marginal training data.
C1 [Leventhal, Samuel; Gyulassy, Attila; Pascucci, Valerio] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
   [Leventhal, Samuel; Gyulassy, Attila; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Heimann, Mark] Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
C3 Utah System of Higher Education; University of Utah; Utah System of
   Higher Education; University of Utah; United States Department of Energy
   (DOE); Lawrence Livermore National Laboratory
RP Leventhal, S (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.; Leventhal, S (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM samlev@cs.utah.edu; jediati@sci.utah.edu; heimann2@llnl.gov;
   pascucci@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022
OI Gyulassy, Attila/0000-0002-6046-8022; pascucci,
   valerio/0000-0002-8877-2042; Leventhal, Samuel/0000-0003-2454-0311
FU NSF OAC [2138811]; NSF CI CoE [2127548]; Department of Energy(DoE)
   [DE-FE0031880]; Intel one API Centers of Excellence at University of
   Utah; Exascale Computing Project [17-SC-20-SC]; NNSA; UT-Battelle, LLC
   [DE-AC05-00OR22725]; U.S. DoE [DE-AC52-07NA27344]; LDRD Program
   [21-ERD-012]
FX This work was supported in part by NSF OAC under Grant 2138811,in part
   by NSF CI CoE under Grant 2127548, in part by Department of Energy(DoE)
   under Grant DE-FE0031880, in part by the Intel one API Centers of
   Excellence at University of Utah, in part by the Exascale Computing
   Project under Grant 17-SC-20-SC, a collaborative effort of the DoE and
   the NNSA,in part by UT-Battelle, LLC under Grant DE-AC05-00OR22725, in
   part by the U.S. DoE by Lawrence Livermore National Laboratory under
   Grant DE-AC52-07NA27344, and in part by LDRD Program under Grant
   21-ERD-012.
CR Acciai L, 2016, NEUROINFORMATICS, V14, P353, DOI 10.1007/s12021-016-9310-0
   Achanta A., 2010, Tech. Rep. 149300
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Acharjya P. P., 2012, Int. J. Comput. Appl., V52, P6
   Banerjee S, 2020, NAT MACH INTELL, V2, P585, DOI 10.1038/s42256-020-0227-9
   Bendich P, 2007, ANN IEEE SYMP FOUND, P536, DOI 10.1109/FOCS.2007.45
   Beucher S., 1931, P IEEE INT C AC SPEE, P1928
   Bhatia H, 2018, J COMPUT CHEM, V39, P936, DOI 10.1002/jcc.25181
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bremer PT, 2011, IEEE T VIS COMPUT GR, V17, P1307, DOI 10.1109/TVCG.2010.253
   Bremer PT, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P139, DOI 10.1109/VISUAL.2003.1250365
   Chen F, 2013, PROC CVPR IEEE, P1870, DOI 10.1109/CVPR.2013.244
   Dey TK, 2019, 27TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2019), P520, DOI 10.1145/3347146.3359348
   Dingding Liu, 2011, Machine Learning and Data Mining in Pattern Recognition. Proceedings 7th International Conference, MLDM 2011, P484, DOI 10.1007/978-3-642-23199-5_36
   Eberle AL, 2015, J MICROSC-OXFORD, V259, P114, DOI 10.1111/jmi.12224
   Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454
   Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1
   Forman R., 2002, Seminaire Lotharingien de Combinatoire, VB48c, P1
   Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195
   Funke J, 2019, IEEE T PATTERN ANAL, V41, P1669, DOI 10.1109/TPAMI.2018.2835450
   Gillette TA, 2011, NEUROINFORMATICS, V9, P233, DOI 10.1007/s12021-011-9117-y
   Gyulassy A, 2006, IEEE T VIS COMPUT GR, V12, P474, DOI 10.1109/TVCG.2006.57
   Gyulassy A., 2018, MSCEER: Morse-Smale complex extraction, exploration, reasoning
   Gyulassy A, 2008, IEEE T VIS COMPUT GR, V14, P1619, DOI 10.1109/TVCG.2008.110
   Gyulassy A, 2019, IEEE T VIS COMPUT GR, V25, P1183, DOI 10.1109/TVCG.2018.2864848
   Hamilton WL, 2017, ADV NEUR IN, V30
   Harer H Edelsbrunner J., 2010, Computational Topology: An Introduction (no. Book, Whole)
   Hebbalaguppe R, 2013, 2013 1ST IEEE WORKSHOP ON USER-CENTERED COMPUTER VISION (UCCV), P19, DOI 10.1109/UCCV.2013.6530803
   Hongjuan G., 2009, Comput. Eng., V35, DOI [10.3969/j.issn.1000-3428.2009.03.090, DOI 10.3969/J.ISSN.1000-3428.2009.03.090]
   Hu XL, 2021, Arxiv, DOI arXiv:2103.09992
   Januszewski M, 2018, NAT METHODS, V15, P605, DOI 10.1038/s41592-018-0049-4
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Kipf TN, 2016, arXiv:1609.02907, P1
   Klibisz A, 2017, LECT NOTES COMPUT SC, V10553, P285, DOI 10.1007/978-3-319-67558-9_33
   Konyushkova K, 2015, IEEE I CONF COMP VIS, P2974, DOI 10.1109/ICCV.2015.340
   Laney D, 2006, IEEE T VIS COMPUT GR, V12, P1053, DOI 10.1109/TVCG.2006.186
   Lindeberg T., 1994, J. Appl. Statist., V21, P270
   Mayer J, 2018, LIGHT-SCI APPL, V7, DOI 10.1038/s41377-018-0068-z
   McDonald T, 2021, IEEE T VIS COMPUT GR, V27, P744, DOI 10.1109/TVCG.2020.3030363
   Moor M., 2020, INT C MACH LEARN, P7045
   Olabarriaga SD, 2001, MED IMAGE ANAL, V5, P127, DOI 10.1016/S1361-8415(00)00041-4
   Pachitariu A. M., 2013, INT C NEURAL INF PRO, P1745
   PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J
   Pape C, 2019, FRONT COMP SCI-SWITZ, V1, DOI 10.3389/fcomp.2019.00006
   Pascucci Valerio., 2010, Topological Methods in Data Analysis and Visualization: Theory, Algorithms, and Applications
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Petruzza S, 2020, IEEE T VIS COMPUT GR, V26, P140, DOI 10.1109/TVCG.2019.2934620
   Pham DL, 2000, ANNU REV BIOMED ENG, V2, P315, DOI 10.1146/annurev.bioeng.2.1.315
   Ramadan H, 2020, COMPUT VIS MEDIA, V6, P355, DOI 10.1007/s41095-020-0177-5
   Ricci P, 2022, PROG BIOPHYS MOL BIO, V168, P52, DOI 10.1016/j.pbiomolbio.2021.07.003
   Robins V, 2011, IEEE T PATTERN ANAL, V33, P1646, DOI 10.1109/TPAMI.2011.95
   Roerdink J. B. T. M., 2000, Fundamenta Informaticae, V41, P187
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shih CT, 2021, FRONT SYST NEUROSCI, V15, DOI 10.3389/fnsys.2021.687182
   Sonka V., 2014, Image Processing, Analysis, andMachine Vision
   Sweet C. P., 1998, FLTK Revision
   Tierny J, 2018, IEEE T VIS COMPUT GR, V24, P832, DOI 10.1109/TVCG.2017.2743938
   van der Merwe EL, 2010, EXP EYE RES, V91, P118, DOI 10.1016/j.exer.2010.04.016
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   Venkat A, 2022, IEEE T VIS COMPUT GR, V28, P76, DOI 10.1109/TVCG.2021.3114819
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Wilburn DB, 2016, J PROTEOMICS, V135, P12, DOI 10.1016/j.jprot.2015.06.007
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Zhao Q, 2020, PR MACH LEARN RES, V108, P2896
   Zhu Y. Yan, 2020, P 34 INT C NEUR INF
NR 66
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3959
EP 3972
DI 10.1109/TVCG.2023.3248632
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700009
PM 37027638
DA 2024-11-06
ER

PT J
AU Bouzbib, E
   Teyssier, M
   Howard, T
   Pacchierotti, C
   Lecuyer, A
AF Bouzbib, Elodie
   Teyssier, Marc
   Howard, Thomas
   Pacchierotti, Claudio
   Lecuyer, Anatole
TI PalmEx: Adding Palmar Force-Feedback for 3D Manipulation With Haptic
   Exoskeleton Gloves
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Exoskeletons; Haptic interfaces; Three-dimensional displays; Taxonomy;
   Training; Wearable computers; Avatars; Artefact; encountered-type of
   haptic device; ETHD; exoskeleton; haptics; on-demand; virtual reality
AB Haptic exoskeleton gloves are a widespread solution for providing force-feedback in Virtual Reality (VR), especially for 3D object manipulations. However, they are still lacking an important feature regarding in-hand haptic sensations: the palmar contact. In this paper, we present PalmEx, a novel approach which incorporates palmar force-feedback into exoskeleton gloves to improve the overall grasping sensations and manual haptic interactions in VR. PalmEx's concept is demonstrated through a self-contained hardware system augmenting a hand exoskeleton with an encountered palmar contact interface - physically encountering the users' palm. We build upon current taxonomies to elicit PalmEx's capabilities for both the exploration and manipulation of virtual objects. We first conduct a technical evaluation optimising the delay between the virtual interactions and their physical counterparts. We then empirically evaluate PalmEx's proposed design space in a user study (n=12) to assess the potential of a palmar contact for augmenting an exoskeleton. Results show that PalmEx offers the best rendering capabilities to perform believable grasps in VR. PalmEx highlights the importance of the palmar stimulation, and provides a low-cost solution to augment existing high-end consumer hand exoskeletons.
C1 [Bouzbib, Elodie; Howard, Thomas; Pacchierotti, Claudio; Lecuyer, Anatole] Univ Rennes, Inria, CNRS, IRISA Rennes, F-35000 Rennes, France.
   [Teyssier, Marc] Leonard Vinci Pole Univ, Res Ctr Paris Def, F-92400 Courbevoie, France.
C3 Inria; Universite de Rennes; Centre National de la Recherche
   Scientifique (CNRS)
RP Bouzbib, E (corresponding author), Univ Rennes, Inria, CNRS, IRISA Rennes, F-35000 Rennes, France.
EM elo.bouzbib@gmail.com; marc.teyssier@devinci.fr; thomas.howard@irisa.fr;
   claudio.pacchierotti@irisa.fr; anatole.lecuyer@inria.fr
RI Bouzbib, Elodie/HKV-5037-2023; Pacchierotti, Claudio/G-7304-2011
OI Bouzbib, Elodie/0000-0002-5221-2991; Teyssier, Marc/0000-0003-2950-7545;
   Howard, Thomas/0000-0003-4904-375X; Pacchierotti,
   Claudio/0000-0002-8006-9168
FU Inria (Associated Teams programme)
FX This work was supported by Inria (Associated Teams programme).
CR Achibet M, 2015, P IEEE VIRT REAL ANN, P63, DOI 10.1109/VR.2015.7223325
   Aggravi Marco, 2018, IEEE Robotics and Automation Letters, V3, P2166, DOI 10.1109/LRA.2018.2810887
   Blake J, 2009, IEEE-ASME T MECH, V14, P606, DOI 10.1109/TMECH.2008.2010934
   Bouzbib E., 2022, P IEEE C VIRT REAL 3, P1
   Bouzbib E., 2021, P 32 C FRANC INT HOM, DOI [10.1145/3450522.3451323, DOI 10.1145/3450522.3451323]
   Bouzit M, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P145, DOI 10.1109/HAPTIC.2002.998952
   Bratt M, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P2716
   Bryson S., 2005, The Visualization Handbook, P413, DOI [10.1016/B978-012387582-2/50023-X, DOI 10.1016/B978-012387582-2/50023-X]
   Choi I, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P986, DOI 10.1109/IROS.2016.7759169
   CUTKOSKY MR, 1989, IEEE T ROBOTIC AUTOM, V5, P269, DOI 10.1109/70.34763
   CUTKOSKY MR, 1990, DEXTROUS ROBOT HANDS, P5
   Dragusanu M, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.706627
   Fang C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376470
   Feix T, 2016, IEEE T HUM-MACH SYST, V46, P66, DOI 10.1109/THMS.2015.2470657
   Gonzalez Eric J., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P732, DOI 10.1145/3472749.3474782
   Kovacs Robert, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P1046, DOI 10.1145/3379337.3415854
   Lawrence D. A., 1996, Proceedings of the ASME Dynamic Systems and Control Division, P441
   Lopes P, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P11, DOI 10.1145/2807442.2807443
   Minamizawa K, 2008, LECT NOTES COMPUT SC, V5024, P458, DOI 10.1007/978-3-540-69057-3_59
   Moon HS, 2023, INT J HUM-COMPUT INT, V39, P2840, DOI 10.1080/10447318.2022.2087000
   Nith R., 2021, P 34 ANN ACM S US IN, P414, DOI DOI 10.1145/3472749.3474759
   Salvato M, 2022, IEEE ROBOT AUTOM LET, V7, P3851, DOI 10.1109/LRA.2022.3148458
   Son B, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P757, DOI 10.1145/3242587.3242656
   Son B, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P486, DOI 10.1145/3242969.3243030
   Steed A, 2020, Arxiv, DOI arXiv:2002.06093
   Teng SY, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P5, DOI 10.1145/3242587.3242628
   Teyssier M, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P307, DOI 10.1145/3332165.3347943
   Tinguy Xavier, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P262, DOI 10.1007/978-3-030-58147-3_29
   Trinitatova D, 2019, SA'19: SIGGRAPH ASIA 2019 XR, P42, DOI 10.1145/3355355.3361896
   Yoshida S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376358
NR 30
TC 2
Z9 2
U1 10
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3973
EP 3980
DI 10.1109/TVCG.2023.3244076
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700061
PM 37022896
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Engel, D
   Hartwig, S
   Ropinski, T
AF Engel, Dominik
   Hartwig, Sebastian
   Ropinski, Timo
TI Monocular Depth Decomposition of Semi-Transparent Volume Renderings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Volume rendering; depth compositing; monocular depth estimation
AB Neural networks have shown great success in extracting geometric information from color images. Especially, monocular depth estimation networks are increasingly reliable in real-world scenes. In this work we investigate the applicability of such monocular depth estimation networks to semi-transparent volume rendered images. As depth is notoriously difficult to define in a volumetric scene without clearly defined surfaces, we consider different depth computations that have emerged in practice, and compare state-of-the-art monocular depth estimation approaches for these different interpretations during an evaluation considering different degrees of opacity in the renderings. Additionally, we investigate how these networks can be extended to further obtain color and opacity information, in order to create a layered representation of the scene based on a single color image. This layered representation consists of spatially separated semi-transparent intervals that composite to the original input rendering. In our experiments we show that existing approaches to monocular depth estimation can be adapted to perform well on semi-transparent volume renderings, which has several applications in the area of scientific visualization, like re-composition with additional objects and labels or additional shading.
C1 [Engel, Dominik; Hartwig, Sebastian; Ropinski, Timo] Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
C3 Ulm University
RP Engel, D (corresponding author), Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
EM dominik.engel@uni-ulm.de; sebastian.hartwig@uni-ulm.de;
   timo.ropinski@uni-ulm.de
OI Engel, Dominik/0000-0002-5766-7215; Ropinski, Timo/0000-0002-7857-5512;
   Hartwig, Sebastian/0000-0001-8642-2789
FU Deutsche Forschungsgemeinschaft (DFG) [391107954]
FX This work was partially supported in part by the Deutsche
   Forschungsgemeinschaft (DFG) under Grant 391107954 (Inviwo).
CR [Anonymous], 2013, International Journal of Robotics Research, V32, P1231, DOI DOI 10.1177/0278364913491297
   Argelaguet F, 2013, COMPUT GRAPH-UK, V37, P121, DOI 10.1016/j.cag.2012.12.003
   Bailey D., 1998, J. Graph. Tools, P1
   Behrendt P., 2017, P EUR WORKSH VIS COM, P159
   Bruckner S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P671
   Bruckner S, 2009, COMPUT GRAPH FORUM, V28, P775, DOI 10.1111/j.1467-8659.2009.01474.x
   Caban JJ, 2008, IEEE T VIS COMPUT GR, V14, P1364, DOI 10.1109/TVCG.2008.169
   Chilamkurthy S, 2018, Arxiv, DOI arXiv:1803.05854
   Choi J, 2021, IEEE INT CONF ROBOT, P467, DOI 10.1109/ICRA48506.2021.9560831
   Correa CD, 2008, IEEE T VIS COMPUT GR, V14, P1380, DOI 10.1109/TVCG.2008.162
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Elmqvist N., 2014, EuroVis -Short Papers, DOI [10.2312/eurovisshort.20141164, DOI 10.2312/EUROVISSHORT.20141164]
   Engel D, 2021, IEEE T VIS COMPUT GR, V27, P1268, DOI 10.1109/TVCG.2020.3030344
   Frey Steffen, 2013, 2013 XXVI Conference on Graphics, Patterns and Images (SIBGRAPI 2013), P123, DOI 10.1109/SIBGRAPI.2013.26
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   KAUFMAN A, 1990, NATO ADV SCI I F-COM, V60, P217
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lee JH, 2019, PROC CVPR IEEE, P9721, DOI 10.1109/CVPR.2019.00996
   Lee M.-K., 2019, arXiv
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lindholm S, 2015, COMPUT GRAPH FORUM, V34, P74, DOI 10.1111/cgf.12460
   Lindholm S, 2014, IEEE T VIS COMPUT GR, V20, P2447, DOI 10.1109/TVCG.2014.2346351
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Maximov M, 2020, PROC CVPR IEEE, P1068, DOI 10.1109/CVPR42600.2020.00115
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   Pajarola R., 2008, P EUR IEEE COMP SOC
   Prassni JS, 2010, IEEE PAC VIS SYMP, P9, DOI 10.1109/PACIFICVIS.2010.5429624
   Ramamonjisoa M, 2020, PROC CVPR IEEE, P14636, DOI 10.1109/CVPR42600.2020.01466
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Ropinski T., 2007, Internal labels as shape cues for medical illustration, P203
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Steenblik R. A., 1987, True Three-Dimensional Imaging Techniques &Display Technologies, P34
   Stoppel S, 2018, IEEE PAC VIS SYMP, P36, DOI 10.1109/PacificVis.2018.00014
   Tikhonova A, 2010, IEEE PAC VIS SYMP, P177, DOI 10.1109/PACIFICVIS.2010.5429595
   Wang J., IEEE Trans. Vis. Comput. Graph., DOI [10.1109/TVCG.2022.3167896.41L, DOI 10.1109/TVCG.2022.3167896.41L]
   Wang LJ, 2020, PROC CVPR IEEE, P538, DOI 10.1109/CVPR42600.2020.00062
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weiss J., 2021, arXiv, DOI 10.48550/arXiv.2106.05429
   Weiss S, 2022, Arxiv, DOI arXiv:2112.01579
   Weiss S, 2022, IEEE T VIS COMPUT GR, V28, P562, DOI 10.1109/TVCG.2021.3114769
   Weiss S, 2022, IEEE T VIS COMPUT GR, V28, P2654, DOI 10.1109/TVCG.2020.3039340
   Weiss S, 2021, IEEE T VIS COMPUT GR, V27, P3064, DOI 10.1109/TVCG.2019.2956697
   Wiebel A., 2011, ZIB-Report
   Wiebel A, 2012, IEEE T VIS COMPUT GR, V18, P2236, DOI 10.1109/TVCG.2012.292
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Zellmann S, 2012, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE 2012, VOL 2, PTS A AND B, P1385
   Zhao YH, 2020, PROC CVPR IEEE, P3327, DOI 10.1109/CVPR42600.2020.00339
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu SJ, 2020, PROC CVPR IEEE, P13113, DOI 10.1109/CVPR42600.2020.01313
NR 53
TC 1
Z9 1
U1 9
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3981
EP 3994
DI 10.1109/TVCG.2023.3245305
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700021
PM 37027532
OA Green Submitted, hybrid
DA 2024-11-06
ER

PT J
AU Liu, XQ
   Li, JT
   Lu, GD
AF Liu, Xinqi
   Li, Jituo
   Lu, Guodong
TI Modeling Realistic Clothing From a Single Image Under Normal Guide
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clothing mask; clothing modeling; normal map; realistic wrinkles
   distribution; single image; stylized cloth
AB We propose a robust and highly realistic clothing modeling method to generate a 3D clothing model with visually consistent clothing style and wrinkles distribution from a single RGB image. Notably, this entire process only takes a few seconds. Our high-quality clothing results benefit from the idea of combining learning and optimization, making it highly robust. First, we use the neural networks to predict the normal map, a clothing mask, and a learning-based clothing model from input images. The predicted normal map can effectively capture high-frequency clothing deformation from image observations. Then, by introducing a normal-guided clothing fitting optimization, the normal maps are used to guide the clothing model to generate realistic wrinkles details. Finally, we utilize a clothing collar adjustment strategy to stylize clothing results using predicted clothing masks. An extended multi-view version of the clothing fitting is naturally developed, which can further improve the realism of the clothing without tedious effort. Extensive experiments have proven that our method achieves state-of-the-art clothing geometric accuracy and visual realism. More importantly, it is highly adaptable and robust to in-the-wild images. Further, our method can be easily extended to multi-view inputs to improve realism. In summary, our method can provide a low-cost and user-friendly solution to achieve realistic clothing modeling.
C1 [Liu, Xinqi; Li, Jituo; Lu, Guodong] Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liu, Xinqi; Li, Jituo; Lu, Guodong] Zhejiang Univ, Robot Inst, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Li, JT (corresponding author), Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Zhejiang, Peoples R China.
EM liuxinqi@zju.edu.cn; jituo_li@zju.edu.cn; lugd@zju.edu.cn
RI Gao, Zihao/KIL-4959-2024; li, jituo/LMP-9875-2024
OI Liu, Xinqi/0000-0002-9105-2417
FU Key Technologies Research and Development Program [2022YFB3303103];
   National Natural Science Foundation of China [52275276]; Research
   Funding of Zhejiang University Robotics Institute
FX This work was supported in part by the Key Technologies Research and
   Development Program under Grant 2022YFB3303103, in part by the National
   Natural Science Foundation of China under Grant 52275276,and in part by
   the Research Funding of Zhejiang University Robotics Institute.
CR Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Baraff D, 2003, ACM T GRAPHIC, V22, P862, DOI 10.1145/882262.882357
   Bertiche H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480479
   Bhatnagar Bharat Lal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P311, DOI 10.1007/978-3-030-58536-5_19
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Boyi Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P18, DOI 10.1007/978-3-030-58565-5_2
   Bozic A, 2021, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR46437.2021.00150
   Bridson S., 2005, P ACM SIGGRAPH EUR S
   Chen JC, 2021, Arxiv, DOI arXiv:2106.13629
   Chen X, 2021, Arxiv, DOI arXiv:1904.02601
   Chen X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11574, DOI 10.1109/ICCV48922.2021.01139
   Chibane Julian, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P717, DOI 10.1007/978-3-030-66096-3_48
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Corona E, 2021, PROC CVPR IEEE, P11870, DOI 10.1109/CVPR46437.2021.01170
   Danerek R, 2017, COMPUT GRAPH FORUM, V36, P269, DOI 10.1111/cgf.13125
   Decaudin P, 2006, COMPUT GRAPH FORUM, V25, P625, DOI 10.1111/j.1467-8659.2006.00982.x
   Deng B., 2020, P COMP VIS ECCV 2020, P612, DOI [10.1007/978-3-030-58571-636, DOI 10.1007/978-3-030-58571-636]
   Dou M., 2017, ACM Trans. Graph., V36, P1
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239500, 10.1145/1276377.1276438]
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gundogdu E, 2022, IEEE T PATTERN ANAL, V44, P181, DOI 10.1109/TPAMI.2020.3010886
   Halimi O., 2022, arXiv
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   He J., 2021, INT C NEURAL INF PRO, P9276
   Heming Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P512, DOI 10.1007/978-3-030-58452-8_30
   Jiang BY, 2022, PROC CVPR IEEE, P5595, DOI 10.1109/CVPR52688.2022.00552
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D.P., 2014, P INT C LEARNING REP
   Kocabas M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11107, DOI 10.1109/ICCV48922.2021.01094
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Li Z, 2022, LECT NOTES COMPUT SC, V13661, P322, DOI 10.1007/978-3-031-19769-7_19
   Liang JB, 2018, COMPUT GRAPH FORUM, V37, P21, DOI 10.1111/cgf.13509
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Moon G, 2022, LECT NOTES COMPUT SC, V13662, P184, DOI 10.1007/978-3-031-20086-1_11
   Muller M., 2008, P 5 WORKSH VIRT REAL, P1
   Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Patel C, 2020, PROC CVPR IEEE, P7363, DOI 10.1109/CVPR42600.2020.00739
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   PROVOT X, 1995, GRAPH INTER, P147
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Saito S, 2021, PROC CVPR IEEE, P2885, DOI 10.1109/CVPR46437.2021.00291
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Santesteban I, 2022, PROC CVPR IEEE, P8130, DOI 10.1109/CVPR52688.2022.00797
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Scholz V, 2005, COMPUT GRAPH FORUM, V24, P439, DOI 10.1111/j.1467-8659.2005.00869.x
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Tiwari Garvita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P1, DOI 10.1007/978-3-030-58580-8_1
   Tiwari G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11688, DOI 10.1109/ICCV48922.2021.01150
   Tiwari L, 2021, IEEE INT CONF COMP V, P1416, DOI 10.1109/ICCVW54120.2021.00163
   Wang HM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459787
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Xiu YL, 2022, PROC CVPR IEEE, P13286, DOI 10.1109/CVPR52688.2022.01294
   Xu HY, 2020, PROC CVPR IEEE, P6183, DOI 10.1109/CVPR42600.2020.00622
   Yu T, 2021, PROC CVPR IEEE, P5742, DOI 10.1109/CVPR46437.2021.00569
   Yu T, 2020, IEEE T PATTERN ANAL, V42, P2523, DOI 10.1109/TPAMI.2019.2928296
   Yuan Y, 2021, PROC CVPR IEEE, P7155, DOI 10.1109/CVPR46437.2021.00708
   Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582
   Zhang HW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11426, DOI 10.1109/ICCV48922.2021.01125
   Zhao H, 2022, PROC CVPR IEEE, P15883, DOI 10.1109/CVPR52688.2022.01544
   Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 78
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3995
EP 4007
DI 10.1109/TVCG.2023.3245583
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700088
PM 37027566
DA 2024-11-06
ER

PT J
AU Wong, KK
   Wang, XB
   Wang, Y
   He, JB
   Zhang, R
   Qu, HM
AF Wong, Kam Kwai
   Wang, Xingbo
   Wang, Yong
   He, Jianben
   Zhang, Rong
   Qu, Huamin
TI Anchorage: Visual Analysis of Satisfaction in Customer Service Videos
   Via Anchor Events
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Videos; Customer services; Behavioral sciences; Customer satisfaction;
   Visual analytics; Visualization; Data visualization; video data; video
   visualization; visual analytics
ID EMOTIONS; FUTURE
AB Delivering customer services through video communications has brought new opportunities to analyze customer satisfaction for quality management. However, due to the lack of reliable self-reported responses, service providers are troubled by the inadequate estimation of customer services and the tedious investigation into multimodal video recordings. We introduce Anchorage, a visual analytics system to evaluate customer satisfaction by summarizing multimodal behavioral features in customer service videos and revealing abnormal operations in the service process. We leverage the semantically meaningful operations to introduce structured event understanding into videos which help service providers quickly navigate to events of their interest. Anchorage supports a comprehensive evaluation of customer satisfaction from the service and operation levels and efficient analysis of customer behavioral dynamics via multifaceted visualization views. We extensively evaluate Anchorage through a case study and a carefully-designed user study. The results demonstrate its effectiveness and usability in assessing customer satisfaction using customer service videos. We found that introducing event contexts in assessing customer satisfaction can enhance its performance without compromising annotation precision. Our approach can be adapted in situations where unlabelled and unstructured videos are collected along with sequential records.
C1 [Wong, Kam Kwai; Wang, Xingbo; He, Jianben; Zhang, Rong; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Wang, Yong] Singapore Management Univ, Singapore 178902, Singapore.
C3 Hong Kong University of Science & Technology; Singapore Management
   University
RP Wang, Y (corresponding author), Singapore Management Univ, Singapore 178902, Singapore.
EM kkwongar@connect.ust.hk; xingbo.wang@connect.ust.hk;
   yongwang@smu.edu.sg; jhebt@connect.ust.hk; rzhangab@connect.ust.hk;
   huamin@cse.ust.hk
RI Wang, Xingbo/JHS-6567-2023
OI Wang, Xingbo/0000-0001-5693-1128; WONG, Kam Kwai/0000-0002-2813-1972
FU  [FSUST19-CWB09]
FX This work was supported in part by under grant FSUST19-CWB09.
CR Al-Otaibi S, 2018, INT J ADV COMPUT SC, V9, P106
   Ando A, 2020, IEEE-ACM T AUDIO SPE, V28, P715, DOI 10.1109/TASLP.2020.2966857
   [Anonymous], 2018, ISO 10004: 2018
   [Anonymous], 2009, P 18 ACM C INF KNOWL
   Blascheck T, 2016, IEEE CONF VIS ANAL, P141, DOI 10.1109/VAST.2016.7883520
   Bredin H, 2020, INT CONF ACOUST SPEE, P7124, DOI [10.1109/ICASSP40776.2020.9052974, 10.1109/icassp40776.2020.9052974]
   Cappers BCM, 2018, IEEE T VIS COMPUT GR, V24, P532, DOI 10.1109/TVCG.2017.2745278
   Chen D, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P3002
   Chen ZT, 2022, IEEE T VIS COMPUT GR, V28, P824, DOI 10.1109/TVCG.2021.3114806
   Cheshin A, 2018, ORGAN BEHAV HUM DEC, V144, P97, DOI 10.1016/j.obhdp.2017.10.002
   de Pinto MG, 2020, IEEE CONF EVOL ADAPT, DOI 10.1109/eais48028.2020.9122698
   Deng D., 2021, P CHI C HUM FACT COM
   Generosi A, 2018, IEEE I C CONS ELECT
   Gleicher M, 2018, IEEE T VIS COMPUT GR, V24, P413, DOI 10.1109/TVCG.2017.2744199
   Gunes H, 2013, IMAGE VISION COMPUT, V31, P120, DOI 10.1016/j.imavis.2012.06.016
   Guo SN, 2022, IEEE T VIS COMPUT GR, V28, P4531, DOI 10.1109/TVCG.2021.3093585
   Guo Y, 2022, IEEE T VIS COMPUT GR, V28, P5091, DOI 10.1109/TVCG.2021.3100413
   Höferlin B, 2015, INFORM VISUAL, V14, P10, DOI 10.1177/1473871613488571
   Kumar S, 2019, INFORM FUSION, V52, P41, DOI 10.1016/j.inffus.2018.11.001
   Kurzhals K, 2016, IEEE T MULTIMEDIA, V18, P2149, DOI 10.1109/TMM.2016.2614184
   Li HT, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445294
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Liljander V, 1997, INT J SERV IND MANAG, V8, P148, DOI 10.1108/09564239710166272
   Lin YT, 2021, IEEE T VIS COMPUT GR, V27, P849, DOI 10.1109/TVCG.2020.3030370
   Liu Z. X., 2019, Design, User Experience, and Usability. Practice and Case Studies, P183
   Liu ZC, 2017, COMPUT GRAPH FORUM, V36, P527, DOI 10.1111/cgf.13208
   Lou J, 2010, PROTEIN ENG DES SEL, V23, P311, DOI 10.1093/protein/gzq001
   Ma CX, 2020, J COMPUT SCI TECH-CH, V35, P576, DOI 10.1007/s11390-020-0271-2
   Magallanes J, 2022, IEEE T VIS COMPUT GR, V28, P901, DOI 10.1109/TVCG.2021.3114868
   Maher K, 2022, IEEE T VIS COMPUT GR, V28, P508, DOI 10.1109/TVCG.2021.3114789
   McDuff D, 2015, IEEE T AFFECT COMPUT, V6, P223, DOI 10.1109/TAFFC.2014.2384198
   McDuff D, 2012, IEEE T AFFECT COMPUT, V3, P456, DOI 10.1109/T-AFFC.2012.19
   Morrow B, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P1, DOI [10.1109/visual.2019.8933582, 10.1109/VISUAL.2019.8933582]
   Nguyen PH, 2019, IEEE T VIS COMPUT GR, V25, P2838, DOI 10.1109/TVCG.2018.2859969
   Oliver R. L., 2010, Satisfaction: A Behavioral Perspective on the Consumer, V2nd, P2
   Peterson R. A., 1992, J. Acad. Marketing Sci., V20, P61, DOI DOI 10.1007/BF02723476
   Polack PJ, 2018, ACM T INTERACT INTEL, V8, DOI 10.1145/3152888
   Liang PP, 2022, Arxiv, DOI arXiv:2207.00056
   Qi D., 2021, arXiv
   González-Rodríguez MR, 2020, TELEMAT INFORM, V51, DOI 10.1016/j.tele.2020.101404
   Saberi M, 2017, BUS PROCESS MANAG J, V23, P574, DOI 10.1108/BPMJ-02-2015-0018
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   See A, 2021, SIGDIAL 2021: 22ND ANNUAL MEETING OF THE SPECIAL INTEREST GROUP ON DISCOURSE AND DIALOGUE (SIGDIAL 2021), P1
   Seng KP, 2018, IEEE T HUM-MACH SYST, V48, P266, DOI 10.1109/THMS.2017.2695613
   Shi CL, 2014, IEEE T VIS COMPUT GR, V20, P1733, DOI 10.1109/TVCG.2014.2346912
   Slim M, 2018, INT MULTICONF SYST, P502, DOI 10.1109/SSD.2018.8570588
   Soure EJ, 2022, IEEE T VIS COMPUT GR, V28, P643, DOI 10.1109/TVCG.2021.3114822
   Sugianto N., 2018, P IEEE 15 INT C ADV, P1
   Tang T, 2022, IEEE T VIS COMPUT GR, V28, P846, DOI 10.1109/TVCG.2021.3114781
   Tronvoll B, 2011, J SERV MANAGE, V22, P111, DOI 10.1108/09564231111106947
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang XB, 2022, IEEE T VIS COMPUT GR, V28, P4609, DOI 10.1109/TVCG.2021.3097709
   Wang XB, 2022, IEEE T VIS COMPUT GR, V28, P802, DOI 10.1109/TVCG.2021.3114794
   Wang XB, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376726
   Wong A., 2004, Manag. Service Quality: An Int. J., V14, P365
   Wu AY, 2020, IEEE T VIS COMPUT GR, V26, P2429, DOI 10.1109/TVCG.2018.2889081
   Xu W, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P117
   Yang LN, 2023, IEEE T VIS COMPUT GR, V29, P1638, DOI 10.1109/TVCG.2021.3128157
   Yang TY, 2019, PROC CVPR IEEE, P1087, DOI 10.1109/CVPR.2019.00118
   Ye N., 2000, P IEEE SYST MAN CYB
   Yeshchenko A, 2022, IEEE T VIS COMPUT GR, V28, P3050, DOI 10.1109/TVCG.2021.3050071
   Yolcu G, 2020, J AMB INTEL HUM COMP, V11, P237, DOI 10.1007/s12652-019-01310-5
   Zeng HP, 2023, IEEE T VIS COMPUT GR, V29, P3685, DOI 10.1109/TVCG.2022.3169175
   Zeng HP, 2021, IEEE T VIS COMPUT GR, V27, P3168, DOI 10.1109/TVCG.2019.2963659
   Zeng HP, 2020, IEEE T VIS COMPUT GR, V26, P927, DOI 10.1109/TVCG.2019.2934656
   Zhang Q, 2020, MARKET SCI, V39, P285, DOI 10.1287/mksc.2019.1215
   Zhang Wei, 2023, IEEE Trans Vis Comput Graph, V29, P756, DOI 10.1109/TVCG.2022.3209483
   Zhou JH, 2023, IEEE T VIS COMPUT GR, V29, P809, DOI 10.1109/TVCG.2022.3209391
NR 68
TC 3
Z9 3
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4008
EP 4022
DI 10.1109/TVCG.2023.3245609
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700092
PM 37027780
OA Green Accepted, Green Submitted
DA 2024-11-06
ER

PT J
AU Fukuoka, M
   Nakamura, F
   Verhulst, A
   Inami, M
   Kitazaki, M
   Sugimoto, M
AF Fukuoka, Masaaki
   Nakamura, Fumihiko
   Verhulst, Adrien
   Inami, Masahiko
   Kitazaki, Michiteru
   Sugimoto, Maki
TI Sensory Attenuation With a Virtual Robotic Arm Controlled Using Facial
   Movements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Manipulators; Robot sensing systems; Rubber; Cognition; Virtual
   environments; Task analysis; Standards; Sensory attenuation; human
   augmentation; robotic arm; virtual reality
ID SELF-INITIATED SOUNDS; BODY OWNERSHIP; INTERNAL-MODEL; AUDITORY ERPS;
   AGENCY; INTEGRATION; PERCEPTION; AWARENESS; HEARING; SPEECH
AB When humans generate stimuli voluntarily, they perceive the stimuli more weakly than those produced by others, which is called sensory attenuation (SA). SA has been investigated in various body parts, but it is unclear whether an extended body induces SA. This study investigated the SA of audio stimuli generated by an extended body. SA was assessed using a sound comparison task in a virtual environment. We prepared robotic arms as extended bodies, and the robotic arms were controlled by facial movements. To evaluate the SA of robotic arms, we conducted two experiments. Experiment 1 investigated the SA of the robotic arms under four conditions. The results showed that robotic arms manipulated by voluntary actions attenuated audio stimuli. Experiment 2 investigated the SA of the robotic arm and innate body under five conditions. The results indicated that the innate body and robotic arm induced SA, while there were differences in the sense of agency between the innate body and robotic arm. Analysis of the results indicated three findings regarding the SA of the extended body. First, controlling the robotic arm with voluntary actions in a virtual environment attenuates the audio stimuli. Second, there were differences in the sense of agency related to SA between extended and innate bodies. Third, the SA of the robotic arm was correlated with the sense of body ownership.
C1 [Fukuoka, Masaaki; Nakamura, Fumihiko; Sugimoto, Maki] Keio Univ, Fac Sci & Technol, Tokyo 1088345, Japan.
   [Verhulst, Adrien] Keio Univ, Sony Comp Sci Labs, Tokyo 1088345, Japan.
   [Inami, Masahiko] Univ Tokyo, Dept Adv Interdisciplinary Studies, Tokyo 1138654, Japan.
   [Kitazaki, Michiteru] Toyohashi Univ Technol, Dept Comp Sci & Engn, Toyohashi 4418580, Japan.
C3 Keio University; Keio University; University of Tokyo; Toyohashi
   University of Technology
RP Fukuoka, M (corresponding author), Keio Univ, Fac Sci & Technol, Tokyo 1088345, Japan.
EM mskifukuoka@imlab.ics.keio.ac.jp; f.nakamura@imlab.ics.keio.ac.jp;
   adrien.verhulst@imlab.ics.keio.ac.jp; inami@star.rcast.u-tokyo.ac.jp;
   mich@tut.jp; sugimoto@imlab.ics.keio.ac.jp
RI Kitazaki, Mitchiteru/AAI-6554-2020
OI INAMI, MASAHIKO/0000-0002-8652-0730; Fukuoka,
   Masaaki/0000-0001-7892-4623; Kitazaki, Michiteru/0000-0003-0966-4842;
   Nakamura, Fumihiko/0000-0001-6285-3963
FU JST ERATO [JP-MJER1701]
FX This project was supported by JST ERATO under Grant JP-MJER1701.
CR Abdi E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134501
   Baess P, 2011, PSYCHOPHYSIOLOGY, V48, P1276, DOI 10.1111/j.1469-8986.2011.01196.x
   Bashford L, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156591
   Bien Z, 2004, AUTON ROBOT, V16, P165, DOI 10.1023/B:AURO.0000016864.12513.77
   Blakemore SJ, 2000, NEUROREPORT, V11, pR11, DOI 10.1097/00001756-200008030-00002
   Blakemore SJ, 1999, J COGNITIVE NEUROSCI, V11, P551, DOI 10.1162/089892999563607
   Bock R.D., 1968, The measurement and prediction of judgment and choice
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Burin D, 2017, J ADV RES, V8, P649, DOI 10.1016/j.jare.2017.08.001
   Burin D, 2017, COGNITION, V166, P164, DOI 10.1016/j.cognition.2017.05.035
   Cardoso-Leite P, 2010, PSYCHOL SCI, V21, P1740, DOI 10.1177/0956797610389187
   Desantis A, 2014, COGNITION, V132, P243, DOI 10.1016/j.cognition.2014.04.010
   Dewey JA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110118
   Divenyi PL, 1997, EAR HEARING, V18, P42, DOI 10.1097/00003446-199702000-00005
   Dogge M, 2019, TRENDS COGN SCI, V23, P743, DOI 10.1016/j.tics.2019.06.008
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Fitts P., 1964, CATEGORIES HUM LEARN, P243
   FRANCIS BA, 1976, AUTOMATICA, V12, P457, DOI 10.1016/0005-1098(76)90006-6
   Frith CD, 2000, PHILOS T R SOC B, V355, P1771, DOI 10.1098/rstb.2000.0734
   Fritsch A, 2021, CONSCIOUS COGN, V88, DOI 10.1016/j.concog.2020.103073
   Fukuoka M., 2020, Trans. Virtual Reality Soc. Jpn., V25, P451
   Fukuoka M, 2019, SA'19: SIGGRAPH ASIA 2019 XR, P9, DOI 10.1145/3355355.3361888
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Guterstam A, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017208
   Hesse MD, 2010, CEREB CORTEX, V20, P425, DOI 10.1093/cercor/bhp110
   Horváth J, 2013, BIOL PSYCHOL, V93, P81, DOI 10.1016/j.biopsycho.2012.12.008
   Horváth J, 2013, PSYCHOPHYSIOLOGY, V50, P266, DOI 10.1111/psyp.12009
   Houde JF, 2002, J COGNITIVE NEUROSCI, V14, P1125, DOI 10.1162/089892902760807140
   Humes LE, 2009, ATTEN PERCEPT PSYCHO, V71, P860, DOI 10.3758/APP.71.4.860
   Hussain I, 2017, INT C REHAB ROBOT, P1177, DOI 10.1109/ICORR.2017.8009409
   Imamizu H, 2012, CEREBELLUM, V11, P325, DOI 10.1007/s12311-010-0241-2
   Inamura T, 2017, ADV ROBOTICS, V31, P97, DOI 10.1080/01691864.2016.1264885
   International Organization for Standardization, 2023, ISO 226:2003
   Jazbec M, 2017, IEEE SYS MAN CYBERN, P1471, DOI 10.1109/SMC.2017.8122821
   Jeunet C, 2018, IEEE T VIS COMPUT GR, V24, P1486, DOI 10.1109/TVCG.2018.2794598
   KAWATO M, 1987, BIOL CYBERN, V57, P169, DOI 10.1007/BF00364149
   Kilteni K, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.100843
   Kilteni K, 2019, ELIFE, V8, DOI 10.7554/eLife.42888
   Kilteni K, 2017, P NATL ACAD SCI USA, V114, P8426, DOI 10.1073/pnas.1703347114
   Kilteni K, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00141
   Kilteni K, 2013, IEEE T VIS COMPUT GR, V19, P597, DOI 10.1109/TVCG.2013.29
   Klaffehn AL, 2019, NEUROPSYCHOLOGIA, V132, DOI 10.1016/j.neuropsychologia.2019.107145
   Laak KJ, 2017, NEUROSCI CONSCIOUS, V3, DOI 10.1093/nc/niw025
   Lindquist K, 2008, ADV PSYCHOL, V139, P167
   Ma K, 2015, CONSCIOUS COGN, V36, P277, DOI 10.1016/j.concog.2015.07.008
   Maindonald J., 2003, Data Analysis and Graphics Using R - an ExampleBased Approach
   Miall RC, 1996, NEURAL NETWORKS, V9, P1265, DOI 10.1016/S0893-6080(96)00035-4
   Mifsud NG, 2017, NEUROPSYCHOLOGIA, V103, P38, DOI 10.1016/j.neuropsychologia.2017.07.019
   Nakamura T., 2021, Determine the subjective point of equivalence (PSE) using R
   Newport R, 2010, EXP BRAIN RES, V204, P385, DOI 10.1007/s00221-009-2104-y
   Oppenheimer DM, 2009, J EXP SOC PSYCHOL, V45, P867, DOI 10.1016/j.jesp.2009.03.009
   OUCHI T, 1994, ACTA OTO-LARYNGOL, P89
   Paillard J., 1993, The use of tools by human and non-human primates, P36
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Pyasik M, 2021, NEUROIMAGE, V229, DOI 10.1016/j.neuroimage.2021.117727
   Reznik D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0127651
   Rosa N, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3341225
   Sanchez-Vives MV, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010381
   Sasaki T., 2017, ACM SIGGRAPH Emerging Technologies, P1, DOI DOI 10.1145/3084822.3084837
   Sato A, 2008, CONSCIOUS COGN, V17, P1219, DOI 10.1016/j.concog.2008.01.003
   Sato Y, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.172170
   Schwarz KA, 2018, J EXP PSYCHOL GEN, V147, P418, DOI 10.1037/xge0000353
   Steptoe W, 2013, IEEE T VIS COMPUT GR, V19, P583, DOI 10.1109/TVCG.2013.32
   Tsakiris M, 2005, J EXP PSYCHOL HUMAN, V31, P80, DOI 10.1037/0096-1523.31.1.80
   United States Department of Labor, 2023, Calculations and application of age corrections to audiograms
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Watt R.J., 1981, CURRENT PSYCHOL REV, V1, P205
   Wei Wang, 2012, Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO), P1733, DOI 10.1109/ROBIO.2012.6491218
   Weiss C, 2011, COGNITION, V121, P207, DOI 10.1016/j.cognition.2011.06.011
   Weller L, 2017, BIOL PSYCHOL, V123, P241, DOI 10.1016/j.biopsycho.2016.12.015
   Wolpe N, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13034
   Wolpert DM, 2000, NAT NEUROSCI, V3, P1212, DOI 10.1038/81497
   WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
   Yuan Y, 2010, P IEEE VIRT REAL ANN, P95, DOI 10.1109/VR.2010.5444807
NR 75
TC 0
Z9 0
U1 5
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4023
EP 4038
DI 10.1109/TVCG.2023.3246092
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700042
PM 37027567
DA 2024-11-06
ER

PT J
AU Garretón, M
   Morini, F
   Celhay, P
   Dörk, M
   Parra, D
AF Garreton, Manuela
   Morini, Francesca
   Celhay, Pablo
   Doerk, Marian
   Parra, Denis
TI Attitudinal Effects of Data Visualizations and Illustrations in Data
   Stories
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Attitude change; data stories; emotions; quantitative and qualitative
   evaluation
ID NEGATIVE AFFECT; NEWS; COMMUNICATION; MEMORABILITY; RECOGNITION;
   PHOTOGRAPHS; VALIDATION; RESPONSES; PICTURE; MEMORY
AB Journalism has become more data-driven and inherently visual in recent years. Photographs, illustrations, infographics, data visualizations, and general images help convey complex topics to a wide audience. The way that visual artifacts influence how readers form an opinion beyond the text is an important issue to research, but there are few works about this topic. In this context, we research the persuasive, emotional and memorable dimensions of data visualizations and illustrations in journalistic storytelling for long-form articles. We conducted a user study and compared the effects which data visualizations and illustrations have on changing attitude towards a presented topic. While visual representations are usually studied along one dimension, in this experimental study, we explore the effects on readers' attitudes along three: persuasion, emotion, and information retention. By comparing different versions of the same article, we observe how attitudes differ based on the visual stimuli present, and how they are perceived when combined. Results indicate that the narrative using only data visualization elicits a stronger emotional impact than illustration-only visual support, as well as a significant change in the initial attitude about the topic. Our findings contribute to a growing body of literature on how visual artifacts may be used to inform and influence public opinion and debate. We present ideas for future work to generalize the results beyond the domain studied, the water crisis.
C1 [Garreton, Manuela; Parra, Denis] Pontificia Univ Catolica Chile, Dept Comp Sci, Santiago 8331150, Chile.
   [Morini, Francesca; Doerk, Marian] Univ Appl Sci Potsdam, Urban Complex Lab, D-14469 Potsdam, Germany.
   [Celhay, Pablo] Pontificia Univ Catolica Chile, Santiago 8331150, Chile.
C3 Pontificia Universidad Catolica de Chile; Pontificia Universidad
   Catolica de Chile
RP Garretón, M (corresponding author), Pontificia Univ Catolica Chile, Dept Comp Sci, Santiago 8331150, Chile.
EM manuela.garreton@uc.cl; morini@fh-potsdam.de; pacelhay@uc.cl;
   doerk@fh-potsdam.de; dparra@ing.puc.cl
RI Parra, Denis/D-1388-2014
OI Parra, Denis/0000-0001-9878-8761; Garreton, Manuela/0000-0003-4626-3984;
   Dork, Marian/0000-0002-3469-7841; Morini, Francesca/0000-0002-5677-8175
FU ANID; Millennium Science Initiative Programs [ICN17_002, ICN2021_004];
   Basal Funds [FB210017]; German Federal Ministry of Education and
   Research; Bundesministerium fur Bildung und Forschungunder Grant [FK
   13FH126PX6]
FX This work was supported in part by ANID - Doctorate Grant and partially
   funded in part by ANID, Millennium Science Initiative Programs under
   Grants ICN17_002 (IMFD) and ICN2021_004 (iHealth), in part by Basal
   Funds under Grant FB210017 (CENIA), and in part by the German Federal
   Ministry of Education and Research, Bundesministerium fur Bildung und
   Forschungunder Grant FK 13FH126PX6.
CR Albarracin D., 2008, Attitudes and Attitude Change
   [Anonymous], 2019, GUARDIAN
   [Anonymous], 2019, The New York Times
   Bach B, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3670, DOI 10.1145/2858036.2858387
   Barrett LF, 1999, CURR DIR PSYCHOL SCI, V8, P10, DOI 10.1111/1467-8721.00003
   Bateman S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2573
   Besancon L., 2017, P 29 C FRANC INT HOM, P11
   Blascheck T, 2019, IEEE T VIS COMPUT GR, V25, P630, DOI 10.1109/TVCG.2018.2865142
   Bless H., 2001, SOCIAL INFLUENCE DIR, P167
   Borgo R, 2012, IEEE T VIS COMPUT GR, V18, P2759, DOI 10.1109/TVCG.2012.197
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Bounegru L., 2012, DATA JOURNALISM HDB
   Boy J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5462, DOI 10.1145/3025453.3025512
   Brantner C, 2011, J MASS COMMUN Q, V88, P523, DOI 10.1177/107769901108800304
   Bucher HJ, 2006, COMMUNICATIONS-GER, V31, P347, DOI 10.1515/COMMUN.2006.022
   Byrne L, 2016, IEEE T VIS COMPUT GR, V22, P509, DOI 10.1109/TVCG.2015.2467321
   CACIOPPO JT, 1984, ADV CONSUM RES, V11, P673
   Campbell Sarah, 2019, Information Design Journal, V25, P71, DOI 10.1075/idj.25.1.06cam
   CARD SK, 1999, READINGS INFORM VISU, P1
   Charmaz K., 2013, Estrategias de investigacion cualitativa, VIII., P270
   D'Ignazio C, 2020, STRONG IDEAS SERIES, P97
   David P, 1998, HUM COMMUN RES, V25, P180, DOI 10.1111/j.1468-2958.1998.tb00442.x
   DIgnazio Catherine, 2016, P WORKSH VIS DIG HUM
   Dragicevic P, 2016, HUM-COMPUT INT-SPRIN, P291, DOI 10.1007/978-3-319-26633-6_13
   Dufey M., 2012, Revista Iberoamericana Diagnstico Evaluacin Avaliao Psicol, V34, P157
   Garry M, 2007, APPL COGNITIVE PSYCH, V21, P995, DOI 10.1002/acp.1362
   Haroz S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1191, DOI 10.1145/2702123.2702275
   Hohman F., 2020, Distill.
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2231, DOI 10.1109/TVCG.2011.255
   Iyer A, 2006, EUR J SOC PSYCHOL, V36, P635, DOI 10.1002/ejsp.316
   Jacobson S, 2016, JOURNALISM, V17, P527, DOI 10.1177/1464884914568079
   Jones J.G., 2017, Persuasion in society
   Kelders SM, 2012, J MED INTERNET RES, V14, P17, DOI 10.2196/jmir.2104
   Kennedy H, 2018, SOCIOLOGY, V52, P830, DOI 10.1177/0038038516674675
   Kensicki L.J., 2001, Journal of Communication Inquiry, V25, P147, DOI [DOI 10.1177/0196859901025002005, https://doi.org/10.1177/0196859901025002005]
   Kim Y.-S., 2019, P CHI C HUM FACT COM, P1
   Kim YS, 2021, IEEE T VIS COMPUT GR, V27, P989, DOI 10.1109/TVCG.2020.3028984
   Kim YS, 2018, IEEE T VIS COMPUT GR, V24, P760, DOI 10.1109/TVCG.2017.2745240
   King C, 2005, JOURNALISM MASS COMM, V82, P623, DOI 10.1177/107769900508200309
   Kong HK, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300576
   Kong HK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174012
   Kosara R, 2013, COMPUTER, V46, P44, DOI 10.1109/MC.2013.36
   Krzywinski M, 2013, NAT METHODS, V10, P921, DOI 10.1038/nmeth.2659
   Liem J, 2020, COMPUT GRAPH FORUM, V39, P277, DOI 10.1111/cgf.13980
   Matthes J, 2009, J MASS COMMUN Q, V86, P349, DOI 10.1177/107769900908600206
   Mitchelstein E., 2009, JOURNALISM, V10, P562, DOI [DOI 10.1177/1464884909106533, 10.1177/1464884909106533]
   Morseletto N, 2017, ENVIRON SCI POLICY, V78, P40, DOI 10.1016/j.envsci.2017.08.021
   Moya M., 1999, Psicologia Social, P153
   Obie HO, 2019, J COMPUT LANG, V52, P113, DOI 10.1016/j.cola.2019.04.006
   Ohira H, 1998, PERS SOC PSYCHOL B, V24, P986, DOI 10.1177/0146167298249006
   Pandey AV, 2014, IEEE T VIS COMPUT GR, V20, P2211, DOI 10.1109/TVCG.2014.2346419
   Parrott S, 2019, J BROADCAST ELECTRON, DOI 10.1080/08838151.2019.1681860
   Peck EM, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300474
   Peña-Araya V, 2020, IEEE T VIS COMPUT GR, V26, P375, DOI 10.1109/TVCG.2019.2934807
   Petty R.E., 2010, Advanced social psychology: The state of the science, P217, DOI DOI 10.1016/B978-0-12-375000-6.00040-9
   Petty RE, 2015, COGNITION EMOTION, V29, P1, DOI 10.1080/02699931.2014.967183
   Powell TE, 2015, J COMMUN, V65, P997, DOI 10.1111/jcom.12184
   Rall K, 2016, J HUM RIGHTS PRACT, V8, P171, DOI 10.1093/jhuman/huw011
   Reardon K. K., 1991, Persuasion in Practice
   Reuters, 2019, The race to save the river ganges
   Riche NH., 2018, Data-driven storytelling, DOI [10.1201/9781315281575, DOI 10.1201/9781315281575]
   Rodriguez L., 2011, J. Vis. Lit., V30, P48, DOI [10.1080/23796529.2011.11674684, DOI 10.1080/23796529.2011.11674684]
   Sawyer A., 1982, Cognitive Responses in Persuasion, P237
   Schneider Birgit., 2014, Image Politics of Climate Change: Visualizations, Imaginations, Documentations
   Segel E, 2010, IEEE T VIS COMPUT GR, V16, P1139, DOI 10.1109/TVCG.2010.179
   Svicarovic L., 2021, P EUROVIS SHORT PAP, P79
   Thompson ER, 2007, J CROSS CULT PSYCHOL, V38, P227, DOI 10.1177/0022022106297301
   TindallFord S, 1997, J EXP PSYCHOL-APPL, V3, P257, DOI 10.1037/1076-898X.3.4.257
   van Beek L, 2020, ENVIRON SCI POLICY, V114, P497, DOI 10.1016/j.envsci.2020.09.011
   Wang ZZ, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300483
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Weber W., 2018, Stud Commun Sci, V18, P191, DOI [10.24434/j.scoms.2018.01.013, DOI 10.24434/J.SCOMS.2018.01.013]
NR 74
TC 2
Z9 2
U1 4
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4039
EP 4054
DI 10.1109/TVCG.2023.3248319
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700075
PM 37027584
OA hybrid
DA 2024-11-06
ER

PT J
AU Zhao, MY
   Huang, XS
   Jiang, JE
   Mou, LT
   Yan, DM
   Ma, L
AF Zhao, Mingyang
   Huang, Xiaoshui
   Jiang, Jingen
   Mou, Luntian
   Yan, Dong-Ming
   Ma, Lei
TI Accurate Registration of Cross-Modality Geometry via Consistent
   Clustering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud compression; Geometry; Three-dimensional displays; Laser
   radar; Tensors; Clustering algorithms; Solid modeling; Cross-modality
   geometry; point cloud registration; 3D reconstruction; adaptive fuzzy
   clustering; CAD
ID ITERATIVE CLOSEST POINT; ICP
AB The registration of unitary-modality geometric data has been successfully explored over past decades. However, existing approaches typically struggle to handle cross-modality data due to the intrinsic difference between different models. To address this problem, in this article, we formulate the cross-modality registration problem as a consistent clustering process. First, we study the structure similarity between different modalities based on an adaptive fuzzy shape clustering, from which a coarse alignment is successfully operated. Then, we optimize the result using fuzzy clustering consistently, in which the source and target models are formulated as clustering memberships and centroids, respectively. This optimization casts new insight into point set registration, and substantially improves the robustness against outliers. Additionally, we investigate the effect of fuzzier in fuzzy clustering on the cross-modality registration problem, from which we theoretically prove that the classical Iterative Closest Point (ICP) algorithm is a special case of our newly defined objective function. Comprehensive experiments and analysis are conducted on both synthetic and real-world cross-modality datasets. Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art approaches with higher accuracy and robustness. Our code is publicly available at https://github.com/zikai1/CrossModReg.
C1 [Zhao, Mingyang] Chinese Acad Sci, Beijing Acad Artificial Intelligence, Inst Automat, Beijing 100045, Peoples R China.
   [Zhao, Mingyang; Jiang, Jingen; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100045, Peoples R China.
   [Huang, Xiaoshui] Shanghai AI Lab, Shanghai 200433, Peoples R China.
   [Jiang, Jingen; Yan, Dong-Ming] Chinese Acad Sci, State Key Lab Multimodal Artificial Intelligence, Inst Automat, Beijing 100045, Peoples R China.
   [Jiang, Jingen; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch AI, Beijing 101408, Peoples R China.
   [Mou, Luntian] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100021, Peoples R China.
   [Ma, Lei] Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
   [Ma, Lei] Peking Univ, Sch Comp Sci, Beijing Acad Artificial Intelligence, Beijing 100871, Peoples R China.
   [Ma, Lei] Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS; Shanghai Artificial
   Intelligence Laboratory; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Beijing University of Technology; Peking
   University; Peking University; Peking University
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100045, Peoples R China.; Yan, DM (corresponding author), Chinese Acad Sci, State Key Lab Multimodal Artificial Intelligence, Inst Automat, Beijing 100045, Peoples R China.; Ma, L (corresponding author), Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.; Ma, L (corresponding author), Peking Univ, Sch Comp Sci, Beijing Acad Artificial Intelligence, Beijing 100871, Peoples R China.; Ma, L (corresponding author), Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
EM zhaomingyang16@mails.ucas.ac.cn; huangxiaoshui@163.com;
   jiangjingen@ia.ac.cn; ltmou@bjut.edu.cn; yandongming@gmail.com;
   lei.ma@pku.edu.cn
RI Mou, Luntian/ACX-6553-2022; Ma, Lei/AEI-0577-2022; Zhao,
   Ming-yang/ABD-1741-2021
OI Ma, Lei/0000-0001-6024-3854; Yan, Dong-Ming/0000-0003-2209-2404; Huang,
   Xiaoshui/0000-0002-3579-538X
FU National Key R&D Program of China [2022ZD0116305]; National Natural
   Science Foundation of China [62172415]; Open Research Fund Program of
   State Key Laboratory of Hydroscience and Engineering, Tsinghua
   University [klhse-2022-D-04]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2022ZD0116305, in part by the National Natural Science
   Foundation of China under Grant 62172415, and in part by the Open
   Research Fund Program of State Key Laboratory of Hydroscience and
   Engineering, Tsinghua University under Grants klhse-2022-D-04.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bolles RC, 1981, IJCAI, P637, DOI DOI 10.5555/1623264.1623272
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Campbell D, 2015, IEEE I CONF COMP VIS, P4292, DOI 10.1109/ICCV.2015.488
   CHARALAMBOUS C, 1976, INT J SYST SCI, V7, P377, DOI 10.1080/00207727608941924
   Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Eckart B, 2018, LECT NOTES COMPUT SC, V11219, P730, DOI 10.1007/978-3-030-01267-0_43
   Ferrari V, 2022, IEEE T VIS COMPUT GR, V28, P1608, DOI 10.1109/TVCG.2020.3021534
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forsyth D.A., 2003, Comput. Vis. Mod. Approach, V17, P21
   Gao W, 2019, PROC CVPR IEEE, P11087, DOI 10.1109/CVPR.2019.01135
   Ghahremani P, 2022, IEEE T VIS COMPUT GR, V28, P4951, DOI 10.1109/TVCG.2021.3109460
   Harders M, 2009, IEEE T VIS COMPUT GR, V15, P138, DOI 10.1109/TVCG.2008.63
   Hartley I., 1997, IEEETrans.PatternAnal. Mach. Intell., V19, P593
   Huang X., 2016, P INT C DIG IM COMP, P1
   Huang XS, 2019, IEEE INT CON MULTI, P1552, DOI 10.1109/ICME.2019.00268
   Huang XS, 2017, IEEE T IMAGE PROCESS, V26, P3261, DOI 10.1109/TIP.2017.2695888
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Jubran Ibrahim., 2021, IEEE INT C COMPUT VI, P13269
   Kwon SH, 1998, ELECTRON LETT, V34, P2176, DOI 10.1049/el:19981523
   Liao QF, 2021, IEEE T PATTERN ANAL, V43, P3229, DOI 10.1109/TPAMI.2020.2978477
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674
   Magnusson M., 2009, Ph.D. dissertation
   Mellado N, 2016, IEEE T VIS COMPUT GR, V22, P2160, DOI 10.1109/TVCG.2015.2505287
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Myronenko X., 2009, IEEETrans.PatternAnal. Mach. Intell., V19, P580
   Pavlov AL, 2018, IEEE INT CONF ROBOT, P3407
   Peng FR, 2014, IEEE IMAGE PROC, P2026, DOI 10.1109/ICIP.2014.7025406
   Pomerleau F, 2015, Foundations and Trends in Robotics, V4, P1, DOI DOI 10.1561/2300000035
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tazir ML, 2018, ROBOT AUTON SYST, V108, P66, DOI 10.1016/j.robot.2018.07.003
   Wang WN, 2007, FUZZY SET SYST, V158, P2095, DOI 10.1016/j.fss.2007.03.004
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Yue, 2019, Advances in Neural Information Processing Systems, V32
   Weinmann M., 2013, ISPRS Annals of the Photogrammetry, Remote Sensing Spatial Information Sciences, V2, P313, DOI [DOI 10.5194/ISPRSANNALS-II-5-W2-313-2013, 10.5194/isprsannals-II-5-W2-313-2013]
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiaoshui Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11363, DOI 10.1109/CVPR42600.2020.01138
   Yan FL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980241
   Yan ZH, 2022, IEEE T VIS COMPUT GR, V28, P4304, DOI 10.1109/TVCG.2021.3086113
   Yang H, 2021, IEEE T ROBOT, V37, P314, DOI 10.1109/TRO.2020.3033695
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang JY, 2022, IEEE T PATTERN ANAL, V44, P3450, DOI 10.1109/TPAMI.2021.3054619
   Zhao MY, 2022, IEEE T IMAGE PROCESS, V31, P7449, DOI 10.1109/TIP.2022.3223793
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
NR 56
TC 4
Z9 4
U1 7
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4055
EP 4067
DI 10.1109/TVCG.2023.3247169
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700043
PM 37027717
DA 2024-11-06
ER

PT J
AU Fu, Q
   Zhang, F
   Li, XM
   Fu, HB
AF Fu, Qiang
   Zhang, Fan
   Li, Xueming
   Fu, Hongbo
TI Magic Furniture: Design Paradigm of Multi-Function Assembly
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Three-dimensional displays; Solid modeling; Urban areas; Rails;
   Merging; Media; Assembly-based modeling; multi-function design; shape
   reconfiguration
AB Assembly-based furniture with movable parts enables shape and structure reconfiguration, thus supporting multiple functions. Although a few attempts have been made for facilitating the creation of multi-function objects, designing such a multi-function assembly with the existing solutions often requires high imagination of designers. We develop the Magic Furniture system for users to easily create such designs simply given multiple cross-category objects. Our system automatically leverages the given objects as references to generate a 3D model with movable boards driven by back-and-forth movement mechanisms. By controlling the states of these mechanisms, a designed multi-function furniture object can be reconfigured to approximate the shapes and functions of the given objects. To ensure the designed furniture easy to transform between different functions, we perform an optimization algorithm to choose a proper number of movable boards and determine their shapes and sizes, following a set of design guidelines. We demonstrate the effectiveness of our system through various multi-function furniture designed with different sets of reference inputs and various movement constraints. We also evaluate the design results through several experiments including comparative and user studies.
C1 [Fu, Qiang; Zhang, Fan; Li, Xueming] Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing 100876, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Beijing University of Posts & Telecommunications; City University of
   Hong Kong
RP Fu, Q (corresponding author), Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing 100876, Peoples R China.
EM fu.john.qiang@gmail.com; zhangfan.kanv@gmail.com; lixm@bupt.edu.cn;
   fuplus@gmail.com
RI Fu, Qiang/AAF-2612-2021; Li, Xueming/W-8707-2019
OI LI, xueming/0000-0003-1058-2799; Fu, Qiang/0000-0002-8944-8981; Zhang,
   Fan/0000-0003-3486-6040; FU, Hongbo/0000-0002-0284-726X
FU NSFC [61902032]
FX This work was supported by NSFC under Grant 61902032.
CR Alhashim I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601102
   Bennell JA, 2008, EUR J OPER RES, V184, P397, DOI 10.1016/j.ejor.2006.11.038
   Fu CW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766892
   Fu Q, 2017, IEEE T VIS COMPUT GR, V23, P2574, DOI 10.1109/TVCG.2017.2739159
   Fu Q, 2016, COMPUT GRAPH FORUM, V35, P27, DOI 10.1111/cgf.12808
   Gao B, 2016, COMPUT GRAPH-UK, V58, P102, DOI 10.1016/j.cag.2016.05.006
   Garg A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925900
   Hu RZ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201287
   Hu RZ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130811
   Hu RZ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925870
   Hu RZ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766914
   Huang SS, 2016, IEEE T VIS COMPUT GR, V22, P2024, DOI 10.1109/TVCG.2015.2473845
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Koo BJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661289
   Li H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766939
   Li HH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366177
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Mitra NJ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778795
   Saul G, 2011, TEI 2011: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION, P73
   Schulz A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601127
   Song P, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130803
   Su XY, 2016, COMPUT GRAPH-UK, V54, P145, DOI 10.1016/j.cag.2015.06.009
   Suzuki R, 2020, TEI'20: PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION, P143, DOI 10.1145/3374920.3374941
   Umetani N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185582
   Wang ZQ, 2021, COMPUT GRAPH FORUM, V40, P633, DOI 10.1111/cgf.142660
   Wang ZQ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356489
   Wang ZQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275034
   Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091
   Xiong GM, 2021, IEEE T VIS COMPUT GR, V27, P4413, DOI 10.1109/TVCG.2020.3005680
   Zhou J, 2018, COMPUT GRAPH-UK, V70, P165, DOI 10.1016/j.cag.2017.07.033
NR 30
TC 2
Z9 2
U1 4
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4068
EP 4079
DI 10.1109/TVCG.2023.3250488
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700057
PM 37028007
DA 2024-11-06
ER

PT J
AU Qu, LZ
   Shang, JX
   Han, XG
   Fu, HB
AF Qu, Linzi
   Shang, Jiaxiang
   Han, Xiaoguang
   Fu, Hongbo
TI ReenactArtFace: Artistic Face Image Reenactment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Faces; Three-dimensional displays; Image reconstruction; Videos;
   Geometry; Generators; Fitting; 3DMM; artistic faces; face reenactment;
   generative models
AB Large-scale datasets and deep generative models have enabled impressive progress in human face reenactment. Existing solutions for face reenactment have focused on processing real face images through facial landmarks by generative models. Different from real human faces, artistic human faces (e.g., those in paintings, cartoons, etc.) often involve exaggerated shapes and various textures. Therefore, directly applying existing solutions to artistic faces often fails to preserve the characteristics of the original artistic faces (e.g., face identity and decorative lines along face contours) due to the domain gap between real and artistic faces. To address these issues, we present ReenactArtFace, the first effective solution for transferring the poses and expressions from human videos to various artistic face images. We achieve artistic face reenactment in a coarse-to-fine manner. First, we perform 3D artistic face reconstruction, which reconstructs a textured 3D artistic face through a 3D morphable model (3DMM) and a 2D parsing map from an input artistic image. The 3DMM can not only rig the expressions better than facial landmarks but also render images under different poses/expressions as coarse reenactment results robustly. However, these coarse results suffer from self-occlusions and lack contour lines. Second, we thus perform artistic face refinement by using a personalized conditional adversarial generative model (cGAN) fine-tuned on the input artistic image and the coarse reenactment results. For high-quality refinement, we propose a contour loss to supervise the cGAN to faithfully synthesize contour lines. Quantitative and qualitative experiments demonstrate that our method achieves better results than the existing solutions.
C1 [Qu, Linzi; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Shang, Jiaxiang] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Han, Xiaoguang] Chinese Univ Hong Kong, SSE, Shenzhen 518172, Peoples R China.
C3 City University of Hong Kong; Hong Kong University of Science &
   Technology; The Chinese University of Hong Kong, Shenzhen
RP Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
EM linziqu2-c@my.cityu.edu.hk; jshang@cse.ust.hk; hanxiaoguang@cuhk.edu.cn;
   hongbofu@cityu.edu.hk
OI FU, Hongbo/0000-0002-0284-726X; Han, Xiaoguang/0000-0003-0162-3296;
   Shang, Jiaxiang/0000-0001-7161-9765; Qu, Linzi/0000-0002-8731-5501
FU Adobe; Centre for Applied Computing and Interactive Media (ACIM) of the
   School of Creative Media, City University of Hong Kong
FX This work was supported by unrestricted gifts from Adobe and the Centre
   for Applied Computing and Interactive Media (ACIM) of the School of
   Creative Media, City University of Hong Kong.
CR Averbuch-Elor H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130818
   Burkov E, 2020, PROC CVPR IEEE, P13783, DOI 10.1109/CVPR42600.2020.01380
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Chen Zhuo, 2020, P IEEE CVF C COMP VI, P13518
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Fiser J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073660
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Futschik D, 2021, COMPUT GRAPH FORUM, V40, P563, DOI 10.1111/cgf.142655
   Gecer B, 2021, PROC CVPR IEEE, P7624, DOI 10.1109/CVPR46437.2021.00754
   Goodfellow I., 2014, Adv. NeuralInf. Process. Syst., V2
   Ha S, 2020, AAAI CONF ARTIF INTE, V34, P10893
   Heusel H., 2017, inProc. Adv. Neural Inf. Process. Syst., V45
   Huang PH, 2020, PROC CVPR IEEE, P7082, DOI 10.1109/CVPR42600.2020.00711
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiaxiang Shang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P53, DOI 10.1007/978-3-030-58555-6_4
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Lin YM, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104190
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Mirza M, 2014, Arxiv, DOI arXiv:1411.1784
   Nagano K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356568
   Nagrani A, 2018, Arxiv, DOI arXiv:1706.08612
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Rao S., 2020, Proc. SIGGRAPH Asia Posters, P1
   Ren YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13739, DOI 10.1109/ICCV48922.2021.01350
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Siarohin A, 2021, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR46437.2021.01344
   Siarohin S., 2019, Adv. Neural Inf. Process.Syst.
   Sorkine O., 2004, P S GEOM PROC, P175, DOI [DOI 10.1145/1057432.1057456, 10.]
   Sun P, 2021, Arxiv, DOI arXiv:2011.00269
   Texler A, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451270
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tripathy S, 2021, IEEE WINT CONF APPL, P1328, DOI 10.1109/WACV48630.2021.00137
   Tu XG, 2022, IEEE T CIRC SYST VID, V32, P1805, DOI 10.1109/TCSVT.2021.3083257
   Wang D., 2022, arXiv
   Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Yaniv J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322984
   Yao GM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1773, DOI 10.1145/3394171.3413865
   Zakharov Egor, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P524, DOI 10.1007/978-3-030-58610-2_31
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhang Y., 2019, arXiv
   Zhou H, 2020, PROC CVPR IEEE, P5910, DOI 10.1109/CVPR42600.2020.00595
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 53
TC 1
Z9 1
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4080
EP 4092
DI 10.1109/TVCG.2023.3253184
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700001
PM 37028041
DA 2024-11-06
ER

PT J
AU Zhao, JQ
   Wang, YX
   Mancenido, MV
   Chiou, EK
   Maciejewski, R
AF Zhao, Jieqiong
   Wang, Yixuan
   Mancenido, Michelle V.
   Chiou, Erin K.
   Maciejewski, Ross
TI Evaluating the Impact of Uncertainty Visualization on Model Reliance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Uncertainty; Predictive models; Task analysis; Data visualization; Data
   models; Computational modeling; Prediction algorithms; Human-machine
   collaborations; model reliance; trust; uncertainty
ID DECISION-MAKING; AUTOMATION; TRUST; ERROR; COMPLACENCY; ALGORITHMS;
   ENCODINGS; HUMANS; DESIGN; BIAS
AB Machine learning models have gained traction as decision support tools for tasks that require processing copious amounts of data. However, to achieve the primary benefits of automating this part of decision-making, people must be able to trust the machine learning model's outputs. In order to enhance people's trust and promote appropriate reliance on the model, visualization techniques such as interactive model steering, performance analysis, model comparison, and uncertainty visualization have been proposed. In this study, we tested the effects of two uncertainty visualization techniques in a college admissions forecasting task, under two task difficulty levels, using Amazon's Mechanical Turk platform. Results show that (1) people's reliance on the model depends on the task difficulty and level of machine uncertainty and (2) ordinal forms of expressing model uncertainty are more likely to calibrate model usage behavior. These outcomes emphasize that reliance on decision support tools can depend on the cognitive accessibility of the visualization technique and perceptions of model performance and task difficulty.
C1 [Zhao, Jieqiong; Wang, Yixuan; Maciejewski, Ross] Arizona State Univ, Sch Comp & Augmented Intelligence, Tempe, AZ 85281 USA.
   [Mancenido, Michelle V.] Arizona State Univ, Sch Math & Nat Sci, Tempe, AZ 85281 USA.
   [Chiou, Erin K.] Arizona State Univ, Human Syst Engn Polytech Sch, Tempe, AZ 85281 USA.
C3 Arizona State University; Arizona State University-Tempe; Arizona State
   University; Arizona State University-Tempe; Arizona State University;
   Arizona State University-Tempe
RP Maciejewski, R (corresponding author), Arizona State Univ, Sch Comp & Augmented Intelligence, Tempe, AZ 85281 USA.
EM Jieqiong.Zhao@asu.edu; ywan1290@asu.edu; mickey.mancenido@asu.edu;
   Erin.Chiou@asu.edu; rmacieje@asu.edu
RI Zhao, Jieqiong/HLH-8586-2023
OI Chiou, Erin/0000-0002-7201-8483; Mancenido,
   Michelle/0000-0002-3000-8922; Zhao, Jieqiong/0000-0002-4303-7722; Wang,
   Yixuan/0000-0003-0195-1193
FU U.S. Department of Homeland Security [2017-ST-061-QA0001,
   17STQAC00001-03-03]; National Science Foundation Program on Fairness;
   Amazon [1939725]
FX This work was supported in part by the U.S. Department of Homeland
   Security under Grants 2017-ST-061-QA0001 and 17STQAC00001-03-03, and in
   part by the National Science Foundation Program on Fairness in AI in
   collaboration with Amazon under Grant 1939725.
CR Alberdi E, 2004, ACAD RADIOL, V11, P909, DOI 10.1016/j.acra.2004.05.012
   Alberdi E, 2008, INT J COMPUT ASS RAD, V3, P115, DOI 10.1007/s11548-008-0213-x
   Bahner J. Elin, 2008, Proceedings of the Human Factors and Ergonomics Society. 52nd Annual Meeting, P1330
   Belia S, 2005, PSYCHOL METHODS, V10, P389, DOI 10.1037/1082-989X.10.4.389
   Bonneau G.-P., 2014, Overview and State-of-the-Art of Uncertainty Visualization, V37, P3, DOI [10.1007/978-1-4471-6497-5_1, DOI 10.1007/978-1-4471-6497-5_1]
   Brodlie K., 2012, Expanding the Frontiers of Visual Analytics and Visualization, P81, DOI [10.1007/978-1-4471-2804-5_6, 10.1007/978-1-4471-2804-56, DOI 10.1007/978-1-4471-2804-56]
   Bussone A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI 2015), P160, DOI 10.1109/ICHI.2015.26
   Cai H., 2010, P HUMAN FACTORS ERGO, V54, P2437
   Conway D., 2016, P ACM C HUM FACT COM, P3035
   Correa Carlos D., 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P51, DOI 10.1109/VAST.2009.5332611
   Correll M, 2014, IEEE T VIS COMPUT GR, V20, P2142, DOI 10.1109/TVCG.2014.2346298
   Dietvorst BJ, 2018, MANAGE SCI, V64, P1155, DOI 10.1287/mnsc.2016.2643
   Dietvorst BJ, 2015, J EXP PSYCHOL GEN, V144, P114, DOI 10.1037/xge0000033
   Dimara E, 2022, IEEE T VIS COMPUT GR, V28, P1128, DOI 10.1109/TVCG.2021.3114813
   Du F, 2016, IEEE CONF VIS ANAL, P61, DOI 10.1109/VAST.2016.7883512
   Dzindolet MT, 2003, INT J HUM-COMPUT ST, V58, P697, DOI 10.1016/S1071-5819(03)00038-7
   Eroglu C, 2010, INT J FORECASTING, V26, P116, DOI 10.1016/j.ijforecast.2009.02.005
   Eyal P, 2022, BEHAV RES METHODS, V54, P1643, DOI 10.3758/s13428-021-01694-3
   Fernandes M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173718
   Fildes R, 2007, INTERFACES, V37, P570, DOI 10.1287/inte.1070.0309
   Fox G. R., 2011, Perspectives on Thinking, Judging, and Decision Making, P21
   Gschwandtner T, 2016, IEEE T VIS COMPUT GR, V22, P539, DOI 10.1109/TVCG.2015.2467752
   Guo SN, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI [10.1145/3290605.3300803, 10.1109/peds44367.2019.8998889]
   Gutzwiller Robert S., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P217, DOI 10.1177/1071181319631201
   Highhouse S, 2008, IND ORGAN PSYCHOL-US, V1, P333, DOI 10.1111/j.1754-9434.2008.00058.x
   Hoekstra R, 2014, PSYCHON B REV, V21, P1157, DOI 10.3758/s13423-013-0572-3
   Hüllermeier E, 2021, MACH LEARN, V110, P457, DOI 10.1007/s10994-021-05946-3
   Hullman J., 2016, PLoS One, V10, P1
   Hullman J, 2020, IEEE T VIS COMPUT GR, V26, P130, DOI 10.1109/TVCG.2019.2934287
   Hullman J, 2019, IEEE T VIS COMPUT GR, V25, P903, DOI 10.1109/TVCG.2018.2864889
   Jacobs M, 2021, TRANSL PSYCHIAT, V11, DOI 10.1038/s41398-021-01224-x
   Jian J.-Y., 2000, Int. J. Cognitive Ergonomics, V4, P53, DOI [10.1207/s15327566ijce040104, DOI 10.1207/S15327566IJCE040104, 10.1207/S15327566IJCE0401_04, DOI 10.1207/S15327566IJCE0401_04, 10.1207/S15327566IJCE040104]
   Kale A, 2021, IEEE T VIS COMPUT GR, V27, P272, DOI 10.1109/TVCG.2020.3030335
   Kay M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5092, DOI 10.1145/2858036.2858558
   Kohn SC, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.604977
   Lai V, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P29, DOI 10.1145/3287560.3287590
   Lawrence M, 2006, INT J FORECASTING, V22, P493, DOI 10.1016/j.ijforecast.2006.03.007
   Lee JD, 2004, HUM FACTORS, V46, P50, DOI 10.1518/hfes.46.1.50.30392
   Logg JM, 2019, ORGAN BEHAV HUM DEC, V151, P90, DOI 10.1016/j.obhdp.2018.12.005
   Mancenido M., 2021, P IISE ANN C, P175
   Manzey D, 2012, J COGN ENG DECIS MAK, V6, P57, DOI 10.1177/1555343411433844
   Merritt SM, 2013, HUM FACTORS, V55, P520, DOI 10.1177/0018720812465081
   Mohseni S., 2021, P INT AAAI C WEB SOC, P421
   Montgomery D.C., 2021, Introduction to Linear Regression Analysis
   Mosier K.L., 2018, Automation and Human Performance: Theory and Applications, P201
   MOSIER KL, 1992, PROCEEDINGS OF THE HUMAN FACTORS SOCIETY, 36TH ANNUAL MEETING, VOLS 1 AND 2, P7, DOI 10.1177/154193129203600104
   Mosier KL, 1998, INT J AVIAT PSYCHOL, V8, P47, DOI 10.1207/s15327108ijap0801_3
   MUIR BM, 1987, INT J MAN MACH STUD, V27, P527, DOI 10.1016/S0020-7373(87)80013-5
   Myers R.H., 2016, Response Surface Methodology: Process and Product Optimization Using Designed Experiments
   Neuburger J, 2017, BMJ QUAL SAF, V26, P919, DOI 10.1136/bmjqs-2016-005526
   Newman GE, 2012, PSYCHON B REV, V19, P601, DOI 10.3758/s13423-012-0247-5
   Padilla LMK, 2021, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.579267
   Parasuraman R, 1997, HUM FACTORS, V39, P230, DOI 10.1518/001872097778543886
   Parasuraman R, 2010, HUM FACTORS, V52, P381, DOI 10.1177/0018720810376055
   Potter K, 2010, COMPUT GRAPH FORUM, V29, P823, DOI 10.1111/j.1467-8659.2009.01677.x
   Procopio M, 2022, IEEE T VIS COMPUT GR, V28, P3093, DOI 10.1109/TVCG.2021.3051013
   Reynolds MR, 1999, J QUAL TECHNOL, V31, P87
   Sacha D, 2016, IEEE T VIS COMPUT GR, V22, P240, DOI 10.1109/TVCG.2015.2467591
   Stahler GJ, 2013, CRIM JUSTICE BEHAV, V40, P690, DOI 10.1177/0093854812469609
   Stevens NT, 2019, J QUAL TECHNOL, V51, P109, DOI 10.1080/00224065.2019.1571344
   Stroup WW., 2012, Generalized linear mixed models: Modern concepts, methods and applications
   Tomsett R, 2020, PATTERNS, V1, DOI 10.1016/j.patter.2020.100049
   Tsai TL, 2003, J AM MED INFORM ASSN, V10, P478, DOI 10.1197/jamia.M1279
   Visser Ewart J., 2014, Virtual, Augmented and Mixed Reality. Designing and Developing Virtual and Augmented Environments. 6th International Conference, VAMR 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8525, P251, DOI 10.1007/978-3-319-07458-0_24
   Vrieze SI, 2009, PROF PSYCHOL-RES PR, V40, P525, DOI 10.1037/a0014693
   Wickens CD, 2015, HUM FACTORS, V57, P728, DOI 10.1177/0018720815581940
   Yang FM, 2020, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2020, P189, DOI 10.1145/3377325.3377480
   Zhang DC, 2019, J BEHAV DECIS MAKING, V32, P152, DOI 10.1002/bdm.2102
NR 68
TC 2
Z9 2
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4093
EP 4107
DI 10.1109/TVCG.2023.3251950
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700055
PM 37028077
DA 2024-11-06
ER

PT J
AU Lin, YN
   Li, HT
   Wu, AY
   Wang, Y
   Qu, HM
AF Lin, Yanna
   Li, Haotian
   Wu, Aoyu
   Wang, Yong
   Qu, Huamin
TI DMiner: Dashboard Design Mining and Recommendation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Layout; Encoding; Feature extraction; Data mining;
   Visualization; Software development management; Dashboards; design
   mining; multiple-view visualization; visualization recommendation
ID MULTIPLE VIEWS
AB Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement, which describes the position, size, and layout of each view in the display space; and coordination, which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders.
C1 [Lin, Yanna; Li, Haotian; Wu, Aoyu; Qu, Huamin] Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
   [Wang, Yong] Singapore Management Univ, Singapore 178902, Singapore.
C3 Hong Kong University of Science & Technology; Singapore Management
   University
RP Lin, YN (corresponding author), Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
EM ylindg@connect.ust.hk; haotian.li@connect.ust.hk; awuac@connect.ust.hk;
   yongwang@smu.edu.sg; huamin@cse.ust.hk
RI wu, au/KLC-5346-2024; Wang, Yong/HKF-3903-2023
OI Wu, Aoyu/0000-0001-9187-9265; Lin, Yanna/0000-0003-3730-0827
FU HK RGC GRF [16210722]; Singapore Ministry of Education (MOE) Academic
   Research Fund (AcRF) Tier 2 [T2EP20222-0049]
FX This work was partially supported by HK RGC GRF under Grant 16210722,
   and in part by the Singapore Ministry of Education (MOE) Academic
   Research Fund (AcRF) Tier 2 under Grant T2EP20222-0049.
CR Al-maneea HM, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P121, DOI [10.1109/VISUAL.2019.8933655, 10.1109/visual.2019.8933655]
   [Anonymous], Binary vector-wikipedia
   [Anonymous], Data Visualization | Microsoft Power BI
   Bach B., 2022, arXiv
   Bartram L., 2002, Information Visualization, V1, P66, DOI 10.1057/palgrave/ivs/9500005
   Cao YR, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517648
   Chen R, 2022, IEEE T VIS COMPUT GR, V28, P4127, DOI 10.1109/TVCG.2021.3076222
   Chen X, 2021, IEEE T VIS COMPUT GR, V27, P1514, DOI 10.1109/TVCG.2020.3030338
   Deng DZ, 2022, Arxiv, DOI arXiv:2208.01232
   Dibia V, 2019, IEEE COMPUT GRAPH, V39, P33, DOI 10.1109/MCG.2019.2924636
   Ding R, 2019, INT CONF MANAGE DATA, P317, DOI 10.1145/3299869.3314037
   Dowding D, 2018, APPL CLIN INFORM, V9, P511, DOI 10.1055/s-0038-1666842
   Forsell C., 2010, Proceedings of the International Conference on Advanced Visual Interfaces, DOI [10.1145/1842993.18430292, DOI 10.1145/1842993.18430292, 10.1145/1842993.1843029., DOI 10.1145/1842993.1843029]
   Frieman JH, 2008, ANN APPL STAT, V2, P916, DOI 10.1214/07-AOAS148
   github, About us
   Greco S., 2016, Multiple Criteria Decision Analysis, P497, DOI DOI 10.1016/j.ecolind.2014.05.014
   Hu K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300358
   Hullman J, 2013, IEEE T VIS COMPUT GR, V19, P2406, DOI 10.1109/TVCG.2013.119
   Key A., 2012, P ACM SIGMOD INT C M, P681, DOI DOI 10.1145/2213836.2213931
   Kim Y, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2628, DOI 10.1145/3025453.3025866
   Langner R, 2019, IEEE T VIS COMPUT GR, V25, P608, DOI 10.1109/TVCG.2018.2865235
   Lee S, 2018, KOREAN J ANESTHESIOL, V71, P353, DOI 10.4097/kja.d.18.00242
   Li HT, 2022, IEEE T VIS COMPUT GR, V28, P195, DOI 10.1109/TVCG.2021.3114863
   Lu M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376263
   Ming Y, 2019, IEEE T VIS COMPUT GR, V25, P342, DOI 10.1109/TVCG.2018.2864812
   Moritz D, 2019, IEEE T VIS COMPUT GR, V25, P438, DOI 10.1109/TVCG.2018.2865240
   North C., 2000, Proceedings of the the working conference on Advanced visual interfaces (AVI) 2000, P128, DOI DOI 10.1145/345513.345282
   Qu ZN, 2018, IEEE T VIS COMPUT GR, V24, P468, DOI 10.1109/TVCG.2017.2744198
   Roberts JC, 1998, IEEE INFOR VIS, P8, DOI 10.1109/IV.1998.694193
   Roberts JC, 2007, CMV 2007: FIFTH INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P61, DOI 10.1109/CMV.2007.20
   Sadana R, 2016, COMPUT GRAPH FORUM, V35, P261, DOI 10.1111/cgf.12902
   Saket B., 2018, arXiv
   Sarikaya A, 2019, IEEE T VIS COMPUT GR, V25, P682, DOI 10.1109/TVCG.2018.2864903
   Satyanarayan A, 2020, IEEE T VIS COMPUT GR, V26, P461, DOI 10.1109/TVCG.2019.2934281
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Shao LD, 2021, J VISUAL-JAPAN, V24, P1237, DOI 10.1007/s12650-021-00781-z
   Shi DQ, 2021, IEEE T VIS COMPUT GR, V27, P453, DOI 10.1109/TVCG.2020.3030403
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Singh C., 2021, imodels: A Python package for fitting interpretable models, V6
   Smith VS, 2013, New Dir Eval, V2013, P21, DOI [DOI 10.1002/EV.200722, DOI 10.1002/EV.20072, 10.1002/ev.20072]
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Stasko J, 2008, INFORM VISUAL, V7, P118, DOI 10.1057/palgrave.ivs.9500180
   Sun MY, 2022, IEEE T VIS COMPUT GR, V28, P4741, DOI 10.1109/TVCG.2021.3102966
   Sun MY, 2022, IEEE T VIS COMPUT GR, V28, P54, DOI 10.1109/TVCG.2021.3114801
   tableau, ABOUT US
   Tufte E. R., 1985, TLS-TIMES LIT SUPPL, V7
   Tundo Alessandro, 2020, 2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), P215, DOI 10.1109/ISSREW51248.2020.00075
   Vartak M, 2015, PROC VLDB ENDOW, V8, P2182, DOI 10.14778/2831360.2831371
   Wang Baldonado M. Q., 2000, P WORK C ADV VIS INT, P110, DOI [DOI 10.1145/345513.345271, 10/cqq2cj]
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P895, DOI 10.1109/TVCG.2019.2934398
   Wongsuphasawat K, 2016, IEEE T VIS COMPUT GR, V22, P649, DOI 10.1109/TVCG.2015.2467191
   Wu AY, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517618
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P162, DOI 10.1109/TVCG.2021.3114826
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
NR 55
TC 3
Z9 3
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4108
EP 4121
DI 10.1109/TVCG.2023.3251344
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700016
PM 37028006
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Narciso, D
   Melo, M
   Rodrigues, S
   Cunha, JP
   Vasconcelos-Raposo, J
   Bessa, M
AF Narciso, David
   Melo, Miguel
   Rodrigues, Susana
   Cunha, Joao Paulo
   Vasconcelos-Raposo, Jose
   Bessa, Maximino
TI Studying the Influence of Multisensory Stimuli on a Firefighting
   Training Virtual Environment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Human factors; Heart rate variability; Visualization; Virtual
   environments; Market research; Cybersickness; Biofeedback; computer
   graphics; professional training; virtual reality
ID ENHANCES REALISTIC RESPONSE; STRESS ASSESSMENT
AB How we perceive and experience the world around us is inherently multisensory. Most of the Virtual Reality (VR) literature is based on the senses of sight and hearing. However, there is a lot of potential for integrating additional stimuli into Virtual Environments (VEs), especially in a training context. Identifying the relevant stimuli for obtaining a virtual experience that is perceptually equivalent to a real experience will lead users to behave the same across environments, which adds substantial value for several training areas, such as firefighters. In this article, we present an experiment aiming to assess the impact of different sensory stimuli on stress, fatigue, cybersickness, Presence and knowledge transfer of users during a firefighter training VE. The results suggested that the stimulus that significantly impacted the user's response was wearing a firefighter's uniform and combining all sensory stimuli under study: heat, weight, uniform, and mask. The results also showed that the VE did not induce cybersickness and that it was successful in the task of transferring knowledge.
C1 [Narciso, David; Cunha, Joao Paulo; Bessa, Maximino] Univ Tras os Montes & Alto Douro UTAD, P-5000801 Vila Real, Portugal.
   [Narciso, David; Melo, Miguel; Rodrigues, Susana; Cunha, Joao Paulo; Vasconcelos-Raposo, Jose; Bessa, Maximino] Inst Syst & Comp Engn Technol & Sci INESC TEC, P-4200465 Porto, Portugal.
   [Cunha, Joao Paulo] Fac Engn Univ Porto FEUP, P-4200465 Porto, Portugal.
C3 University of Tras-os-Montes & Alto Douro; INESC TEC; Universidade do
   Porto
RP Narciso, D (corresponding author), Univ Tras os Montes & Alto Douro UTAD, P-5000801 Vila Real, Portugal.
EM davidnarciso@utad.pt; mcmelo@inesctec.pt;
   susana.c.rodrigues@inesctec.pt; jpcunha@fe.up.pt; jvraposo@utad.pt;
   maxbessa@utad.pt
RI Bessa, Maximino/B-4729-2012; Melo, Miguel/AAN-1855-2020;
   VASCONCELOS-RAPOSO, JOSE/JMB-6306-2023; Cunha, Joao Paulo/F-9039-2010;
   Branco VASCONCELOS-RAPOSO, JOSE Jacinto/G-3743-2010
OI Cunha, Joao Paulo/0000-0003-4131-9045; Branco VASCONCELOS-RAPOSO, JOSE
   Jacinto/0000-0002-3456-9727; Melo, Miguel/0000-0003-4050-3473;
   Rodrigues, Susana/0000-0001-6546-340X; Bessa,
   Maximino/0000-0002-3002-704X
FU European Union [833573]; FCT-Fundacao para a Ciencia ea Tecnologia
   [SFRH/BD/147334/2019]
FX The research leading to these results has received funding fromt he
   European Union's Horizon 2020 - The EU Framework Programme for Research
   and Innovation 2014-2020, under Grant 833573. This work was also
   partially financed by the National funding by FCT-Fundacao para a
   Ciencia ea Tecnologia, through the PhD Research Scholarship
   SFRH/BD/147334/2019.
CR Bhagat KK, 2016, VIRTUAL REAL-LONDON, V20, P127, DOI 10.1007/s10055-016-0284-x
   Biodevices, 2019, Biodevices - Solucoes de Engenharia Biomedica
   Brogni A., 2007, Int. J. Virtual Reality, V6, P1
   Camm AJ, 1996, CIRCULATION, V93, P1043
   Castaldo R, 2015, BIOMED SIGNAL PROCES, V18, P370, DOI 10.1016/j.bspc.2015.02.012
   Clifford G. D., 2006, Advanced Methods and Tools for ECG Data Analysis, P55
   Deniaud C, 2015, 2015 SCIENCE AND INFORMATION CONFERENCE (SAI), P739, DOI 10.1109/SAI.2015.7237225
   Dillon C., 2001, P 4 ANN INT WORKSH P, P21
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Egan D, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Engelbrecht H, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00101
   Frohlich Julia, 2013, Virtual Augmented and Mixed Reality. Designing and Developing Augmented and Virtual Environments. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings: LNCS 7936, P159, DOI 10.1007/978-3-642-39405-8_19
   Gardé A, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188638
   George D., 2019, IBM SPSS STAT 26 STE, DOI DOI 10.4324/9780429056765
   Gonçalves G, 2020, IEEE T VIS COMPUT GR, V26, P3231, DOI 10.1109/TVCG.2019.2926978
   Gupta A, 2018, IEEE INT CON AUTO SC, P433, DOI 10.1109/COASE.2018.8560602
   Hulsmann F., 2014, Virtual Reality Broadcast, V11, P1
   Jang DP, 2002, CYBERPSYCHOL BEHAV, V5, P11, DOI 10.1089/109493102753685845
   Jones S, 2018, PROGR IS, P183, DOI 10.1007/978-3-319-64027-3_13
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Lesage FX, 2012, OCCUP MED-OXFORD, V62, P600, DOI 10.1093/occmed/kqs140
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, pJCMC321, DOI 10.1111/j.1083-6101.1997.tb00072.x
   Martin D, 2022, Arxiv, DOI arXiv:2101.07906
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Melo M, 2022, IEEE T VIS COMPUT GR, V28, P1428, DOI 10.1109/TVCG.2020.3010088
   Mossel A, 2017, P IEEE VIRT REAL ANN, P357, DOI 10.1109/VR.2017.7892324
   Munyan BG III, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157568
   Narciso D., 2022, IEEE Trans. Vis. Comput. Graph., DOI [10.1109/TVCG.20223156734, DOI 10.1109/TVCG.20223156734]
   Narciso D, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380903
   Pulijala Y, 2018, J ORAL MAXIL SURG, V76, P1065, DOI 10.1016/j.joms.2017.10.002
   Ragan ED, 2015, IEEE T VIS COMPUT GR, V21, P794, DOI 10.1109/TVCG.2015.2403312
   Sallnäs EL, 2010, LECT NOTES COMPUT SC, V6192, P178, DOI 10.1007/978-3-642-14075-4_26
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Shaw E, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300856
   Slater M., 1996, VRST'96. Proceedings of the ACM Symposium on Virtual Reality and Technology, P163
   Slater M, 2006, PRESENCE-VIRTUAL AUG, V15, P553, DOI 10.1162/pres.15.5.553
   Slater M, 2009, IEEE COMPUT GRAPH, V29, P76, DOI 10.1109/MCG.2009.55
   Sobel-Fox RM, 2013, J PSYCHOSOC ONCOL, V31, P413, DOI 10.1080/07347332.2013.798760
   Vasconcelos-Raposo J, 2016, PRESENCE-VIRTUAL AUG, V25, P191, DOI 10.1162/PRES_a_00261
   Walter Hannah, 2019, DRUM
   Wheeler SG, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.671664
   Wiederhold B.K., 2001, CYBERPSYCHOLOGY MIND, P175
   Wiederhold BK, 2002, CYBERPSYCHOL BEHAV, V5, P77, DOI 10.1089/109493102753685908
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yu I, 2012, IEEE COMPUT GRAPH, V32, P36, DOI 10.1109/MCG.2012.121
   Zimmons P, 2003, P IEEE VIRT REAL ANN, P293, DOI 10.1109/VR.2003.1191170
NR 46
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4122
EP 4136
DI 10.1109/TVCG.2023.3251188
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700031
PM 37028005
DA 2024-11-06
ER

PT J
AU Suh, A
   Appleby, G
   Anderson, EW
   Finelli, L
   Chang, R
   Cashman, D
AF Suh, Ashley
   Appleby, Gabriel
   Anderson, Erik W.
   Finelli, Luca
   Chang, Remco
   Cashman, Dylan
TI Are Metrics Enough? Guidelines for Communicating and Visualizing
   Predictive Models to Subject Matter Experts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data models; Data visualization; Predictive models; Interviews; Data
   science; Guidelines; Stakeholders; Data communications aspects; human
   factors; modeling and prediction; visualization techniques and
   methodologies
ID DATA-SCIENCE; INNOVATIONS; CHALLENGES; ART
AB Presenting a predictive model's performance is a communication bottleneck that threatens collaborations between data scientists and subject matter experts. Accuracy and error metrics alone fail to tell the whole story of a model - its risks, strengths, and limitations - making it difficult for subject matter experts to feel confident in their decision to use a model. As a result, models may fail in unexpected ways or go entirely unused, as subject matter experts disregard poorly presented models in favor of familiar, yet arguably substandard methods. In this paper, we describe an iterative study conducted with both subject matter experts and data scientists to understand the gaps in communication between these two groups. We find that, while the two groups share common goals of understanding the data and predictions of the model, friction can stem from unfamiliar terms, metrics, and visualizations - limiting the transfer of knowledge to SMEs and discouraging clarifying questions being asked during presentations. Based on our findings, we derive a set of communication guidelines that use visualization as a common medium for communicating the strengths and weaknesses of a model. We provide a demonstration of our guidelines in a regression modeling scenario and elicit feedback on their use from subject matter experts. From our demonstration, subject matter experts were more comfortable discussing a model's performance, more aware of the trade-offs for the presented model, and better equipped to assess the model's risks - ultimately informing and contextualizing the model's use beyond text and numbers.
C1 [Suh, Ashley; Appleby, Gabriel; Chang, Remco] Tufts Univ, Medford, MA 02155 USA.
   [Anderson, Erik W.; Finelli, Luca; Cashman, Dylan] Novartis Pharmaceut, Data Sci & AI, CH-4033 Basel, Switzerland.
C3 Tufts University; Novartis
RP Suh, A (corresponding author), Tufts Univ, Medford, MA 02155 USA.
EM ashleysuh1@gmail.com; gabriel.appleby@tufts.edu;
   erik.anderson@novartis.com; luca.finelli@novartis.com;
   remco@cs.tufts.edu; dylancash88@yahoo.com
RI Cashman, Dylan/ABC-6776-2021
OI Anderson, Erik/0000-0002-0334-8497; Suh, Ashley/0000-0001-6513-8447;
   Cashman, Dylan/0000-0003-4853-5701; Chang, Remco/0000-0002-6484-6430
FU National Science Foundation [IIS1452977, OAC-1940175, OAC-1939945,
   OAC-2118201, NRT-2021874]; DOD [HQ0860-20-C-7137]
FX This work was supported in part by National Science Foundation under
   Grants IIS1452977, OAC-1940175, OAC-1939945, OAC-2118201, NRT-2021874,
   and in part by DOD under Grant HQ0860-20-C-7137.
CR Algorithmia, 2021, 2021 enterprise trends in machine learning
   Almenoff JS, 2007, DRUG SAFETY, V30, P631, DOI 10.2165/00002018-200730070-00013
   Alspaugh S, 2019, IEEE T VIS COMPUT GR, V25, P22, DOI 10.1109/TVCG.2018.2865040
   Bäuerle A, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502102
   Berinato S, 2019, HARVARD BUS REV, V97, P126
   Börner K, 2019, P NATL ACAD SCI USA, V116, P1857, DOI 10.1073/pnas.1807180116
   Braun K., 2006, Qualitative. Psychol. Res., V3, P77, DOI DOI 10.1191/1478088706QP063OA
   Bttinger Michael., 2020, Foundations of Data Visualization, P297, DOI [DOI 10.1007/978-3-030-34444-3, 10.1007/978-3-030-34444-316, DOI 10.1007/978-3-030-34444-316]
   Chatzimparmpas A, 2020, COMPUT GRAPH FORUM, V39, P713, DOI 10.1111/cgf.14034
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Cresswell K, 2013, INT J MED INFORM, V82, pE73, DOI 10.1016/j.ijmedinf.2012.10.007
   Crisan A, 2021, IEEE T VIS COMPUT GR, V27, P1860, DOI 10.1109/TVCG.2020.3030340
   Davis B, 2020, 2020 IEEE WORKSHOP ON TRUST AND EXPERTISE IN VISUAL ANALYTICS (TREX 2020), P1, DOI 10.1109/TREX51495.2020.00005
   DeCuir-Gunby JT, 2011, FIELD METHOD, V23, P136, DOI 10.1177/1525822X10388468
   Donoho D., 1982, P VERS 2 15 18 AUG 1
   Doshi-Velez F, 2017, Arxiv, DOI [arXiv:1702.08608, DOI 10.48550/ARXIV.1702.08608]
   Emanuel EJ, 2019, JAMA-J AM MED ASSOC, V321, P2281, DOI 10.1001/jama.2019.4914
   Evans SJW, 2000, STAT MED, V19, P3199, DOI 10.1002/1097-0258(20001215)19:23<3199::AID-SIM621>3.0.CO;2-Q
   FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797
   Friendly M, 2013, STAT SCI, V28, P1, DOI 10.1214/12-STS402
   Frisch B., 2020, WHAT IT TAK RUN GREA
   Furman J., 2019, Innov. Policy Econ, V19, P161, DOI DOI 10.1086/699936
   Harvey J, 2002, WORK STRESS, V16, P18, DOI 10.1080/02678370110113226
   Hohman F, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P151, DOI [10.1109/visual.2019.8933695, 10.1109/VISUAL.2019.8933695]
   Hohman F, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300809
   Hong Sungsoo Ray, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3392878
   Hopkins A, 2021, AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P134, DOI 10.1145/3461702.3462527
   Weerts HJP, 2019, Arxiv, DOI arXiv:1907.03324
   Jiang F, 2017, STROKE VASC NEUROL, V2, P230, DOI 10.1136/svn-2017-000101
   Kandel S, 2012, IEEE T VIS COMPUT GR, V18, P2917, DOI 10.1109/TVCG.2012.219
   Kessler JS, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P85, DOI 10.18653/v1/P17-4015
   Kosara R, 2013, COMPUTER, V46, P44, DOI 10.1109/MC.2013.36
   Krause J, 2017, IEEE CONF VIS ANAL, P162, DOI 10.1109/VAST.2017.8585720
   Lam H, 2008, IEEE T VIS COMPUT GR, V14, P1149, DOI 10.1109/TVCG.2008.109
   Lipton Z. C., 2018, Queue, V16, P31, DOI [10.1145/3236386.3241340, DOI 10.1145/3236386.3241340]
   Ma KL, 2012, IEEE COMPUT GRAPH, V32, P12, DOI 10.1109/MCG.2012.24
   MacQueen KM., 1998, CAM J, V10, P31, DOI [10.1177/1525822X980100020301, DOI 10.1177/1525822X980100020301]
   Mohseni S, 2020, Arxiv, DOI arXiv:1811.11839
   Mosca A., 2019, Proc. EuroVis-Short Papers
   Nittas Vasileios, 2023, PLOS Digit Health, V2, pe0000189, DOI 10.1371/journal.pdig.0000189
   Pace RK, 1997, STAT PROBABIL LETT, V33, P291, DOI 10.1016/s0167-7152(96)00140-x
   Passi Samir, 2018, Proceedings of the ACM on Human-Computer Interaction, V2, DOI 10.1145/3274405
   Hoffman RR, 2019, Arxiv, DOI arXiv:1812.04608
   Ransbotham S., 2020, MIT Sloan Management Review, P1
   Rudin C, 2022, STAT SURV, V16, P1, DOI 10.1214/21-SS133
   Sacha D, 2019, IEEE T VIS COMPUT GR, V25, P385, DOI 10.1109/TVCG.2018.2864838
   Sambasivan Nithya, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3445518
   Schelp C., 2018, An alternative way to plot the covariance ellipse
   Segel E, 2010, IEEE T VIS COMPUT GR, V16, P1139, DOI 10.1109/TVCG.2010.179
   Seneviratne MG, 2020, BMJ INNOV, V6, P45, DOI 10.1136/bmjinnov-2019-000359
   Shah NH, 2019, JAMA-J AM MED ASSOC, V322, P1351, DOI 10.1001/jama.2019.10306
   Shilo S, 2020, NAT MED, V26, P29, DOI 10.1038/s41591-019-0727-5
   Sievert C., 2020, Interactive Web-Based Data Visualization with R, plotly, and shiny
   Smiciklas M., 2012, The power of infographics: Using pictures to communicate and connect with your audiences
   Suresh H, 2022, IUI'22: 27TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P767, DOI 10.1145/3490099.3511160
   Suresh H, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445088
   Wang DD, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI [10.1145/3290605.3300831, 10.1109/icocn.2019.8934212]
   Webb S, 2018, NATURE, V554, P555, DOI 10.1038/d41586-018-02174-z
   Yang FM, 2020, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2020, P189, DOI 10.1145/3377325.3377480
   Yifan Wu, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P152, DOI 10.1145/3379337.3415851
   Zytek A, 2022, IEEE T VIS COMPUT GR, V28, P1161, DOI 10.1109/TVCG.2021.3114864
NR 61
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4137
EP 4153
DI 10.1109/TVCG.2023.3259341
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700026
PM 37030764
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Kawabe, T
   Ujitoko, Y
AF Kawabe, Takahiro
   Ujitoko, Yusuke
TI Softness Perception of Visual Objects Controlled by Touchless Inputs:
   The Role of Effective Distance of Hand Movements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deformation; Visualization; Cameras; Material properties; Springs;
   Monitoring; Fabrics; Material perception; pseudo-haptics; touchless
   inputs; softness
ID HAPTIC FEEDBACK; SEEING LIQUIDS; RATIO
AB Feedback on the material properties of a visual object is essential in enhancing the users' perceptual experience of the object when users control the object with touchless inputs. Focusing on the softness perception of the object, we examined how the effective distance of hand movements influenced the degree of the object's softness perceived by users. In the experiments, participants moved their right hand in front of a camera which tracked their hand position. A textured 2D or 3D object on display deformed depending on the participant's hand position. In addition to establishing a ratio of deformation magnitude to the distance of hand movements, we altered the effective distance of hand movement, within which the hand movement could deform the object. Participants rated the strength of perceived softness (Experiments 1 and 2) and other perceptual impressions (Experiment 3). A longer effective distance produced a softer impression of the 2D and 3D objects. The saturation speed of object deformation due to the effective distance was not a critical determinant. The effective distance also modulated other perceptual impressions than softness. The role of the effective distance of hand movements on perceptual impressions of objects under touchless control is discussed.
C1 [Kawabe, Takahiro; Ujitoko, Yusuke] NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Kawabe, T (corresponding author), NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
EM takkawabe@gmail.com; yusuke.ujitoko@gmail.com
RI Ujitoko, Yusuke/AAV-2457-2021; Kawabe, Takahiro/V-3676-2017
OI Ujitoko, Yusuke/0000-0001-6059-9324; Kawabe,
   Takahiro/0000-0002-9888-8866
CR Adelson EH, 2001, PROC SPIE, V4299, P1, DOI 10.1117/12.429489
   Argelaguet F, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2501599
   Ban YK, 2018, IEEE HAPTICS SYM, P278, DOI 10.1109/HAPTICS.2018.8357188
   Ban Y, 2014, IEEE HAPTICS SYM, P557, DOI 10.1109/HAPTICS.2014.6775516
   Bi B., 2016, P ACM S APPL PERC, P19, DOI [10.1145/2931002.2931016, DOI 10.1145/2931002.2931016]
   Bi WY, 2019, J VISION, V19, DOI 10.1167/19.5.18
   Bi WY, 2018, J VISION, V18, DOI 10.1167/18.5.12
   Biocca F, 2001, PRESENCE-VIRTUAL AUG, V10, P247, DOI 10.1162/105474601300343595
   Cauquis J, 2022, 28TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2022, DOI 10.1145/3562939.3565607
   De Paolis LT, 2022, VIRTUAL REAL-LONDON, V26, P1551, DOI 10.1007/s10055-022-00647-1
   Dominjon L, 2005, P IEEE VIRT REAL ANN, P19
   Elkin Lisa A., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P754, DOI 10.1145/3472749.3474784
   Fakhoury E, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P88, DOI 10.1109/WHC.2015.7177696
   Fleming R. W., 2016, Annu. Rev. Vis. Sci., V3, P1
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Gaffary Y, 2017, IEEE T VIS COMPUT GR, V23, P2372, DOI 10.1109/TVCG.2017.2735078
   Gaucher P, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P55, DOI 10.1109/3DUI.2013.6550197
   Georgiou O, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3299030
   Greaves GN, 2011, NAT MATER, V10, P823, DOI [10.1038/NMAT3134, 10.1038/nmat3134]
   Habibi P, 2021, INT J HUM-COMPUT ST, V149, DOI 10.1016/j.ijhcs.2021.102600
   Hachisu T., 2011, 2011 IEEE International Symposium on VR Innovation (ISVRI), P327, DOI 10.1109/ISVRI.2011.5759662
   Hachisu T., 2015, Haptic Interaction, P297
   Hachisu T, 2011, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2011)
   Harrington K, 2018, AUTOMOTIVEUI'18: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P11, DOI 10.1145/3239060.3239089
   Hirao Y, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P305, DOI [10.1109/VRW50115.2020.00069, 10.1109/VRW50115.2020.0-207]
   Jung PG, 2015, IEEE T IND INFORM, V11, P485, DOI 10.1109/TII.2015.2405413
   Kawabe T., 2016, P ACM S APPL PERC, P121, DOI DOI 10.1145/2931002.2931008
   Kawabe T, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.811881
   Kawabe T, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.652781
   Kawabe T, 2021, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.612368
   Kawabe T, 2020, IEEE T HAPTICS, V13, P18, DOI 10.1109/TOH.2019.2961883
   Kawabe T, 2015, VISION RES, V109, P125, DOI 10.1016/j.visres.2014.07.003
   Kawasaki T., 2019, Scientific reports, V9, P1, DOI [10.1038/s41598-018-37186-2, DOI 10.1038/S41598-018-37186-2]
   Kim J, 2022, IEEE ACCESS, V10, P5129, DOI 10.1109/ACCESS.2022.3140438
   Le TR, 2015, EUR MICROW CONF, P371, DOI 10.1109/EuMC.2015.7345777
   Lecuyer A., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P83, DOI 10.1109/VR.2000.840369
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Li QS, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-019-57204-1
   Malecki K, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186620
   Mandryk R., 2018, P CHI C HUM FACT COM, P1
   McIntosh J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2332, DOI 10.1145/2858036.2858093
   Ota Yusuke, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P33, DOI 10.1007/978-3-030-58147-3_4
   Paulun VC, 2020, J VISION, V20, DOI 10.1167/jov.20.6.6
   Paulun VC, 2017, J VISION, V17, DOI 10.1167/17.1.20
   Paulun VC, 2015, VISION RES, V115, P163, DOI 10.1016/j.visres.2015.01.023
   Polhemus, 2012, 3SpaceR fastrakR user manual
   Samae M, 2019, BIOMED ENG INT CONF, DOI 10.1145/3290605.3300550
   Sato M., 2016, P ACM SIGGRAPH, P1
   Schmidt F, 2017, J VISION, V17, DOI 10.1167/17.3.18
   Srinivasan M. A., 1996, Proceedings of the ASME Dynamic Systems and Control Division, P555
   Ujitoko Y, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-05864-x
   Ujitoko Y, 2021, IEEE T HAPTICS, V14, P699, DOI 10.1109/TOH.2021.3077619
   Ujitoko Y, 2019, IEEE T VIS COMPUT GR, V25, P1981, DOI 10.1109/TVCG.2019.2898820
   van Assen JJR, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008018
   Van Assen JJR, 2018, CURR BIOL, V28, P452, DOI 10.1016/j.cub.2017.12.037
   Wang DX, 2020, IEEE T IND ELECTRON, V67, P610, DOI 10.1109/TIE.2019.2920602
   Watanabe J., 2013, ITE Trans. Media Technol. Appl., V1, P199
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Yabe S.-I., 2014, P INT C ART REAL TEL, P1
   Zhao N, 2015, 2015 IEEE 12TH INTERNATIONAL CONFERENCE ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS (BSN)
NR 60
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4154
EP 4169
DI 10.1109/TVCG.2023.3254522
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700065
PM 37028284
DA 2024-11-06
ER

PT J
AU Han, D
   Lee, R
   Kim, K
   Kang, H
AF Han, Dongheun
   Lee, Roun
   Kim, Kyeongmin
   Kang, Hyeongyeop
TI VR-HandNet: A Visually and Physically Plausible Hand Manipulation System
   in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tracking; Three-dimensional displays; Shape; Physics; Grasping; Deep
   learning; Visualization; Hand manipulation; physics-based animation;
   reinforcement learning; virtual reality
AB This study aims to allow users to perform dexterous hand manipulation of objects in virtual environments with hand-held VR controllers. To this end, the VR controller is mapped to the virtual hand and the hand motions are dynamically synthesized when the virtual hand approaches an object. At each frame, given the information about the virtual hand, VR controller input, and hand-object spatial relations, the deep neural network determines the desired joint orientations of the virtual hand model in the next frame. The desired orientations are then converted into a set of torques acting on hand joints and applied to a physics simulation to determine the hand pose at the next frame. The deep neural network, named VR-HandNet, is trained with a reinforcement learning-based approach. Therefore, it can produce physically plausible hand motion since the trial-and-error training process can learn how the interaction between hand and object is performed under the environment that is simulated by a physics engine. Furthermore, we adopted an imitation learning paradigm to increase visual plausibility by mimicking the reference motion datasets. Through the ablation studies, we validated the proposed method is effectively constructed and successfully serves our design goal. A live demo is demonstrated in the supplementary video.
C1 [Han, Dongheun; Lee, Roun; Kim, Kyeongmin; Kang, Hyeongyeop] Kyung Hee Univ, Dept Software Convergence, IIIXR LAB, Yongin 17104, Gyeonggi Do, South Korea.
C3 Kyung Hee University
RP Kang, H (corresponding author), Kyung Hee Univ, Dept Software Convergence, IIIXR LAB, Yongin 17104, Gyeonggi Do, South Korea.
EM hand32@khu.ac.kr; dlfhdns@khu.ac.kr; kgm031189@gmail.com;
   siamiz@khu.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kim, KyeongMin/0000-0001-5168-9563; Kang,
   HyeongYeop/0000-0001-5292-4342; Han, DongHeun/0000-0001-7693-7674
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [NRF-2020R1F1A1076528]
FX . This work was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Education under Grant NRF-2020R1F1A1076528.
CR Andrews S, 2013, COMPUT GRAPH-UK, V37, P830, DOI 10.1016/j.cag.2013.04.007
   Andrychowicz M, 2020, INT J ROBOT RES, V39, P3, DOI 10.1177/0278364919887447
   Argelaguet F, 2016, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2016.7504682
   Bergamin K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356536
   Bonardi A, 2020, IEEE ROBOT AUTOM LET, V5, P3533, DOI 10.1109/LRA.2020.2977835
   Chen T, 2021, PR MACH LEARN RES, V164, P297
   Choi I, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174228
   Christen S., 2021, arXiv
   Christen S, 2019, IEEE INT CONF ROBOT, P2161, DOI [10.1109/ICRA.2019.8794065, 10.1109/icra.2019.8794065]
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   Delrieu T, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P266, DOI [10.1109/VR46266.2020.00-58, 10.1109/VR46266.2020.1581332505844]
   Fernández UJ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12094193
   Freeman CD, 2021, Arxiv, DOI arXiv:2106.13281
   Garcia-Hernando G, 2020, IEEE INT C INT ROBOT, P9561, DOI 10.1109/IROS45743.2020.9340947
   Grady P, 2021, PROC CVPR IEEE, P1471, DOI 10.1109/CVPR46437.2021.00152
   Gupta A, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P3786, DOI 10.1109/IROS.2016.7759557
   Haidacher S, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1597, DOI 10.1109/ROBOT.2002.1014771
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   Ho J, 2016, ADV NEUR IN, V29
   Höll M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P175, DOI 10.1109/VR.2018.8448284
   Humberston B., 2015, P 14 ACM SIGGRAPH EU, P63, DOI 10.1145/2786784.2786794
   Jiang HW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11087, DOI 10.1109/ICCV48922.2021.01092
   Jiang YF, 2021, PROCEEDINGS OF ACM SIGGRAPH SYMPOSIUM ON COMPUTER ANIMATION, SCA 2021, DOI 10.1145/3475946.3480950
   Jorg S., 2020, P SIGGRAPH ASIA2020, P1
   KANEKO M, 1994, IEEE T ROBOTIC AUTOM, V10, P355, DOI 10.1109/70.294210
   Karunratanakul K, 2020, INT CONF 3D VISION, P333, DOI 10.1109/3DV50981.2020.00043
   Kim JS, 2019, IEEE INT C INT ROBOT, P3235, DOI [10.1109/iros40897.2019.8968088, 10.1109/IROS40897.2019.8968088]
   Kingma D.P., 2014, P INT C LEARNING REP
   Kumar V, 2016, Arxiv, DOI arXiv:1611.05095
   Lenz I, 2015, INT J ROBOT RES, V34, P705, DOI 10.1177/0278364914549607
   Levine S, 2016, J MACH LEARN RES, V17
   Li Y, 2007, IEEE T VIS COMPUT GR, V13, P732, DOI [10.1109/TVCG.2007.1033, 10.1109/TVCG.2007.1033.]
   Liu CK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531365
   Liu C. Karen, 2008, P SCA 08 SCA 08, P163
   Liu YX, 2018, IEEE INT CONF ROBOT, P1118
   Lougiakis C, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P510, DOI [10.1109/VR46266.2020.1581086151885, 10.1109/VR46266.2020.00-32]
   Mnih V, 2016, PR MACH LEARN RES, V48
   Oculus VR, 2017, Distance grab sample now available in oculus unity sample framework
   Oprea S, 2019, COMPUT GRAPH-UK, V83, P77, DOI 10.1016/j.cag.2019.07.003
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459670, 10.1145/3197517.3201311]
   Radosavovic I, 2021, IEEE INT C INT ROBOT, P7865, DOI 10.1109/IROS51168.2021.9636557
   Rajeswaran A, 2018, Arxiv, DOI [arXiv:1709.10087, DOI 10.48550/ARXIV.1709.10087]
   Schulman J, 2018, Arxiv, DOI [arXiv:1506.02438, 10.48550/arXiv.1506.02438, DOI 10.48550/ARXIV.1506.02438]
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Schulman J, 2015, PR MACH LEARN RES, V37, P1889
   Sermanet P, 2017, Arxiv, DOI arXiv:1612.06699
   Song P, 2018, VISUAL COMPUT, V34, P257, DOI 10.1007/s00371-016-1333-x
   Verschoor M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P183, DOI 10.1109/VR.2018.8447555
   Voigt-Antons JN, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123085
   Wei Quan, 2020, 2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS). Proceedings, P772, DOI 10.1109/ICITBS49701.2020.00169
   Ye YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185537
   Yu H, 2022, MULTIMEDIA SYST, V28, P1845, DOI 10.1007/s00530-021-00874-7
   Zhang H, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322998
   Zhang H, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459830, 10.1145/3448978]
NR 54
TC 4
Z9 4
U1 8
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4170
EP 4182
DI 10.1109/TVCG.2023.3255991
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700036
PM 37028286
DA 2024-11-06
ER

PT J
AU Ji, B
   Pan, Y
   Yan, YC
   Chen, RZ
   Yang, XK
AF Ji, Bin
   Pan, Ye
   Yan, Yichao
   Chen, Ruizhao
   Yang, Xiaokang
TI StyleVR: Stylizing Character Animations With Normalizing Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Character animation; motion generation; style transfer; normalizing
   flow; virtual reality
AB The significance of artistry in creating animated virtual characters is widely acknowledged, and motion style is a crucial element in this process. There has been a long-standing interest in stylizing character animations with style transfer methods. However, this kind of models can only deal with short-term motions and yield deterministic outputs. To address this issue, we propose a generative model based on normalizing flows for stylizing long and aperiodic animations in the VR scene. Our approach breaks down this task into two sub-problems: motion style transfer and stylized motion generation, both formulated as the instances of conditional normalizing flows with multi-class latent space. Specifically, we encode high-frequency style features into the latent space for varied results and control the generation process with style-content labels for disentangled edits of style and content. We have developed a prototype, StyleVR, in Unity, which allows casual users to apply our method in VR. Through qualitative and quantitative comparisons, we demonstrate that our system outperforms other methods in terms of style transfer as well as stochastic stylized motion generation.
C1 [Ji, Bin; Pan, Ye; Yan, Yichao; Chen, Ruizhao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Pan, Y (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
EM bin.ji@sjtu.edu.cn; whitneypanye@sjtu.edu.cn; yanyichao@sjtu.edu.cn;
   stelledge@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Yan, Yichao/ADT-5511-2022
OI Ji, Bin/0000-0002-8981-5251; Chen, Ruizhao/0000-0002-3056-0186
FU Shanghai Sailing Program [20YF1421200]; National Natural Science
   Foundation of China (NSFC) [62102255, 62201342]; Shanghai Municipal
   Science and Technology Major Project [2021SHZDZX0102]
FX This work was supported in part by Shanghai Sailing Program under Grant
   20YF1421200, in part by the National Natural Science Foundation of China
   (NSFC) under Grant 62102255, in part by the National Natural Science
   Foundation of China (NSFC) under Grant 62201342 and in part by Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0102.
CR Gatys LA, 2015, Arxiv, DOI [arXiv:1508.06576, 10.1167/16.12.326, DOI 10.1167/16.12.326]
   Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392469
   Alexanderson G. E., 2020, arXiv
   Alexanderson S, 2020, COMPUT GRAPH FORUM, V39, P487, DOI 10.1111/cgf.13946
   Amaya K, 1996, PROC GRAPH INTERF, P222
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Child R, 2019, Arxiv, DOI arXiv:1904.10509
   Dinh L, 2017, Arxiv, DOI arXiv:1605.08803
   Du Han., 2019, Eurographics 2019 - Short Papers, DOI [10.2312/egs.20191002, DOI 10.2312/EGS.20191002]
   Garcia R., 2019, P 12 ACM SIGGRAPH C, P1
   Harvey FG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392480
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Holden D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392440
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Huang RZ, 2021, Arxiv, DOI arXiv:2006.06119
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Izmailov Pavel, 2020, P MACHINE LEARNING R, V119
   Li PZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530157
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Liu CK, 2005, ACM T GRAPHIC, V24, P1071, DOI 10.1145/1073204.1073314
   Lockwood Noah., 2012, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '12, P43, DOI [10.2312/SCA/SCA12/043-052, DOI 10.2312/SCA/SCA12/043-052]
   Ma L., 2020, P S INT 3D GRAPH GAM, P1
   Mason I, 2018, COMPUT GRAPH FORUM, V37, P143, DOI 10.1111/cgf.13555
   Mason I, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522618
   Kingma DP, 2018, Arxiv, DOI [arXiv:1807.03039, 10.48550/arXiv.1807.03039]
   Pan Y, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P759, DOI [10.1109/VRW50115.2020.00-45, 10.1109/VRW50115.2020.00230]
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Salimans T, 2016, ADV NEUR IN, V29
   Smith HJ, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340254
   Starke S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530178
   Starke S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392450
   Starke S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356505
   Taylor Graham W., 2009, P 26 ANN INT C MACH, P1025
   Thorne M, 2004, ACM T GRAPHIC, V23, P424, DOI 10.1145/1015706.1015740
   Unuma M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P91, DOI 10.1145/218380.218419
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vogel P., 2018, P IEEE C VIRT REAL 3, P1
   Wang ZY, 2021, IEEE T VIS COMPUT GR, V27, P14, DOI 10.1109/TVCG.2019.2938520
   Wen YH, 2021, PROC CVPR IEEE, P13607, DOI 10.1109/CVPR46437.2021.01340
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Xu Z, 2022, PROC CVPR IEEE, P11625, DOI 10.1109/CVPR52688.2022.01134
   Ye H, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392404
   Yumer ME, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925955
   Zhang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201366
NR 49
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4183
EP 4196
DI 10.1109/TVCG.2023.3259183
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700023
PM 37030765
DA 2024-11-06
ER

PT J
AU Mahmood, S
   Mueller, K
AF Mahmood, Salman
   Mueller, Klaus
TI Interactive Subspace Cluster Analysis Guided by Semantic Attribute
   Associations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Semantics; Visual analytics; Data visualization; Three-dimensional
   displays; Task analysis; Standards; Space exploration; Cluster analysis;
   high-dimensional data; multivariate data; subspace clustering; subspace
   analysis
ID HIGH-DIMENSIONAL DATA; VISUAL EXPLORATION
AB Multivariate datasets with many variables are increasingly common in many application areas. Most methods approach multivariate data from a singular perspective. Subspace analysis techniques, on the other hand. provide the user a set of subspaces which can be used to view the data from multiple perspectives. However, many subspace analysis methods produce a huge amount of subspaces, a number of which are usually redundant. The enormity of the number of subspaces can be overwhelming to analysts, making it difficult for them to find informative patterns in the data. In this article, we propose a new paradigm that constructs semantically consistent subspaces. These subspaces can then be expanded into more general subspaces by ways of conventional techniques. Our framework uses the labels/meta-data of a dataset to learn the semantic meanings and associations of the attributes. We employ a neural network to learn a semantic word embedding of the attributes and then divide this attribute space into semantically consistent subspaces. The user is provided with a visual analytics interface that guides the analysis process. We show via various examples that these semantic subspaces can help organize the data and guide the user in finding interesting patterns in the dataset.
C1 [Mahmood, Salman; Mueller, Klaus] SUNY Stony Brook, Comp Sci Dept, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University
RP Mueller, K (corresponding author), SUNY Stony Brook, Comp Sci Dept, Stony Brook, NY 11794 USA.
EM samahmood@cs.stonybrook.edu; mueller@cs.stonybrook.edu
OI Mueller, Klaus/0000-0002-0996-8590
FU NSF [IIS 1941613, IIS1527200]
FX This work was partially supported by NSF under Grants IIS 1941613 and
   IIS1527200.
CR Aggarwal CC, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P61, DOI 10.1145/304181.304188
   Assent I., 2007, ACM SIGKDD Explorations Newsletter, V9, P5, DOI DOI 10.1145/1345448.1345451
   Assent I, 2007, IEEE DATA MINING, P409, DOI 10.1109/ICDM.2007.49
   Baumgartner C, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P11, DOI 10.1109/ICDM.2004.10112
   Chen TL, 2021, J BIOMED INFORM, V113, DOI 10.1016/j.jbi.2020.103665
   Cheng C.-H., 1999, P INT C KNOWL DISC D, P84, DOI [10.1145/312129.312199, DOI 10.1145/312129.312199]
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Erk K., 2009, P 13 C COMP NAT LANG, P65
   Ester M., 1996, P KDD, P226
   Estivill-Castro V., 2002, ACM SIGKDD explorations newsletter, V4, P65, DOI [10.1145/568574.568575, DOI 10.1145/568574.568575]
   Eysenck M.W., 2018, FUNDAMENTALS COGNITI
   Gansner ER, 2009, LECT NOTES COMPUT SC, V5417, P206, DOI 10.1007/978-3-642-00219-9_20
   Gleicher M, 2013, IEEE T VIS COMPUT GR, V19, P2042, DOI 10.1109/TVCG.2013.157
   Greenacre MJ, 2010, Biplots in Practice
   Hartigan J. A., 1975, Journal of Statistical Computation and Simulation, V4, P187, DOI 10.1080/00949657508810123
   Hinton G. E., 1986, Proceedings of the Eighth Annual Conference of the Cognitive Science Society, P1
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Jäckle D, 2017, IEEE CONF VIS ANAL, P1, DOI 10.1109/VAST.2017.8585613
   Jolliffe I., 2022, Principal Component Analysis, P150, DOI [DOI 10.1016/0169-7439(87)80084-9, 10.1007/0-387-22440-87, 10.1007/b98835]
   Kenter T, 2015, P 24 ACM INT C INFOR, P1411, DOI 10.1145/2806416.2806475
   Kim H, 2016, IEEE T VIS COMPUT GR, V22, P131, DOI 10.1109/TVCG.2015.2467615
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Kusner MJ, 2015, PR MACH LEARN RES, V37, P957
   Lee D., 2021, Information, V13
   LEE DT, 1980, INT J COMPUT INF SCI, V9, P219, DOI 10.1007/BF00977785
   Liu S, 2015, COMPUT GRAPH FORUM, V34, P271, DOI 10.1111/cgf.12639
   Mahmood S, 2020, IEEE T VIS COMPUT GR, V26, P2875, DOI 10.1109/TVCG.2019.2895642
   Mikolov T, 2013, 1 INT C LEARN REPR M, DOI DOI 10.48550/ARXIV.1301.3781
   Mikolov T, 2013, Arxiv, DOI arXiv:1301.3781
   Nam EJ, 2007, IEEE CONF VIS ANAL, P75
   Nam JE, 2013, IEEE T VIS COMPUT GR, V19, P291, DOI 10.1109/TVCG.2012.65
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P5509, DOI 10.1109/TNNLS.2020.2968848
   Ren Y., 2022, arXiv, DOI DOI 10.48550/ARXIV.2210.04142
   Risch J, 2019, DATA TECHNOL APPL, V53, P108, DOI 10.1108/DTA-01-2019-0002
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Satopaa V., 2011, Proceedings of the 2011 31st International Conference on Distributed Computing Systems Workshops (ICDCS Workshops), P166, DOI 10.1109/ICDCSW.2011.20
   Schick T, 2019, Arxiv, DOI arXiv:1904.01617
   Tatu Andrada, 2012, Tsinghua Science and Technology, V17, P419
   Tatu A, 2012, IEEE CONF VIS ANAL, P63, DOI 10.1109/VAST.2012.6400488
   Dang TN, 2014, IEEE PAC VIS SYMP, P73, DOI 10.1109/PacificVis.2014.42
   Turkay C, 2011, IEEE T VIS COMPUT GR, V17, P2591, DOI 10.1109/TVCG.2011.178
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang B, 2018, IEEE T VIS COMPUT GR, V24, P1204, DOI 10.1109/TVCG.2017.2672987
   Wang B, 2014, 2014 IEEE VIS INTERNATIONAL WORKSHOP ON 3DVIS (3DVIS), P37, DOI 10.1109/3DVis.2014.7160098
   Wang JP, 2019, INFORM VISUAL, V18, P94, DOI 10.1177/1473871617733996
   Watanabe K, 2015, IEEE PAC VIS SYMP, P287, DOI 10.1109/PACIFICVIS.2015.7156389
   Yuan XR, 2013, IEEE T VIS COMPUT GR, V19, P2625, DOI 10.1109/TVCG.2013.150
   Zhang ZY, 2012, IEEE PAC VIS SYMP, P17
   Zhou FF, 2016, IEEE PAC VIS SYMP, P128, DOI 10.1109/PACIFICVIS.2016.7465260
NR 50
TC 1
Z9 1
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4197
EP 4210
DI 10.1109/TVCG.2023.3256376
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700078
PM 37028285
DA 2024-11-06
ER

PT J
AU Li, XL
   Liu, ZN
   Chen, T
   Mu, TJ
   Martin, RR
   Hu, SM
AF Li, Xiang-Li
   Liu, Zheng-Ning
   Chen, Tuo
   Mu, Tai-Jiang
   Martin, Ralph R.
   Hu, Shi-Min
TI Mesh Neural Networks Based on Dual Graph Pyramids
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neural networks; Convolution; Three-dimensional displays; Feature
   extraction; Faces; Shape; Point cloud compression; Geometric
   understanding; mesh processing; neural networks; shape analysis
ID SHAPE
AB Deep neural networks (DNNs) have been widely used for mesh processing in recent years. However, current DNNs can not process arbitrary meshes efficiently. On the one hand, most DNNs expect 2-manifold, watertight meshes, but many meshes, whether manually designed or automatically generated, may have gaps, non-manifold geometry, or other defects. On the other hand, the irregular structure of meshes also brings challenges to building hierarchical structures and aggregating local geometric information, which is critical to conduct DNNs. In this paper, we present DGNet, an efficient, effective and generic deep neural mesh processing network based on dual graph pyramids; it can handle arbitrary meshes. First, we construct dual graph pyramids for meshes to guide feature propagation between hierarchical levels for both downsampling and upsampling. Second, we propose a novel convolution to aggregate local features on the proposed hierarchical graphs. By utilizing both geodesic neighbors and euclidean neighbors, the network enables feature aggregation both within local surface patches and between isolated mesh components. Experimental results demonstrate that DGNet can be applied to both shape analysis and large-scale scene understanding. Furthermore, it achieves superior performance on various benchmarks, including ShapeNetCore, HumanBody, ScanNet and Matterport3D. Code and models will be available at https://github.com/li-xl/DGNet.
C1 [Li, Xiang-Li; Chen, Tuo; Mu, Tai-Jiang; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Liu, Zheng-Ning] Fitten Tech Co Ltd, Beijing 100084, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci Informat, Cardiff CF10 3AT, Wales.
C3 Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM lixl19@mails.tsinghua.edu.cn; lzhengning@gmail.com; 2509976039@qq.com;
   taijiang@tsinghua.edu.cn; martinrr@cardiff.ac.uk; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Mu, Tai-Jiang/JWO-1381-2024
OI Hu, Shi-Min/0000-0001-7507-6542; Mu, Tai-Jiang/0000-0002-9197-346X; Liu,
   Zhengning/0000-0001-6643-6016; Martin, Ralph/0000-0002-8495-8536
FU National Key R&D Program of China [2021ZD0112902]; Natural Science
   Foundation of China [62220106003]; Research Grant of Beijing Higher
   Institution Engineering Research Center; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This work was supported in part by National Key R&D Program of China
   under Grant 2021ZD0112902, in part by the Natural Science Foundation of
   China under Grant 62220106003, in part by Research Grant of Beijing
   Higher Institution Engineering Research Center and Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology.
CR [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI [DOI 10.2312/3DOR/3DOR11/079-088, 10.2312/3DOR/3DOR11/079-088]
   Boscaini Davide, 2016, ADV NEURAL INFORM PR, P3197
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Chen Y, 2021, IEEE T MULTIMEDIA, V23, P3098, DOI 10.1109/TMM.2020.3020693
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dong QJ, 2023, Arxiv, DOI arXiv:2202.00307
   Engel N, 2021, IEEE ACCESS, V9, P134826, DOI 10.1109/ACCESS.2021.3116304
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13244
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo MH, 2022, Arxiv, DOI arXiv:2209.08575
   Guo MH, 2022, Arxiv, DOI arXiv:2202.09741
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Haim N, 2019, IEEE I CONF COMP VIS, P632, DOI 10.1109/ICCV.2019.00072
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J., 2018, P 2018 IEEE CVF C CO, P7132
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   HU ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15468, DOI 10.1109/ICCV48922.2021.01520
   Huang J, 2016, INT C PATT RECOG, P2670, DOI 10.1109/ICPR.2016.7900038
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Lahav A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417806
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei H, 2021, PROC CVPR IEEE, P13849, DOI 10.1109/CVPR46437.2021.01364
   Li YZ, 2018, ADV NEUR IN, V31
   Liang YQ, 2022, Arxiv, DOI arXiv:2207.10228
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Milano F., 2020, Adv. Neural Inf. Process. Syst, V33, P952
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Pfaff T., 2021, P INT C LEARN REPR
   Pfaff T, 2021, Arxiv, DOI arXiv:2010.03409
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rossignac J., 1993, MODELING COMPUTER GR, P455, DOI [10.1007/978-3-642-78114-8_29, DOI 10.1007/978-3-642-78114-8_29]
   Sanchez-Gonzalez A., 2020, PMLR, P8459
   Savva M., 2016, P EUR WORKSH 3D OBJ, V10
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schult J., 2020, P IEEECVF C COMPUTER, P8612, DOI 10.1109/CVPR42600.2020.00864
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Singh VV, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4883, DOI 10.1145/3474085.3475468
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Socher Richard, 2012, Advances in Neural Information Processing Systems, P656
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang PS, 2020, IEEE COMPUT SOC CONF, P1074, DOI 10.1109/CVPRW50498.2020.00141
   Wang PS, 2021, AAAI CONF ARTIF INTE, V35, P2773
   Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Yang YQ, 2020, PROC CVPR IEEE, P13575, DOI 10.1109/CVPR42600.2020.01359
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 70
TC 2
Z9 2
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4211
EP 4224
DI 10.1109/TVCG.2023.3257035
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700010
PM 37028344
DA 2024-11-06
ER

PT J
AU Zhang, HL
   Cheng, S
   El Amm, C
   Kim, J
AF Zhang, Haoliang
   Cheng, Samuel
   El Amm, Christian
   Kim, Jonghoon
TI Efficient Pooling Operator for 3D Morphable Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Convolution; Faces; Three-dimensional displays; Shape; Task analysis;
   Computational modeling; Geometry; Latent representation; mesh
   reconstruction; morphable model
AB Learning the latent representation of three-dimensional (3D) morphable geometry is useful for several tasks, such as 3D face tracking, human motion analysis, and character generation and animation. For unstructured surface meshes, previous state-of-the-art methods focus on designing convolution operators and share the same pooling and unpooling operations to encode neighborhood information. Previous models use a mesh pooling operation based on edge contraction, which is based on the euclidean distance of vertices rather than the actual topology. In this study, we investigated whether such a pooling operation can be improved, introducing an improved pooling layer that combines the vertex normals and adjacent faces area. Furthermore, to prevent template overfitting, we increased the receptive field and improved low-resolution projection in the unpooling stage. This increase did not affect processing efficiency because the operation was implemented once on the mesh. We performed experiments to evaluate the proposed method, whose results indicated that the proposed operations outperformed Neural3DMM with 14% lower reconstruction errors and outperformed CoMA by 15% by modifying the pooling and unpooling matrices.
C1 [Zhang, Haoliang; Cheng, Samuel; El Amm, Christian] Univ Oklahoma, Norman, OK 73019 USA.
   [Kim, Jonghoon] Chungnam Natl Univ, Daejeon 34134, South Korea.
C3 University of Oklahoma System; University of Oklahoma - Norman; Chungnam
   National University
RP Kim, J (corresponding author), Chungnam Natl Univ, Daejeon 34134, South Korea.
EM mars_zhang@ou.edu; samuel.cheng@ou.edu; christianamm@hotmail.com;
   qwzxas@hanmail.net
OI Cheng, Samuel/0000-0002-5439-1137
CR Amberg R., 2008, P IEEE 8 INT C AUT F, P1
   [Anonymous], 2020, P IEEECVF C COMPUTER
   Atwood J., 2016, Advances in Neural Information Processing Systems, P1993, DOI DOI 10.5555/3157096.3157320
   Bagautdinov T, 2018, PROC CVPR IEEE, P3877, DOI 10.1109/CVPR.2018.00408
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bogo F, 2017, PROC CVPR IEEE, P5573, DOI 10.1109/CVPR.2017.591
   Boscaini Davide, 2016, ADV NEURAL INFORM PR, P3197
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Bouritsas G, 2019, IEEE I CONF COMP VIS, P7212, DOI 10.1109/ICCV.2019.00731
   Brock A, 2016, Arxiv, DOI [arXiv:1608.04236, 10.48550/arXiv.1608.04236]
   Chen T.-K., 2021, P IEEE CVF C COMP VI, p13 164
   Chung F. C., 1997, Spectral Graph Theory, P39
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Defferrard M, 2016, ADV NEUR IN, V29
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   GOURAUD H, 1971, IEEE T COMPUT, VC 20, P623, DOI 10.1109/T-C.1971.223313
   Hahner J., 2022, P IEEE CVF WINT C AP, P885
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Hoppe H., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P59, DOI 10.1109/VISUAL.1999.809869
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kingma D.P., 2014, P INT C LEARNING REP
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Li YZ, 2018, ADV NEUR IN, V31
   Lim I, 2019, LECT NOTES COMPUT SC, V11131, P349, DOI 10.1007/978-3-030-11015-4_26
   Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202
   Liu Y, 2006, ACM T GRAPHIC, V25, P681, DOI 10.1145/1141911.1141941
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Malik J, 2018, INT CONF 3D VISION, P110, DOI 10.1109/3DV.2018.00023
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Masci D., 2015, P INT C COMP VIS WOR, P37, DOI DOI 10.1109/ICCVW.2015.112
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Kipf TN, 2017, Arxiv, DOI [arXiv:1609.02907, 10.48550/arXiv.1609.02907]
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Romero J., 2022, arXiv
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang Chengcheng., 2014, ACM T GRAPHIC, V33, P70, DOI DOI 10.1145/2601097.2601213
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Tretschk Edgar, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P601, DOI 10.1007/978-3-030-58548-8_35
   Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275
   Yang J., 2011, ACM SIGGRAPH, P1
   Yao L, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/428917
   Zhou Yi, 2020, Advances in neural information processing systems, V33, P9251
NR 52
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4225
EP 4233
DI 10.1109/TVCG.2023.3255820
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700007
PM 37028282
DA 2024-11-06
ER

PT J
AU Guo, JP
   Zhang, WX
   Ye, CY
   Fu, XM
AF Guo, Jia-Peng
   Zhang, Wen-Xiang
   Ye, Chunyang
   Fu, Xiao-Ming
TI Robust Coarse Cage Construction With Small Approximation Errors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Complexity theory; Robustness; Manifolds; Approximation error;
   Approximation algorithms; Filtering; Geometry; cage construction;
   conformal tetrahedral meshing; mesh complexity; tetrahedral subdivision
ID LOCAL REFINEMENT; SUBDIVISION
AB We propose a robust and automatic method to construct manifold cages for 3D triangular meshes. The cage contains hundreds of triangles to tightly enclose the input mesh without self-intersections. To generate such cages, our algorithm consists of two phases: (1) construct manifold cages satisfying the tightness, enclosing, and intersection-free requirements and (2) reduce mesh complexities and approximation errors without violating the enclosing and intersection-free requirements. To theoretically make the first stage have those properties, we combine the conformal tetrahedral meshing and tetrahedral mesh subdivision. The second step is a constrained remeshing process using explicit checks to ensure that the enclosing and intersection-free constraints are always satisfied. Both phases use a hybrid coordinate representation, i.e., rational numbers and floating point numbers, combined with exact arithmetic and floating point filtering techniques to guarantee the robustness of geometric predicates with a favorable speed. We extensively test our method on a data set of over 8500 models, demonstrating robustness and performance. Compared to other state-of-the-art methods, our method possesses much stronger robustness.
C1 [Guo, Jia-Peng] Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Anhui, Peoples R China.
   [Zhang, Wen-Xiang; Ye, Chunyang; Fu, Xiao-Ming] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM gjp171499@mail.ustc.edu.cn; zwx111@mail.ustc.edu.cn;
   yechyang@mail.ustc.edu.cn; fuxm@ustc.edu.cn
RI Fu, Xiao-Ming/V-8253-2019
OI Fu, Xiao-Ming/0000-0001-8479-0107
FU National Natural Science Foundation of China [62272429]; Major Project
   of Science and Technology of Anhui Province [202203a05020050]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272429 and in part by the Major
   Project of Science and Technology of Anhui Province under Grant
   202203a05020050.
CR Attene M, 2020, COMPUT AIDED DESIGN, V126, DOI 10.1016/j.cad.2020.102856
   Attene M, 2009, COMPUT AIDED GEOM D, V26, P850, DOI 10.1016/j.cagd.2009.06.002
   Ben-Chen M., 2009, P ACM SIGGRAPH EUR S, P67, DOI DOI 10.1145/1599470.1599479
   Bey J, 1995, COMPUTING, V55, P355, DOI 10.1007/BF02238487
   Botsch Mario, 2004, P 2004 EUR ACM SIGGR, P185, DOI DOI 10.1145/1057432.1057457
   Brönnimann H, 2001, DISCRETE APPL MATH, V109, P25, DOI 10.1016/S0166-218X(00)00231-6
   Bronnimann Herve, 2022, CGAL User and Reference Manual, V5.5
   Burkhart D, 2010, COMPUT GRAPH FORUM, V29, P117, DOI 10.1111/j.1467-8659.2009.01581.x
   Calderon S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073714
   Casti S, 2019, COMPUT GRAPH-UK, V81, P140, DOI 10.1016/j.cag.2019.04.004
   Chen X, 2014, COMPUT ANIMAT VIRT W, V25, P447, DOI 10.1002/cav.1577
   Cheng XX, 2019, COMPUT GRAPH-UK, V82, P163, DOI 10.1016/j.cag.2019.05.019
   Cherchi G, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417818
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Deng ZJ, 2011, J COMPUT SCI TECH-CH, V26, P538, DOI 10.1007/s11390-011-1153-4
   Diazzi L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480564
   Doi A, 1997, FIFTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P206, DOI 10.1109/PCCGA.1997.626208
   Escobar JM, 2005, COMPUT STRUCT, V83, P2423, DOI 10.1016/j.compstruc.2005.03.022
   Faraj N, 2012, COMPUT GRAPH-UK, V36, P562, DOI 10.1016/j.cag.2012.03.020
   Forest C, 2005, MED IMAGE ANAL, V9, P113, DOI 10.1016/j.media.2004.11.003
   Fortune S, 1996, ACM T GRAPHIC, V15, P223, DOI 10.1145/231731.231735
   Fousse L, 2007, ACM T MATH SOFTWARE, V33, DOI 10.1145/1236463.1236468
   García FG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487232
   Greiner G, 2000, VISUAL COMPUT, V16, P357, DOI 10.1007/PL00007214
   Hemmer M, 2022, CGAL user and reference manual
   Hu KM, 2017, IEEE T VIS COMPUT GR, V23, P2560, DOI 10.1109/TVCG.2016.2632720
   Hu YX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323011
   Hu YX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201353
   Jiang ZS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417769
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Ju T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409075
   Khan D, 2022, IEEE T VIS COMPUT GR, V28, P1680, DOI 10.1109/TVCG.2020.3016645
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   Le B.H., 2017, P 21 ACM SIGGRAPH S, P1
   Li C, 2015, VISUAL COMPUT, V31, P937, DOI 10.1007/s00371-015-1117-8
   Liao WT, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459794
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   Liu AW, 1996, MATH COMPUT, V65, P1183, DOI 10.1090/S0025-5718-96-00748-X
   Liu YJ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818076
   Liu ZY, 2021, COMPUT AIDED DESIGN, V139, DOI 10.1016/j.cad.2021.103080
   Liu ZY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459843
   Meyer A., 2008, Real Numbers Comput., P47
   Nesme M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531358
   Platis N, 2003, COMPUT GRAPH FORUM, V22, P107, DOI 10.1111/1467-8659.00653
   Portaneri C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530152
   Sacht L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818093
   Sander PV, 2000, COMP GRAPH, P327, DOI 10.1145/344779.344935
   Schaefer S., 2004, P EUR ACM SIGGRAPH S, P147
   Stuart A., 2013, P MOT GAM
   Wang BL, 2022, COMPUT GRAPH FORUM, V41, P355, DOI 10.1111/cgf.14479
   Wang BL, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3460775
   Wang BL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392426
   Xian CH, 2015, COMPUT ANIMAT VIRT W, V26, P173, DOI 10.1002/cav.1571
   Xian CH, 2012, VISUAL COMPUT, V28, P21, DOI 10.1007/s00371-011-0595-6
   Xian CH, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P21, DOI 10.1109/SMI.2009.5170159
   Yang XS, 2013, VISUAL COMPUT, V29, P369, DOI 10.1007/s00371-012-0739-3
   Yang Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392434
   Yifan W, 2020, PROC CVPR IEEE, P72, DOI 10.1109/CVPR42600.2020.00015
   Zhang WX, 2022, COMPUT GRAPH FORUM, V41, P237, DOI 10.1111/cgf.14471
   Zhao Y, 2009, J COMPUT SCI TECH-CH, V24, P47, DOI 10.1007/s11390-009-9213-8
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
   Zhou QN, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925901
NR 63
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4234
EP 4245
DI 10.1109/TVCG.2023.3255207
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700074
PM 37028283
DA 2024-11-06
ER

PT J
AU Ma, XH
   Yu, YX
   Wu, HZ
   Zhou, K
AF Ma, Xiaohe
   Yu, Yaxin
   Wu, Hongzhi
   Zhou, Kun
TI Efficient Reflectance Capture With a Deep Gated Mixture-of-Experts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Image reconstruction; Decoding; Optimization; Light emitting
   diodes; Cameras; Neural networks; Anisotropic reflectance; computational
   illumination; SVBRDF
ID REPRESENTATION; APPEARANCE
AB We present a novel framework to efficiently acquire anisotropic reflectance in a pixel-independent fashion, using a deep gated mixture-of-experts. While existing work employs a unified network to handle all possible input, our network automatically learns to condition on the input for enhanced reconstruction. We train a gating module that takes photometric measurements as input and selects one out of a number of specialized decoders for reflectance reconstruction, essentially trading generality for quality. A common pre-trained latent-transform module is also appended to each decoder, to offset the burden of the increased number of decoders. In addition, the illumination conditions during acquisition can be jointly optimized. The effectiveness of our framework is validated on a wide variety of challenging near-planar samples with a lightstage. Compared with the state-of-the-art technique, our quality is improved with the same number of input images, and our input image number can be reduced to about 1/3 for equal-quality results. We further generalize the framework to enhance a state-of-the-art technique on non-planar reflectance scanning.
C1 [Ma, Xiaohe; Yu, Yaxin; Wu, Hongzhi; Zhou, Kun] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Zhou, Kun] ZJU FaceUn Joint Lab Intelligent G, Hangzhou 310058, Peoples R China.
C3 Zhejiang University
RP Wu, HZ (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM xiaohema1998@gmail.com; 22021283@zju.edu.cn; hwu@acm.org;
   kunzhou@acm.org
RI Zhou, Kun/G-1249-2010
OI Ma, Xiaohe/0009-0000-8924-2046
FU NSF China [62022072, 62227806]; Zhejiang Provincial Key RD Program
   [2022C01057]; XPLORER PRIZE
FX This work was partially supported in part by NSF China under Grants
   62022072 & 62227806, in part by Zhejiang Provincial Key R&D Program
   under Grant 2022C01057, and in part by the XPLORER PRIZE
CR Aittala M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925917
   Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967
   Aittala M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461978
   Chen GJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601180
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765
   Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378
   Dong Y, 2019, VIS INFORM, V3, P59, DOI 10.1016/j.visinf.2019.07.003
   Dong Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778835
   Gao D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417767
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Gardner A, 2003, ACM T GRAPHIC, V22, P749, DOI 10.1145/882262.882342
   Ghosh A, 2009, COMPUT GRAPH FORUM, V28, P1161, DOI 10.1111/j.1467-8659.2009.01493.x
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459854
   Guo J, 2020, IEEE T VIS COMPUT GR, V26, P1476, DOI 10.1109/TVCG.2018.2872709
   Guo Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417779
   Henzler P, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480507
   Hu BY, 2020, COMPUT GRAPH FORUM, V39, P157, DOI 10.1111/cgf.13920
   Hui Z, 2017, IEEE I CONF COMP VIS, P5372, DOI 10.1109/ICCV.2017.573
   Kang KZ, 2023, IEEE T VIS COMPUT GR, V29, P1450, DOI 10.1109/TVCG.2021.3117370
   Kang KZ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356492
   Kang KZ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201279
   Lawrence J, 2006, ACM T GRAPHIC, V25, P735, DOI 10.1145/1141911.1141949
   Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891
   Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641
   Morales JL, 2011, ACM T MATH SOFTWARE, V38, DOI 10.1145/2049662.2049669
   Ma XH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459679
   Marschner SR, 1999, SPRING EUROGRAP, P131
   Nam G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275017
   Nam G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980220
   Rainer G, 2020, COMPUT GRAPH FORUM, V39, P167, DOI 10.1111/cgf.13921
   Ren PR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964940
   Riquelme C, 2021, Arxiv, DOI [arXiv:2106.05974, arXiv:2106.05974]
   Shazeer N., 2017, arXiv
   Tunwattanapong B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461944
   Walter B., 2007, RENDERING TECHNIQUES
   Wang JP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360640
   Weinmann R., 2015, SIGGRAPH Asia Courses
   Weyrich T, 2008, FOUND TRENDS COMPUT, V4, P75, DOI 10.1561/0600000022
   Zhou XL, 2021, COMPUT GRAPH FORUM, V40, P315, DOI 10.1111/cgf.142635
NR 41
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4246
EP 4256
DI 10.1109/TVCG.2023.3261872
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700053
PM 37030776
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Benda, B
   Sargunam, SP
   Nourani, M
   Ragan, ED
AF Benda, Brett
   Sargunam, Shyam Prathish
   Nourani, Mahsan
   Ragan, Eric D.
TI An Evaluation of View Rotation Techniques for Seated Navigation in
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Turning; Tracking; Teleportation; Virtual
   environments; Navigation; Three-dimensional displays; Human-centered
   computing; human-computer interaction; virtual reality
ID HEAD MOVEMENTS; ENVIRONMENTS; LOCOMOTION; TRAVEL
AB Head tracking is commonly used in VR applications to allow users to naturally view 3D content using physical head movement, but many applications also support turning with hand-held controllers. Controller and joystick controls are convenient for practical settings where full 360-degree physical rotation is not possible, such as when the user is sitting at a desk. Though controller-based rotation provides the benefit of convenience, previous research has demonstrated that virtual or joystick-controlled view rotation to have drawbacks of sickness and disorientation compared to physical turning. To combat such issues, researchers have considered various techniques such as speed adjustments or reduced field of view, but data is limited on how different variations for joystick rotation influences sickness and orientation perception. Our studies include different variations of techniques such as joystick rotation, resetting, and field-of-view reduction. We investigate trade-offs among different techniques in terms of sickness and the ability to maintain spatial orientation. In two controlled experiments, participants traveled through a sequence of rooms and were tested on spatial orientation, and we also collected subjective measures of sickness and preference. Our findings indicate a preference by users towards directly-manipulated joystick-based rotations compared to user-initiated resetting and minimal effects of technique on spatial awareness.
C1 [Benda, Brett; Nourani, Mahsan; Ragan, Eric D.] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
   [Sargunam, Shyam Prathish] Autodesk, San Rafael, CA 94903 USA.
C3 State University System of Florida; University of Florida; Autodesk,
   Inc.
RP Benda, B (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
EM brett.benda@ufl.edu; shyam.prathish@gmail.com; mahsannourani@ufl.edu;
   eragan@ufl.edu
RI Sargunam, Shyam Prathish/JZD-0838-2024
OI Benda, William/0000-0002-1825-6392
CR Adhikari A, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P370, DOI 10.1109/VRW52623.2021.00074
   Bacim F., 2013, Proceedings of Graphics Interface 2013, VVolume 2, P25
   Bölling L, 2019, IEEE T VIS COMPUT GR, V25, P2032, DOI 10.1109/TVCG.2019.2899228
   Bolte B., 2011, P VIRT REAL INT C
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Bozgeyikli E, 2019, INT J HUM-COMPUT ST, V122, P38, DOI 10.1016/j.ijhcs.2018.08.002
   Bozgeyikli E, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P205, DOI 10.1145/2967934.2968105
   Buttussi F, 2021, IEEE T VIS COMPUT GR, V27, P125, DOI 10.1109/TVCG.2019.2928304
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   Cohen J., 1975, APPL MULTIPLE REGRES
   Coomer N, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225175
   Davis S., 2014, P C INT ENT, P1
   Farmani Y., 2018, P 44 GRAPH INT C, P168, DOI [10.20380/GI2018.21, DOI 10.20380/GI2018.23, 10.20380/GI201 8.23, 10.20380/GI2018.23]
   Farmani Y, 2020, VIRTUAL REAL-LONDON, V24, P645, DOI 10.1007/s10055-020-00425-x
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Jay C, 2003, PRESENCE-VIRTUAL AUG, V12, P268, DOI 10.1162/105474603765879521
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kolasinski EugeniaM., 1995, Simulator sickness in virtual environments
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Langbehn E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201335
   Langbehn Eike, 2019, P MENSCH COMP 2019 H, P235, DOI [10.1145/3340764.3340778, DOI 10.1145/3340764.3340778]
   LaViola JosephJ., 2001, Proceedings Symposium on Interactive 3D Graphics, P9
   Lee JY, 2017, SIGGRAPH ASIA 2017 POSTERS (SA'17), DOI 10.1145/3145690.3145697
   Lin JJW, 2002, P IEEE VIRT REAL ANN, P164, DOI 10.1109/VR.2002.996519
   Ngoc LL, 2013, P IEEE VIRT REAL ANN, P51, DOI 10.1109/VR.2013.6549359
   McCauley Michael E, 1992, Presence: Teleoperators & Virtual Environments, V1, P311, DOI [10.1162/pres.1992.1.3.311, DOI 10.1162/PRES.1992.1.3.311]
   Moghadam K, 2020, IEEE T VIS COMPUT GR, V26, P2273, DOI 10.1109/TVCG.2018.2884468
   Morganti Francesca, 2014, Stud Health Technol Inform, V196, P278
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Peck TC, 2009, IEEE T VIS COMPUT GR, V15, P383, DOI 10.1109/TVCG.2008.191
   Poupyrev I., 1999, CHI'99 extended abstracts on Human factors in computing systems, P256
   Prabhat, 2008, IEEE T VIS COMPUT GR, V14, P551, DOI 10.1109/TVCG.2007.70433
   Ragan E. D., 2012, P JOINT VIRT REAL C, P81
   Ragan ED, 2017, IEEE T VIS COMPUT GR, V23, P1880, DOI 10.1109/TVCG.2016.2601607
   Ragan ED, 2013, IEEE T VIS COMPUT GR, V19, P886, DOI 10.1109/TVCG.2012.163
   Razzaque S., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P123
   Razzaque S., 2001, P 22 ANN C EUR ASS C, P289, DOI DOI 10.2312/EGS.20011036
   Riecke BernhardE., 2012, P ACM S APPL PERCEPT, P17, DOI [10.1145/2338676.2338680, DOI 10.1145/2338676.2338680]
   Ruddle RA, 1999, PRESENCE-TELEOP VIRT, V8, P157, DOI 10.1162/105474699566143
   Ryge AN, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P675, DOI 10.1109/VR.2018.8446206
   Sargunam SP, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON INTERACTIVE AND SPATIAL COMPUTING (IWISC 18), P74, DOI 10.1145/3191801.3191815
   Sargunam SP, 2017, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2017.7892227
   Slater M., 1995, ACM Transactions on Computer Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   Stebbins T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P377, DOI [10.1109/vr.2019.8797994, 10.1109/VR.2019.8797994]
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Ware C., 1990, Computer Graphics, V24, P175, DOI 10.1145/91394.91442
   Weissker T, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P97, DOI 10.1109/VR.2018.8446620
   WELLS MJ, 1990, OPT ENG, V29, P870, DOI 10.1117/12.55672
   Weniger G, 2011, NEUROPSYCHOLOGIA, V49, P518, DOI 10.1016/j.neuropsychologia.2010.12.031
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
   Zielasko D., 2018, P IEEE VR WORKSH EV, P1
   Zielasko D, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P693, DOI 10.1109/VR51125.2022.00090
   Zielasko D, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P398, DOI [10.1109/VR46266.2020.1581426770550, 10.1109/VR46266.2020.00-44]
   Zielasko D, 2017, 2017 IEEE 3RD WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), DOI 10.1109/WEVR.2017.7957707
NR 55
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4257
EP 4270
DI 10.1109/TVCG.2023.3258693
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700048
PM 37030847
DA 2024-11-06
ER

PT J
AU Yang, SP
   Zhao, YL
   Luo, YZ
   Wang, H
   Sun, HY
   Li, C
   Cai, BH
   Jin, XG
AF Yang, Sipeng
   Zhao, Yunlu
   Luo, Yuzhe
   Wang, He
   Sun, Hongyu
   Li, Chen
   Cai, Binghuang
   Jin, Xiaogang
TI MNSS: Neural Supersampling Framework for Real-Time Rendering on Mobile
   Devices
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Real-time systems; Image reconstruction;
   Image resolution; Videos; Artificial intelligence; Neural networks; Deep
   learning; neural supersampling; real-time rendering
ID IMAGE SUPERRESOLUTION; QUALITY ASSESSMENT
AB Although neural supersampling has achieved great success in various applications for improving image quality, it is still difficult to apply it to a wide range of real-time rendering applications due to the high computational power demand. Most existing methods are computationally expensive and require high-performance hardware, preventing their use on platforms with limited hardware, such as smartphones. To this end, we propose a new supersampling framework for real-time rendering applications to reconstruct a high-quality image out of a low-resolution one, which is sufficiently lightweight to run on smartphones within a real-time budget. Our model takes as input the renderer-generated low resolution content and produces high resolution and anti-aliased results. To maximize sampling efficiency, we propose using an alternate sub-pixel sample pattern during the rasterization process. This allows us to create a relatively small reconstruction model while maintaining high image quality. By accumulating new samples into a high-resolution history buffer, an efficient history check and re-usage scheme is introduced to improve temporal stability. To our knowledge, this is the first research in pushing real-time neural supersampling on mobile devices. Due to the absence of training data, we present a new dataset containing 57 training and test sequences from three game scenes. Furthermore, based on the rendered motion vectors and a visual perception study, we introduce a new metric called inter-frame structural similarity (IF-SSIM) to quantitatively measure the temporal stability of rendered videos. Extensive evaluations demonstrate that our supersampling model outperforms existing or alternative solutions in both performance and temporal stability.
C1 [Yang, Sipeng; Zhao, Yunlu; Luo, Yuzhe; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Wang, He] Univ Leeds, Sch Comp, Leeds LS2 9JT, England.
   [Sun, Hongyu; Li, Chen; Cai, Binghuang] OPPO US Res Ctr, Bellevue, WA 98005 USA.
C3 Zhejiang University; University of Leeds
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM 12121024@zju.edu.cn; yunlu.zhao@zju.edu.cn; 22221149@zju.edu.cn;
   realcrane@gmail.com; dr.sunhongyu@gmail.com; chenlixyz@gmail.com;
   bhcai8@gmail.com; jin@cad.zju.edu.cn
RI Zhao, Yunlu/ISU-1989-2023; hongyu, sun/JGM-2944-2023; Wang,
   He/ABD-8303-2021
OI Wang, He/0000-0002-2281-5679; Luo, Yuzhe/0009-0008-5796-6144; Sun,
   Hongyu/0009-0002-4118-4777; Jin, Xiaogang/0000-0001-7339-2920
FU Key R#x0026;D Program of Zhejiang [2023C01047]; National Natural Science
   Foundation of China [62036010, 61972344]
FX This work was supported in part by Key R&D Program of Zhejiang under
   Grant 2023C01047, and in part by the National Natural Science Foundation
   of China under Grants 62036010, 61972344.
CR Akeley K., 1993, Computer Graphics Proceedings, P109, DOI 10.1145/166117.166131
   AMD, 2022, FidelityFX Super Resolution 2.0
   AMD, 2021, Fidelityfx super resolution
   Aydin TO, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866187
   Barten PGJ, 2004, P SOC PHOTO-OPT INS, V5294, P231, DOI 10.1117/12.537476
   Blackwell H. R., 1972, Visual Psychophysics, P78
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Damera-Venkata N, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477935
   Deore M., 2017, Hexagon DSP CPU offload
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dou H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1891, DOI 10.1145/3394171.3413590
   Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247
   Edelsten A., 2019, P GAM DEV C
   El Mansouri J. E., 2016, P GAM DEV C
   Fuoli D, 2019, IEEE INT CONF COMP V, P3476, DOI 10.1109/ICCVW.2019.00431
   Games E., 2021, Unreal engine 4.27: Screen percentage with temporal upsam- ple
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480531
   Guo J, 2017, AAAI CONF ARTIF INTE, P4053
   HOBBY JD, 1990, ACM T GRAPHIC, V9, P262, DOI 10.1145/78964.78966
   Huang Y, 2015, ADV NEUR IN, V28
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Jimenez J, 2012, COMPUT GRAPH FORUM, V31, P355, DOI 10.1111/j.1467-8659.2012.03014.x
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karis B., 2014, P ADV REAL TIM REND, V1, P1
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim SY, 2019, IEEE IMAGE PROC, P2831, DOI [10.1109/icip.2019.8803297, 10.1109/ICIP.2019.8803297]
   Li Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30338
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu E., 2020, P GPU TECHN C
   Liu HY, 2022, Arxiv, DOI arXiv:2007.12928
   Liu SL, 2021, IEEE COMPUT SOC CONF, P2480, DOI 10.1109/CVPRW53098.2021.00281
   Mantiuk RK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459831
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Mueller JH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3446790
   Polesel A, 2000, IEEE T IMAGE PROCESS, V9, P505, DOI 10.1109/83.826787
   Poynton C, 2012, DIGITAL VIDEO AND HD: ALGORITHMS AND INTERFACES, 2ND EDITION, P1
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Thomas MM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417786
   U. Technologies, 2005, Unity engine
   Vaidyanathan Karthik, 2014, P HIGH PERF GRAPH, P9
   Wang LD, 2018, IEEE ENG MED BIO, P514, DOI 10.1109/EMBC.2018.8512300
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wei XY, 2021, Arxiv, DOI arXiv:2108.06915
   Xiao L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392376
   Yang L, 2020, COMPUT GRAPH FORUM, V39, P607, DOI 10.1111/cgf.14018
   Yang L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618481
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P1500, DOI 10.1109/LSP.2020.3013518
   Zeng Z, 2021, COMPUT GRAPH FORUM, V40, P79, DOI 10.1111/cgf.142616
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 58
TC 4
Z9 4
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4271
EP 4284
DI 10.1109/TVCG.2023.3259141
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700089
PM 37030766
DA 2024-11-06
ER

PT J
AU Cortes, CAT
   Thurow, S
   Ong, A
   Sharples, JJ
   Bednarz, T
   Stevens, G
   Favero, DD
AF Cortes, Carlos A. Tirado
   Thurow, Susanne
   Ong, Alex
   Sharples, Jason J.
   Bednarz, Tomasz
   Stevens, Grant
   Favero, Dennis Del
TI Analysis of Wildfire Visualization Systems for Research and Training:
   Are They Up for the Challenge of the Current State of Wildfires?
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Immersive wildfire training; immersive wildfire visualization;
   modelling; simulation; wildfire; wildfire visualization
ID VIRTUAL-REALITY; SIMULATION; DYNAMICS; DESIGN
AB Wildfires affect many regions across the world. The accelerated progression of global warming has amplified their frequency and scale, deepening their impact on human life, the economy, and the environment. The temperature rise has been driving wildfires to behave unpredictably compared to those previously observed, challenging researchers and fire management agencies to understand the factors behind this behavioral change. Furthermore, this change has rendered fire personnel training outdated and lost its ability to adequately prepare personnel to respond to these new fires. Immersive visualization can play a key role in tackling the growing issue of wildfires. Therefore, this survey reviews various studies that use immersive and non-immersive data visualization techniques to depict wildfire behavior and train first responders and planners. This paper identifies the most useful characteristics of these systems. While these studies support knowledge creation for certain situations, there is still scope to comprehensively improve immersive systems to address the unforeseen dynamics of wildfires.
C1 [Cortes, Carlos A. Tirado; Thurow, Susanne; Ong, Alex; Favero, Dennis Del] Univ New South Wales, iCinema Res Ctr, Kensington, NSW 2052, Australia.
   [Sharples, Jason J.] Univ New South Wales, Sch Sci, Canberra, ACT 2600, Australia.
   [Stevens, Grant] Univ New South Wales, Sch Art & Design, Paddington, NSW 2021, Australia.
C3 University of New South Wales Sydney; University of New South Wales
   Sydney; University of New South Wales Sydney
RP Cortes, CAT (corresponding author), Univ New South Wales, iCinema Res Ctr, Kensington, NSW 2052, Australia.
EM c.tirado@unsw.edu.au; s.thurow@unsw.edu.au; alex.ong@unsw.edu.au;
   j.sharples@adfa.edu.au; tomasz.bednarz@gmail.com;
   grant.stevens@unsw.edu.au; d.delfavero@unsw.edu.au
RI Tirado Cortes, Carlos Alfredo/HGB-5257-2022; Bednarz,
   Tomasz/AAQ-2605-2021; Bednarz, Tomasz/A-7376-2011
OI Stevens, Grant/0000-0003-1620-9127; Bednarz, Tomasz/0000-0001-9240-0922;
   Del Favero, Dennis/0000-0002-6761-0992; Tirado Cortes, Carlos
   Alfredo/0000-0003-0626-0914; Sharples, Jason/0000-0002-7816-6989
FU Australian Government through the Australian Research Councils Laureate
   funding scheme [FL200100004]; Australian Research Council [FL200100004]
   Funding Source: Australian Research Council
FX This work was supported by the Australian Government through the
   Australian Research Councils Laureate funding scheme under Grant
   FL200100004.
CR A4VR, 2022, Flamecoachs
   Abouali A, 2021, COMBUST FLAME, V234, DOI 10.1016/j.combustflame.2021.111724
   Abram NJ, 2021, COMMUN EARTH ENVIRON, V2, DOI 10.1038/s43247-020-00065-8
   Altintas I, 2015, PROCEDIA COMPUT SCI, V51, P1633, DOI 10.1016/j.procs.2015.05.296
   Anthes C., 2016, P IEEE AER C P, P1
   Badlan RL, 2021, INT J WILDLAND FIRE, V30, P484, DOI 10.1071/WF20040
   Badlan RL, 2021, INT J WILDLAND FIRE, V30, P498, DOI 10.1071/WF20041
   Bednarz T., 2011, P 10 INT C VIRT REAL, P459, DOI [DOI 10.1145/2087756, 10.1145/2087756.2087845, DOI 10.1145/2087756.2087845]
   Berntsen K., 2016, Proceedings ofthe Fourth International Conference on Technological Ecosystemsfor Enhancing Multiculturality, P435, DOI DOI 10.1145/3012430.3012553
   Billen MI, 2008, COMPUT GEOSCI-UK, V34, P1056, DOI 10.1016/j.cageo.2007.11.009
   Black J., 2007, Transactions in GIS, V11, P621, DOI 10.1111/j.1467-9671.2007.01063.x
   Bogdos N, 2013, ENVIRON MODELL SOFTW, V46, P182, DOI 10.1016/j.envsoft.2013.03.009
   Bot K, 2022, INVENTIONS-BASEL, V7, DOI 10.3390/inventions7010015
   Braun Philipp, 2022, Virtual Reality & Intelligent Hardware, V4, P406, DOI [10.1016/j.vrih.2022.08.006, DOI 10.1016/J.VRIH.2022.08.006]
   Bryce R., 2010, Information Report NOR-X-417
   Byari M, 2022, CHAOS SOLITON FRACT, V164, DOI 10.1016/j.chaos.2022.112653
   Calandra D, 2023, VIRTUAL REAL-LONDON, V27, P985, DOI 10.1007/s10055-022-00704-9
   Calkin DE, 2021, FORESTS, V12, DOI 10.3390/f12101407
   Castellnou M, 2018, ADVANCES IN FOREST FIRE RESEARCH 2018, P447, DOI 10.14195/978-989-26-16-506_48
   Castellnou M, 2019, FIRE ECOL, V15, DOI 10.1186/s42408-019-0048-6
   Castrillón M, 2011, COMPUT GEOSCI-UK, V37, P390, DOI 10.1016/j.cageo.2010.04.011
   Cha M, 2012, FIRE SAFETY J, V50, P12, DOI 10.1016/j.firesaf.2012.01.004
   Chandler T, 2015, 2015 BIG DATA VISUAL ANALYTICS (BDVA)
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chen HS, 2021, AUTOMAT CONSTR, V125, DOI 10.1016/j.autcon.2021.103631
   Chen S. Y., 2022, Front. Psychol., V13
   Chertoff DB, 2010, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VR.2010.5444804
   Clifford R. M., 2018, P IEEE 10 INT C VIRT, P1
   Clifford R. M. S., 2020, Ph.D. dissertation
   Clifford RMS, 2021, VISUAL COMPUT, V37, P63, DOI 10.1007/s00371-020-01816-6
   Clifford RMS, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P181, DOI [10.1109/VR.2019.8797889, 10.1109/vr.2019.8797889]
   Clifford RMS, 2018, 2018 IEEE WORKSHOP ON AUGMENTED AND VIRTUAL REALITIES FOR GOOD (VAR4GOOD)
   Coen JL, 2018, ECOL APPL, V28, P1565, DOI 10.1002/eap.1752
   Coen JL, 2013, J APPL METEOROL CLIM, V52, P16, DOI 10.1175/JAMC-D-12-023.1
   Cordeil M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P200, DOI [10.1109/vr.2019.8797978, 10.1109/VR.2019.8797978]
   Crawl D, 2017, PROCEDIA COMPUT SCI, V108, P2230, DOI 10.1016/j.procs.2017.05.174
   Danyluk K, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445098
   Daylamani-Zad D, 2022, INT J HUM-COMPUT ST, V162, DOI 10.1016/j.ijhcs.2022.102790
   Denham MM, 2022, ENVIRON MODELL SOFTW, V158, DOI 10.1016/j.envsoft.2022.105526
   Desmond M, 2011, QUAL SOCIOL, V34, P59, DOI 10.1007/s11133-010-9176-7
   Diamond R.M., 1989, DESIGNING IMPROVING
   Bui DT, 2019, J ENVIRON MANAGE, V237, P476, DOI 10.1016/j.jenvman.2019.01.108
   Dimara E, 2020, IEEE T VIS COMPUT GR, V26, P119, DOI 10.1109/TVCG.2019.2934283
   Doroudian S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P378, DOI 10.1109/VRW55335.2022.00085
   Dwyer G, 2021, AUST J PUBL ADMIN, V80, P602, DOI 10.1111/1467-8500.12476
   Ens Barrett., 2021, P 2021 CHI C HUMAN F, P1, DOI DOI 10.1145/3411764.3446866
   ETC Simulation, 2023, Advanced disaster management simulator
   Ewer J., 2010, P 5 EUR C COMP FLUID, P14
   Favero D. D., 2021, Theatre Perform. Des., V7, P82, DOI [10.1080/23322551.2021.1919488, DOI 10.1080/23322551.2021.1919488]
   Finney M., 1995, P BISW S FIR ISS SOL, P55
   Finney MA, 2002, CAN J FOREST RES, V32, P1420, DOI [10.1139/x02-068, 10.1139/X02-068]
   FLAIM Systems Pty Ltd, 2023, FLAIM: Fully immersive VR learning solutions for training in hazardous and emergency situations
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Gabbert B., 2020, Wildfire Today
   Galati A, 2021, IEEE T VIS COMPUT GR, V27, P2714, DOI 10.1109/TVCG.2021.3067693
   Grabowski A, 2021, FIRE SAFETY J, V125, DOI 10.1016/j.firesaf.2021.103440
   Green S., 2014, P ACM SIGGRAPH COMP, P1, DOI [10.1145/2633956.2658828, DOI 10.1145/2633956.2658828]
   Hädrich T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459954
   Han YC, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519903
   Haskins J, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P57, DOI [10.1109/VRW50115.2020.00018, 10.1109/VRW50115.2020.0-258]
   Heirman J, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P266, DOI 10.1109/AIVR50618.2020.00055
   Hilton JE, 2015, ENVIRON MODELL SOFTW, V67, P118, DOI 10.1016/j.envsoft.2015.01.015
   Hoang RV, 2010, COMPUT GRAPH-UK, V34, P655, DOI 10.1016/j.cag.2010.09.014
   Hodges JL, 2019, FIRE TECHNOL, V55, P2115, DOI 10.1007/s10694-019-00846-4
   Huang H., 2012, P 20 INT C GEOINF, P1
   Jarvis C., 2021, Interactive Data Processing and 3D Visualization of the Solid Earth, P273, DOI [10.1007/978-3-030-90716-7_8, DOI 10.1007/978-3-030-90716-7_8]
   Jeon SG, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364268
   Kalabokidis K, 2016, NAT HAZARD EARTH SYS, V16, P643, DOI 10.5194/nhess-16-643-2016
   Keefe DF, 2009, IEEE T VIS COMPUT GR, V15, P1383, DOI 10.1109/TVCG.2009.152
   KESSEN W, 1966, SCIENCE, V152, P193, DOI 10.1126/science.152.3719.193
   Kim YS, 2006, COMPUT IND, V57, P653, DOI 10.1016/j.compind.2006.02.005
   Kraus M, 2020, IEEE T VIS COMPUT GR, V26, P525, DOI 10.1109/TVCG.2019.2934395
   Kraus M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376675
   Kyron MJ, 2022, ARCH ENVIRON OCCUP H, V77, P282, DOI 10.1080/19338244.2021.1893631
   Lareau NP, 2018, GEOPHYS RES LETT, V45, P13107, DOI 10.1029/2018GL080667
   Lee B, 2021, IEEE T VIS COMPUT GR, V27, P1095, DOI 10.1109/TVCG.2020.3030435
   Lee J., 2010, P 9 ACM SIGGRAPH C V, P299, DOI [10.1145/1900179.1900242, DOI 10.1145/1900179.1900242]
   Lee SC, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19116633
   Leuenberger M, 2018, ENVIRON MODELL SOFTW, V101, P194, DOI 10.1016/j.envsoft.2017.12.019
   Li N, 2022, NAT HAZARDS, V112, P1851, DOI 10.1007/s11069-022-05277-z
   Lino H, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489957
   Liu DL, 2021, J LOSS PREVENT PROC, V71, DOI 10.1016/j.jlp.2021.104505
   Liu DL, 2019, FIRE SAFETY J, V109, DOI 10.1016/j.firesaf.2019.102863
   Liu H., 2021, P IEEE 4 INT EL EN C, P1
   Liu RC, 2021, BIOINFORMATICS, V37, P2033, DOI 10.1093/bioinformatics/btab052
   Lorusso P, 2022, BUILDINGS-BASEL, V12, DOI 10.3390/buildings12020223
   Ludus Tech S.L., 2023, Ludus: Virtual reality training for organizations
   Ma C, 2022, GEOSCIENCES, V12, DOI 10.3390/geosciences12060237
   Mandel J, 2014, NAT HAZARD EARTH SYS, V14, P2829, DOI 10.5194/nhess-14-2829-2014
   Manjrekar S, 2014, UKSIM INT CONF COMP, P131, DOI 10.1109/UKSim.2014.20
   Marriott K, 2018, LECT NOTES COMPUT SC, V11190, P259, DOI 10.1007/978-3-030-01388-2_9
   McCormick PS, 1998, IEEE COMPUT GRAPH, V18, P17, DOI 10.1109/38.656785
   McCurdy N, 2019, IEEE T VIS COMPUT GR, V25, P925, DOI 10.1109/TVCG.2018.2864913
   Mcrae RHD, 2015, NAT HAZARD EARTH SYS, V15, P417, DOI 10.5194/nhess-15-417-2015
   Mcrae RHD, 2013, NAT HAZARDS, V65, P1801, DOI 10.1007/s11069-012-0443-7
   Meng QK, 2023, COMPUT GRAPH-UK, V110, P58, DOI 10.1016/j.cag.2022.12.002
   Morélot S, 2021, COMPUT EDUC, V166, DOI 10.1016/j.compedu.2021.104145
   Mossel A, 2021, VIRTUAL REAL-LONDON, V25, P745, DOI 10.1007/s10055-020-00487-x
   Mossel A, 2017, P IEEE VIRT REAL ANN, P357, DOI 10.1109/VR.2017.7892324
   Munzner T, 2009, IEEE T VIS COMPUT GR, V15, P921, DOI 10.1109/TVCG.2009.111
   Mystakidis S, 2022, EDUC SCI, V12, DOI 10.3390/educsci12040281
   NAFZCO, 2022, Protected by Naffco
   Nahavandi S., 2019, Intelligent Computing. Proceedings of the 2019 Computing Conference. Advances in Intelligent Systems and Computing (AISC 997), P11, DOI 10.1007/978-3-030-22871-2_2
   Narciso D, 2023, IEEE T VIS COMPUT GR, V29, P3238, DOI 10.1109/TVCG.2022.3156734
   Narciso D, 2020, MULTIMED TOOLS APPL, V79, P6227, DOI 10.1007/s11042-019-08323-4
   Nur AS, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174416
   OMalley N., 2020, The Sydney Morning Herald, P3
   ONEBONSAI, 2023, VR fire training
   Pedram S., 2021, Front. Virtual Reality, V2, P7, DOI [10.3389/frvir.2021.627333/full, DOI 10.3389/FRVIR.2021.627333/FULL]
   Peterson DA, 2021, NPJ CLIM ATMOS SCI, V4, DOI 10.1038/s41612-021-00192-9
   Peterson DA, 2018, NPJ CLIM ATMOS SCI, V1, DOI 10.1038/s41612-018-0039-3
   Pinto D, 2019, PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON GRAPHICS AND INTERACTION (ICGI 2019), P130, DOI 10.1109/ICGI47575.2019.8955091
   Pirk S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130814
   Preston A, 2019, IEEE COMPUT GRAPH, V39, P72, DOI 10.1109/MCG.2019.2918158
   Pyne StevenJ., 2021, PYROCENE WE CREATED
   RE-liON Group BV, 2022, RE-liON for fire & rescue
   Reality in Virtual Reality Limited, 2021, RiVR
   REW R, 1990, IEEE COMPUT GRAPH, V10, P76, DOI 10.1109/38.56302
   Rhyne TM, 2021, IEEE COMPUT GRAPH, V41, P125, DOI 10.1109/MCG.2021.3075258
   Richards L., 2020, 201920 Australian bushfires frequently asked questions: A quick guide
   Rink K, 2020, J HYDROL, V582, DOI 10.1016/j.jhydrol.2019.124507
   Rosenthal A, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0248617
   Saghafian M, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.593466
   Salehi M, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P245, DOI 10.1145/2939672.2939685
   Schultze U, 2010, J INF TECHNOL-UK, V25, P434, DOI 10.1057/jit.2010.25
   Schumaker NH, 2022, LAND-BASEL, V11, DOI 10.3390/land11081288
   Scott J. H., 2013, Tech. Rep. 315 RMRS-GTR
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Sereno M, 2022, IEEE T VIS COMPUT GR, V28, P2530, DOI 10.1109/TVCG.2020.3032761
   Seydi ST, 2022, ECOL INDIC, V140, DOI 10.1016/j.ecolind.2022.108999
   Sha Alan, 2021, Design for TomorrowVolume 2. Proceedings of ICoRD 2021. Smart Innovation, Systems and Technologies (SIST 222), P829, DOI 10.1007/978-981-16-0119-4_67
   Sharma SK, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14071645
   Sharples J. J., 2010, P 6 INT C FOR FIR RE, P15
   Sharples JJ, 2020, FRONT MECH ENG-SWITZ, V5, DOI 10.3389/fmech.2019.00069
   Sharples JJ, 2016, CLIMATIC CHANGE, V139, P85, DOI 10.1007/s10584-016-1811-1
   Shen SJ, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.597487
   Shuai Yun, 2011, Proceedings of the 2011 IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services (ICSDM 2011), P315, DOI 10.1109/ICSDM.2011.5969054
   Sicat R, 2019, IEEE T VIS COMPUT GR, V25, P715, DOI 10.1109/TVCG.2018.2865152
   Simpson CC, 2013, INT J WILDLAND FIRE, V22, P599, DOI 10.1071/WF12072
   Soltani P., 2021, Research Anthology on Business Strategies, Health Factors, and Ethical Implications in Sports and eSports, P734, DOI [10.4018/978-1-7998-7707-3.ch041, DOI 10.4018/978-1-7998-7707-3.CH041]
   Storey MA, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245132
   Strobelt H, 2016, IEEE T VIS COMPUT GR, V22, P399, DOI 10.1109/TVCG.2015.2467911
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Sullivan AL, 2009, INT J WILDLAND FIRE, V18, P387, DOI 10.1071/WF06144
   Sullivan AL, 2009, INT J WILDLAND FIRE, V18, P349, DOI 10.1071/WF06143
   Sullivan AL, 2009, INT J WILDLAND FIRE, V18, P369, DOI 10.1071/WF06142
   Piralilou ST, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14030672
   Tedim F, 2018, FIRE-BASEL, V1, DOI 10.3390/fire1010009
   Tolhurst K, 2008, AUST J EMERG MANAG, V23, P47
   Tory M, 2004, IEEE T VIS COMPUT GR, V10, P72, DOI 10.1109/TVCG.2004.1260759
   Trucchia A, 2022, FIRE-BASEL, V5, DOI 10.3390/fire5010030
   Trucchia A, 2020, FIRE-BASEL, V3, DOI 10.3390/fire3030026
   Vahdatikhaki F, 2019, AUTOMAT CONSTR, V106, DOI 10.1016/j.autcon.2019.102853
   van Dam A, 2002, COMPUT GRAPH-UK, V26, P535, DOI 10.1016/S0097-8493(02)00113-9
   Visner M, 2021, BUILDINGS-BASEL, V11, DOI 10.3390/buildings11020037
   VR Vision Group, 2021, Alchemy systems fire safety VR training
   VSTEP BV, Response simulator
   Wagner JA, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P483, DOI 10.1109/VR.2018.8447558
   Wahlqvist J, 2021, SAFETY SCI, V136, DOI 10.1016/j.ssci.2020.105145
   Waidelich S, 2018, J COMPUT SCI TECHNOL, V18, P239, DOI 10.24215/16666038.18.e27
   Wang P, 2021, ROBOT CIM-INT MANUF, V72, DOI 10.1016/j.rcim.2020.102071
   Wang Z., 2017, GeoRich 2017-4th International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data, in Conjunction with SIGMOD 2017, P19, DOI DOI 10.1145/3080546.3080549
   Wang ZJ, 2023, VIRTUAL REAL-LONDON, V27, P1145, DOI 10.1007/s10055-022-00718-3
   Weir J., 2017, P BUSHF NAT HAZ CRC
   Weissker Tim, 2021, IEEE Transactions on Visualization and Computer Graphics, V27, P2524, DOI 10.1109/TVCG.2021.3067756
   Whitlock M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P616, DOI [10.1109/VR46266.2020.00-20, 10.1109/VR46266.2020.1582298687237]
   Wijkmark C. H., 2022, P 1 IEEE INT C COGN
   Wildfire Training Solutions Inc, 2022, Lorby wildfire response
   Williams BJ, 2013, FOREST ECOL MANAG, V294, P107, DOI 10.1016/j.foreco.2012.12.008
   Williams-Bell FM, 2015, FIRE TECHNOL, V51, P553, DOI 10.1007/s10694-014-0398-1
   Witt P. W., 1954, Audiovisual Commun. Rev., V2, P291
   WOLFRAM S, 1984, NATURE, V311, P419, DOI 10.1038/311419a0
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Wu R, 2016, IEEE IND ELEC, P4964, DOI 10.1109/IECON.2016.7793478
   Xi MZ, 2014, 2014 IEEE VIRTUAL REALITY (VR), P139, DOI 10.1109/VR.2014.6802090
   Xie B, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.645153
   Xu L, 2021, J MANAGE ENG, V37, DOI 10.1061/(ASCE)ME.1943-5479.0000879
   Xu Z, 2014, ADV ENG SOFTW, V68, P1, DOI 10.1016/j.advengsoft.2013.10.004
   XVR Simulation B.V, 2021, XVR simulation platform
   Yan FT, 2020, MULTIMED TOOLS APPL, V79, P31541, DOI 10.1007/s11042-020-08863-0
   Yang L, 2018, 2018 4TH IEEE INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC 2018), P453, DOI 10.1109/CIC.2018.00068
   Yang YL, 2018, COMPUT GRAPH FORUM, V37, P427, DOI 10.1111/cgf.13431
   You JW, 2022, COMPUT GRAPH-UK, V103, P109, DOI 10.1016/j.cag.2022.01.009
   Yu XH, 2022, BUILDINGS-BASEL, V12, DOI 10.3390/buildings12101523
   Zhang Q, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020397
   Zhou XJ, 2021, VIRTUAL REAL-LONDON, V25, P421, DOI 10.1007/s10055-020-00465-3
NR 186
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4285
EP 4303
DI 10.1109/TVCG.2023.3258440
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700046
PM 37030767
DA 2024-11-06
ER

PT J
AU Wang, XJ
   Lv, QX
   Chen, G
   Zhang, J
   Wei, ZQ
   Dong, JY
   Fu, HB
   Zhu, ZP
   Liu, JX
   Jin, XG
AF Wang, Xinjie
   Lv, Qingxuan
   Chen, Guo
   Zhang, Jing
   Wei, Zhiqiang
   Dong, Junyu
   Fu, Hongbo
   Zhu, Zhipeng
   Liu, Jingxin
   Jin, Xiaogang
TI MobileSky: Real-Time Sky Replacement for Mobile AR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Real-time systems; Cameras; Streaming media; Image color analysis;
   Performance evaluation; Motion segmentation; Task analysis; Mobile
   augmented reality; semantic segmentation; sky replacement
ID IMAGE SEGMENTATION; VIDEO
AB We present MobileSky, the first automatic method for real-time high-quality sky replacement for mobile AR applications. The primary challenge of this task is how to extract sky regions in camera feed both quickly and accurately. While the problem of sky replacement is not new, previous methods mainly concern extraction quality rather than efficiency, limiting their application to our task. We aim to provide higher quality, both spatially and temporally consistent sky mask maps for all camera frames in real time. To this end, we develop a novel framework that combines a new deep semantic network called FSNet with novel post-processing refinement steps. By leveraging IMU data, we also propose new sky-aware constraints such as temporal consistency, position consistency, and color consistency to help refine the weakly classified part of the segmentation output. Experiments show that our method achieves an average of around 30 FPS on off-the-shelf smartphones and outperforms the state-of-the-art sky replacement methods in terms of execution speed and quality. In the meantime, our mask maps appear to be visually more stable across frames. Our fast sky replacement method enables several applications, such as AR advertising, art making, generating fantasy celestial objects, visually learning about weather phenomena, and advanced video-based visual effects. To facilitate future research, we also create a new video dataset containing annotated sky regions with IMU data.
C1 [Wang, Xinjie; Lv, Qingxuan; Wei, Zhiqiang; Dong, Junyu] Ocean Univ China, Dept Comp Sci & Technol, Qingdao 266005, Shandong, Peoples R China.
   [Chen, Guo; Zhang, Jing; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
   [Zhu, Zhipeng; Liu, Jingxin] Guangdong OPPO Mobile Telecommun Corp Ltd, Dongguan 523860, Peoples R China.
C3 Ocean University of China; Zhejiang University; City University of Hong
   Kong
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM wangxinjie@ouc.edu.cn; lvqingxuan@stu.ouc.edu.cn; guo_chenoo@zju.edu.cn;
   jing_z99@163.com; weizhiqiang@ouc.edu.cn; dongjunyu@ouc.edu.cn;
   hongbofu@cityu.edu.hk; zhuzhipeng@oppo.com; liujingxin@oppo.com;
   jin@cad.zju.edu.cn
OI Dong, Junyu/0000-0001-7012-2087; FU, Hongbo/0000-0002-0284-726X;
   /0000-0003-3157-7358; Jin, Xiaogang/0000-0001-7339-2920
FU Shandong Provincial Natural Science Foundation of China [ZR2021QF124];
   China Postdoctoral Science Foundation [2021M703031]; Open Project
   Program of the State Key Lab of CAD&CG , Zhejiang University [A2219];
   National Natural Science Foundation of China [62036010]; Key R&D Program
   of Zhejiang [2022C03126]
FX The work of Xinjie Wang was supported in part by the Shandong Provincial
   Natural Science Foundation of China under Grant ZR2021QF124, in part by
   China Postdoctoral Science Foundation under Grant 2021M703031, andin
   part by the Open Project Program of the State Key Lab of CAD&CG under
   Grant A2219, Zhejiang University. The work of Xiaogang Jin was supported
   inpart by the National Natural Science Foundation of China under Grant
   62036010 and in part by the Key R&D Program of Zhejiang under Grant
   2022C03126.
CR Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   BROWN MB, 1974, J AM STAT ASSOC, V69, P364, DOI 10.2307/2285659
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chen LC, 2017, Arxiv, DOI arXiv:1706.05587
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Cho SH, 2020, Arxiv, DOI arXiv:2002.03651
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Delgado JMD, 2020, ADV ENG INFORM, V45, DOI 10.1016/j.aei.2020.101122
   DUNN OJ, 1964, TECHNOMETRICS, V6, P241, DOI 10.2307/1266041
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Guo YH, 2019, AAAI CONF ARTIF INTE, P8368
   Halperin T, 2019, COMPUT GRAPH FORUM, V38, P207, DOI 10.1111/cgf.13631
   He J. Sun, 2009, IEEE transactions on pattern analysis and machine intelligence, V28, P1, DOI [DOI 10.1109/TPAMI.2010.168, 10.1109/CVPRW.2009.5206515]
   He KM, 2015, Arxiv, DOI [arXiv:1505.00996, 10.48550/arXiv.1505.00996, DOI 10.48550/ARXIV.1505.00996]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Jiang XT, 2020, Arxiv, DOI arXiv:2002.12418
   Jung S., 2022, P CHI C HUM FACT COM, P1
   Kaufman L, 2012, COMPUT GRAPH FORUM, V31, P2528, DOI 10.1111/j.1467-8659.2012.03225.x
   KRUSKAL WH, 1952, J AM STAT ASSOC, V47, P583, DOI 10.1080/01621459.1952.10483441
   Lalonde JF, 2010, INT J COMPUT VISION, V88, P24, DOI 10.1007/s11263-009-0291-4
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Likert R., 1932, Arch Psychol, V140, P5, DOI DOI 10.4135/9781412961288.N454
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Liu C, 2019, PROC CVPR IEEE, P10978, DOI 10.1109/CVPR.2019.01124
   Liu J. Xie, 2021, IEEE Trans. Multimedia, V24, P2727
   Liu SG, 2022, IEEE T MULTIMEDIA, V24, P1299, DOI 10.1109/TMM.2021.3063605
   Lobo J, 2007, INT J ROBOT RES, V26, P561, DOI 10.1177/0278364907079276
   Loesdau M, 2014, LECT NOTES COMPUT SC, V8509, P203, DOI 10.1007/978-3-319-07998-1_23
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Meister Philippe, 2022, Journal of Air Transportation, P113, DOI 10.2514/1.D0308
   Mihail R. P., 2016, P IEEE WINT C APPL C, P6
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Paszke A, 2016, Arxiv, DOI [arXiv:1606.02147, 10.48550/arXiv.1606.02147]
   Paszke A, 2019, ADV NEUR IN, V32
   Poudel R.P., 2019, arXiv, DOI DOI 10.48550/ARXIV.1902.04502
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sun Y., 2021, P IEEE CVF C COMP VI, P11120
   Sun YN, 2021, PROC CVPR IEEE, P6971, DOI 10.1109/CVPR46437.2021.00690
   Szeliski R, 2011, TEXTS COMPUT SCI, P1
   Tan KS, 2011, PATTERN RECOGN, V44, P1, DOI 10.1016/j.patcog.2010.07.013
   Tan MX, 2019, Arxiv, DOI arXiv:1907.09595
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Tao LT, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531374
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Tobias OJ, 2002, IEEE T IMAGE PROCESS, V11, P1457, DOI 10.1109/TIP.2002.806231
   Tran Y., 2020, P CVPR WORKSH COMP V, P1
   Tsai YH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925942
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Xu Z., 2019, PROC ACMEUROGRAPHICS, P21, DOI DOI 10.2312/EXP.20191073
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
   Zou ZX, 2022, IEEE T IMAGE PROCESS, V31, P5067, DOI 10.1109/TIP.2022.3192717
NR 71
TC 2
Z9 2
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4304
EP 4320
DI 10.1109/TVCG.2023.3257840
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700014
PM 37030763
DA 2024-11-06
ER

PT J
AU Hoang, D
   Bhatia, H
   Lindstrom, P
   Pascucci, V
AF Hoang, Duong
   Bhatia, Harsh
   Lindstrom, Peter
   Pascucci, Valerio
TI Progressive Tree-Based Compression of Large-Scale Particle Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Encoding; Decoding; Image reconstruction; Data models; Computational
   modeling; Compressors; Task analysis; Coarse approximation; compression
   (coding); data compaction and compression; hierarchical;
   multiresolution; particle datasets; progressive decompression; tree
   traversal; visualization
ID POINT CLOUDS; MOLECULAR-DYNAMICS; SELF-SIMILARITY; FRAMEWORK; EFFICIENT;
   SURFACES; SCHEME; SYSTEM
AB Scientific simulations and observations using particles have been creating large datasets that require effective and efficient data reduction to store, transfer, and analyze. However, current approaches either compress only small data well while being inefficient for large data, or handle large data but with insufficient compression. Toward effective and scalable compression/decompression of particle positions, we introduce new kinds of particle hierarchies and corresponding traversal orders that quickly reduce reconstruction error while being fast and low in memory footprint. Our solution to compression of large-scale particle data is a flexible block-based hierarchy that supports progressive, random-access, and error-driven decoding, where error estimation heuristics can be supplied by the user. For low-level node encoding, we introduce new schemes that effectively compress both uniform and densely structured particle distributions. Our proposed methods thus target all three phases of a tree-based particle compression pipeline, namely tree construction, tree traversal, and node encoding. The improved efficacy and flexibility of these methods over existing compressors are demonstrated through extensive experimentation, using a wide range of scientific particle datasets.
C1 [Hoang, Duong; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Bhatia, Harsh; Lindstrom, Peter] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
C3 Utah System of Higher Education; University of Utah; United States
   Department of Energy (DOE); Lawrence Livermore National Laboratory
RP Hoang, D (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM duong@sci.utah.edu; hbhatia@llnl.gov; pl@llnl.gov; pascucci@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022
OI Lindstrom, Peter/0000-0003-3817-4199; pascucci,
   valerio/0000-0002-8877-2042; Hoang, Duong/0000-0003-4707-7198
FU U.S. Department of Energy (DOE) [DE-FE0031880]; Exascale Computing
   Project [17-SC-20-SC]; National Nuclear Security Administration; NSF OAC
   award [1842042, 1941085]; NSF CMMI award [1629660]; DOE
   [DE-AC52-07NA27344]; LLNL-LDRD Program [17-SI-004]
FX This work was supported in part by the U.S. Department of Energy
   (DOE)under Grant DE-FE0031880 and in part by the Exascale Computing
   Project under Grant 17-SC-20-SC, a collaborative effort of the DOE
   Office of Science and the National Nuclear Security Administration. This
   work was supported in part by the NSF OAC award under Grant 1842042, in
   part by the NSF OAC award under Grant 1941085, and in part by the NSF
   CMMI award under Grant 1629660. This work was also performed under the
   auspices of the DOE by Lawrence Livermore National Laboratory under
   Grant DE-AC52-07NA27344 and supported in part by the LLNL-LDRD Program
   under Project under Grant 17-SI-004.
CR Alexiou E, 2017, PROC SPIE, V10396, DOI 10.1117/12.2275142
   [Anonymous], 2016, Scientific visualization contest
   Berzins M, 2016, SIAM J SCI COMPUT, V38, pS101, DOI 10.1137/15M1023270
   Botsch M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P53
   Byna S., 2013, P CRAY US GROUP C
   Cai K., 2006, PROC ACM VIRTUAL REA, P83
   Chen SH, 2018, IEEE T SIGNAL PROCES, V66, P666, DOI 10.1109/TSP.2017.2771730
   Cirio G, 2010, GRAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P5
   Devillers O, 2000, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2000.885711
   Digne J, 2014, COMPUT GRAPH FORUM, V33, P155, DOI 10.1111/cgf.12305
   Dodge Y, 2008, The concise encyclopedia of statistics
   Dovrat O, 2019, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR.2019.00287
   Draco, Geometric coding for dynamic voxelized point clouds
   Fan YX, 2013, ASIAPAC SIGN INFO PR
   Fleishman S, 2003, ACM T GRAPHIC, V22, P997, DOI 10.1145/944020.944023
   Fraedrich R, 2009, IEEE T VIS COMPUT GR, V15, P1251, DOI 10.1109/TVCG.2009.142
   Garcia DC, 2020, IEEE T IMAGE PROCESS, V29, P313, DOI 10.1109/TIP.2019.2931466
   github, MPEG-PCC-TMC13: Geometrybased point cloud compression
   Gobbetti E, 2004, COMPUT GRAPH-UK, V28, P815, DOI 10.1016/j.cag.2004.08.010
   Golla T, 2015, IEEE INT C INT ROBOT, P5087, DOI 10.1109/IROS.2015.7354093
   Goswami P, 2013, VISUAL COMPUT, V29, P69, DOI 10.1007/s00371-012-0675-2
   Grottel S, 2012, IEEE T VIS COMPUT GR, V18, P2061, DOI 10.1109/TVCG.2012.282
   Guarda AFR, 2021, IEEE J-STSP, V15, P415, DOI 10.1109/JSTSP.2020.3047520
   Gumhold S., 2005, P ACM SIGGRAPH SKETC
   Habib S, 2016, NEW ASTRON, V42, P49, DOI 10.1016/j.newast.2015.06.003
   Hoang D., 2022, Progressive particle compression
   Hoang D, 2021, SYMP LARG DATA ANAL, P32, DOI 10.1109/LDAV53230.2021.00011
   Hoang D, 2021, IEEE T VIS COMPUT GR, V27, P603, DOI 10.1109/TVCG.2020.3030381
   Hopf M, 2004, IEEE COMPUT GRAPH, V24, P64, DOI 10.1109/MCG.2004.7
   Hosseini M, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P25, DOI 10.1145/3210424.3210429
   Huang LL, 2020, PROC CVPR IEEE, P1310, DOI 10.1109/CVPR42600.2020.00139
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   Hubo E, 2008, COMPUT GRAPH-UK, V32, P221, DOI 10.1016/j.cag.2008.01.012
   Hubo E, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P105
   Isenburg M, 2013, PHOTOGRAMM ENG REM S, V79, P209, DOI 10.14358/PERS.79.2.209
   Jin SA, 2020, INT PARALL DISTRIB P, P105, DOI 10.1109/IPDPS47924.2020.00021
   Khalil JE, 2017, COMPUT GRAPH FORUM, V36, P275, DOI 10.1111/cgf.12938
   Kim TJ, 2010, IEEE T VIS COMPUT GR, V16, P273, DOI 10.1109/TVCG.2009.71
   King D, 1999, COMP GEOM-THEOR APPL, V14, P91, DOI 10.1016/S0925-7721(99)00025-5
   Krivokuca M., 2018, ISO/IEC JTC1/SC29 WG11 (MPEG) input document m42914
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Krüger J, 2006, VISUAL COMPUT, V22, P517, DOI 10.1007/s00371-006-0026-2
   Lasserre S, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P145, DOI 10.1145/3304109.3306224
   Le Muzic Mathieu, 2015, Eurographics Workshop Vis Comput Biomed, V2015, P61, DOI 10.2312/vcbm.20151209
   Lee H, 2003, ACM T GRAPHIC, V22, P471, DOI 10.1145/882262.882294
   Lee H, 2012, VISUAL COMPUT, V28, P137, DOI 10.1007/s00371-011-0602-y
   Lindstrom P., 2003, PROC SYMPOS INTERACT, P93
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Maglo A, 2012, COMPUT GRAPH-UK, V36, P349, DOI 10.1016/j.cag.2012.03.023
   Martinez Rubi O., 2015, Capturing Reality Forum, DOI [10.13140/RG.2.1.1731.4326/1, DOI 10.13140/RG.2.1.1731.4326/1]
   McGuire M., 2017, Computer Graphics Archive
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Merry B, 2006, COMPUT GRAPH FORUM, V25, P709, DOI 10.1111/j.1467-8659.2006.00993.x
   Metere A, 2015, SOFT MATTER, V11, P4606, DOI 10.1039/c5sm00570a
   Milani S, 2020, IEEE T IMAGE PROCESS, V29, P8213, DOI 10.1109/TIP.2020.3011811
   Moffat A, 1998, ACM T INFORM SYST, V16, P256, DOI 10.1145/290159.290162
   Moffat A, 2000, INFORM RETRIEVAL, V3, P25, DOI 10.1023/A:1013002601898
   Nguyen DT, 2021, IEEE T CIRC SYST VID, V31, P4617, DOI 10.1109/TCSVT.2021.3100279
   Ochotta T, 2008, COMPUT GRAPH FORUM, V27, P1647, DOI 10.1111/j.1467-8659.2008.01178.x
   Omeltchenko A, 2000, COMPUT PHYS COMMUN, V131, P78, DOI 10.1016/S0010-4655(00)00083-7
   OpenTopography, About us
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Pascucci V., 2001, P INT C HIGH PERF CO, P45
   Patwary MMA, 2015, PROCEEDINGS OF SC15: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/2807591.2807616
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Peng JL, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1
   Peng JL, 2003, PROC SPIE, V5242, P301, DOI 10.1117/12.510857
   Qingyu Meng, 2012, 2012 SC Companion: High-Performance Computing, Networking, Storage and Analysis (SCC), P2441, DOI 10.1109/SCC.2012.6674233
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rizzi S., 2015, P EUR S PAR GRAPH VI, P1
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Schatz K, 2016, SYMP LARG DATA ANAL, P56, DOI 10.1109/LDAV.2016.7874310
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Schnabel R., 2006, P S POINT BAS GRAPH, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Schütz M, 2020, COMPUT GRAPH FORUM, V39, P155, DOI 10.1111/cgf.14134
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Singh JM, 2006, LECT NOTES COMPUT SC, V4338, P364
   Slattery Stuart, 2019, Zenodo, DOI 10.5281/ZENODO.2591488
   Smith J, 2012, COMPUT GRAPH-UK, V36, P341, DOI 10.1016/j.cag.2012.03.032
   Sweldens W, 1996, Z ANGEW MATH MECH, V76, P41
   Tao DW, 2017, IEEE INT CONF BIG DA, P486, DOI 10.1109/BigData.2017.8257962
   Teuhola J, 2009, COMPUT J, V52, P368, DOI 10.1093/comjnl/bxn030
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Usher W, 2021, INT PARALL DISTRIB P, P547, DOI 10.1109/IPDPS49936.2021.00063
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
   Valette S, 2009, COMPUT GRAPH FORUM, V28, P1301, DOI 10.1111/j.1467-8659.2009.01507.x
   Wald I., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P9, DOI 10.1109/PBG.2005.194058
   Wald I, 2017, IEEE T VIS COMPUT GR, V23, P931, DOI 10.1109/TVCG.2016.2599041
   Wald I, 2015, 2015 IEEE SCIENTIFIC VISUALIZATION CONFERENCE (SCIVIS), P57, DOI 10.1109/SciVis.2015.7429492
   Wang JQ, 2021, IEEE T CIRC SYST VID, V31, P4909, DOI 10.1109/TCSVT.2021.3051377
   WASCHBUSCH M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P95
   Woodring J, 2011, COMPUT GRAPH FORUM, V30, P1151, DOI 10.1111/j.1467-8659.2011.01964.x
   Zhang DZ, 2008, J COMPUT PHYS, V227, P3159, DOI 10.1016/j.jcp.2007.11.021
   Zhiyan Du, 2009, 2009 Data Compression Conference. DCC 2009, P420, DOI 10.1109/DCC.2009.73
NR 96
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4321
EP 4338
DI 10.1109/TVCG.2023.3260628
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700094
PM 37027261
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Qiu, JX
   Yin, ZX
   Cheng, MM
   Ren, B
AF Qiu, Jiaxiong
   Yin, Ze-Xin
   Cheng, Ming-Ming
   Ren, Bo
TI NeRC: Rendering Planar Caustics by Learning Implicit Neural
   Representations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Caustics; centering calibration; photon mapping; sine activation; volume
   rendering
ID GLOBAL ILLUMINATION
AB Caustics are challenging light transport effects for photo-realistic rendering. Photon mapping techniques play a fundamental role in rendering caustics. However, photon mapping methods render single caustics under the stationary light source in a fixed scene view. They require significant storage and computing resources to produce high-quality results. In this paper, we propose efficiently rendering more diverse caustics of a scene with the camera and the light source moving. We present a novel learning-based volume rendering approach with implicit representations for our proposed task. Considering the variety of materials and textures of planar caustic receivers, we decompose the output appearance into two components: the diffuse and specular parts with a probabilistic module. Unlike NeRF, we construct weights for rendering each component from the implicit signed distance function (SDF). Moreover, we introduce the centering calibration and the sine activation function to improve the performance of the color prediction network. Extensive experiments on the synthetic and real-world datasets illustrate that our method achieves much better performance than baselines in the quantitative and qualitative comparison, for rendering caustics in novel views with the dynamic light source. Especially, our method outperforms the baseline on the temporal consistency across frames.
C1 [Qiu, Jiaxiong; Yin, Ze-Xin; Cheng, Ming-Ming; Ren, Bo] Nankai Univ, Coll Comp Sci, VCIP, Tianjin 300000, Peoples R China.
C3 Nankai University
RP Ren, B (corresponding author), Nankai Univ, Coll Comp Sci, VCIP, Tianjin 300000, Peoples R China.
EM qiujiaxiong727@gmail.com; Zexin.Yin.cn@gmail.com; cmm@nankai.edu.cn;
   rb@nankai.edu.cn
RI Qiu, Jiaxiong/KDM-8471-2024; Cheng, Ming-Ming/A-2527-2009
OI Yin, Ze-Xin/0009-0004-1478-7868; Cheng, Ming-Ming/0000-0001-5550-8758;
   Qiu, Jiaxiong/0000-0002-6065-7296
FU National Key Research and Development Program of China [2018AAA0100400];
   NSFC [61922046, 62132012]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0100400, in part by the NSFC under
   Grant 61922046, and in part by the NSFC under Grant 62132012.
CR Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Bemana M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417827
   Bojanowski P, 2019, Arxiv, DOI arXiv:1707.05776
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Gao SH, 2021, PROC CVPR IEEE, P8665, DOI 10.1109/CVPR46437.2021.00856
   Glorot X., 2011, JMLR WORKSHOP C P, P315, DOI DOI 10.1002/ECS2.1832
   Gropp A, 2020, Arxiv, DOI [arXiv:2002.10099, 10.48550/arXiv.2002.10099]
   Hachisuka S., 2008, P ACM SIGGRAPH C AS, P1
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Ioffe Sergey, 2015, P MACHINE LEARNING R, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Jensen H.W., 2001, Realistic Image Synthesis Using Photon Mapping
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kaza S., 2019, Differentiable volume rendering using signed distancefunctions
   Lombardi S, 2019, Arxiv, DOI arXiv:1906.07751
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Pauly M, 2000, SPRING COMP SCI, P11
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Shi XH, 2021, IEEE T VIS COMPUT GR, V27, P4183, DOI 10.1109/TVCG.2021.3106488
   Shirley P, 1995, SPRING COMP SCI, P219
   Sitzmann V., 2020, Adv. Neural Inf. Process. Syst., V33, P7462
   Srinivasan PP, 2021, PROC CVPR IEEE, P7491, DOI 10.1109/CVPR46437.2021.00741
   Trevithick A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15162, DOI 10.1109/ICCV48922.2021.01490
   Tse H, 2012, Gopro Hero 2 Dive Housing - Swimming Pool Test 1080p HD30fps
   Veach E., 1997, Robust monte carlo methods for light transport simulation
   Verbin D, 2022, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR52688.2022.00541
   Walter B, 1997, ACM T GRAPHIC, V16, P217, DOI 10.1145/256157.256158
   Wang PA, 2021, ADV NEUR IN, V34
   Xu B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356547
   Zhang K, 2020, Arxiv, DOI arXiv:2010.07492
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480496
   Zhu SL, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3476828
   Zhu SL, 2020, COMPUT GRAPH FORUM, V39, P35, DOI 10.1111/cgf.14052
NR 36
TC 0
Z9 0
U1 6
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4339
EP 4348
DI 10.1109/TVCG.2023.3259382
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700072
PM 37030762
DA 2024-11-06
ER

PT J
AU Dong, QJ
   Wang, ZX
   Li, MY
   Gao, JJ
   Chen, SM
   Shu, ZY
   Xin, SQ
   Tu, CH
   Wang, WP
AF Dong, Qiujie
   Wang, Zixiong
   Li, Manyi
   Gao, Junjie
   Chen, Shuangmin
   Shu, Zhenyu
   Xin, Shiqing
   Tu, Changhe
   Wang, Wenping
TI Laplacian2Mesh: Laplacian-Based Mesh Understanding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Task analysis; Convolution; Laplace equations; Three-dimensional
   displays; Deep learning; Kernel; Geometric deep learning; laplacian
   pooling; laplacian-beltrami space; mesh understanding
ID SHAPE; DESCRIPTORS
AB Geometric deep learning has sparked a rising interest in computer graphics to perform shape understanding tasks, such as shape classification and semantic segmentation. When the input is a polygonal surface, one has to suffer from the irregular mesh structure. Motivated by the geometric spectral theory, we introduce Laplacian2Mesh, a novel and flexible convolutional neural network (CNN) framework for coping with irregular triangle meshes (vertices may have any valence). By mapping the input mesh surface to the multi-dimensional Laplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis tasks directly using the mature CNNs, without the need to deal with the irregular connectivity of the mesh structure. We further define a mesh pooling operation such that the receptive field of the network can be expanded while retaining the original vertex set as well as the connections between them. Besides, we introduce a channel-wise self-attention block to learn the individual importance of feature ingredients. Laplacian2Mesh not only decouples the geometry from the irregular connectivity of the mesh structure but also better captures the global features that are central to shape classification and segmentation. Extensive tests on various datasets demonstrate the effectiveness and efficiency of Laplacian2Mesh, particularly in terms of the capability of being vulnerable to noise to fulfill various learning tasks.
C1 [Dong, Qiujie; Wang, Zixiong; Gao, Junjie; Xin, Shiqing; Tu, Changhe] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Shandong, Peoples R China.
   [Li, Manyi] Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
   [Chen, Shuangmin] Qingdao Univ Sci & Technol, Sch Informat & Technol, Qingdao 266061, Shandong, Peoples R China.
   [Shu, Zhenyu] Zhejiang Univ, Ningbo Inst Technol, Sch Comp & Data Engn, Ningbo 315100, Zhejiang, Peoples R China.
   [Wang, Wenping] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
C3 Shandong University; Shandong University; Qingdao University of Science
   & Technology; Zhejiang University; Texas A&M University System; Texas
   A&M University College Station
RP Xin, SQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Shandong, Peoples R China.
EM qiujie.jay.dong@gmail.com; zixiong_wang@outlook.com; manyili@sdu.edu.cn;
   gjjsdnu@163.com; csmqq@163.com; shuzhenyu@nit.zju.edu.cn;
   xinshiqing@sdu.edu.cn; chtu@sdu.edu.cn; wenping@cs.hku.hk
RI Tu, Changhe/H-5162-2013; Gao, Junjie/F-7601-2012; Wang,
   Zixiong/JBS-2459-2023
OI Xin, Shiqing/0000-0001-8452-8723; shu, zhenyu/0000-0001-5733-6638; Dong,
   Qiujie/0000-0001-6271-2546; Wang, Zixiong/0000-0002-6170-7339
FU National Key R&D Program of China [2021YFB1715900]; National Natural
   Science Foundation of China [62002190, 62272277, 62172356, 62072284];
   Natural Science Foundation of Shandong Province [ZR2020MF036]; Zhejiang
   Provincial Natural Science Foundation of China [LY22F020026];
   Fundamental Research Funds of Shandong University
FX This work is supported by National Key R&D Program of China under Grant
   2021YFB1715900, in part by the National Natural Science Foundation of
   China under Grants 62002190, 62272277, 62172356, and 62072284, in part
   by the Natural Science Foundation of Shandong Province under Grant
   ZR2020MF036, and in part by the Zhejiang Provincial Natural Science
   Foundation of China under Grant LY22F020026 and the Fundamental Research
   Funds of Shandong University.
CR Adobe, 2006, Adobe fuse 3D characters
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2017, ACM Trans. Graph, DOI DOI 10.1145/3072959.3073616
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Brock A, 2016, Arxiv, DOI [arXiv:1608.04236, 10.48550/arXiv.1608.04236]
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bronstein MM, 2011, IEEE T PATTERN ANAL, V33, P1065, DOI 10.1109/TPAMI.2010.210
   Bruna J., 2013, ARXIV
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Defferrard M, 2016, ADV NEUR IN, V29
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13244
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097
   Giorgi Daniela, 2007, SHREC COMPETITION, V8, P7
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Haim N, 2019, IEEE I CONF COMP VIS, P632, DOI 10.1109/ICCV.2019.00072
   Hanocka H.-T. D., 2021, ACM SIGGRAPH 2021 CO, P1
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3267347
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   He WC, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2214, DOI 10.1145/3394486.3403272
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Lahav A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417806
   Latecki LJ, 2000, IEEE T PATTERN ANAL, V22, P1185, DOI 10.1109/34.879802
   Le T, 2017, COMPUT GRAPH-UK, V66, P103, DOI 10.1016/j.cag.2017.05.011
   Levy H., 2010, ACM SIGGRAPH COURS, P1
   Li R., 2018, INT C NEURAL INF PRO, V31
   Lian Z., 2011, P 3DOR EUR
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   Loop C.T., 1987, Smooth Subdivision Surfaces Based on Triangles
   Bronstein MMM, 2021, Arxiv, DOI arXiv:2104.13478
   Masci D., 2015, P INT C COMP VIS WOR, P37, DOI DOI 10.1109/ICCVW.2015.112
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Milano F, 2020, Arxiv, DOI arXiv:2010.12455
   Kipf TN, 2017, Arxiv, DOI [arXiv:1609.02907, 10.48550/arXiv.1609.02907]
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Pinkall U., 1993, Exp. Math, V2, P15
   Qi CR, 2017, ADV NEUR IN, V30
   Qiao YL, 2022, IEEE T VIS COMPUT GR, V28, P1317, DOI 10.1109/TVCG.2020.3014449
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rustamov R. M., 2007, P S GEOM PROC, V257, P225, DOI 10.1145/1281991.1282022
   Schonsheck S. C., 2018, arXiv
   Schult J., 2020, P IEEECVF C COMPUTER, P8612, DOI 10.1109/CVPR42600.2020.00864
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Sharp N, 2020, COMPUT GRAPH FORUM, V39, P69, DOI 10.1111/cgf.14069
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050
   Wang Y, 2019, HANDB NUM ANAL, V20, P41, DOI 10.1016/bs.hna.2019.08.003
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Xu H., 2017, P IEEE INT C COMP VI, P2698
NR 70
TC 11
Z9 11
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4349
EP 4361
DI 10.1109/TVCG.2023.3259044
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700082
PM 37030768
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Li, RY
   Han, BX
   Li, HW
   Ma, LF
   Zhang, XR
   Zhao, Z
   Liao, HE
AF Li, Ruiyang
   Han, Boxuan
   Li, Haowei
   Ma, Longfei
   Zhang, Xinran
   Zhao, Zhe
   Liao, Hongen
TI A Comparative Evaluation of Optical See-Through Augmented Reality in
   Surgical Guidance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; optical see-through; surgical guidance; head-mounted
   display; autostereoscopic image overlay
ID TOTAL HIP-ARTHROPLASTY; IMAGE OVERLAY; CONTEXT VISUALIZATION; MOUNTED
   DISPLAYS; VIDEO SEE; NAVIGATION; SYSTEM; REGISTRATION; PLACEMENT; FOCUS
AB During traditional surgeries, planning and instrument guidance is displayed on an external screen. Recent developments of augmented reality (AR) techniques can overcome obstacles including hand-eye discoordination and heavy mental load. Among these AR technologies, optical see-through (OST) schemes with stereoscopic displays can provide depth perception and retain the physical scene for safety considerations. However, limitations still exist in certain AR systems and the influence of these factors on surgical performance is yet to explore. To this end, experiments of multi-scale surgical tasks were carried out to compare head-mounted display (HMD) AR and autostereoscopic image overlay (AIO) AR, concerning objective performance and subjective evaluation. To solely analyze effects brought by display techniques, the tracking system in each included display system was identical and similar tracking accuracy was proved by a preliminary experiment. Focus and context rendering was utilized to enhance in-situ visualization for surgical guidance. Latency values of all display systems were assessed and a delay experiment proved the latency differences had no significant impact on user performance. Results of multi-scale surgical tasks showed that HMD outperformed in detailed operations probably due to stable resolution along the depth axis, while AIO had better performance in larger-scale operations for better depth perception. This article helps point out the critical limitations of current OST AR techniques and potentially promotes the progress of AR applications in surgical guidance.
C1 [Li, Ruiyang; Han, Boxuan; Li, Haowei; Ma, Longfei; Zhang, Xinran; Liao, Hongen] Tsinghua Univ, Sch Med, Dept Biomed Engn, Beijing 100084, Peoples R China.
   [Zhao, Zhe] Tsinghua Univ, Beijing Tsinghua Changgung Hosp, Sch Clin Med, Dept Orthopaed, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Liao, HE (corresponding author), Tsinghua Univ, Sch Med, Dept Biomed Engn, Beijing 100084, Peoples R China.
EM liruiruiyang@outlook.com; hbxbobo@hotmail.com;
   lihaowei19991202@gmail.com; malongfei@tsinghua.edu.cn;
   zhangxinran@tsinghua.edu.cn; zhaozhao_02@163.com; liao@tsinghua.edu.cn
OI Zhang, Xinran/0000-0001-9049-2691; haowei, li/0000-0001-6558-1369; Li,
   Ruiyang/0000-0003-3880-068X; Liao, Hongen/0000-0003-3847-9347
FU National Natural Science Foundation of China [82027807, U22A2051,
   81901844]; National Key Research and Development Program of China
   [2022YFC2405200]; Beijing Municipal Natural Science Foundation [7212202,
   L192013]; Institute for Intelligent Healthcare, Tsinghua University
   [2022ZLB001]; Tsinghua-Foshan Innovation Special Fund [2021THFS0104]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 82027807, U22A2051, and 81901844, in
   partby the National Key Research and Development Program of China under
   Grant 2022YFC2405200, in part by Beijing Municipal Natural Science
   Foundation under Grants 7212202 and L192013, in part by Institute for
   Intelligent Healthcare, Tsinghua University under Grant 2022ZLB001, and
   in part by Tsinghua-Foshan Innovation Special Fund under Grant
   2021THFS0104.
CR Abbey B, 2008, NAT PHYS, V4, P394, DOI 10.1038/nphys896
   ADAMS L, 1990, IEEE COMPUT GRAPH, V10, P43, DOI 10.1109/38.55152
   Alexander C, 2020, J ARTHROPLASTY, V35, P1636, DOI 10.1016/j.arth.2020.01.025
   Baumeister J, 2017, IEEE T VIS COMPUT GR, V23, P2378, DOI 10.1109/TVCG.2017.2735098
   Bichlmeier C., 2007, Proc. IEEE ACM Int. Sym. on Mix. Aug. Reality, P1, DOI [DOI 10.1109/ISMAR.2007.4538837.18M, DOI 10.1109/ISMAR.2007.4538837]
   Biedermann R, 2005, J BONE JOINT SURG BR, V87B, P762, DOI 10.1302/0301-620X.87B6
   Birlo M, 2022, MED IMAGE ANAL, V77, DOI 10.1016/j.media.2022.102361
   Brooke J., 1996, Usability Eval. Ind./Taylor Fr, V189, P4, DOI DOI 10.1201/9781498710411-35
   Casari FA, 2021, CURR REV MUSCULOSKE, V14, P192, DOI 10.1007/s12178-021-09699-3
   Condino S, 2020, IEEE T BIO-MED ENG, V67, P411, DOI 10.1109/TBME.2019.2914517
   Cutolo F, 2014, INT SYM MIX AUGMENT, P393
   De Paolis LT, 2019, MED BIOL ENG COMPUT, V57, P995, DOI 10.1007/s11517-018-1929-6
   El-Hariri H, 2018, HEALTHC TECHNOL LETT, V5, P189, DOI 10.1049/htl.2018.5061
   Ewurum CH, 2018, ADV EXP MED BIOL, V1093, P47, DOI 10.1007/978-981-13-1396-7_4
   Fan ZC, 2016, J DISP TECHNOL, V12, P1185, DOI 10.1109/JDT.2016.2569452
   Feng J.E, 2019, JBJS Rev., V7
   Fotouhi J, 2021, IEEE T MED IMAGING, V40, P765, DOI 10.1109/TMI.2020.3037013
   Furman AA, 2021, CURR REV MUSCULOSKE, V14, P397, DOI 10.1007/s12178-021-09728-1
   Gavaghan KA, 2011, IEEE T BIO-MED ENG, V58, P1855, DOI 10.1109/TBME.2011.2126572
   Han BX, 2022, INT J MED ROBOT COMP, V18, DOI 10.1002/rcs.2404
   HART S G, 1988, P139
   Heinrich F, 2020, IEEE T VIS COMPUT GR, V26, P3568, DOI 10.1109/TVCG.2020.3023637
   Tran HH, 2011, LECT NOTES COMPUT SC, V6891, P81, DOI 10.1007/978-3-642-23623-5_11
   Kalkofen D, 2009, IEEE T VIS COMPUT GR, V15, P193, DOI 10.1109/TVCG.2008.96
   Kersten-Oertel M, 2014, IEEE T VIS COMPUT GR, V20, P391, DOI 10.1109/TVCG.2013.240
   Learmonth ID, 2007, LANCET, V370, P1508, DOI 10.1016/S0140-6736(07)60457-7
   Lerotic M, 2007, LECT NOTES COMPUT SC, V4792, P102
   Liao H, 2004, IEEE T INF TECHNOL B, V8, P114, DOI 10.1109/TITB.2004.826734
   Liao HG, 2013, COMPUT MED IMAG GRAP, V37, P81, DOI 10.1016/j.compmedimag.2013.04.001
   Liao HE, 2010, IEEE T BIO-MED ENG, V57, P1476, DOI 10.1109/TBME.2010.2040278
   Liebmann F, 2019, INT J COMPUT ASS RAD, V14, P1157, DOI 10.1007/s11548-019-01973-7
   Liu H, 2018, ANN BIOMED ENG, V46, P1595, DOI 10.1007/s10439-018-2055-1
   Ma LF, 2017, INT J COMPUT ASS RAD, V12, P2205, DOI 10.1007/s11548-017-1652-z
   Macedo MCF, 2015, COMPUT GRAPH-UK, V53, P196, DOI 10.1016/j.cag.2015.09.007
   Martin-Gomez A, 2022, IEEE T VIS COMPUT GR, V28, P4156, DOI 10.1109/TVCG.2021.3079849
   Mavrogenis AF, 2013, ORTHOPEDICS, V36, P631, DOI 10.3928/01477447-20130724-10
   Mezger U, 2013, LANGENBECK ARCH SURG, V398, P501, DOI 10.1007/s00423-013-1059-4
   Müller M, 2013, INT J COMPUT ASS RAD, V8, P663, DOI 10.1007/s11548-013-0828-4
   Ogawa H, 2018, J ARTHROPLASTY, V33, P1833, DOI 10.1016/j.arth.2018.01.067
   Paolis L.T., 2010, ADV BIOMEDICAL SENSI, V55, P305, DOI [DOI 10.1007/978-3-642-05167-8_17, 10.1007/978-3-642-05167-8_17]
   Peters TM, 2006, PHYS MED BIOL, V51, pR505, DOI 10.1088/0031-9155/51/14/R01
   Qian L, 2017, INT J COMPUT ASS RAD, V12, P901, DOI 10.1007/s11548-017-1564-y
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Rolland JP, 2000, PRESENCE-VIRTUAL AUG, V9, P287, DOI 10.1162/105474600566808
   Scheerlinck T, 2014, ACTA ORTHOP BELG, V80, P336
   Shen FY, 2013, INT J COMPUT ASS RAD, V8, P169, DOI 10.1007/s11548-012-0775-5
   Tarwala R, 2011, CURR REV MUSCULOSKE, V4, P151, DOI 10.1007/s12178-011-9086-7
   Wang HX, 2016, INT ORTHOP, V40, P1941, DOI 10.1007/s00264-015-3028-8
   Wang JC, 2014, IEEE T BIO-MED ENG, V61, P1295, DOI 10.1109/TBME.2014.2301191
   Wen R, 2014, COMPUT METH PROG BIO, V116, P68, DOI 10.1016/j.cmpb.2013.12.018
   Xu K, 2014, INT J SURG, V12, P528, DOI 10.1016/j.ijsu.2014.02.014
   Zhang RW, 2021, J MECH MED BIOL, V21, DOI 10.1142/S0219519421500676
   Zhang XR, 2017, IEEE T BIO-MED ENG, V64, P1815, DOI 10.1109/TBME.2016.2624632
NR 53
TC 2
Z9 2
U1 2
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4362
EP 4374
DI 10.1109/TVCG.2023.3260001
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700047
PM 37030748
DA 2024-11-06
ER

PT J
AU Rogha, M
   Sah, S
   Karduni, A
   Markant, D
   Dou, WW
AF Rogha, Milad
   Sah, Subham
   Karduni, Alireza
   Markant, Douglas
   Dou, Wenwen
TI The Impact of Elicitation and Contrasting Narratives on Engagement,
   Recall and Attitude Change With News Articles Containing Data
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Market research; Visualization; Uncertainty; Data
   models; Correlation; Attitude control; Belief elicitation; visual
   elicitation; data visualization; contrasting narratives
ID COGNITION; BIASES; POWER
AB News articles containing data visualizations play an important role in informing the public on issues ranging from public health to politics. Recent research on the persuasive appeal of data visualizations suggests that prior attitudes can be notoriously difficult to change. Inspired by an NYT article, we designed two experiments to evaluate the impact of elicitation and contrasting narratives on attitude change, recall, and engagement. We hypothesized that eliciting prior beliefs leads to more elaborative thinking that ultimately results in higher attitude change, better recall, and engagement. Our findings revealed that visual elicitation leads to higher engagement in terms of feelings of surprise. While there is an overall attitude change across all experiment conditions, we did not observe a significant effect of belief elicitation on attitude change. With regard to recall error, while participants in the draw trend elicitation exhibited significantly lower recall error than participants in the categorize trend condition, we found no significant difference in recall error when comparing elicitation conditions to no elicitation. In a follow-up study, we added contrasting narratives with the purpose of making the main visualization (communicating data on the focal issue) appear strikingly different. Compared to the results of Study 1, we found that contrasting narratives improved engagement in terms of surprise and interest but interestingly resulted in higher recall error and no significant change in attitude. We discuss the effects of elicitation and contrasting narratives in the context of topic involvement and the strengths of temporal trends encoded in the data visualization.
C1 [Rogha, Milad; Sah, Subham; Dou, Wenwen] Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA.
   [Markant, Douglas] Univ North Carolina Charlotte, Dept Psychol Sci, Charlotte, NC 28223 USA.
   [Karduni, Alireza] Simon Fraser Univ, Sch Interact Arts & Technol, Burnaby, BC V5A 1S6, Canada.
C3 University of North Carolina; University of North Carolina Charlotte;
   University of North Carolina; University of North Carolina Charlotte;
   Simon Fraser University
RP Rogha, M (corresponding author), Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA.
EM mrogha@charlotte.edu; ssah1@uncc.edu; alireza116@gmail.com;
   dmarkant@charlotte.edu; wdou1@uncc.edu
OI Markant, Douglas/0000-0003-0568-2648; Sah, Subham/0009-0007-9446-6731;
   Karduni, Alireza/0000-0001-9719-7513; Dou, Wenwen/0000-0003-0319-9484;
   Rogha, Milad/0000-0002-1464-2157
FU NSF [CNS-1747785, CNS-2323795]
FX This work was supported in part by the NSF under Grant CNS-1747785 and
   by the NFS under Grant CNS-2323795.
CR Brinol R. E., 2005, Individual differences in attitude change, P575
   Brod G, 2018, LEARN INSTR, V55, P22, DOI 10.1016/j.learninstruc.2018.01.013
   Bürkner PC, 2017, J STAT SOFTW, V80, P1, DOI 10.18637/jss.v080.i01
   Bycoffe E., 2008, Howunpopular is Joe Biden?
   Ceja CR, 2021, IEEE T VIS COMPUT GR, V27, P1054, DOI 10.1109/TVCG.2020.3030422
   Christie D., 2022, Comprehensive renewable energy, V2nd, P149, DOI [10.1016/B978-0-12-819727-1.00083-2, DOI 10.1016/B978-0-12-819727-1.00083-2]
   Curran PJ, 2009, PSYCHOL METHODS, V14, P81, DOI 10.1037/a0015914
   Fazio LK, 2009, PSYCHON B REV, V16, P88, DOI 10.3758/PBR.16.1.88
   FiveThirtyEight, about us
   Gupta A., 2021, Comput. Graph. Forum, V40, P207
   Haddock G, 2019, ADV EXP SOC PSYCHOL, V59, P53, DOI 10.1016/bs.aesp.2018.10.002
   Heyer J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376887
   Josh K., 2017, You draw it: Just how bad is the drug overdose epidemic?
   Karduni A, 2021, IEEE T VIS COMPUT GR, V27, P978, DOI 10.1109/TVCG.2020.3029412
   Kim L. A., 2019, P CHI C HUM FACT COM, P1
   Kim YS, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1375, DOI 10.1145/3025453.3025592
   Kim YS, 2018, IEEE T VIS COMPUT GR, V24, P760, DOI 10.1109/TVCG.2017.2745240
   Kirsh D, 2010, AI SOC, V25, P441, DOI 10.1007/s00146-010-0272-8
   Koonchanok G. Y., 2023, arXiv
   Koonchanok P., 2021, P CHI C HUM FACT COM, P1
   Kruschke JK, 2018, PSYCHON B REV, V25, P178, DOI 10.3758/s13423-016-1221-4
   Liao Q Vera, 2013, P SIGCHI C HUM FACT, P2359, DOI DOI 10.1145/2470654.24813
   Liem J, 2020, COMPUT GRAPH FORUM, V39, P277, DOI 10.1111/cgf.13980
   Mahajan S, 2022, COMPUT GRAPH FORUM, V41, P477, DOI 10.1111/cgf.14556
   Makowski D, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02767
   Mantri Prateek, 2023, IEEE Trans Vis Comput Graph, V29, P1005, DOI 10.1109/TVCG.2022.3209467
   Markant D, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581330
   McColeman CM, 2021, IEEE T VIS COMPUT GR, V27, P1063, DOI 10.1109/TVCG.2020.3030345
   O'Brien HL, 2018, INT J HUM-COMPUT ST, V112, P28, DOI 10.1016/j.ijhcs.2018.01.004
   OBrien Heather, 2016, Why engagement matters: Cross-disciplinary perspectives of user engagement in digital media, P1
   Pandey AV, 2014, IEEE T VIS COMPUT GR, V20, P2211, DOI 10.1109/TVCG.2014.2346419
   Petty J., 2002, Mass Media Attitude Change: Impli-cations on the Elaboration Likelihood Model of Persuasion, P155
   Petty RE, 1999, DUAL-PROCESS THEORIES IN SOCIAL PSYCHOLOGY, P41
   Petty RE, 2001, SOC COGNITION, V19, P418, DOI 10.1521/soco.19.4.418.20758
   Ritchie H, 1999, Drug use
   Sanborn AN, 2010, COGNITIVE PSYCHOL, V60, P63, DOI 10.1016/j.cogpsych.2009.07.001
   Sanborn T., ADV NEURAL INF PROCE
   Upshot, 2014, The New York TimesApr. 22,
   Vehtari A, 2017, STAT COMPUT, V27, P1413, DOI 10.1007/s11222-016-9696-4
   Wagner R. E., 2011, TheoriesSoc. Psychol., V1, P96
   Wallace-Wells B, 2021, Is there a case for legalizing heroin?
   Xiong Cindy, 2023, IEEE Trans Vis Comput Graph, V29, P493, DOI 10.1109/TVCG.2022.3209405
NR 42
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4375
EP 4389
DI 10.1109/TVCG.2024.3355884
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700015
PM 38241101
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Miandji, E
   Tongbuasirilai, T
   Hajisharif, S
   Kavoosighafi, B
   Unger, J
AF Miandji, Ehsan
   Tongbuasirilai, Tanaboon
   Hajisharif, Saghi
   Kavoosighafi, Behnaz
   Unger, Jonas
TI FROST-BRDF: A Fast and Robust Optimal Sampling Technique for BRDF
   Acquisition
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Dictionaries; Image reconstruction; Compressed sensing;
   Sensors; Optimization; Rendering (computer graphics); Rendering;
   compressed sensing; multiple measurement vector; SOMP; BRDF measurement;
   BRDF reconstruction
ID REFLECTANCE; ALGORITHMS; RECOVERY; FIELD
AB Efficient and accurate BRDF acquisition of real world materials is a challenging research problem that requires sampling millions of incident light and viewing directions. To accelerate the acquisition process, one needs to find a minimal set of sampling directions such that the recovery of the full BRDF is accurate and robust given such samples. In this article, we formulate BRDF acquisition as a compressed sensing problem, where the sensing operator is one that performs sub-sampling of the BRDF signal according to a set of optimal sample directions. To solve this problem, we propose the Fast and Robust Optimal Sampling Technique (FROST) for designing a provably optimal sub-sampling operator that places light-view samples such that the recovery error is minimized. FROST casts the problem of designing an optimal sub-sampling operator for compressed sensing into a sparse representation formulation under the Multiple Measurement Vector (MMV) signal model. The proposed reformulation is exact, i.e. without any approximations, hence it converts an intractable combinatorial problem into one that can be solved with standard optimization techniques. As a result, FROST is accompanied by strong theoretical guarantees from the field of compressed sensing. We perform a thorough analysis of FROST-BRDF using a 10-fold cross-validation with publicly available BRDF datasets and show significant advantages compared to the state-of-the-art with respect to reconstruction quality. Finally, FROST is simple, both conceptually and in terms of implementation, it produces consistent results at each run, and it is at least two orders of magnitude faster than the prior art.
C1 [Miandji, Ehsan] Linkoping Univ, Dept Sci & Technol, Comp Graph, S-58183 Linkoping, Sweden.
   [Hajisharif, Saghi] Linkoping Univ, Comp Graph & Image Proc CGIP Lab, S-58183 Linkoping, Sweden.
   [Kavoosighafi, Behnaz] Linkoping Univ, Comp Graph & Image Proc, S-58183 Linkoping, Sweden.
   [Unger, Jonas] Linkoping Univ, Comp Graph, S-58183 Linkoping, Sweden.
   [Tongbuasirilai, Tanaboon] Linkoping Univ, S-58183 Linkoping, Sweden.
   [Tongbuasirilai, Tanaboon] Kasetsart Univ, Bangkok 10900, Thailand.
C3 Linkoping University; Linkoping University; Linkoping University;
   Linkoping University; Linkoping University; Kasetsart University
RP Miandji, E (corresponding author), Linkoping Univ, Dept Sci & Technol, Comp Graph, S-58183 Linkoping, Sweden.
EM ehsan.miandji@liu.se; tanaboon.to@ku.th; saghi.hajisharif@liu.se;
   behnaz.kavoosighafi@liu.se; jonas.unger@liu.se
RI Tongbuasirilai, Tanaboon/JDM-2247-2023; Miandji, Ehsan/ACN-7116-2022
OI Kavoosighafi, Behnaz/0000-0002-1951-7515; Unger,
   Jonas/0000-0002-7765-1747; Hajisharif, Saghi/0000-0002-0176-5852;
   Tongbuasirilai, Tanaboon/0000-0002-3239-8581
FU European Union [956585]
FX This work was supported by European Union's Horizon 2020 research and
   innovation programme under the Marie Skodowska-Curie under Grant Grant
   956585 (PRIME).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967
   Bagher MM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907941
   Ben-Artzi A, 2006, ACM T GRAPHIC, V25, P945, DOI 10.1145/1141911.1141979
   Bilgili A, 2011, COMPUT GRAPH FORUM, V30, P2427, DOI 10.1111/j.1467-8659.2011.02072.x
   Blanz V, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P293, DOI 10.1109/TDPVT.2004.1335212
   Boss M, 2019, Arxiv, DOI arXiv:1910.05148
   Boss M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12664, DOI 10.1109/ICCV48922.2021.01245
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chen Z, 2022, IEEE T PATTERN ANAL, V44, P9380, DOI 10.1109/TPAMI.2021.3129537
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378
   Dupuy J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275059
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Gardner A, 2003, ACM T GRAPHIC, V22, P749, DOI 10.1145/882262.882342
   Ghosh A, 2010, INT J COMPUT VISION, V90, P183, DOI 10.1007/s11263-008-0151-7
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Guo Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417779
   Hajisharif S, 2020, COMPUT GRAPH FORUM, V39, P463, DOI 10.1111/cgf.13944
   Han NN, 2021, IEEE SIGNAL PROC LET, V28, P1320, DOI 10.1109/LSP.2021.3089434
   Henzler P, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480507
   Jakob W, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601139
   Kang KZ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356492
   Kang KZ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201279
   Kautz J, 1999, SPRING EUROGRAP, P247
   Kim J, 2020, IEEE T INFORM THEORY, V66, P5072, DOI 10.1109/TIT.2020.2986917
   Lavoué G, 2021, COMPUT GRAPH FORUM, V40, P327, DOI 10.1111/cgf.142636
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Lensch H. P. A., 2005, P ACM SIGGRAPH 2005, P1
   Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5
   Lin X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661262
   Löw J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077350
   Marschner SR, 1999, SPRING EUROGRAP, P131
   Marwah K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461914
   Matusik W., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P241
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Miandji E., 2018, Ph.D. dissertation
   Miandji E, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269980
   Miandji E, 2017, IEEE SIGNAL PROC LET, V24, P1646, DOI 10.1109/LSP.2017.2753939
   Miandji E, 2015, COMPUT GRAPH FORUM, V34, P33, DOI 10.1111/cgf.12539
   Nicodemus F.E., 1992, NBS Monogr, V160, P4
   Nielsen JB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818085
   Otani H, 2019, LECT NOTES COMPUT SC, V11542, P483, DOI 10.1007/978-3-030-22514-8_48
   Peers P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477929
   Pharr M., 2010, PHYS BASED RENDERING
   Romeiro F, 2008, LECT NOTES COMPUT SC, V5305, P859, DOI 10.1007/978-3-540-88693-8_63
   Romeiro F, 2010, LECT NOTES COMPUT SC, V6311, P45, DOI 10.1007/978-3-642-15549-9_4
   Rubinstein R., 2012, Ph.D. dissertation
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Sen P, 2011, IEEE T VIS COMPUT GR, V17, P487, DOI 10.1109/TVCG.2010.46
   Sen P, 2009, COMPUT GRAPH FORUM, V28, P609, DOI 10.1111/j.1467-8659.2009.01401.x
   Seylan N, 2013, WSCG 2013, FULL PAPERS PROCEEDINGS, P88
   Shi L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417781
   Soler C, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13348
   Sun TC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275026
   Sztrajman A, 2021, COMPUT GRAPH FORUM, V40, P332, DOI 10.1111/cgf.14335
   Tongbuasirilai T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3533427
   Tongbuasirilai T, 2020, VISUAL COMPUT, V36, P855, DOI 10.1007/s00371-019-01664-z
   Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Tunwattanapong B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461944
   Wang RM, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2557449
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Xu YY, 2014, INVERSE PROBL IMAG, V8, P901, DOI 10.3934/ipi.2014.8.901
   Xu ZX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201313
   Xu ZX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982396
   Yu Jiyang, 2016, P EUR WORKSH MAT APP, P19
   Zhou XL, 2021, COMPUT GRAPH FORUM, V40, P315, DOI 10.1111/cgf.142635
   Zupancic B., 2013, P SIGGRAPH AS 2013
NR 72
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4390
EP 4402
DI 10.1109/TVCG.2024.3355200
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700081
PM 38231803
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Skrodzki, M
   van Geffen, H
   Chaves-de-Plaza, NF
   Höllt, T
   Eisemann, E
   Hildebrandt, K
AF Skrodzki, Martin
   van Geffen, Hunter
   Chaves-de-Plaza, Nicolas F.
   Hollt, Thomas
   Eisemann, Elmar
   Hildebrandt, Klaus
TI Accelerating Hyperbolic t-SNE
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dimensionality reduction; t-SNE; hyperbolic embedding; acceleration
   structure
ID DIMENSIONALITY REDUCTION; MDS
AB The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This article introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We compare our approach with existing methods and demonstrate that it computes embeddings of similar quality in significantly less time. Implementation and scripts for the experiments can be found at https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.
C1 [Skrodzki, Martin; van Geffen, Hunter; Chaves-de-Plaza, Nicolas F.; Hollt, Thomas; Eisemann, Elmar; Hildebrandt, Klaus] Delft Univ Technol, NL-2628CD Delft, Netherlands.
C3 Delft University of Technology
RP Skrodzki, M (corresponding author), Delft Univ Technol, NL-2628CD Delft, Netherlands.
EM m.skrodzki@tudelft.nl; huntervangeffen@gmail.com;
   n.f.chavesdeplaza@tudelft.nl; T.Hollt-1@tudelft.nl;
   e.eisemann@tudelft.nl; k.a.hildebrandt@tudelft.nl
RI Skrodzki, Martin/AAU-7540-2021
OI Hildebrandt, Klaus/0000-0002-9196-3923; Chaves-de-Plaza, Nicolas
   F./0000-0003-4971-3151; Skrodzki, Martin/0000-0002-8126-0511
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [455095046]; Varian; Siemens Healthineers Company, through the Holland
   PTC-Varian Consortium [2019022]; Surcharge for Top Consortia for
   Knowledge and Innovation (TKIs) Ministry of Economic Affairs and Climate
FX The work of Martin Skrodzki was supported by the Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) under Grant
   455095046. The work of Nicolas F. Chaves-de-Plaza was supported in
   partby Varian, a Siemens Healthineers Company, through the Holland
   PTC-Varian Consortium under Grant 2019022, and in part by the Surcharge
   for Top Consortia for Knowledge and Innovation (TKIs) from the Ministry
   of Economic Affairs and Climate.
CR Andrews TS, 2021, NAT PROTOC, V16, P1, DOI 10.1038/s41596-020-00409-w
   Becht E, 2019, NAT BIOTECHNOL, V37, P38, DOI 10.1038/nbt.4314
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkina AC, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-13055-y
   Bläsius T, 2018, IEEE ACM T NETWORK, V26, P920, DOI 10.1109/TNET.2018.2810186
   Boguñá M, 2010, NAT COMMUN, V1, DOI 10.1038/ncomms1063
   Bohm J.N., 2022, J. Mach. Learn. Res., V23, P1
   Cannon J.W., 1997, MATH SCI RES I PUBL, V31, P59
   Cao XF, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109302
   Cho M, 2017, ADV NEUR IN, V30
   Conway J. H., 2008, The symmetries of things
   Cvetkovski A., 2016, Appl. Math, V10, P125
   Eppstein D., 2021, Graph Drawing and Network Visualization-29th International Symposium, GD 2021, Tubingen, Germany, September 14-17, 2021, P343, DOI 10.1007/978-3-030-92931-2
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Fellbaum C, 1998, LANG SPEECH & COMMUN, P1
   Friedrich T, 2015, IEEE INFOCOM SER, DOI 10.1109/INFOCOM.2015.7218533
   Ganea OE, 2018, ADV NEUR IN, V31
   Gulcehre C, 2018, Arxiv, DOI arXiv:1805.09786
   Guo YH, 2022, PROC CVPR IEEE, P11, DOI 10.1109/CVPR52688.2022.00011
   Ingram S, 2009, IEEE T VIS COMPUT GR, V15, P249, DOI 10.1109/TVCG.2008.85
   Isozaki H., 2014, Introduction to Spectral Theory and Inverse Problem on Asymptotically Hyperbolic Manifolds, V32, P11, DOI DOI 10.2969/MSJMEMOIRS/03201C010
   JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Keller-Ressel M, 2020, J COMPLEX NETW, V8, DOI 10.1093/comnet/cnaa002
   Klimovskaia A, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16822-4
   Kobak D, 2021, NAT BIOTECHNOL, V39, DOI 10.1038/s41587-020-00809-z
   Kobak D, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-13056-x
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kopczynski E., 2021, arXiv
   Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106
   Krumsiek J, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022649
   Lamping J., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P401
   Lin Y.-W. E., 2023, P 40 INT C MACH LEAR
   Linderman GC, 2019, NAT METHODS, V16, P243, DOI 10.1038/s41592-018-0308-4
   Lou A., 2020, P MACHINE LEARNING R, P6393
   Miller J, 2022, IEEE PAC VIS SYMP, P71, DOI 10.1109/PacificVis53943.2022.00016
   Narechania A, 2022, IEEE T VIS COMPUT GR, V28, P486, DOI 10.1109/TVCG.2021.3114820
   Nickel M., 2017, P 31 INT C NEUR INF, P6341
   Ontrup J, 2006, NEURAL NETWORKS, V19, P751, DOI 10.1016/j.neunet.2006.05.015
   Pezzotti N, 2016, COMPUT GRAPH FORUM, V35, P21, DOI 10.1111/cgf.12878
   Pezzotti N, 2020, IEEE T VIS COMPUT GR, V26, P1172, DOI 10.1109/TVCG.2019.2934307
   Pirolli P., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P506, DOI 10.1145/365024.365337
   Plass M, 2018, SCIENCE, V360, DOI 10.1126/science.aaq1723
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sala F, 2018, PR MACH LEARN RES, V80
   Sarkar R, 2012, LECT NOTES COMPUT SC, V7034, P355
   Senning J. R., 2019, Computing and estimating the rate of convergence
   van de Ruit M, 2022, IEEE T VIS COMPUT GR, V28, P614, DOI 10.1109/TVCG.2021.3114817
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Verbeek K, 2016, COMP GEOM-THEOR APPL, V59, P1, DOI 10.1016/j.comgeo.2016.08.003
   von Looz M, 2015, LECT NOTES COMPUT SC, V9472, P467, DOI 10.1007/978-3-662-48971-0_40
   Walter JA, 2004, INFORM SYST, V29, P273, DOI 10.1016/j.is.2003.10.002
   Wang JP, 2022, IEEE T VIS COMPUT GR, V28, P4141, DOI 10.1109/TVCG.2021.3076749
   Wu J, 2022, IEEE T VIS COMPUT GR, V28, P835, DOI 10.1109/TVCG.2021.3114832
   Wu ZJ, 2020, GENOME BIOL, V21, DOI 10.1186/s13059-020-02027-x
   Xia JZ, 2022, IEEE T VIS COMPUT GR, V28, P529, DOI 10.1109/TVCG.2021.3114694
   Xiao H, 2022, P 2022 C EMP METH NA, P5228, DOI [10.18653/V1/2022.EMNLP-MAIN, DOI 10.18653/V1/2022.EMNLP-MAIN]
   Zhang Chenyang, 2023, IEEE Trans Vis Comput Graph, V29, P767, DOI 10.1109/TVCG.2022.3209440
   Zhou YS, 2022, NEURAL COMPUT, V34, P1637, DOI 10.1162/neco_a_01504
   Zhou YS, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.102225
   Zhou YS, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aaq1458
NR 62
TC 0
Z9 0
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4403
EP 4415
DI 10.1109/TVCG.2024.3364841
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700070
PM 38345956
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Xu, RK
   Zhang, L
   Zhang, FL
AF Xu, Rong-Kai
   Zhang, Lei
   Zhang, Fang-Lue
TI Intrinsic Omnidirectional Image Decomposition With Illumination
   Pre-Extraction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Reflectivity; Light sources; Rendering (computer graphics);
   Image decomposition; Geometry; Data mining; Intrinsic decomposition;
   omnidirectional image; reflectance; shading
ID RETINEX; MODEL
AB Capturing an omnidirectional image with a 360-degree field of view entails capturing intricate spatial and lighting details of the scene. Consequently, existing intrinsic image decomposition methods face significant challenges when attempting to separate reflectance and shading components from a low dynamic range (LDR) omnidirectional images. To address this, our article introduces a novel method specifically designed for the intrinsic decomposition of omnidirectional images. Leveraging the unique characteristics of the 360-degree scene representation, we employ a pre-extraction technique to isolate specific illumination information. Subsequently, we establish new constraints based on these extracted details and the inherent characteristics of omnidirectional images. These constraints limit the illumination intensity range and incorporate spherical-based illumination variation. By formulating and solving an objective function that accounts for these constraints, our method achieves a more accurate separation of reflectance and shading components. Comprehensive qualitative and quantitative evaluations demonstrate the superiority of our proposed method over state-of-the-art intrinsic decomposition methods.
C1 [Xu, Rong-Kai; Zhang, Lei] Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
C3 Beijing Institute of Technology; Victoria University Wellington
RP Zhang, L (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM zjlxxrk@gmail.com; leizhang@bit.edu.cn; fanglue.zhang@vuw.ac.nz
OI Xu, RongKai/0009-0004-2836-2676
FU National Natural Science Foundation of China [62132012]; Marsden Fund
   Council [MFP-20-VUW-180]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62132012, and in part by Marsden Fund Council managed
   by the Royal Society of New Zealand under Grant MFP-20-VUW-180.
CR Armeni I., 2017, arXiv
   Baslamisli AS, 2021, J OPT SOC AM A, V38, DOI 10.1364/JOSAA.414682
   Bell Sean, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601206
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149
   Das Partha, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P605, DOI 10.1007/978-3-031-25066-8_35
   Das P, 2022, PROC CVPR IEEE, P19758, DOI 10.1109/CVPR52688.2022.01917
   Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932
   Fu G, 2019, IEEE INT CON MULTI, P175, DOI 10.1109/ICME.2019.00038
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Garces E, 2022, INT J COMPUT VISION, V130, P836, DOI 10.1007/s11263-021-01563-8
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Gardner MA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130891
   Gardner MA, 2019, IEEE I CONF COMP VIS, P7174, DOI 10.1109/ICCV.2019.00727
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Jia Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P519, DOI 10.1007/978-3-030-58545-7_30
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   Kovacs B, 2017, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2017.97
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li J., 2021, P IEEE C COMP VIS PA, p10 586
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Li Z, 2022, PROC CVPR IEEE, P12703, DOI 10.1109/CVPR52688.2022.01238
   Li ZQ, 2022, LECT NOTES COMPUT SC, V13666, P555, DOI 10.1007/978-3-031-20068-7_32
   Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255
   Luo JD, 2020, IEEE T VIS COMPUT GR, V26, P3434, DOI 10.1109/TVCG.2020.3023565
   Ma WY, 2012, INVERSE PROBL IMAG, V6, P697, DOI 10.3934/ipi.2012.6.697
   Ma WY, 2011, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2011.5995422
   Meka A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3374753
   Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907
   Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342
   Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192
   Qian YL, 2021, IEEE WINT CONF APPL, P3168, DOI 10.1109/WACV48630.2021.00321
   Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869
   Shen HL, 2009, APPL OPTICS, V48, P2711, DOI 10.1364/AO.48.002711
   Shen JB, 2011, PROC CVPR IEEE
   Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619
   Wang FE, 2020, PROC CVPR IEEE, P459, DOI 10.1109/CVPR42600.2020.00054
   Wang Z., 2021, P IEEE INT C COMP VI, P12538
   Weber H, 2018, INT CONF 3D VISION, P199, DOI 10.1109/3DV.2018.00032
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yue HJ, 2017, IEEE T IMAGE PROCESS, V26, P3981, DOI 10.1109/TIP.2017.2703078
   Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77
   Zhou H, 2019, IEEE I CONF COMP VIS, P7819, DOI 10.1109/ICCV.2019.00791
   Zhu JS, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555407
   Zhu R, 2022, PROC CVPR IEEE, P2812, DOI 10.1109/CVPR52688.2022.00284
NR 47
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4416
EP 4428
DI 10.1109/TVCG.2024.3366343
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700003
PM 38358860
DA 2024-11-06
ER

PT J
AU Hogräfer, M
   Schulz, HJ
AF Hografer, Marius
   Schulz, Hans-Jorg
TI Tailorable Sampling for Progressive Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Pipelines; Data visualization; Visual analytics;
   Switches; Data structures; Image color analysis; Progressive visual
   analytics; sampling; visual analytics
ID VISUALIZATIONS
AB Progressive visual analytics (PVA) allows analysts to maintain their flow during otherwise long-running computations by producing early, incomplete results that refine over time, for example, by running the computation over smaller partitions of the data. These partitions are created using sampling, whose goal it isto draw samples of the dataset such that the progressive visualization becomes as useful as possible as soon as possible. What makes the visualization useful depends on the analysis task and, accordingly, some task-specific sampling methods have been proposed for PVA to address this need. However, as analysts see more and more of their data during the progression, the analysis task at hand often changes, which means that analysts need to restart the computation to switch the sampling method, causing them to lose their analysis flow. This poses a clear limitation to the proposed benefits of PVA. Hence, we propose a pipeline for PVA-sampling that allows tailoring the data partitioning to analysis scenarios by switching out modules in a way that does not require restarting the analysis. To that end, we characterize the problem of PVA-sampling, formalize the pipeline in terms of data structures, discuss on-the-fly tailoring, and present additional examples demonstrating its usefulness.
C1 [Hografer, Marius; Schulz, Hans-Jorg] Aarhus Univ, Dept Comp Sci, DK-8000 Aarhus, Denmark.
C3 Aarhus University
RP Hogräfer, M (corresponding author), Aarhus Univ, Dept Comp Sci, DK-8000 Aarhus, Denmark.
EM mhograefer@cs.au.dk; hjschulz@cs.au.dk
RI Schulz, Hans-Jorg/G-1788-2013
OI Schulz, Hans-Jorg/0000-0001-9974-535X
FU Innovationsfonden
FX No Statement Available
CR Agarwal S, 2012, PROC VLDB ENDOW, V5, P1902, DOI 10.14778/2367502.2367533
   Agarwal Sameer, 2013, 8 EUR C 2013 EUROSYS, P29
   Angelini M, 2018, INFORMATICS-BASEL, V5, DOI 10.3390/informatics5030031
   Angelini Marco, 2019, P 10 INT EUROVIS WOR, P25, DOI [10.2312/eurova.20191120, DOI 10.2312/EUROVA.20191120]
   Badam SK, 2017, COMPUT GRAPH FORUM, V36, P491, DOI 10.1111/cgf.13205
   Berg L, 2019, PROC VLDB ENDOW, V12, P1814, DOI 10.14778/3352063.3352073
   Card SK., 1999, READINGS INFORM VISU
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chen X, 2022, IEEE T VIS COMPUT GR, V28, P593, DOI 10.1109/TVCG.2021.3114880
   Chi EH, 2000, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2000, P69, DOI 10.1109/INFVIS.2000.885092
   Cui Z, 2019, 2019 IEEE VISUALIZATION IN DATA SCIENCE (VDS), P48, DOI [10.1109/vds48975.2019.8973384, 10.1109/VDS48975.2019.8973384]
   Demir I, 2014, IEEE T VIS COMPUT GR, V20, P2694, DOI 10.1109/TVCG.2014.2346448
   Ding BL, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P679, DOI 10.1145/2882903.2915249
   Ellis G., 2005, CHI'05 Extended Abstracts on Human factors in Computing Systems, P1351, DOI [10.1145/1056808.1056914, DOI 10.1145/1056808.1056914]
   Elmqvist N, 2011, INFORM VISUAL, V10, P327, DOI 10.1177/1473871611413180
   Fekete JD, 2016, Arxiv, DOI arXiv:1607.05162
   Fisher D., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P73, DOI 10.1109/LDAV.2011.6092320
   Gu B., 2001, MACHINE LEARNING ECM, P192, DOI DOI 10.1007/3-540-44795-4_17
   Heer J, 2006, IEEE T VIS COMPUT GR, V12, P853, DOI 10.1109/TVCG.2006.178
   Hografer M., 2022, P INT EUROVIS WORKSH, P49, DOI [10.2312/eurova.20221079, DOI 10.2312/EUROVA.20221079]
   Hogräfer M, 2022, ACM T INTEL SYST TEC, V13, DOI 10.1145/3531229
   Jo J, 2021, IEEE T VIS COMPUT GR, V27, P3109, DOI 10.1109/TVCG.2019.2962404
   Kwon BC, 2017, IEEE COMPUT GRAPH, V37, P100, DOI 10.1109/MCG.2017.6
   Li JPK, 2020, IEEE T VIS COMPUT GR, V26, P1151, DOI 10.1109/TVCG.2019.2934537
   Losing V, 2016, IEEE DATA MINING, P291, DOI [10.1109/ICDM.2016.0040, 10.1109/ICDM.2016.141]
   Micallef Luana, 2019, IN PRESS, P19, DOI [10.2312/evs.20191164, DOI 10.2312/EVS.20191164]
   OLKEN F, 1990, LECT NOTES COMPUT SC, V420, P92
   Onus M, 2007, SIAM PROC S, P99
   Patil Ameya, 2023, IEEE Trans Vis Comput Graph, V29, P407, DOI 10.1109/TVCG.2022.3209426
   Procopio M, 2022, IEEE T VIS COMPUT GR, V28, P3093, DOI 10.1109/TVCG.2021.3051013
   Procopio M, 2019, INFORMATICS-BASEL, V6, DOI 10.3390/informatics6010014
   Provost F., 1999, P ACM SIGKDD INT C K, P23, DOI [10.1145/312129.312188, DOI 10.1145/312129.312188]
   Rahman S, 2017, PROC VLDB ENDOW, V10, P1262, DOI 10.14778/3137628.3137637
   Schulz HJ, 2016, IEEE T VIS COMPUT GR, V22, P1830, DOI 10.1109/TVCG.2015.2462356
   Sculley D. etal., 2010, P 19 INT C WORLD WID, DOI [https://doi.org/10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]
   Settles B., 2009, Tech. Rep. 1649
   Stolper CD, 2014, IEEE T VIS COMPUT GR, V20, P1653, DOI 10.1109/TVCG.2014.2346574
   Turkay C, 2017, IEEE T VIS COMPUT GR, V23, P131, DOI 10.1109/TVCG.2016.2598470
   VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165
   Williams M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P57, DOI 10.1109/INFVIS.2004.60
   Yang J, 2003, VLDB J, V12, P262, DOI [10.1007/s00778-003-0107-z, 10.1007/S00778-003-0107-Z]
   Zgraggen E, 2017, IEEE T VIS COMPUT GR, V23, P1977, DOI 10.1109/TVCG.2016.2607714
   Zheng Y., 2013, P ACM SIGMOD INT C M, P433, DOI DOI 10.1145/2463676,2465319
   Zheng Y, 2017, 2017 IEEE VISUALIZATION IN DATA SCIENCE (VDS), P23, DOI 10.1109/VDS.2017.8573446
   Zhou ZG, 2021, IEEE T VIS COMPUT GR, V27, P1709, DOI 10.1109/TVCG.2020.3030440
NR 45
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4809
EP 4824
DI 10.1109/TVCG.2023.3278084
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400020
PM 37204960
OA hybrid
DA 2024-11-06
ER

PT J
AU Dennig, FL
   Miller, M
   Keim, DA
   El-Assady, M
AF Dennig, Frederik L.
   Miller, Matthias
   Keim, Daniel A.
   El-Assady, Mennatallah
TI FS/DS: A Theoretical Framework for the Dual Analysis of Feature Space
   and Data Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Analytical models; Data models; Visual analytics;
   Task analysis; Adaptation models; Taxonomy; dual analysis; feature
   space; data space; feature exploration; mixed data; high-dimensional
   data
ID VISUAL ANALYTICS; QUALITY METRICS; EXPLORATION; VISUALIZATION;
   GENERATION; TYPOLOGY; GUIDANCE; MODEL
AB With the surge of data-driven analysis techniques, there is a rising demand for enhancing the exploration of large high-dimensional data by enabling interactions for the joint analysis of features (i.e., dimensions). Such a dual analysis of the feature space and data space is characterized by three components, 1) a view visualizing feature summaries, 2) a view that visualizes the data records, and 3) a bidirectional linking of both plots triggered by human interaction in one of both visualizations, e.g., Linking & Brushing. Dual analysis approaches span many domains, e.g., medicine, crime analysis, and biology. The proposed solutions encapsulate various techniques, such as feature selection or statistical analysis. However, each approach establishes a new definition of dual analysis. To address this gap, we systematically reviewed published dual analysis methods to investigate and formalize the key elements, such as the techniques used to visualize the feature space and data space, as well as the interaction between both spaces. From the information elicited during our review, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field. We apply our proposed formalization describing the interactions between each component and relate them to the addressed tasks. Additionally, we categorize the existing approaches using our framework and derive future research directions to advance dual analysis by including state-of-the-art visual analysis techniques to improve data exploration.
C1 [Dennig, Frederik L.] Univ Konstanz, Data Anal & Visualizat Res Grp, D-78457 Constance, Germany.
   [Miller, Matthias] Univ Konstanz, AI Ctr, D-78457 Constance, Germany.
C3 University of Konstanz; University of Konstanz
RP Dennig, FL (corresponding author), Univ Konstanz, Data Anal & Visualizat Res Grp, D-78457 Constance, Germany.
EM frederik.dennig@uni-konstanz.de; matthias.miller@uni-konstanz.de;
   keim@uni-konstanz.de; melassady@ai.ethz.ch
RI Dennig, Frederik/HTL-3123-2023
OI Dennig, Frederik L./0000-0003-1116-8450; El-Assady,
   Mennatallah/0000-0001-8526-2613
FU Deutsche Forschungsgemeinschaft(DFG, German Research Foundation)
   [251654672 - TRR 161]; Federal Ministry of Education and the Research of
   Germany (BMBF) through PEGASUS under the Program "Forschung fur
   diezivile Sicherheit"; ETH AI Center
FX This work was supported in part by the Deutsche
   Forschungsgemeinschaft(DFG, German Research Foundation) under Grant
   251654672 - TRR 161 (Project A03), in part by the Federal Ministry of
   Education and the Research of Germany (BMBF) through PEGASUS under the
   Program "Forschung fur diezivile Sicherheit 20182023" and its
   announcement "Zivile Sicherheit - Schutzvor organisierter Kriminalitat
   II," and in part by ETH AI Center.
CR Abuthawabeh M., P 21 EUR C VIS
   Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1
   Artur E, 2019, COMPUT GRAPH-UK, V84, P160, DOI 10.1016/j.cag.2019.08.015
   Baumgartner C, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P11, DOI 10.1109/ICDM.2004.10112
   Behrisch M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13446
   Bertin J., 1983, Semiology of Graphics
   Bertini E., 2009, P ACM SIGKDD WORKSHO, P12, DOI DOI 10.1145/1562849.15628512
   Bertini E, 2011, IEEE T VIS COMPUT GR, V17, P2203, DOI 10.1109/TVCG.2011.229
   Bibal A, 2021, ARRAY-NY, V11, DOI 10.1016/j.array.2021.100080
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Brownlee J., 2017, Why one-hot encode data in machine learning?
   Card SK., 1999, READINGS INFORM VISU
   Chen W, 2009, IEEE T VIS COMPUT GR, V15, P1433, DOI 10.1109/TVCG.2009.112
   Dennig FL, 2021, COMPUT GRAPH FORUM, V40, P375, DOI 10.1111/cgf.14314
   Dennig FL, 2019, IEEE CONF VIS ANAL, P69, DOI [10.1109/vast47406.2019.8986940, 10.1109/VAST47406.2019.8986940]
   Dimara E, 2020, IEEE T VIS COMPUT GR, V26, P119, DOI 10.1109/TVCG.2019.2934283
   Doleisch H, 2006, SIMUL-T SOC MOD SIM, V82, P851, DOI 10.1177/0037549707078278
   Dowling M, 2019, IEEE T VIS COMPUT GR, V25, P172, DOI 10.1109/TVCG.2018.2865047
   Endert A, 2014, IEEE COMPUT GRAPH, V34, P8, DOI 10.1109/MCG.2014.73
   Ester H., P 2 C KNOWL DISC
   Evans DW, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038966
   Fernstad SJ, 2013, INFORM VISUAL, V12, P44, DOI 10.1177/1473871612460526
   Fisher M. D., 2018, Making Data Visual - A Practical Guide toUsing Visualization for Insight
   Fuchs J, 2017, IEEE T VIS COMPUT GR, V23, P1863, DOI 10.1109/TVCG.2016.2549018
   Fujiwara T, 2020, IEEE T VIS COMPUT GR, V26, P45, DOI 10.1109/TVCG.2019.2934251
   Garrison L, 2021, IEEE T VIS COMPUT GR, V27, P2908, DOI 10.1109/TVCG.2021.3057519
   Görtler J, 2020, IEEE T VIS COMPUT GR, V26, P822, DOI 10.1109/TVCG.2019.2934812
   Green TM, 2008, IEEE CONF VIS ANAL, P91, DOI 10.1109/VAST.2008.4677361
   Hagele David, 2023, IEEE Trans Vis Comput Graph, V29, P23, DOI 10.1109/TVCG.2022.3209420
   Han M., 2000, Data Mining: Concepts and Techniques
   Hinterreiter A, 2021, ACM T INTERACT INTEL, V11, DOI 10.1145/3387165
   Ingwer Borg PJFG., 2005, Modern multidimensional scaling: Theory and applications
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Itoh T, 2017, J VISUAL LANG COMPUT, V43, P1, DOI 10.1016/j.jvlc.2017.03.001
   Jentner W, 2018, VISUAL COMPUT, V34, P1225, DOI 10.1007/s00371-018-1483-0
   Jolliffe I., 2022, Principal Component Analysis, P150, DOI [DOI 10.1016/0169-7439(87)80084-9, 10.1007/0-387-22440-87, 10.1007/b98835]
   Kailing K, 2004, SIAM PROC S, P246
   Kailing K, 2003, LECT NOTES ARTIF INT, V2838, P241
   Kandogan E., 2000, P IEEE INF VIS S, P12
   Keim D. A., 1996, SIGMOD Record, V25, P35, DOI 10.1145/245882.245896
   Keim J., 2010, Mastering theInformation Age - Solving Problems With Visual Analytics, P32
   Koprinska Irena, 2009, New Frontiers in Applied Data Mining. PAKDD 2009. International Workshops. Revised Selected Papers, P106
   Krause J, 2016, SYMP LARG DATA ANAL, P11, DOI 10.1109/LDAV.2016.7874305
   Kriegel HP, 2012, WIRES DATA MIN KNOWL, V2, P351, DOI 10.1002/widm.1057
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Lekschas F, 2021, IEEE T VIS COMPUT GR, V27, P358, DOI 10.1109/TVCG.2020.3028948
   Mehri M, 2018, J IMAGING, V4, DOI 10.3390/jimaging4080097
   Miller M, 2022, COMPUT GRAPH FORUM, V41, P283, DOI 10.1111/cgf.14540
   Minghim R, 2006, PROC SPIE, V6060, DOI 10.1117/12.650880
   Muller J, 2021, IEEE T VIS COMPUT GR, V27, P2953, DOI 10.1109/TVCG.2021.3056424
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Nonato LG, 2019, IEEE T VIS COMPUT GR, V25, P2650, DOI 10.1109/TVCG.2018.2846735
   Nováková L, 2009, LECT NOTES COMPUT SC, V5722, P56, DOI 10.1007/978-3-642-04125-9_9
   Pages J., 2014, MULTIPLE FACTOR ANAL
   Perez-Messina I, 2022, COMPUT GRAPH FORUM, V41, P465, DOI 10.1111/cgf.14555
   Rauber PE, 2018, INFORM VISUAL, V17, P282, DOI 10.1177/1473871617713337
   Sacha D, 2017, IEEE T VIS COMPUT GR, V23, P241, DOI 10.1109/TVCG.2016.2598495
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Self R. K., 2016, P WORKSH HUM IN THE
   Simoudis E., 1996, Data Mining, p65 H
   Snyder H, 2019, J BUS RES, V104, P333, DOI 10.1016/j.jbusres.2019.07.039
   Soriano-Vargas A, 2020, INFORM VISUAL, V19, P3, DOI 10.1177/1473871619858937
   Sperrle Fabian, 2023, IEEE Trans Vis Comput Graph, V29, P1124, DOI 10.1109/TVCG.2022.3209393
   Steinparz A. P., 2019, P 10 INT EUROVIS WOR, P19
   Turkay C, 2017, NEUROCOMPUTING, V268, P153, DOI 10.1016/j.neucom.2016.11.087
   Turkay C, 2017, IEEE T VIS COMPUT GR, V23, P131, DOI 10.1109/TVCG.2016.2598470
   Turkay C, 2014, IEEE T VIS COMPUT GR, V20, P2033, DOI 10.1109/TVCG.2014.2346265
   Turkay C, 2014, IEEE COMPUT GRAPH, V34, P38, DOI 10.1109/MCG.2014.1
   Turkay C, 2012, IEEE T VIS COMPUT GR, V18, P2621, DOI 10.1109/TVCG.2012.256
   Turkay C, 2011, IEEE T VIS COMPUT GR, V17, P2591, DOI 10.1109/TVCG.2011.178
   Tzeng E. B., P IEEE 14 VIS C
   van der Corput P, 2016, COMPUT GRAPH FORUM, V35, P31, DOI 10.1111/cgf.12879
   van Wijk JJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P79
   von Landesberger Tatiana, 2014, Handbook of Human Centric Visualization, P653
   Wang L, 2012, IEEE T VIS COMPUT GR, V18, P121, DOI 10.1109/TVCG.2011.23
   Wang YH, 2011, IEEE T VIS COMPUT GR, V17, P1560, DOI 10.1109/TVCG.2011.97
   Ward M. O., 2009, Encyclopedia of Database Systems, P1626
   Weaver C, 2010, IEEE T VIS COMPUT GR, V16, P192, DOI 10.1109/TVCG.2009.94
   Wei H., P IEEE PAC VIS S
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
   Yuan XR, 2013, IEEE T VIS COMPUT GR, V19, P2625, DOI 10.1109/TVCG.2013.150
   Zanabria GG, 2016, COMPUT GRAPH-UK, V60, P107, DOI 10.1016/j.cag.2016.08.007
   Zhang ZY, 2015, IEEE T VIS COMPUT GR, V21, P289, DOI 10.1109/TVCG.2014.2350494
   Zhao JQ, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P161, DOI [10.1109/VISUAL.2019.8933619, 10.1109/visual.2019.8933619]
NR 84
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5165
EP 5182
DI 10.1109/TVCG.2023.3288356
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400025
PM 37342951
DA 2024-11-06
ER

PT J
AU Fu, Y
   Stasko, J
AF Fu, Yu
   Stasko, John
TI More Than Data Stories: Broadening the Role of Visualization in
   Contemporary Journalism
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surveys; Bridges; Computational modeling; Biological system modeling;
   Instruments; Data visualization; Journalism; data visualization;
   computational journalism; data-driven storytelling
ID INFORMATION VISUALIZATION; NARRATIVE VISUALIZATION; AUTOMATED
   JOURNALISM; INTERACTIVE FEATURES; VISUAL ANALYTICS; NEWS; DESIGN;
   REFLECTIONS; EXPLORATION; GENERATION
AB Data visualization and journalism are deeply connected. From early infographics to recent data-driven storytelling, visualization has become an integrated part of contemporary journalism, primarily as a communication artifact to inform the general public. Data journalism, harnessing the power of data visualization, has emerged as a bridge between the growing volume of data and our society. Visualization research that centers around data storytelling has sought to understand and facilitate such journalistic endeavors. However, a recent metamorphosis in journalism has brought broader challenges and opportunities that extend beyond mere communication of data. We present this article to enhance our understanding of such transformations and thus broaden visualization research's scope and practical contribution to this evolving field. We first survey recent significant shifts, emerging challenges, and computational practices in journalism. We then summarize six roles of computing in journalism and their implications. Based on these implications, we provide propositions for visualization research concerning each role. Ultimately, by mapping the roles and propositions onto a proposed ecological model and contextualizing existing visualization research, we surface seven general topics and a series of research agendas that can guide future visualization research at this intersection.
C1 [Fu, Yu] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
   [Stasko, John] Georgia Inst Technol, Coll Comp, Atlanta, GA USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology
RP Fu, Y (corresponding author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
EM fuyu@gatech.edu; stasko@cc.gatech.edu
OI Fu, Yu/0000-0001-5076-6299
CR Abebe R, 2020, FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P252, DOI 10.1145/3351095.3372871
   Ahva L., 2019, The Handbook of Journalism Studies, P38
   Alper B, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5485, DOI 10.1145/3025453.3025877
   Amini F, 2017, IEEE T VIS COMPUT GR, V23, P501, DOI 10.1109/TVCG.2016.2598647
   Anderson Benedict., 2006, Imagined communities. Reflections on the origin and spread of nationalism
   [Anonymous], 2019, VisualizeNews
   [Anonymous], 2009, P REP CTR ADV STUD B
   [Anonymous], 2020, The New York Times
   Arksey H., 2005, INT J SOC RES METHOD, V8, P19, DOI [10.1080/1364557032000119616, DOI 10.1080/1364557032000119616]
   Bach Benjamin, 2023, IEEE Trans Vis Comput Graph, V29, P342, DOI 10.1109/TVCG.2022.3209448
   Boczkowski PJ, 2012, HUM COMMUN RES, V38, P1, DOI 10.1111/j.1468-2958.2011.01418.x
   Bogost S., 2012, Newsgames: Journalism at Play
   Bongshin Lee, 2021, Foundations and Trends in Human-Computer Interaction, V14, P1, DOI 10.1561/1100000081
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Boss K, 2017, IFLA J-INT FED LIBR, V43, P150, DOI 10.1177/0340035216686355
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Bounegru L., 2012, DATA JOURNALISM HDB
   Brehmer M, 2014, IEEE T VIS COMPUT GR, V20, P2271, DOI 10.1109/TVCG.2014.2346431
   Broussard M., 2015, Newspaper Research Journal, V36, P299
   Broussard M., 2012, Archiving data journalism
   Broussard M, 2018, DIGIT JOURNAL, V6, P1206, DOI 10.1080/21670811.2018.1505437
   Bruns A, 2011, BRAZ JOURNAL RES, V7, P117, DOI 10.25200/BJR.v7n2.2011.355
   Bryan C, 2017, IEEE T VIS COMPUT GR, V23, P511, DOI 10.1109/TVCG.2016.2598876
   Burrell J, 2016, BIG DATA SOC, V3, P1, DOI 10.1177/2053951715622512
   C. I. of Health Research, 2010, A guide to knowledge synthesis - CIHR
   Cairo A., 2012, The Functional Art: An Introduction to Information Graph. and Visualization
   Cao J. L. E., 2023, P CHI C HUM FACT COM, DOI [10.1145/3544548.3581472113A, DOI 10.1145/3544548.3581472113A]
   Card SK., 1999, READINGS INFORM VISU
   Carlson M, 2015, DIGIT JOURNAL, V3, P416, DOI 10.1080/21670811.2014.976412
   Carlson Matt., 2020, The Handbook of Journalism Studies, P123, DOI DOI 10.4324/9781315167497--8/BOUNDARY-WORK-MATT-CARLSON-SETH-LEWIS
   Carpendale S, 2008, LECT NOTES COMPUT SC, V4950, P19, DOI 10.1007/978-3-540-70956-5_2
   Chen H., 2022, P CHI C HUM FACT COM, DOI [10.1145/3491102.3517485112Y, DOI 10.1145/3491102.3517485112Y]
   Chen SM, 2020, IEEE T VIS COMPUT GR, V26, P2499, DOI 10.1109/TVCG.2018.2889054
   Chen Zhutian, 2023, IEEE Trans Vis Comput Graph, V29, P918, DOI 10.1109/TVCG.2022.3209497
   Chen ZT, 2022, IEEE T VIS COMPUT GR, V28, P824, DOI 10.1109/TVCG.2021.3114806
   Chevalier F., 2018, Data-driven storytelling, P151
   Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
   Chung DS, 2008, J COMPUT-MEDIAT COMM, V13, P658, DOI 10.1111/j.1083-6101.2008.00414.x
   Chung DS, 2008, MASS COMMUN SOC, V11, P375, DOI 10.1080/15205430701791048
   Coddington M, 2015, DIGIT JOURNAL, V3, P331, DOI 10.1080/21670811.2014.976400
   Cohen S., 2011, The Data Journalism Handbook
   Cohen S, 2011, COMMUN ACM, V54, P66, DOI 10.1145/2001269.2001288
   Conlen Matthew, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P1, DOI 10.1145/3472749.3474731
   Conlen M, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P977, DOI 10.1145/3242587.3242600
   Correll M, 2022, 2022 IEEE 9TH WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2022), P1, DOI 10.1109/BELIV57783.2022.00005
   Dalen A. V., 2019, The Handbook of Journalism Studies, V2nd, P356, DOI DOI 10.4324/9781315167497-23
   Danziger M., 2008, Information visualization for the people
   Diakopoulos N., 2011, A Functional Roadmap for Innovation in Computational Journalism
   Diakopoulos N, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1717
   Diakopoulos N, 2017, DIGIT JOURNAL, V5, P809, DOI 10.1080/21670811.2016.1208053
   Diakopoulos N, 2015, DIGIT JOURNAL, V3, P398, DOI 10.1080/21670811.2014.976411
   Domingo D, 2008, J COMPUT-MEDIAT COMM, V13, P680, DOI 10.1111/j.1083-6101.2008.00415.x
   Domingo D, 2008, JOURNAL PRACT, V2, P326, DOI 10.1080/17512780802281065
   Dong ES, 2020, LANCET INFECT DIS, V20, P533, DOI 10.1016/S1473-3099(20)30120-1
   Elmqvist N, 2011, INFORM VISUAL, V10, P327, DOI 10.1177/1473871611413180
   Erickson T., 2000, ACM Transactions on Computer-Human Interaction, V7, P59, DOI 10.1145/344949.345004
   Eslami M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P153, DOI 10.1145/2702123.2702556
   Fan A, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502138
   Fekete JD, 2008, LECT NOTES COMPUT SC, V4950, P1, DOI 10.1007/978-3-540-70956-5_1
   Flaxman S, 2016, PUBLIC OPIN QUART, V80, P298, DOI 10.1093/poq/nfw006
   Flew T, 2012, JOURNAL PRACT, V6, P157, DOI 10.1080/17512786.2011.616655
   Fu J., 2022, P CHI C HUM FACT COM, DOI [10.1145/3491102.3502078118S, DOI 10.1145/3491102.3502078118S]
   G. T. G. Center, 2008, JOURN 3G FUT TECHN F
   Gillmor D., 2006, WE MEDIA GRASSROOTS
   Görg C, 2014, INFORM VISUAL, V13, P336, DOI 10.1177/1473871613495674
   Görg C, 2013, IEEE T VIS COMPUT GR, V19, P1646, DOI 10.1109/TVCG.2012.324
   Graefe A., 2016, Columbia Journalism Rev.
   Gratzl S, 2016, COMPUT GRAPH FORUM, V35, P491, DOI 10.1111/cgf.12925
   Graves L., 2016, Deciding whats true: The rise of political fact-checking in American journalism
   Gray J., 2012, The Data Journalism Handbook: How Journalists Can Use Data to Improve News
   Gynnild A, 2014, JOURNALISM, V15, P713, DOI 10.1177/1464884913486393
   Haim M, 2018, DIGIT JOURNAL, V6, P330, DOI 10.1080/21670811.2017.1338145
   Hammond P, 2017, JOURNALISM, V18, P408, DOI 10.1177/1464884915620205
   Hayes G., 2014, KNOWING DOING ACTION, P49, DOI DOI 10.1007/978-1-4939-0378-83
   Hayes GR, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1993060.1993065
   Herdel V, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501881
   Hermida A., 2011, Mechanisms of participation: How audience options shape the conversation Participatory Journalism, P11, DOI [10.1002/9781444340747.ch2, DOI 10.1002/9781444340747.CH2]
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Holan A. D., 2020, Politifact
   Holovaty Adrian., 2006, A fundamental Way Newspaper Sites Need to Change"
   Hopkins AK, 2020, COMPUT GRAPH FORUM, V39, P219, DOI 10.1111/cgf.13975
   Houston B., 2021, The history of data journalism: A historical take on every critical breakthrough from the 1950s until today
   Howard A. B., 2014, The art and science of data-driven journalism, DOI [10.7916/D8Q531V1?ref=https://githubhelp.com, DOI 10.7916/D8Q531V1?REF=HTTPS://GITHUBHELP.COM]
   Hujanen J, 2004, NEW MEDIA SOC, V6, P383, DOI 10.1177/1461444804042521
   Hullman J, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P1170, DOI 10.1145/2675133.2675207
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2231, DOI 10.1109/TVCG.2011.255
   Hutchins J. D., 1985, Hum.-Comput. Interact., V1, P311, DOI [DOI 10.1207/S15327051HCI0104, DOI 10.1207/S15327051HCI0104_2]
   Isenberg P, 2018, LECT NOTES COMPUT SC, V11190, P165, DOI 10.1007/978-3-030-01388-2_6
   Jones S., 2005, Gatewatching: Collaborative Online News Production
   Kang YA, 2011, IEEE T VIS COMPUT GR, V17, P570, DOI 10.1109/TVCG.2010.84
   Karduni A., 2018, P 12 INT AAAI C WEB, P151
   Karduni A, 2019, PROCEEDINGS OF IUI 2019, P312, DOI 10.1145/3301275.3302320
   Kastrenakes J., 2020, The VergeMay
   Kong HK, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300576
   Kong HK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174012
   Kosara R., 2016, eagereyesJul.
   Kosara R, 2013, COMPUTER, V46, P44, DOI 10.1109/MC.2013.36
   Kovach B., 2021, The Elements of Journalism, Revised and Updated: What Newspeople Should Know and the Public Should Expect, V4th
   Ladd JM, 2012, WHY AMERICANS HATE THE MEDIA AND HOW IT MATTERS, P1
   LaFrance A., 2019, Nieman ReportsFeb
   Lan XY, 2021, IEEE T VIS COMPUT GR, V27, P2796, DOI 10.1109/TVCG.2021.3074582
   Law PM, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P171, DOI 10.1109/VIS47514.2020.00041
   Lazer DMJ, 2018, SCIENCE, V359, P1094, DOI 10.1126/science.aao2998
   Lee BS, 2020, IEEE COMPUT GRAPH, V40, P82, DOI 10.1109/MCG.2020.2968244
   Lee B, 2018, AVI'18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, DOI 10.1145/3206505.3206602
   Lee B, 2015, IEEE COMPUT GRAPH, V35, P84, DOI 10.1109/MCG.2015.99
   Lee B, 2013, IEEE T VIS COMPUT GR, V19, P2416, DOI 10.1109/TVCG.2013.191
   Lee S, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519711
   Lee-Robbins Elsie, 2023, IEEE Trans Vis Comput Graph, V29, P1, DOI 10.1109/TVCG.2022.3209500
   Leppanen L., 2017, P 10 INT C NAT LANG, P188, DOI DOI 10.18653/V1/W17-3528
   Leppänen L, 2020, MEDIA COMMUN-LISBON, V8, P39, DOI 10.17645/mac.v8i3.3022
   Lewis SC, 2015, DIGIT JOURNAL, V3, P447, DOI 10.1080/21670811.2014.976418
   Lewis SC, 2015, DIGIT JOURNAL, V3, P19, DOI 10.1080/21670811.2014.927986
   Lewis SC, 2010, JOURNAL PRACT, V4, P163, DOI 10.1080/14616700903156919
   Lin Tica, 2022, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2022.3209353
   Liu ZC, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173697
   Long DR, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376727
   Lupi S., 2016, Dear Data
   Lupton D, 2017, NEW MEDIA SOC, V19, P1599, DOI 10.1177/1461444817717515
   McCurdy N, 2016, BEYOND TIME AND ERRORS: NOVEL EVALUATION METHODS FOR VISUALIZATION, BELIV 2016, P10, DOI 10.1145/2993901.2993916
   McNair B, 2009, INT COMMUN ASSOC HAN, P237
   McNutt A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376420
   Meijer I.C., 2019, The Handbook of Journalism Studies, P389, DOI [DOI 10.4324/9781315167497-25, DOI 10.4324/9781315167497]
   Mele N., 2017, COMBATING FAKE NEWS
   Meyer M, 2020, IEEE T VIS COMPUT GR, V26, P87, DOI 10.1109/TVCG.2019.2934539
   MEYER P, 1988, JOURNALISM QUART, V65, P567, DOI 10.1177/107769908806500301
   Meyer Philip., 2002, PRECISION JOURNALISM, V4th
   Mittelstadt B, 2016, INT J COMMUN-US, V10, P4991
   Munson S., 2021, P INT AAAI C WEB SOC, P419, DOI DOI 10.1609/ICWSM.V7I1.14429
   Munzner T, 2008, LECT NOTES COMPUT SC, V4950, P134, DOI 10.1007/978-3-540-70956-5_6
   Narechania A, 2022, IEEE T VIS COMPUT GR, V28, P1009, DOI 10.1109/TVCG.2021.3114827
   Nicoletti L., 2021, The Pudding
   Nigro N., 2022, Hamilton 2.0 dashboard
   Norman Donald A., 1993, Things that Make Us Smart: Defending Human Attributes in the Age of the Machine
   Oh C., 2020, P CHI C HUM FACT COM, P1, DOI DOI 10.1145/3313831.3376811
   Otten JJ, 2015, HEALTH AFFAIR, V34, P1901, DOI 10.1377/hlthaff.2015.0642
   Pandey AV, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1469, DOI 10.1145/2702123.2702608
   Parasie S, 2013, NEW MEDIA SOC, V15, P853, DOI 10.1177/1461444812463345
   Pariser Eli, 2011, The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think
   Pavlik J.V., 2016, Journalism and New Media, DOI DOI 10.7312/PAVL11482
   Pavlik John, 2000, Journalism studies, V1, P229, DOI DOI 10.1080/14616700050028226
   Peck EM, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300474
   Peters C, 2013, RETHINKING JOURNALISM: TRUST AND PARTICIPATION IN A TRANSFORMED NEWS LANDSCAPE, P1
   Pham MT, 2014, RES SYNTH METHODS, V5, P371, DOI 10.1002/jrsm.1123
   Pousman Z, 2007, IEEE T VIS COMPUT GR, V13, P1145, DOI 10.1109/TVCG.2007.70541
   Powers E, 2017, DIGIT JOURNAL, V5, P1315, DOI 10.1080/21670811.2017.1286943
   Powers M, 2012, J COMMUN INQ, V36, P24, DOI 10.1177/0196859911426009
   Rashkin H., 2017, P 2017 C EMP METH NA, P2931
   Reeves S., 2005, P SIGCHI C HUM FACT, P741, DOI [DOI 10.1145/1054972.1055074, 10.1145/1054972.1055074147Q, DOI 10.1145/1054972.1055074147Q]
   Ren DH, 2019, IEEE T VIS COMPUT GR, V25, P789, DOI 10.1109/TVCG.2018.2865158
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Riche C., 2018, Data-DrivenStorytelling
   Robinson S., 2010, Convergence: The International Journal of Research into New Media Technologies, V16, P125, DOI [10.1177/1354856509347719, DOI 10.1177/1354856509347719]
   Rogowitz E. E., 1996, Computers in Physics, V10, P268
   Saket B, 2016, BEYOND TIME AND ERRORS: NOVEL EVALUATION METHODS FOR VISUALIZATION, BELIV 2016, P133, DOI 10.1145/2993901.2993903
   Samek W, 2017, Arxiv, DOI arXiv:1708.08296
   Sarikaya A, 2019, IEEE T VIS COMPUT GR, V25, P682, DOI 10.1109/TVCG.2018.2864903
   Satyanarayan A, 2020, IEEE T VIS COMPUT GR, V26, P461, DOI 10.1109/TVCG.2019.2934281
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Satyanarayan A, 2016, IEEE T VIS COMPUT GR, V22, P659, DOI 10.1109/TVCG.2015.2467091
   Satyanarayan A, 2014, COMPUT GRAPH FORUM, V33, P351, DOI 10.1111/cgf.12391
   Satyanarayan A, 2014, COMPUT GRAPH FORUM, V33, P361, DOI 10.1111/cgf.12392
   Schudson M., 2008, Why Democracies Need an Unlovable Press
   Schudson M., 2008, Hedgehog Rev., V10, P7
   Scott Ben, 2005, TELEV NEW MEDIA, V6, P89, DOI [DOI 10.1177/1527476403255824, 10.1177/1527476403255824]
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Segel E, 2010, IEEE T VIS COMPUT GR, V16, P1139, DOI 10.1109/TVCG.2010.179
   Shi DQ, 2021, IEEE T VIS COMPUT GR, V27, P453, DOI 10.1109/TVCG.2020.3030403
   Shin M, 2023, IEEE T VIS COMPUT GR, V29, P2980, DOI 10.1109/TVCG.2022.3146329
   Shneiderman B., 2006, P AVI WORKSH TIM ERR, P1, DOI DOI 10.1145/1168149.1168158
   Shoemaker P. J., 2019, The Handbook of Journalism Studies, V2nd
   Shu K, 2019, COMPUT MATH ORGAN TH, V25, P60, DOI 10.1007/s10588-018-09280-3
   Singer J.B., 2011, Participatory journalism: Guarding open gates at online newspapers
   Sprague D, 2012, INFORM VISUAL, V11, P106, DOI 10.1177/1473871611433710
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Stasko J., 2014, P 5 WORKSH TIM ERR N, P46, DOI [DOI 10.1145/2669557.2669579, 10.1145/2669557.26695792,9, DOI 10.1145/2669557.26695792,9]
   Stasko J, 2008, INFORM VISUAL, V7, P118, DOI 10.1057/palgrave.ivs.9500180
   Stolte C, 2002, IEEE T VIS COMPUT GR, V8, P52, DOI 10.1109/2945.981851
   Stroud N.J., 2016, Engaging News Project, P1
   Sunstein CR, 2017, CORNELL LAW REV, V102, P1431
   Tang T, 2020, J VISUAL-JAPAN, V23, P707, DOI 10.1007/s12650-020-00644-z
   The White House, 2022, We just learned that President Biden's first year in office was the strongest year for economic growth since 1984
   Thompson Z., 2021, P CHI C HUM FACT COM, DOI [10.1145/3411764.3445747110M, DOI 10.1145/3411764.3445747110M]
   Thudt Alice, 2018, Data-Driven Storytelling, P59
   Thurman N., 2019, The Handbook of Journalism Studies, P180
   Thurman N, 2012, JOURNALISM STUD, V13, P775, DOI 10.1080/1461670X.2012.664341
   Usher N., 2016, Interactive Journalism: Hackers, Data, and Code, DOI [10.5406/j.ctt1hfr048, DOI 10.5406/J.CTT1HFR048]
   Usher N., 2009, Michael Schudson: Why democracies need an unlovable press
   Vázquez-Herrero J, 2020, STUD BIG DATA, V70, P29, DOI 10.1007/978-3-030-36315-4_3
   Viégas FB, 2007, IEEE T VIS COMPUT GR, V13, P1121, DOI 10.1109/TVCG.2007.70577
   Wahl-Jorgensen K., 2019, The handbook of journalism studies, P3, DOI DOI 10.4324/9781315167497-1/JOURNALISM-STUDIES-KARIN-WAHL-JORGENSEN-THOMAS-HANITZSCH
   WahlJorgensen K, 2009, INT COMMUN ASSOC HAN, P1
   Wall E, 2022, IEEE T VIS COMPUT GR, V28, P966, DOI 10.1109/TVCG.2021.3114862
   Wang Y, 2019, IEEE COMPUT GRAPH, V39, P8, DOI 10.1109/MCG.2019.2923483
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P895, DOI 10.1109/TVCG.2019.2934398
   Wang ZJ, 2021, IEEE T VIS COMPUT GR, V27, P1396, DOI 10.1109/TVCG.2020.3030418
   Wattenberg M, 2005, INFOVIS 05: IEEE Symposium on Information Visualization, Proceedings, P1, DOI 10.1109/INFVIS.2005.1532122
   Wohlin C., 2014, P 18 INT C EV ASS SO, P1, DOI [10.1145/2601248.2601268, DOI 10.1145/2601248.2601268]
   Wolfgang JD, 2016, DIGIT JOURNAL, V4, P764, DOI 10.1080/21670811.2015.1090882
   Young ML, 2018, JOURNAL PRACT, V12, P115, DOI 10.1080/17512786.2016.1270171
   Zamith R, 2014, DIGIT JOURNAL, V2, P558, DOI 10.1080/21670811.2014.882066
   Zhang Yixuan, 2023, IEEE Trans Vis Comput Graph, V29, P1037, DOI 10.1109/TVCG.2022.3209493
   Zhi S., 2019, P CHI C HUM FACT COM, P1, DOI [10.1145/3290605.3300499148C, DOI 10.1145/3290605.3300499148C]
   Zhou JW, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581318
   Zimmerman J., 2014, Ways of Knowing in HCI, P167, DOI [DOI 10.1007/978-1-4939-0378-88, DOI 10.1007/978-1-4939-0378-8_8]
   Zimmerman J, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P493
   Zong J, 2021, IEEE T VIS COMPUT GR, V27, P304, DOI 10.1109/TVCG.2020.3030367
NR 208
TC 4
Z9 4
U1 7
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5240
EP 5259
DI 10.1109/TVCG.2023.3287585
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400009
PM 37339040
DA 2024-11-06
ER

PT J
AU Chen, Q
   Cao, SX
   Wang, JZ
   Cao, N
AF Chen, Qing
   Cao, Shixiong
   Wang, Jiazhe
   Cao, Nan
TI How Does Automation Shape the Process of Narrative Visualization: A
   Survey of Tools
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Automation; Authoring systems;
   Videos; Flowcharts; Taxonomy; Authoring tools; automatic visualization;
   data visualization; design space; narrative visualization; survey
ID DESIGN; GENERATION; INFOGRAPHICS; STORIES; IMAGES; EVOLUTION; CHARTS
AB In recent years, narrative visualization has gained much attention. Researchers have proposed different design spaces for various narrative visualization genres and scenarios to facilitate the creation process. As users' needs grow and automation technologies advance, increasingly more tools have been designed and developed. In this study, we summarized six genres of narrative visualization (annotated charts, infographics, timelines & storylines, data comics, scrollytelling & slideshow, and data videos) based on previous research and four types of tools (design spaces, authoring tools, ML/AI-supported tools and ML/AI-generator tools) based on the intelligence and automation level of the tools. We surveyed 105 papers and tools to study how automation can progressively engage in visualization design and narrative processes to help users easily create narrative visualizations. This research aims to provide an overview of current research and development in the automation involvement of narrative visualization tools. We discuss key research problems in each category and suggest new opportunities to encourage further research in the related domain.
C1 [Chen, Qing; Cao, Shixiong; Cao, Nan] Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai 200070, Peoples R China.
   [Wang, Jiazhe] Ant Grp, Shanghai Hangzhou 310000, Peoples R China.
C3 Tongji University
RP Cao, N (corresponding author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai 200070, Peoples R China.
EM qingchen@tongji.edu.cn; caoshixiong@tongji.edu.cn;
   jiazhe.wjz@antgroup.com; nan.cao@tongji.edu.cn
RI wang, jiazhe/HSH-2060-2023; Cao, Nan/O-5397-2014
OI Cao, Nan/0000-0003-1316-7515; Cao, ShiXiong/0009-0002-9640-7771
FU NSFC [62002267, 62072338, 62061136003]; NSF Shanghai [23ZR1464700];
   Shanghai Education Development Foundation "Chen-Guang Project" [21CGA75]
FX This work was supported in part by the NSFC under Grants 62002267,
   62072338, and 62061136003, in part by NSF Shanghai under Grant
   23ZR1464700, and in part by Shanghai Education Development Foundation
   "Chen-Guang Project" under Grant 21CGA75.
CR Adobe Systems Incorporated, 2023, Adobe illustrator
   Amini F, 2017, IEEE T VIS COMPUT GR, V23, P501, DOI 10.1109/TVCG.2016.2598647
   Amini F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1459, DOI 10.1145/2702123.2702431
   [Anonymous], 1983, Narrative Discourse: An Essay in Method
   [Anonymous], 2021, Cartographic J, V58, P83
   [Anonymous], 2016, Microsoft Powerpoint
   AppleKeynote, 2003, ABOUT US
   Bach B, 2017, IEEE COMPUT GRAPH, V37, P6, DOI 10.1109/MCG.2017.33
   Bach B, 2016, IEEE T VIS COMPUT GR, V22, P559, DOI 10.1109/TVCG.2015.2467851
   Bach Z., 2018, P CHI C HUM FACT COM, P1
   Bocklandt S, 2021, LECT NOTES COMPUT SC, V12822, P445, DOI 10.1007/978-3-030-86331-9_29
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Botero K.-H., 2010, P DES COMPL DRS INT, P1
   Brand A, 2019, ANN INTERN MED, V170, P579, DOI 10.7326/M18-2976
   Brath M., 2018, P IEEE VIS WORKSH VI, P1
   Brehmer M., 2019, P COMP JOURN S
   Brehmer M, 2017, IEEE T VIS COMPUT GR, V23, P2151, DOI 10.1109/TVCG.2016.2614803
   Bryan C, 2017, IEEE T VIS COMPUT GR, V23, P511, DOI 10.1109/TVCG.2016.2598876
   Canva, 2018, ABOUT US
   Cao RC, 2020, VIS INFORM, V4, P8, DOI 10.1016/j.visinf.2019.12.002
   Chen J., 2010, Extended Abstr. Hum. Factors Comput. Syst., P3703
   Chen ZT, 2022, IEEE T VIS COMPUT GR, V28, P824, DOI 10.1109/TVCG.2021.3114806
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P917, DOI 10.1109/TVCG.2019.2934810
   Cmeciu M., 2016, Stud. Media Commun., V4, P54
   Coding B., 2010, Sketch - professional digital design for mac
   Coelho D, 2020, COMPUT GRAPH FORUM, V39, P593, DOI 10.1111/cgf.14004
   Conlen M, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P977, DOI 10.1145/3242587.3242600
   Cui WW, 2022, IEEE T VIS COMPUT GR, V28, P173, DOI 10.1109/TVCG.2021.3114856
   Cui WW, 2020, IEEE T VIS COMPUT GR, V26, P906, DOI 10.1109/TVCG.2019.2934785
   Di Giacomo Emilio, 2020, Graph Drawing and Network Visualization. 28th International Symposium, GD 2020. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12590), P324, DOI 10.1007/978-3-030-68766-3_25
   Diakopoulos N, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1717
   Dragicevic P., 2020, Data Physicalization, P1, DOI DOI 10.1007/978-3-319-27648-9_94-1
   Dukes D., 2010, Dipity
   Dunlap J. C., 2016, Journal of Visual Literacy, V35, P42, DOI [10.1080/1051144X.2016.1205832, DOI 10.1080/1051144X.2016.1205832]
   Elias M, 2018, LECT NOTES COMPUT SC, V10896, P172, DOI 10.1007/978-3-319-94277-3_29
   Fan Y., 2022, P CHI C HUM FACT COM, P1
   Fischer G, 2006, HUM COM INT, V9, P427
   Fu JY, 2021, IEEE T VIS COMPUT GR, V27, P337, DOI 10.1109/TVCG.2020.3030351
   Fulda J, 2016, IEEE T VIS COMPUT GR, V22, P300, DOI 10.1109/TVCG.2015.2467531
   Ge B., 2021, P CHI C HUM FACT COM, P1
   Godulla C., 2017, Digitale Langformen Im Journalismus UndCorporate Publishing
   Gomez-Rubio V., 2017, J STAT SOFTW, V77, P1
   Gronemann M, 2016, LECT NOTES COMPUT SC, V9801, P367, DOI 10.1007/978-3-319-50106-2_29
   Harrison L, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1187, DOI 10.1145/2702123.2702545
   Hasan A., 2022, P CHI C HUM FACT COM, P1
   Heer J, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1029
   Hogan T, 2017, INTERACT COMPUT, V29, P147, DOI 10.1093/iwc/iww015
   Hullman J., 2013, P SIGCHI C HUM FACT, P2707
   Hullman J, 2013, IEEE T VIS COMPUT GR, V19, P2406, DOI 10.1109/TVCG.2013.119
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2231, DOI 10.1109/TVCG.2011.255
   Infogram, 2012, ABOUT US
   Isenberg P, 2018, LECT NOTES COMPUT SC, V11190, P165, DOI 10.1007/978-3-030-01388-2_6
   Kandogan E, 2012, IEEE CONF VIS ANAL, P73, DOI 10.1109/VAST.2012.6400487
   Kang T., 2021, P CHI C HUM FACT COM, P1
   Karyda M, 2021, IEEE COMPUT GRAPH, V41, P74, DOI 10.1109/MCG.2020.3025078
   Kashan O., 2012, Timeline of the universe
   Kim N. W., 2019, P CHI C HUM FACT COM, P1
   Kim NW, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300309
   Kim NW, 2018, IEEE T VIS COMPUT GR, V24, P595, DOI 10.1109/TVCG.2017.2744118
   Kim NW, 2017, IEEE T VIS COMPUT GR, V23, P491, DOI 10.1109/TVCG.2016.2598620
   Kim Y, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P201, DOI [10.1109/VIS49827.2021.00048, 10.1109/VIS49827.2021.9623291]
   Kittivorawong C, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P101, DOI 10.1109/VIS47514.2020.00027
   Kong HK, 2017, COMPUT GRAPH FORUM, V36, P515, DOI 10.1111/cgf.13207
   Kong N, 2012, IEEE T VIS COMPUT GR, V18, P2631, DOI 10.1109/TVCG.2012.229
   Kosara R, 2013, COMPUTER, V46, P44, DOI 10.1109/MC.2013.36
   Lan XY, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445344
   Lan XY, 2022, IEEE T VIS COMPUT GR, V28, P933, DOI 10.1109/TVCG.2021.3114775
   Lan XY, 2021, IEEE T VIS COMPUT GR, V27, P2796, DOI 10.1109/TVCG.2021.3074582
   Latif S, 2022, IEEE T VIS COMPUT GR, V28, P184, DOI 10.1109/TVCG.2021.3114802
   Leake H. Valentina, 2020, P CHI C HUM FACT COM, P1
   Lee B, 2015, IEEE COMPUT GRAPH, V35, P84, DOI 10.1109/MCG.2015.99
   Lee B, 2013, IEEE T VIS COMPUT GR, V19, P2416, DOI 10.1109/TVCG.2013.191
   Lee T., 2021, P CHI C HUM FACT COM, P1
   Li WC, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P141, DOI 10.1109/VIS47514.2020.00035
   Liu C, 2020, IEEE PAC VIS SYMP, P191, DOI 10.1109/PacificVis48177.2020.1043
   Liu SX, 2013, IEEE T VIS COMPUT GR, V19, P2436, DOI 10.1109/TVCG.2013.196
   Liu Z., 2018, P CHI C HUM FACT COM, P1
   Lu JH, 2021, IEEE PAC VIS SYMP, P21, DOI 10.1109/PacificVis52677.2021.00011
   Lu M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376263
   Luo YY, 2018, PROC INT CONF DATA, P101, DOI 10.1109/ICDE.2018.00019
   Lyra KT, 2016, IEEE INT CONF ADV LE, P366, DOI 10.1109/ICALT.2016.83
   Ma KL, 2012, IEEE COMPUT GRAPH, V32, P12, DOI 10.1109/MCG.2012.24
   Marshall CC, 1997, ACM DIGITAL LIBRARIES '97, P131
   McKenna S, 2017, COMPUT GRAPH FORUM, V36, P377, DOI 10.1111/cgf.13195
   McKenna S, 2014, IEEE T VIS COMPUT GR, V20, P2191, DOI 10.1109/TVCG.2014.2346331
   MichaelAlbers J., 2017, Int. J. Multimedia Appl., V9, P1
   Naparin A. Binti, 2017, Int.J.Multimedia Appl., V9, P15
   Nguyen PH, 2016, INFORM VISUAL, V15, P253, DOI 10.1177/1473871615605347
   Nieto GMF, 2022, LAK22 CONFERENCE PROCEEDINGS: THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS & KNOWLEDGE, P219, DOI 10.1145/3506860.3506895
   Northwestern University Knight Lab, 2013, Timelinejs
   Oppermann M, 2020, 2020 IEEE WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2020), P74, DOI 10.1109/BELIV51497.2020.00016
   Otten JJ, 2015, HEALTH AFFAIR, V34, P1901, DOI 10.1377/hlthaff.2015.0642
   Qian CY, 2021, IEEE T VIS COMPUT GR, V27, P443, DOI 10.1109/TVCG.2020.3030448
   Ren DH, 2017, IEEE PAC VIS SYMP, P230, DOI 10.1109/PACIFICVIS.2017.8031599
   Sallam Y., 2022, P CHI C HUM FACT COM, P1
   Satyanarayan A, 2014, COMPUT GRAPH FORUM, V33, P361, DOI 10.1111/cgf.12392
   Schulz HJ, 2011, IEEE T VIS COMPUT GR, V17, P393, DOI 10.1109/TVCG.2010.79
   Schulz Hans-Jorg, 2010, PhD dissertation
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Segel E, 2010, IEEE T VIS COMPUT GR, V16, P1139, DOI 10.1109/TVCG.2010.179
   Seyser D, 2018, IEEE INT CON INF VIS, P401, DOI 10.1109/iV.2018.00075
   Shaw J., 2011, Time line setter
   Shi D, 2021, COMPUT GRAPH FORUM, V40, P495, DOI 10.1111/cgf.14324
   Shi DQ, 2021, IEEE T VIS COMPUT GR, V27, P453, DOI 10.1109/TVCG.2020.3030403
   Shi X., 2021, P CHI C HUM FACT COM, P1
   Shrinivasan Yedendra B., 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P123, DOI 10.1109/VAST.2009.5333023
   Shu XH, 2021, IEEE T VIS COMPUT GR, V27, P1492, DOI 10.1109/TVCG.2020.3030396
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Subramonyam H, 2019, IEEE T VIS COMPUT GR, V25, P597, DOI 10.1109/TVCG.2018.2865231
   Suh S, 2022, Arxiv, DOI arXiv:2208.12981
   Sultanum N, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445354
   Suprata F., 2019, J. Metris, V20, P1
   Tanahashi Y, 2012, IEEE T VIS COMPUT GR, V18, P2679, DOI 10.1109/TVCG.2012.212
   Tang T, 2021, IEEE T VIS COMPUT GR, V27, P294, DOI 10.1109/TVCG.2020.3030467
   Tang T, 2020, J VISUAL-JAPAN, V23, P707, DOI 10.1007/s12650-020-00644-z
   Tang T, 2019, IEEE T VIS COMPUT GR, V25, P769, DOI 10.1109/TVCG.2018.2864899
   Thompson J, 2020, COMPUT GRAPH FORUM, V39, P207, DOI 10.1111/cgf.13974
   Thompson Z., 2021, P CHI C HUM FACT COM, P1
   Tong C, 2018, INFORMATION, V9, DOI 10.3390/info9030065
   Tyagi A., 2021, arXiv
   Vartak M, 2015, PROC VLDB ENDOW, V8, P2182, DOI 10.14778/2831360.2831371
   Visme, 2013, ABOUT US
   Wang H., 2019, P CHI C HUM FACT COM, P1
   Wang J., 2020, IEEE Trans. Vis. Comput. Graph., V27, P967
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wang S., 2019, P CHI C HUM FACT COM, P1
   Wang Y, 2021, COMPUT GRAPH FORUM, V40, P507, DOI 10.1111/cgf.14325
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P895, DOI 10.1109/TVCG.2019.2934398
   Wang Y, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173909
   Wang Z., 2016, P S VIS, P1
   Wang ZZ, 2022, IEEE T VIS COMPUT GR, V28, P944, DOI 10.1109/TVCG.2021.3114849
   Webalon, 2011, Tiki-toki
   Westerlund B., 2005, P NORD RES C, P1
   Wickham H, 2009, USE R, P1, DOI 10.1007/978-0-387-98141-3
   Winters T, 2019, LECT NOTES COMPUT SC, V11453, P127, DOI 10.1007/978-3-030-16667-0_9
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Xia N. H., 2018, P CHI C HUM FACT COM, P1
   Xu L., 2022, P CHI C HUM FACT COM, P1
   Yang Chen, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P155, DOI 10.1109/VAST.2010.5652885
   Yuan LP, 2022, IEEE T VIS COMPUT GR, V28, P4252, DOI 10.1109/TVCG.2021.3085327
   Zhang JE, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376172
   Zhang PY, 2021, IEEE T VIS COMPUT GR, V27, P326, DOI 10.1109/TVCG.2020.3030343
   Zhao J, 2021, Arxiv, DOI arXiv:2103.03996
   Zhao R. Marr, 2015, Tech. Rep. HCIL-2015-15
   Zhenpeng Zhao, 2019, Information in Contemporary Society. 14th International Conference, iConference 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11420), P327, DOI 10.1007/978-3-030-15742-5_32
   Zhu SJ, 2020, VIS INFORM, V4, P24, DOI 10.1016/j.visinf.2020.07.002
   Zikas P, 2020, VISUAL COMPUT, V36, P1965, DOI 10.1007/s00371-020-01919-0
NR 149
TC 10
Z9 10
U1 9
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4429
EP 4448
DI 10.1109/TVCG.2023.3261320
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400016
PM 37030780
OA Green Submitted, Bronze
DA 2024-11-06
ER

PT J
AU Wu, DY
   Le, TNH
   Yao, SY
   Lin, YC
   Lee, TY
AF Wu, Dong-Yi
   Le, Thi-Ngoc-Hanh
   Yao, Sheng-Yi
   Lin, Yun-Chen
   Lee, Tong-Yee
TI Image Collage on Arbitrary Shape via Shape-Aware Slicing and
   Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Layout; Visualization; Optimization; Partitioning algorithms;
   Task analysis; Social networking (online); Image collection
   visualization; image collage; irregular shape layout
ID DECOMPOSITION; PARTS; RULE
AB Image collage is a very useful tool for visualizing an image collection. Most of the existing methods and commercial applications for generating image collages are designed on simple shapes, such as rectangular and circular layouts. This greatly limits the use of image collages in some artistic and creative settings. Although there are some methods that can generate irregularly-shaped image collages, they often suffer from severe image overlapping and excessive blank space. This prevents such methods from being effective information communication tools. In this article, we present a shape slicing algorithm and an optimization scheme that can create image collages of arbitrary shapes in an informative and visually pleasing manner given an input shape and an image collection. To overcome the challenge of irregular shapes, we propose a novel algorithm, called Shape-Aware Slicing, which partitions the input shape into cells based on medial axis and binary slicing tree. Shape-Aware Slicing,which is designed specifically for irregular shapes, takes human perception and shape structure into account to generate visually pleasing partitions. Then, the layout is optimized by analyzing input images with the goal of maximizing the total salient regions of the images. To evaluate our method, we conduct extensive experiments and compare our results against previous work. The evaluations show that our proposed algorithm can efficiently arrange image collections on irregular shapes and create visually superior results than prior work and existing commercial tools.
C1 [Wu, Dong-Yi; Le, Thi-Ngoc-Hanh; Yao, Sheng-Yi; Lin, Yun-Chen; Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Lee, TY (corresponding author), Natl Cheng Kung Univ, Tainan 701, Taiwan.
EM cutechubbit@gmail.com; ngochanh.le1987@gmail.com;
   nd8081018@gs.ncku.edu.tw; f74042060@gmail.com; tonylee@mail.ncku.edu.tw
OI Yao, Sheng-Yi/0000-0002-7233-9615; , Dong-Yi/0000-0002-6889-5520; Le,
   Thi Ngoc Hanh/0000-0001-9667-9780
FU National Science and Technology Council [111-2221-E-006-112-MY3,
   110-2221-E-006-135-MY3]; Republic of China (ROC), Taiwan
FX This work was supported in part by the National Science and Technology
   Council under Grants 111-2221-E-006-112-MY3 and 110-2221-E-006-135-MY3,
   Republic of China (ROC), Taiwan.
CR Adobe, 2021, Photo collage
   [Anonymous], 1997, Pacific J. Math., V181, P57
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   De Winter J, 2006, COGNITION, V99, P275, DOI 10.1016/j.cognition.2005.03.004
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Han XT, 2016, IEEE T CYBERNETICS, V46, P1286, DOI 10.1109/TCYB.2015.2448236
   Hofer C., 2017, ADV NEURAL INFORM PR, P1634
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2
   Latecki LJ, 1999, COMPUT VIS IMAGE UND, V73, P441, DOI 10.1006/cviu.1998.0738
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Lekschas F, 2021, IEEE T VIS COMPUT GR, V27, P358, DOI 10.1109/TVCG.2020.3028948
   Lien JM, 2006, COMP GEOM-THEOR APPL, V35, P100, DOI 10.1016/j.comgeo.2005.10.005
   Liu LJ, 2018, IEEE T VIS COMPUT GR, V24, P1956, DOI 10.1109/TVCG.2017.2703853
   Liu SG, 2017, I C VIRTUAL REALITY, P454, DOI 10.1109/ICVRV.2017.00120
   Luo L, 2015, IEEE T IMAGE PROCESS, V24, P273, DOI 10.1109/TIP.2014.2376188
   Nguyen GP, 2008, J VISUAL LANG COMPUT, V19, P203, DOI 10.1016/j.jvlc.2006.09.002
   Papanelopoulos Y., 2022, Comput. Vis. Image Understanding, V179, P66
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Singh M, 1999, PERCEPT PSYCHOPHYS, V61, P636, DOI 10.3758/BF03205536
   Singh M., 2001, in Psychology, VVolume 130, P401
   Song Y, 2023, IEEE T VIS COMPUT GR, V29, P1330, DOI 10.1109/TVCG.2021.3113031
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tan L, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2011.89
   Wong D. F., 1986, 23rd ACM/IEEE Design Automation Conference. Proceedings 1986 (Cat. No.86CH2288-9), P101, DOI 10.1145/318013.318030
   Wu K., 2013, P AS PAC SIGN INF PR, P1
   Yang JY, 2016, COMPUT VIS IMAGE UND, V145, P43, DOI 10.1016/j.cviu.2016.01.005
   Yu L., 2022, P IEEE CVF C COMP VI, P3729
   Zeng JT, 2008, LECT NOTES COMPUT SC, V5359, P682, DOI 10.1007/978-3-540-89646-3_67
NR 29
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4449
EP 4463
DI 10.1109/TVCG.2023.3262039
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400022
PM 37030778
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Kumar, A
   Zhang, XY
   Xin, HL
   Yan, HF
   Huang, XJ
   Xu, W
   Müller, K
AF Kumar, Ayush
   Zhang, Xinyu
   Xin, Huolin L.
   Yan, Hanfei
   Huang, Xiaojing
   Xu, Wei
   Mueller, Klaus
TI RadVolViz: An Information Display-Inspired Transfer Function Editor for
   Multivariate Volume Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Battery; color mapping; multi channel data; multivariate data; transfer
   function; volume rendering; volume visualization
ID MULTIDIMENSIONAL TRANSFER-FUNCTIONS; COLOR
AB In volume visualization transfer functions are widely used for mapping voxel properties to color and opacity. Typically, volume density data are scalars which require simple 1D transfer functions to achieve this mapping. If the volume densities are vectors of three channels, one can straightforwardly map each channel to either red, green or blue, which requires a trivial extension of the 1D transfer function editor. We devise a new method that applies to volume data with more than three channels. These types of data often arise in scientific scanning applications, where the data are separated into spectral bands or chemical elements. Our method expands on prior work in which a multivariate information display, RadViz, was fused with a radial color map, in order to visualize multi-band 2D images. In this work, we extend this joint interface to blended volume rendering. The information display allows users to recognize the presence and value distribution of the multivariate voxels and the joint volume rendering display visualizes their spatial distribution. We design a set of operators and lenses that allow users to interactively control the mapping of the multivariate voxels to opacity and color. This enables users to isolate or emphasize volumetric structures with desired multivariate properties. Furthermore, it turns out that our method also enables more insightful displays even for RGB data. We demonstrate our method with three datasets obtained from spectral electron microscopy, high energy X-ray scanning, and atmospheric science.
C1 [Kumar, Ayush; Zhang, Xinyu; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Yan, Hanfei; Huang, Xiaojing; Xu, Wei] Brookhaven Natl Lab, Upton, NY 11973 USA.
   [Xin, Huolin L.] Univ Calif Irvine, Dept Phys & Astron, Irvine, CA 92697 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   United States Department of Energy (DOE); Brookhaven National
   Laboratory; University of California System; University of California
   Irvine
RP Kumar, A (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM aykumar@cs.stonybrook.edu; zhang146@cs.stonybrook.edu; huolinx@uci.edu;
   hyan@bnl.gov; xjhuang@bnl.gov; xuw@bnl.gov; mueller@cs.stonybrook.edu
RI Huang, Xiaojing/K-3075-2012; Kumar, Ayush/KLC-5631-2024; Zhang,
   Xinyu/HKF-8200-2023
OI Xu, Wei/0000-0002-4525-4819; Kumar, Ayush/0000-0001-5867-5623; Mueller,
   Klaus/0000-0002-0996-8590; Zhang, Xinyu/0000-0002-7475-8979
FU NSF [IIS 1527200, 1941613, CHE-1900401]; Brookhaven National Laboratory
   LDRD [16-041]; DOE Office of Science by Brookhaven National Laboratory
   [DE-SC0012704]
FX No Statement Available
CR Abbasloo A, 2016, IEEE T VIS COMPUT GR, V22, P975, DOI 10.1109/TVCG.2015.2467031
   Ayachit U., 2015, The Paraview Guide: A Parallel Visualization Application
   Bertini E, 2005, THIRD INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P22
   Bramon R, 2013, IEEE J BIOMED HEALTH, V17, P870, DOI 10.1109/JBHI.2013.2263227
   Cheng SH, 2019, IEEE T VIS COMPUT GR, V25, P1361, DOI 10.1109/TVCG.2018.2808489
   Cheng SH, 2017, PROCESSES, V5, DOI 10.3390/pr5040075
   CIBC, 2016, Sci. Comput. Imag. Inst.
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Ferreira GC, 2020, RADIAT PHYS CHEM, V167, DOI 10.1016/j.radphyschem.2019.02.049
   Giesen J, 2007, IEEE T VIS COMPUT GR, V13, P1664, DOI 10.1109/TVCG.2007.70542
   Guo HQ, 2011, IEEE PAC VIS SYMP, P19, DOI 10.1109/PACIFICVIS.2011.5742368
   Haidacher Martin., 2008, P EUROGRAPHICS WORKS, P101, DOI [DOI 10.2312/VCBM/VCBM08/101-108, 10.2312/VCBM/VCBM08/101-108]
   Hanwell MarcusD., 2019, MICROSC MICROANAL, V25, P408, DOI DOI 10.1017/S1431927619002770
   Harris WM, 2014, NANOSCALE, V6, P4480, DOI 10.1039/c3nr06684c
   Hoffman P, 1997, VISUALIZATION '97 - PROCEEDINGS, P437, DOI 10.1109/VISUAL.1997.663916
   Hovden R, 2020, MRS BULL, V45, P298, DOI 10.1557/mrs.2020.87
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Ip CY, 2012, IEEE T VIS COMPUT GR, V18, P2355, DOI 10.1109/TVCG.2012.231
   Kaufman A.E., 2005, Vis. Handb, V7, P127, DOI DOI 10.1016/B978-012387582-2/50009-5
   Kim HS, 2010, INFORM VISUAL, V9, P167, DOI 10.1057/ivs.2010.6
   Kindlmann G., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P183, DOI 10.1109/VISUAL.1999.809886
   Kindlmann G, 2000, IEEE T VIS COMPUT GR, V6, P124, DOI 10.1109/2945.856994
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Kniss JM, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P287
   Kühne L, 2012, IEEE T VIS COMPUT GR, V18, P2122, DOI 10.1109/TVCG.2012.186
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Li WD, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14589
   Liu SS, 2014, SYMP LARG DATA ANAL, P35, DOI 10.1109/LDAV.2014.7013202
   Ljung P, 2016, COMPUT GRAPH FORUM, V35, P669, DOI 10.1111/cgf.12934
   Lu AD, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P655
   Lundstrom C., 2006, P EG IEEE VGTC WORKS, P1
   Maciejewski R, 2009, IEEE T VIS COMPUT GR, V15, P1473, DOI 10.1109/TVCG.2009.185
   Markus IM, 2014, J PHYS CHEM LETT, V5, P3649, DOI 10.1021/jz5017526
   Mörth E, 2020, LECT NOTES COMPUT SC, V12221, P351, DOI 10.1007/978-3-030-61864-3_29
   Muraki S, 2001, IEEE T VIS COMPUT GR, V7, P265, DOI 10.1109/2945.942694
   Pfaffelmoser T, 2011, COMPUT GRAPH FORUM, V30, P951, DOI 10.1111/j.1467-8659.2011.01944.x
   Pinto Franciscode Moura., 2007, Eurographics IEEE-VGTC Symposium on Visualization, P131
   Sereda P., 2006, P 8 JOINT EUR IEEE V
   Shao YY, 2013, ADV FUNCT MATER, V23, P987, DOI 10.1002/adfm.201200688
   Steiner JD, 2019, ACS APPL MATER INTER, V11, P37885, DOI 10.1021/acsami.9b14729
   Tzeng F.-Y., 2004, S DATA VISUALISATION, P17, DOI DOI 10.2312/VISSYM/VISSYM04/017-024
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   Wang JP, 2017, IEEE T VIS COMPUT GR, V23, P81, DOI 10.1109/TVCG.2016.2598830
   Wang L, 2012, COMPUT GRAPH FORUM, V31, P1305, DOI 10.1111/j.1467-8659.2012.03123.x
   Wang L, 2012, IEEE T VIS COMPUT GR, V18, P121, DOI 10.1109/TVCG.2011.23
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Yan HF, 2020, MRS BULL, V45, P264, DOI 10.1557/mrs.2020.90
   Yan HF, 2018, NANO FUTURES, V2, DOI 10.1088/2399-1984/aab25d
   Zhang JN, 2019, NAT ENERGY, V4, P594, DOI 10.1038/s41560-019-0409-z
NR 49
TC 1
Z9 1
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4464
EP 4479
DI 10.1109/TVCG.2023.3263856
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400070
PM 37030815
DA 2024-11-06
ER

PT J
AU Childs, E
   Mohammad, F
   Stevens, L
   Burbelo, H
   Awoke, A
   Rewkowski, N
   Manocha, D
AF Childs, Elizabeth
   Mohammad, Ferzam
   Stevens, Logan
   Burbelo, Hugo
   Awoke, Amanuel
   Rewkowski, Nicholas
   Manocha, Dinesh
TI An Overview of Enhancing Distance Learning Through Emerging Augmented
   and Virtual Reality Technologies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computer aided instruction; Education; Virtual reality; Headphones;
   Visualization; Pandemics; Sensors; Human-centered computing-Human
   computer interaction (HCI)-Interaction paradigms-Mixed / augmented
   reality; Human-centered computing-Human computer interaction
   (HCI)-Interaction paradigms-Virtual reality
ID SOCIAL-INTERACTION; EDUCATION
AB Although distance learning presents a number of interesting educational advantages as compared to in-person instruction, it is not without its downsides. We first assess the educational challenges presented by distance learning as a whole and identify 4 main challenges that distance learning currently presents as compared to in-person instruction: the lack of social interaction, reduced student engagement and focus, reduced comprehension and information retention, and the lack of flexible and customizable instructor resources. After assessing each of these challenges in-depth, we examine how AR/VR technologies might serve to address each challenge along with their current shortcomings, and finally outline the further research that is required to fully understand the potential of AR/VR technologies as they apply to distance learning.
C1 [Childs, Elizabeth; Mohammad, Ferzam; Stevens, Logan; Burbelo, Hugo; Awoke, Amanuel; Rewkowski, Nicholas; Manocha, Dinesh] Univ Maryland, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Childs, E (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM ehchilds@terpmail.umd.edu; fmoham18@terpmail.umd.edu;
   lsteven7@terpmail.umd.edu; hburbelo@terpmail.umd.edu;
   aawoke@terpmail.umd.edu; nick1@umd.edu; dmanocha@umd.edu
OI Manocha, Dinesh/0000-0001-7047-9801; Childs,
   Elizabeth/0000-0002-5768-6029
CR Abd Majid F., 2019, ASIAN J UNI EDU, V15, P52, DOI [DOI 10.24191/AJUE.V15I2.7556, 10.24191/ajue.v15i2.7556]
   Acosta-Tello E., 2015, J. Instructional Pedagogies, V17
   Allcoat D, 2018, RES LEARN TECHNOL, V26, DOI 10.25304/rlt.v26.2140
   [Anonymous], Zoom video conferencing plans pricing
   [Anonymous], 1993, Early Childhood Res. Quart., DOI DOI 10.1016/S0885-2006(05)80078-X
   Archana M. V., 2014, Int. J. Sci. Eng. Res., V5
   Ayoubi A., 2020, Ikea lanches augmented reality application
   BEARISON DJ, 1986, MERRILL PALMER QUART, V32, P51
   Bergmann J, 2012, PHI DELTA KAPPAN, V94, P25, DOI 10.1177/003172171209400206
   Billinghurst M, 2000, BT TECHNOL J, V18, P80, DOI 10.1023/A:1026582022824
   Billinghurst M, 2001, IEEE COMPUT GRAPH, V21, P6, DOI 10.1109/38.920621
   Blackboard, 2018, Blackboard to acquire Elluminate and Wimba
   Blanco Cindy, 2020, The 2020 duolingo language report
   Carter EW, 2005, RES PRACT PERS SEV D, V30, P179, DOI 10.2511/rpsd.30.4.179
   Chandrasekera Tilanka, 2018, Design and Technology Education: An International Journal, V23, P55
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chen CJ, 2005, J RES TECHNOL EDUC, V38, P123, DOI 10.1080/15391523.2005.10782453
   Chen YT, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P172, DOI [10.1109/VR.2019.8798338, 10.1109/vr.2019.8798338]
   Duenser A., 2012, P 24 AUSTR COMP HUM, P107, DOI DOI 10.1145/2414536.2414554
   Dumford AD, 2018, J COMPUT HIGH EDUC, V30, P452, DOI 10.1007/s12528-018-9179-z
   Egglestone S. R., 2006, P S US INT SOFTW TEC
   Eutsler L, 2021, EDUC TECHNOL SOC, V24, P28
   Everysight, 2020, Raptor everysight
   Ferrer-Torregrosa J, 2016, BMC MED EDUC, V16, DOI 10.1186/s12909-016-0757-3
   Fidan M, 2019, COMPUT EDUC, V142, DOI 10.1016/j.compedu.2019.103635
   Fuste A., 2019, HyperCubes: A Playful Introduction to Compu- tational Thinking in Augmented Reality, DOI DOI 10.1145/3341215.3356264
   Ghuman JK, 1998, INFANT YOUNG CHILD, V11, P21
   Giang V., 2013, Bus. Insider, V18
   Google, 2020, Google cardboard
   Google, 2020, Google AR & VR
   Greenberg G, 1998, IEEE TECHNOL SOC MAG, V17, P36, DOI 10.1109/44.735862
   Hampshire A., 2006, Proceedings of the 18th Australia conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments, P409, DOI [10.1145/1228175.1228259, DOI 10.1145/1228175.1228259]
   Hanoa E., 2021, Kahoot! reaches 5 billion players after a year of recordhigh growth
   Henderson SJ, 2011, INT SYM MIX AUGMENT
   Hong J, 2019, CHI PLAY'19: EXTENDED ABSTRACTS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P15, DOI 10.1145/3341215.3356327
   Horváth I, 2021, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.673826
   Huttner JP, 2017, AMCIS 2017 PROCEEDINGS
   James W.B., 1995, NEW DIRECTIONS ADULT, V67, P19, DOI DOI 10.1002/ACE.36719956705
   Johnson Adrian S., 2013, Virtual Augmented and Mixed Reality. Designing and Developing Augmented and Virtual Environments. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings: LNCS 7936, P169, DOI 10.1007/978-3-642-39405-8_20
   Jorgensen D., 2002, Reference Librarian, P3
   Keller J., 2000, How to integrate learner motivation planning into lesson planning: The arcs model approach
   Kerawalla L., 2006, VIRTUAL REAL, V10, P163, DOI [10.1007/s10055-006-0036-4, DOI 10.1007/S10055-006-0036-4, https://doi.org/10.1007/s10055-006-0036-4]
   Khan T, 2019, ADV HUM-COMPUT INTER, V2019, DOI 10.1155/2019/7208494
   Kiryakova G., 2014, P 9 INT BALK ED SCI, P24
   Küçük S, 2016, ANAT SCI EDUC, V9, P411, DOI 10.1002/ase.1603
   Kytö M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173655
   Le QT, 2015, J INTELL ROBOT SYST, V79, P487, DOI 10.1007/s10846-014-0112-z
   lenovo, 2020, Oculus Rift S-3D virtual reality system
   Li Y., 2010, P 3 INT C ADV COMP T
   Lin Y, 2022, COMPUT APPL ENG EDUC, V30, P396, DOI 10.1002/cae.22462
   Lytridis C, 2018, EDUC SCI, V8, DOI 10.3390/educsci8010006
   Maloney D., 2020, Proc. ACM Hum.-Comput. Interact., V4
   Marcy V., 2001, Perspective on Physician Assistant Education, V12, P117, DOI DOI 10.1097/01367895-200107000-00007
   McBrien JL, 2009, INT REV RES OPEN DIS, V10
   medium, 2020, Mobile augmented reality in 2019
   Michael N, 2017, MULTIMED TOOLS APPL, V76, P14169, DOI 10.1007/s11042-016-3808-1
   Microsoft, 2020, About HoloLens 2
   Mikropoulos T. A., 1998, Education and Information Technologies, V3, P137, DOI 10.1023/A:1009687025419
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Monahan T, 2008, COMPUT EDUC, V50, P1339, DOI 10.1016/j.compedu.2006.12.008
   Moustafa F, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281527
   Mtebe J., 2015, Int. J. Educ. Develop. Using Inf. Commun. Technol., V11, P51
   Mystakidis S, 2020, INT CONF INFORM INTE, P365, DOI 10.1109/iisa50023.2020.9284417
   Mystakidis S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052412
   National teacher and principal survey (ntps), Average age of teachers and percentage distribution of teachers, by age category, school type, and main teaching assignment: 2017-18
   Pejoska J, 2016, BRIT J EDUC TECHNOL, V47, P474, DOI 10.1111/bjet.12442
   Perret-Clermont A.-N, 1980, SOCIAL INTERACTION C
   Ping JM, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1124, DOI [10.1109/vr.2019.8798174, 10.1109/VR.2019.8798174]
   Poupyrev I, 1998, P IEEE VIRT REAL ANN, P126, DOI 10.1109/VRAIS.1998.658467
   QuiverVision, 2020, About quiver education
   Rambach Jason, 2021, Virtual, Augmented and Mixed Reality. 13th International Conference, VAMR 2021 Held as Part of the 23rd HCI International Conference, HCII 2021. Proceedings, P653, DOI 10.1007/978-3-030-77599-5_45
   Raskar G., 1998, P 25 ANN C COMP GRAP, P179
   Rotellar C, 2016, AM J PHARM EDUC, V80, DOI 10.5688/ajpe80234
   Roth D, 2016, P IEEE VIRT REAL ANN, P277, DOI 10.1109/VR.2016.7504761
   Schoop E., 2016, P CHI C HUM FACT COM, P1607
   Seichter H, 2008, INT SYM MIX AUGMENT, P177, DOI 10.1109/ISMAR.2008.4637354
   Simpson O, 2013, OPEN LEARN, V28, P105, DOI 10.1080/02680513.2013.847363
   Southgate E, 2020, PROCEEDINGS OF 2020 6TH INTERNATIONAL CONFERENCE OF THE IMMERSIVE LEARNING RESEARCH NETWORK (ILRN 2020), P38, DOI [10.23919/ilrn47897.2020.9155121, 10.23919/iLRN47897.2020.9155121]
   Steinberg RN, 2000, AM J PHYS, V68, pS37, DOI 10.1119/1.19517
   Stott A., 2013, Analysis of Gamification in Education, V8, P36
   suits-different, 2019, About us
   Hoang TN, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P253, DOI 10.1145/3196709.3196724
   Turner A., 2021, How many smartphones are in the world?
   Tzima S, 2019, EDUC SCI, V9, DOI 10.3390/educsci9020099
   Ventura S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02509
   Vora L. J., 2015, Int. J. Modern Trends Eng. Res., V2, P281
   WICKENS CD, 1992, 1992 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, VOLS 1 AND 2, P842, DOI 10.1109/ICSMC.1992.271688
   Within Unlimited, 2020, Wonderscope: An augmented reality iOS app for kids
   Wu HK, 2013, COMPUT EDUC, V62, P41, DOI 10.1016/j.compedu.2012.10.024
   Xiao R, 2018, IEEE T VIS COMPUT GR, V24, P1653, DOI 10.1109/TVCG.2018.2794222
   Xiong JH, 2020, OSA CONTINUUM, V3, P2730, DOI 10.1364/OSAC.400900
   Youngblut C., 1997, Educational uses of virtual reality technology
   Yuen S., 2011, J. Educ. Technol. Develop. Exchange, V119
   Zhao JY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P155, DOI [10.1109/vr.2019.8797867, 10.1109/VR.2019.8797867]
NR 94
TC 1
Z9 2
U1 9
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4480
EP 4496
DI 10.1109/TVCG.2023.3264577
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400026
PM 37037228
DA 2024-11-06
ER

PT J
AU Angelini, M
   Blasilli, G
   Lenti, S
   Santucci, G
AF Angelini, Marco
   Blasilli, Graziano
   Lenti, Simone
   Santucci, Giuseppe
TI A Visual Analytics Conceptual Framework for Explorable and Steerable
   Partial Dependence Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Predictive models; Computational modeling; Machine learning; Analytical
   models; Visual analytics; Market research; Behavioral sciences; partial
   dependence plot; visual analytics
AB Machine learning techniques are a driving force for research in various fields, from credit card fraud detection to stock analysis. Recently, a growing interest in increasing human involvement has emerged, with the primary goal of improving the interpretability of machine learning models. Among different techniques, Partial Dependence Plots (PDP) represent one of the main model-agnostic approaches for interpreting how the features influence the prediction of a machine learning model. However, its limitations (i.e., visual interpretation, aggregation of heterogeneous effects, inaccuracy, and computability) could complicate or misdirect the analysis. Moreover, the resulting combinatorial space can be challenging to explore both computationally and cognitively when analyzing the effects of more features at the same time. This article proposes a conceptual framework that enables effective analysis workflows, mitigating state-of-the-art limitations. The proposed framework allows for exploring and refining computed partial dependences, observing incrementally accurate results, and steering the computation of new partial dependences on user-selected subspaces of the combinatorial and intractable space. With this approach, the user can save both computational and cognitive costs, in contrast with the standard monolithic approach that computes all the possible combinations of features on all their domains in batch. The framework is the result of a careful design process involving experts' knowledge during its validation and informed the development of a prototype, W4SP(1), that demonstrates its applicability traversing its different paths. A case study shows the advantages of the proposed approach.
C1 [Angelini, Marco; Lenti, Simone] Roma Sapienza Univ Rome, I-00185 Rome, Italy.
RP Lenti, S (corresponding author), Roma Sapienza Univ Rome, I-00185 Rome, Italy.
EM angelini@dis.uniroma1.it; blasilli@diag.uniroma1.it;
   lenti@diag.uniroma1.it; santucci@diag.uniroma1.it
RI Blasilli, Graziano/HLQ-6056-2023; Lenti, Simone/ABA-3229-2020; Santucci,
   Giuseppe/F-3907-2011
OI Angelini, Marco/0000-0001-9051-6972; Santucci,
   Giuseppe/0000-0003-4350-1123; Lenti, Simone/0000-0001-8281-3723;
   Blasilli, Graziano/0000-0003-3339-6403
CR Angelini G., 2018, Informatics, V5
   Apley DW, 2020, J ROY STAT SOC B, V82, P1059, DOI 10.1111/rssb.12377
   Berk RA, 2013, CRIMINOL PUBLIC POL, V12, P513, DOI 10.1111/1745-9133.12047
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Britton M, 2019, Arxiv, DOI [arXiv:1904.00561, DOI 10.48550/ARXIV.1904.00561.P5,6]
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285
   Collaris J. J., 2020, P 13 INT S VIS INF C
   Cutler DR, 2007, ECOLOGY, V88, P2783, DOI 10.1890/07-0539.1
   Dua D, 2017, UCI MACHINE LEARNING
   Fekete D., 2019, Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik
   Floricel C, 2022, IEEE T VIS COMPUT GR, V28, P151, DOI 10.1109/TVCG.2021.3114810
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Friedman JH, 2003, STAT MED, V22, P1365, DOI 10.1002/sim.1501
   Goldstein A, 2015, J COMPUT GRAPH STAT, V24, P44, DOI 10.1080/10618600.2014.907095
   Green DP, 2012, PUBLIC OPIN QUART, V76, P491, DOI 10.1093/poq/nfs036
   Greenwell BM, 2017, R J, V9, P421
   Gromping U, 2020, Reports in Mathematics, Physics and Chemistry: Department II
   Hastie T., 2009, The Elements of Statistical Learning: Data Mining, Inference, and Prediction
   Hogräfer M, 2022, ACM T INTEL SYST TEC, V13, DOI 10.1145/3531229
   Krause J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5686, DOI 10.1145/2858036.2858529
   Kwon B.C., 2020, P 37 INT C MACH LEAR, DOI [10.13140/RG.2.2.35685.63208, DOI 10.13140/RG.2.2.35685.63208]
   Kwon BC, 2019, IEEE T VIS COMPUT GR, V25, P299, DOI 10.1109/TVCG.2018.2865027
   La Rosa B, 2023, COMPUT GRAPH FORUM, V42, P319, DOI 10.1111/cgf.14733
   Lundberg SM, 2017, ADV NEUR IN, V30
   Greenwell BM, 2018, Arxiv, DOI arXiv:1805.04755
   Ming Y, 2019, IEEE T VIS COMPUT GR, V25, P342, DOI 10.1109/TVCG.2018.2864812
   Molnar C., 2021, INTERPRETABLE MACHIN
   Moosbauer J, 2021, ADV NEUR IN, V34
   Mulder JD, 1999, FUTURE GENER COMP SY, V15, P119, DOI 10.1016/S0167-739X(98)00047-8
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   RAO R, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P318, DOI 10.1145/191666.191776
   Ren DH, 2017, IEEE T VIS COMPUT GR, V23, P61, DOI 10.1109/TVCG.2016.2598828
   Ribeiro M. T., P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Richer A., IEEE Trans. Vis. Comput. Graph., DOI [10.1109/TVCG.2022.3231230.52M, DOI 10.1109/TVCG.2022.3231230.52M]
   Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292
   Sankaran M, 2008, GLOBAL ECOL BIOGEOGR, V17, P236, DOI 10.1111/j.1466-8238.2007.00360.x
   SCOTT DW, 1979, BIOMETRIKA, V66, P605, DOI 10.1093/biomet/66.3.605
   Silverman B. W., 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   Sorensen TA, 2012, ACTA PSYCHOL, V140, P158, DOI 10.1016/j.actpsy.2012.04.004
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Sturges HA, 1926, J AM STAT ASSOC, V21, P65, DOI 10.1080/01621459.1926.10502161
   Talbot J, 2012, IEEE T VIS COMPUT GR, V18, P2613, DOI 10.1109/TVCG.2012.196
   Tamagnini J., 2017, P 2 WORKSH HUM IN TH
   Ribeiro MT, 2016, Arxiv, DOI [arXiv:1606.05386, 10.48550/arXiv.1606.05386]
   Tzeng FY, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P383
   van den Elzen S, 2013, COMPUT GRAPH FORUM, V32, P191, DOI 10.1111/cgf.12106
   Van Someren Y. F., The Think Aloud Method:A Practical Approach to Modelling Cognitive, V11
   vanLiere R, 1997, FUTURE GENER COMP SY, V12, P441, DOI 10.1016/S0167-739X(96)00029-5
   Wang C, 2020, Arxiv, DOI arXiv:2011.02149
   Wexler J, 2020, IEEE T VIS COMPUT GR, V26, P56, DOI 10.1109/TVCG.2019.2934619
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Zhang JW, 2019, IEEE T VIS COMPUT GR, V25, P364, DOI 10.1109/TVCG.2018.2864499
   Zhao QY, 2021, J BUS ECON STAT, V39, P272, DOI 10.1080/07350015.2019.1624293
   Zhao X, 2019, IEEE T VIS COMPUT GR, V25, P407, DOI 10.1109/TVCG.2018.2864475
   Zhu XZ, 2019, BIORESOURCE TECHNOL, V288, DOI 10.1016/j.biortech.2019.121527
NR 56
TC 5
Z9 5
U1 4
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4497
EP 4513
DI 10.1109/TVCG.2023.3263739
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400033
PM 37027262
OA Green Published
DA 2024-11-06
ER

PT J
AU Petrov, D
   Gadelha, M
   Mech, R
   Kalogerakis, E
AF Petrov, Dmitry
   Gadelha, Matheus
   Mech, Radomir
   Kalogerakis, Evangelos
TI ANISE: Assembly-Based Neural Implicit Surface Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D shape representations; single-view reconstruction; neural networks;
   implicit functions; shape modeling
AB We present ANISE, a method that reconstructs a 3D shape from partial observations (images or sparse point clouds) using a part-aware neural implicit shape representation. The shape is formulated as an assembly of neural implicit functions, each representing a different part instance. In contrast to previous approaches, the prediction of this representation proceeds in a coarse-to-fine manner. Our model first reconstructs a structural arrangement of the shape in the form of geometric transformations of its part instances. Conditioned on them, the model predicts part latent codes encoding their surface geometry. Reconstructions can be obtained in two ways: (i) by directly decoding the part latent codes to part implicit functions, then combining them into the final shape; or (ii) by using part latents to retrieve similar part instances in a part database and assembling them in a single shape. We demonstrate that, when performing reconstruction by decoding part representations into implicit functions, our method achieves state-of-the-art part-aware reconstruction results from both images and sparse point clouds. When reconstructing shapes by assembling parts retrieved from a dataset, our approach significantly outperforms traditional shape retrieval methods even when significantly restricting the database size. We present our results in well-known sparse point cloud reconstruction and single-view reconstruction benchmarks.
C1 [Petrov, Dmitry] Univ Massachusetts Amherst, Amherst, MA 01003 USA.
   [Gadelha, Matheus] Adobe Res, San Jose, CA 95110 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   Adobe Systems Inc.
RP Petrov, D (corresponding author), Univ Massachusetts Amherst, Amherst, MA 01003 USA.
EM dmitry.petrov@gmail.com; gadelha@adobe.com; rmech@adobe.com;
   kalo@cs.umass.edu
OI Petrov, Dmitry/0000-0003-0445-3923; Mech, Radomir/0000-0002-5558-0327
FU Adobe Research Internship; Adobe Research gift
FX No Statement Available
CR Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Bokhovkin A, 2021, PROC CVPR IEEE, P7480, DOI 10.1109/CVPR46437.2021.00740
   CHABRA ROHAN, 2020, COMPUTER VISION ECCV, P608
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   Chaudhuri Siddhartha, 2013, P 26 ANN ACM S US IN, P193, DOI DOI 10.1145/2501988.2502008
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Deng BY, 2020, PROC CVPR IEEE, P31, DOI 10.1109/CVPR42600.2020.00011
   Dubrovina A, 2019, IEEE I CONF COMP VIS, P8139, DOI 10.1109/ICCV.2019.00823
   Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gadelha M, 2020, PROC CVPR IEEE, P399, DOI 10.1109/CVPR42600.2020.00048
   Gal R, 2021, IEEE INT CONF COMP V, P2039, DOI 10.1109/ICCVW54120.2021.00231
   Genova K, 2020, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR42600.2020.00491
   Genova K, 2019, IEEE I CONF COMP VIS, P7153, DOI 10.1109/ICCV.2019.00725
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Hanocka R, 2020, Arxiv, DOI arXiv:2005.11084
   Hertz A, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530084
   Huang HB, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12694
   Jiang C., 2020, ADV NEURAL INFORM PR, V33, P9745
   Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Li J., 2020, P AAAI C ART INT, p11 362
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Li MY, 2021, PROC CVPR IEEE, P10241, DOI 10.1109/CVPR46437.2021.01011
   Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071
   Littwin G, 2019, IEEE I CONF COMP VIS, P1824, DOI 10.1109/ICCV.2019.00191
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527
   Museth K, 2002, ACM T GRAPHIC, V21, P330, DOI 10.1145/566570.566585
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156
   Niu CJ, 2018, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2018.00475
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paschalidou D, 2021, PROC CVPR IEEE, P3203, DOI 10.1109/CVPR46437.2021.00322
   Peng S., 2020, ECCV, P523
   Peng SY, 2021, ADV NEUR IN, V34
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Reiner T, 2011, COMPUT GRAPH-UK, V35, P596, DOI 10.1016/j.cag.2011.03.010
   Richter SR, 2018, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR.2018.00207
   Schulz A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983618
   Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209
   Sung M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130821
   Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tian Y., 2019, P INT C LEARN REPR
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   ukta Prasad M., 2006, CVPR'06: Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P1345, DOI [10.1109/CVPR.2006.281, DOI 10.1109/CVPR.2006.281]
   Uy MA, 2021, PROC CVPR IEEE, P11708, DOI 10.1109/CVPR46437.2021.01154
   Wang WY, 2019, ADV NEUR IN, V32
   Williams F, 2019, PROC CVPR IEEE, P10122, DOI 10.1109/CVPR.2019.01037
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
   Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103
NR 59
TC 0
Z9 0
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4514
EP 4526
DI 10.1109/TVCG.2023.3265306
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400002
PM 37023153
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Edirimuni, DD
   Lu, XQ
   Li, G
   Robles-Kelly, A
AF Edirimuni, Dasith de Silva
   Lu, Xuequan
   Li, Gang
   Robles-Kelly, Antonio
TI Contrastive Learning for Joint Normal Estimation and Point Cloud
   Filtering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Filtering; Point cloud compression; Estimation; Three-dimensional
   displays; Task analysis; Noise measurement; Training; Contrastive
   learning; machine learning; normal estimation; point cloud filtering
ID SURFACE
AB Point cloud filtering and normal estimation are two fundamental research problems in the 3D field. Existing methods usually perform normal estimation and filtering separately and often show sensitivity to noise and/or inability to preserve sharp geometric features such as corners and edges. In this article, we propose a novel deep learning method to jointly estimate normals and filter point clouds. We first introduce a 3D patch based contrastive learning framework, with noise corruption as an augmentation, to train a feature encoder capable of generating faithful representations of point cloud patches while remaining robust to noise. These representations are consumed by a simple regression network and supervised by a novel joint loss, simultaneously estimating point normals and displacements that are used to filter the patch centers. Experimental results show that our method well supports the two tasks simultaneously and preserves sharp features and fine details. It generally outperforms state-of-the-art techniques on both tasks.
C1 [Edirimuni, Dasith de Silva; Lu, Xuequan; Li, Gang; Robles-Kelly, Antonio] Deakin Univ, Sch Informat Technol, Waurn Ponds, VIC 3216, Australia.
   [Robles-Kelly, Antonio] Def Sci & Technol Grp, Edinburg, SA 5111, Australia.
C3 Deakin University; Defence Science & Technology
RP Lu, XQ (corresponding author), Deakin Univ, Sch Informat Technol, Waurn Ponds, VIC 3216, Australia.
EM dtdesilva@deakin.edu.au; xuequan.lu@deakin.edu.au;
   gang.li@deakin.edu.au; antonio.robles-kelly@deakin.edu.au
OI Lu, Xuequan/0000-0003-0959-408X; de Silva Edirimuni,
   Dasith/0000-0003-4997-5434
CR Adamson A, 2006, ACM T GRAPHIC, V25, P671, DOI 10.1145/1141911.1141940
   Afham M, 2022, PROC CVPR IEEE, P9892, DOI 10.1109/CVPR52688.2022.00967
   Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Alliegro A, 2021, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR46437.2021.00460
   Alliez Pierre, 2007, P 5 EUROGRAPHICS S G, V7, P39, DOI DOI 10.2312/SGP/SGP07/039-048(VERP.39
   Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911
   BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0
   Bekiroglu Y, 2020, DATA BRIEF, V30, DOI 10.1016/j.dib.2020.105335
   Ben-Shabat Y., 2020, P 16 EUR C COMP VIS, P34
   Ben-Shabat Y, 2019, PROC CVPR IEEE, P10104, DOI 10.1109/CVPR.2019.01035
   Boulch A, 2016, COMPUT GRAPH FORUM, V35, P281, DOI 10.1111/cgf.12983
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Dey T. K., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P39, DOI 10.1109/PBG.2005.194062
   Du B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3133, DOI 10.1145/3474085.3475458
   Gschwandtner Michael, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P199, DOI 10.1007/978-3-642-24031-7_20
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI 10.1109/CVPR.2006.100
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Jiang JC, 2023, Arxiv, DOI arXiv:2110.06632
   Kim Y, 2021, J MECH SCI TECHNOL, V35, P3131
   Kolluri R, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1361192.1361195
   Lal S, 2021, PROC CVPR IEEE, P12482, DOI 10.1109/CVPR46437.2021.01230
   Lenssen JE, 2020, PROC CVPR IEEE, P11244, DOI 10.1109/CVPR42600.2020.01126
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Li B, 2010, COMPUT GRAPH-UK, V34, P94, DOI 10.1016/j.cag.2010.01.004
   Li MT, 2022, PROC CVPR IEEE, P14910, DOI 10.1109/CVPR52688.2022.01451
   Li RH, 2021, PROC CVPR IEEE, P344, DOI 10.1109/CVPR46437.2021.00041
   Liao YY, 2022, Arxiv, DOI arXiv:2109.13410
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239473, 10.1145/1276377.1276405]
   Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222
   Luo CX, 2021, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR46437.2021.00320
   Luo W., 2021, P IEEE CVF INT C COM, P4583
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Remil O, 2017, COMPUT GRAPH FORUM, V36, P63, DOI 10.1111/cgf.13272
   Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344
   Serna Andres, 2014, 3rd International Conference on Pattern Recognition Applications and Methods (ICPRAM 2014). Proceedings, P819
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Urech PRW, 2020, LANDSCAPE URBAN PLAN, V203, DOI 10.1016/j.landurbplan.2020.103903
   Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232
   Wang SY, 2022, COMPUT AIDED DESIGN, V150, DOI 10.1016/j.cad.2022.103281
   Wang Z., 2020, P BRIT MACH VIS C
   Xie SN, 2020, Arxiv, DOI arXiv:2007.10985
   Yoon M, 2007, COMPUT AIDED DESIGN, V39, P408, DOI 10.1016/j.cad.2007.02.008
   Yu L., 2018, EC-Net: An EdgeAware Point Set Consolidation Network
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhang J., 2020, IEEE Access, V8
   Zhang J, 2019, IEEE T VIS COMPUT GR, V25, P1693, DOI 10.1109/TVCG.2018.2827998
   Zhang J, 2013, COMPUT GRAPH-UK, V37, P697, DOI 10.1016/j.cag.2013.05.008
   Zhu R., 2021, P IEEE CVF INT C COM, P6118
NR 62
TC 2
Z9 2
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4527
EP 4541
DI 10.1109/TVCG.2023.3263866
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400004
PM 37030701
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Yang, H
   Li, J
   Chen, SM
AF Yang, Huan
   Li, Jie
   Chen, Siming
TI TopicRefiner: Coherence-Guided Steerable LDA for Visual Topic
   Enhancement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human-in-the-loop; LDA; mixed initiative; topic modeling; visual
   analytics; latent dirichlet allocation
ID LATENT DIRICHLET ALLOCATION; ALGORITHMS
AB This article presents a new Human-steerable Topic Modeling (HSTM) technique. Unlike existing techniques commonly relying on matrix decomposition-based topic models, we extend LDA as the fundamental component for extracting topics. LDA's high popularity and technical characteristics, such as better topic quality and no need to cherry-pick terms to construct the document-term matrix, ensure better applicability. Our research revolves around two inherent limitations of LDA. First, the principle of LDA is complex. Its calculation process is stochastic and difficult to control. We thus give a weighting method to incorporate users' refinements into the Gibbs sampling to control LDA. Second, LDA often runs on a corpus with massive terms and documents, forming a vast search space for users to find semantically relevant or irrelevant objects. We thus design a visual editing framework based on the coherence metric, proven to be the most consistent with human perception in assessing topic quality, to guide users' interactive refinements. Cases on two open real-world datasets, participants' performance in a user study, and quantitative experiment results demonstrate the usability and effectiveness of the proposed technique.
C1 [Yang, Huan; Li, Jie] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
   [Chen, Siming] Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.
C3 Tianjin University; Fudan University
RP Li, J (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
EM yanghuan@tju.edu.cn; jie.li@tju.edu.cn; simingchen3@gmail.com
RI Chen, Siming/AAK-1874-2020; Li, Jie/X-4832-2018
OI Li, Jie/0000-0001-6511-4090
FU National Natural Science Foundation of China [61972278, 62202105];
   Natural Science Foundation of Tianjin [20JCQNJC01620]; Shanghai
   Municipal Science and Technology General Program [21ZR1403300]; Sailing
   Program [21YF1402900]
FX No Statement Available
CR AlSumait L, 2009, LECT NOTES ARTIF INT, V5781, P67, DOI 10.1007/978-3-642-04180-8_22
   Andrzejewski David, 2009, Proc Int Conf Mach Learn, V382, P25
   Blei D. M., 2007, P 20 INT C NEUR INF, P121
   Blei DM, 2004, ADV NEUR IN, V16, P17
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd-Graber J., 2014, HDB MIXED MEMBERSHIP, P225, DOI DOI 10.1201/B17520
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chaney A. J.-B., 2012, P INT AAAI C WEB SOC, P419
   Chang J., 2009, Advances in Neural Information Processing Systems, P288
   Chen S, 2020, IEEE T VIS COMPUT GR, V26, P2775, DOI 10.1109/TVCG.2019.2904069
   Chen Y, 2019, KNOWL-BASED SYST, V163, P1, DOI 10.1016/j.knosys.2018.08.011
   Chen YX, 2016, J IEEE I C DEVELOP L, P217, DOI 10.1109/DEVLRN.2016.7846822
   Choo J, 2013, IEEE T VIS COMPUT GR, V19, P1992, DOI 10.1109/TVCG.2013.212
   Chuang J., 2013, P 30 INT C MACH LEAR, P612
   Chuang J, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P74, DOI 10.1145/2254556.2254572
   Cui WW, 2011, IEEE T VIS COMPUT GR, V17, P2412, DOI 10.1109/TVCG.2011.239
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Dou WW, 2013, IEEE T VIS COMPUT GR, V19, P2002, DOI 10.1109/TVCG.2013.162
   El-Assady M, 2020, IEEE T VIS COMPUT GR, V26, P1001, DOI 10.1109/TVCG.2019.2934654
   El-Assady M, 2019, IEEE T VIS COMPUT GR, V25, P374, DOI 10.1109/TVCG.2018.2864769
   El-Assady M, 2018, IEEE T VIS COMPUT GR, V24, P382, DOI 10.1109/TVCG.2017.2745080
   Ganesan A., 2015, P TEXT VIS WORKSH IU
   Glueck M, 2018, IEEE T VIS COMPUT GR, V24, P371, DOI 10.1109/TVCG.2017.2745118
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Hoque E, 2019, INFORM VISUAL, V18, P318, DOI 10.1177/1473871618757228
   Hu YN, 2014, MACH LEARN, V95, P423, DOI 10.1007/s10994-013-5413-0
   Jänicke S, 2017, COMPUT GRAPH FORUM, V36, P226, DOI 10.1111/cgf.12873
   Kim H, 2021, IEEE T VIS COMPUT GR, V27, P3644, DOI 10.1109/TVCG.2020.2981456
   Kumar V, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6323
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lee H, 2012, COMPUT GRAPH FORUM, V31, P1155, DOI 10.1111/j.1467-8659.2012.03108.x
   Lee TY, 2017, INT J HUM-COMPUT ST, V105, P28, DOI 10.1016/j.ijhcs.2017.03.007
   Li DC, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P940
   Li Jie, 2020, IEEE Trans Vis Comput Graph, V26, P1789, DOI 10.1109/TVCG.2018.2882449
   Liu SX, 2014, IEEE CONF VIS ANAL, P183, DOI 10.1109/VAST.2014.7042494
   Liu Z., 2011, ACM Trans. Intell. Syst. Technol., V2, P1
   Lund J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P896, DOI 10.18653/v1/P17-1083
   Mimno David, 2011, P C EMP METH NAT LAN, P262
   Musialek C., Distill-ery: Iterative topic modeling to improve the content analysis process
   Newman D, 2009, J MACH LEARN RES, V10, P1801
   Newman J. H., 2010, P HUM LANG TECHN ANN, P100
   Sievert C., 2014, P WORKSH INT LANG LE, P63, DOI [DOI 10.3115/V1/W14-3110, 10.3115/v1/W14-3110, DOI 10.1109/DSAA.2017.61]
   Smith A., 2014, P WORKSH INT LANG LE, P71
   Smith A, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P293, DOI 10.1145/3172944.3172965
   SPARCKJONES K, 1972, J DOC, V28, P11, DOI 10.1108/eb026526
   Stevens K., 2012, P JOINT C EMP METH N, P952
   Suri P, 2017, 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE & COMMUNICATION TECHNOLOGY (CICT)
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Teneva N, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P530, DOI 10.18653/v1/P17-2084
   von Landesberger T, 2012, IEEE CONF VIS ANAL, P183, DOI 10.1109/VAST.2012.6400553
   Wallach H. M., 2009, P 26 ANN INT C MACH, P1105, DOI [10.1145/1553374.1553515, DOI 10.1145/1553374.1553515]
   Wang J., 2019, P INT US INT WORKSH
   Wang Y, 2009, LECT NOTES COMPUT SC, V5564, P301, DOI 10.1007/978-3-642-02158-9_26
   Xing LZ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3471
   Xing LZ, 2018, AAAI CONF ARTIF INTE, P6005
   Xue P, 2016, COMM COM INF SC, V626, P97, DOI 10.1007/978-981-10-2209-8_9
   Yang Y., 2015, P C EMP METH NAT LAN, P308
   Yanjun Jiang, 2012, 2012 Third International Conference on Networking and Distributed Computing (ICNDC 2012), P26, DOI 10.1109/ICNDC.2012.14
   Yao LM, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P937
   Zhai K., 2012, WWW
   Zhao H, 2021, PROCEEDINGS OF THE THIRTIETH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2021, P4713
NR 63
TC 0
Z9 0
U1 6
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4542
EP 4557
DI 10.1109/TVCG.2023.3266890
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400039
PM 37053067
DA 2024-11-06
ER

PT J
AU Zhang, SK
   Tam, H
   Li, YK
   Ren, KX
   Fu, HB
   Zhang, SH
AF Zhang, Shao-Kui
   Tam, Hou
   Li, Yike
   Ren, Ke-Xin
   Fu, Hongbo
   Zhang, Song-Hai
TI SceneDirector: Interactive Scene Synthesis by Simultaneously Editing
   Multiple Objects in Real-Time
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Three-dimensional displays; Solid modeling; Real-time systems;
   Process control; Tuning; Shape; 3D scene synthesis; 3D scene editing;
   interactive 3D modeling
AB Intelligent tools for creating synthetic scenes have been developed significantly in recent years. Existing techniques on interactive scene synthesis only incorporate a single object at every interaction, i.e., crafting a scene through a sequence of single-object insertions with user preferences. These techniques suggest objects by considering existent objects in the scene instead of fully picturing the eventual result, which is inherently problematic since the sets of objects to be inserted are seldom fixed during interactive processes. In this article, we introduce SceneDirector, a novel interactive scene synthesis tool to help users quickly picture various potential synthesis results by simultaneously editing groups of objects. Specifically, groups of objects are rearranged in real-time with respect to a position of an object specified by a mouse cursor or gesture, i.e., a movement of a single object would trigger the rearrangement of the existing object group, the insertions of potentially appropriate objects, and the removal of redundant objects. To achieve this, we first propose an idea of coherent group set which expresses various concepts of layout strategies. Subsequently, we present layout attributes, where users can adjust how objects are arranged by tuning the weights of the attributes. Thus, our method gives users intuitive control of both how to arrange groups of objects and where to place them. Through extensive experiments and two applications, we demonstrate the potentiality of our framework and how it enables concurrently effective and efficient interactions of editing groups of objects.
C1 [Zhang, Shao-Kui; Tam, Hou] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Li, Yike; Ren, Ke-Xin] Tsinghua Univ, Acad Arts & Design, Beijing 100084, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Zhang, Song-Hai] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University; City University of Hong Kong;
   Tsinghua University
RP Zhang, SH (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM zhangsk18@mails.tsinghua.edu.cn; th21@mails.tsinghua.edu.cn;
   lyk20@mails.tsinghua.edu.cn; rkx20@mails.tsinghua.edu.cn;
   hongbofu@cityu.edu.hk; shz@tsinghua.edu.cn
OI Zhang, Shao-Kui/0000-0003-0353-1977; Tam, Hou/0009-0009-3107-9552; FU,
   Hongbo/0000-0002-0284-726X
FU Natural Science Foundation of China [62132012]; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62132012 and Tsinghua-Tencent Joint Laboratory for
   Internet Innovation Technology.
CR Chang AE, 2015, Arxiv, DOI arXiv:1505.06289
   Chen K., 2015, Computational Visual Media, V1, P267, DOI [DOI 10.1007/S41095-015-0029-X, 10.1007/s41095-015-0029-x]
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Ching F. D., 2018, INTERIOR DESIGN ILLU
   Dardas N. H., 2011, Res. Bull. Jordan, V2, P86
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Fisher M., 2011, P ACM SIGGRAPH PAP, P1
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Fu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10913, DOI 10.1109/ICCV48922.2021.01075
   Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805
   He Y, 2022, IEEE T VIS COMPUT GR, V28, P3986, DOI 10.1109/TVCG.2021.3111729
   Kela J, 2006, PERS UBIQUIT COMPUT, V10, P285, DOI 10.1007/s00779-005-0033-8
   Kim Hyosun, 2005, IPT EGVE, P191, DOI [10.2312/EGVE/IPT_EGVE2005/191-199, DOI 10.2312/EGVE/IPT_EGVE2005/195-199]
   kujiale.com, 2020, Kujiale
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Li WW, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P539, DOI [10.1109/VR46266.2020.1581008046739, 10.1109/VR46266.2020.00-29]
   Li Y., 2018, P AAAI C ART INT
   Li YR, 2020, COMPUT GRAPH-UK, V93, P1, DOI 10.1016/j.cag.2020.08.002
   Liang W, 2019, IEEE T VIS COMPUT GR, V25, P1836, DOI 10.1109/TVCG.2019.2898721
   Lu Hong-Li, 2010, RESIDENTIAL INTERIOR
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Ma R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275035
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Mitton M., 2016, Residential interior design: A guide to planning spaces / Maureen Mitton, CID, NCIDO, Courtney Nystuen, AIA EMERITUS ; with CAD illustrations by Melissa Brewer, Shelley Pecha, and Jamey Bowe, VThird
   Peng Y.-G., 2008, Architectural Space Combination Theory
   planner5d.com, 2020, Planner5D
   Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618
   Ritchie D, 2019, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2019.00634
   Savva M, 2017, Arxiv, DOI arXiv:1703.00061
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Vasiliki Asaroglou A. B., 2013, Furniture Arrangement: In Residential Spaces
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Weiss T, 2019, IEEE T VIS COMPUT GR, V25, P3231, DOI 10.1109/TVCG.2018.2866436
   Xiong GM, 2021, IEEE T VIS COMPUT GR, V27, P4413, DOI 10.1109/TVCG.2020.3005680
   Xu K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461968
   Yan M, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095169
   Yeh YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185552
   Yu LF, 2016, IEEE T VIS COMPUT GR, V22, P1138, DOI 10.1109/TVCG.2015.2417575
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Yuan Liang, 2017, Next Generation Computer Animation Techniques. Third International Workshop, AniNex 2017. Revised Selected Papers: LNCS 10582, P133, DOI 10.1007/978-3-319-69487-0_10
   Zhang SK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P965, DOI 10.1145/3474085.3475194
   Zhang SK, 2021, GRAPH MODELS, V116, DOI 10.1016/j.gmod.2021.101104
   Zhang SH, 2022, IEEE T VIS COMPUT GR, V28, P3082, DOI 10.1109/TVCG.2021.3050143
   Zhang SH, 2023, IEEE T VIS COMPUT GR, V29, P2080, DOI 10.1109/TVCG.2021.3139990
   Zhang SH, 2020, COMPUT VIS MEDIA, V6, P79, DOI 10.1007/s41095-020-0158-8
   Zhang SY, 2021, IEEE T VIS COMPUT GR, V27, P2250, DOI 10.1109/TVCG.2019.2949295
   Zhang SY, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P353, DOI 10.1145/3013971.3014002
   Zhang YY, 2021, ACTA GEOCHIM, V40, P1, DOI 10.1007/s11631-020-00439-x
   Zhang ZW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3381866
   Zheng Tao-Kai, 2011, INTERIOR DESIGN HOME
NR 53
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4558
EP 4569
DI 10.1109/TVCG.2023.3268115
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400034
PM 37083513
DA 2024-11-06
ER

PT J
AU Xu, WP
   Zhang, P
   Yu, ML
   Yang, L
   Wang, WM
   Liu, LG
AF Xu, Wenpeng
   Zhang, Peng
   Yu, Menglin
   Yang, Li
   Wang, Weiming
   Liu, Ligang
TI Topology Optimization Via Spatially-Varying TPMS
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Microstructure; Optimization; Topology; Lattices; Three-dimensional
   printing; Shape; Three-dimensional displays; Additive manufacturing;
   microstructure; topology optimization; triply periodic minimal surface
ID POROUS SCAFFOLD DESIGN; LEVEL SET METHOD; LATTICE STRUCTURES; FEATURE
   EVOLUTION; CODE WRITTEN; HOMOGENIZATION; MICROSTRUCTURES; SPLINES;
   MODELS; INFILL
AB Structural design with multi-family triply periodic minimal surfaces (TPMS) is a meaningful work that can combine the advantages of different types of TPMS. However, very few methods consider the influence of the blending of different TPMS on structural performance, and the manufacturability of final structure. Therefore, this work proposes a method to design manufacturable microstructures with topology optimization (TO) based on spatially-varying TPMS. In our method, different types of TPMS are simultaneously considered in the optimization to maximize the performance of designed microstructure. The geometric and mechanical properties of the unit cells generated with TPMS, that is minimal surface lattice cell (MSLC), are analyzed to obtain the performance of different types of TPMS. In the designed microstructure, MSLCs of different types are smoothly blended with an interpolation method. To analyze the influence of deformed MSLCs on the performance of the final structure, the blending blocks are introduced to describe the connection cases between different types of MSLCs. The mechanical properties of deformed MSLCs are analyzed and applied in TO process to reduce the influence of deformed MSLCs on the performance of final structure. The infill resolution of MSLC within a given design domain is determined according to the minimal printable wall thickness of MSLC and structural stiffness. Both numerical and physical experimental results demonstrate the effectiveness of the proposed method.
C1 [Xu, Wenpeng; Zhang, Peng] Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454099, Henan, Peoples R China.
   [Yu, Menglin] Henan Polytech Univ, Sch Mech & Power Engn, Jiaozuo 454099, Henan, Peoples R China.
   [Yang, Li] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Wang, Weiming] Dalian Univ Technol, Sch Math Sci, Key Lab Computat Math & Data Intelligence Liaoning, Dalian 116024, Peoples R China.
   [Wang, Weiming] Univ Manchester, Dept Mech Aerosp & Civil Engn, Manchester M13 9PL, England.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Henan Polytechnic University; Henan Polytechnic University; Dalian
   University of Technology; Dalian University of Technology; University of
   Manchester; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, WM (corresponding author), Dalian Univ Technol, Sch Math Sci, Key Lab Computat Math & Data Intelligence Liaoning, Dalian 116024, Peoples R China.; Wang, WM (corresponding author), Univ Manchester, Dept Mech Aerosp & Civil Engn, Manchester M13 9PL, England.
EM wpxu08@gmail.com; hpuzp98@qq.com; 2510392773@qq.com; 690660794@qq.com;
   wwmdlut@gmail.com; lgliu@ustc.edu.cn
RI Xu, Wenpeng/JQW-3191-2023; Zhang, Peng/AGZ-8970-2022; Liu,
   Ligang/IZQ-5817-2023; Wang, Weiming/H-4944-2017
OI Zhang, Peng/0009-0005-1468-7346; Xu, Wenpeng/0000-0002-0852-4904; Wang,
   Weiming/0000-0001-6289-0094
FU National Natural Science Foundation of China [62172073, 62025207];
   Natural Science Foundation of Liaoning Province [2021-MS-110]; Key
   Scientific Research Projects of Colleges and Universities of Henan
   Province [21A520017]; National key research and development program
   [2022YFB3303000]; Fundamental Research Fund [DUT22QN212]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62172073, and 62025207, in part by the Natural
   Science Foundation of Liaoning Province under Grant 2021-MS-110, in part
   by the Key Scientific Research Projects of Colleges and Universities of
   Henan Province under Grant 21A520017, in part by the National key
   research and development program under Grant 2022YFB3303000, and in part
   by Fundamental Research Fund under Grant DUT22QN212.
CR 3ds, 2021, Abaqus
   Abueidda DW, 2016, MECH MATER, V95, P102, DOI 10.1016/j.mechmat.2016.01.004
   Al-Ketan O, 2020, J MECH BEHAV BIOMED, V102, DOI 10.1016/j.jmbbm.2019.103520
   Andreassen E, 2014, MECH MATER, V69, P1, DOI 10.1016/j.mechmat.2013.09.018
   Aremu AO, 2016, C PROC SOC EXP MECH, P83, DOI 10.1007/978-3-319-22443-5_10
   Bendsoe M.P., 2003, Topology Optimization: Theory, Methods, and Applications, DOI DOI 10.1007/978-3-662-05086-6
   BENDSOE MP, 1988, COMPUT METHOD APPL M, V71, P197, DOI 10.1016/0045-7825(88)90086-2
   Brackett D., 2011, P INT SOL FREEF FABR, P1
   Challis VJ, 2008, INT J SOLIDS STRUCT, V45, P4130, DOI 10.1016/j.ijsolstr.2008.02.025
   Cheng L, 2018, STRUCT MULTIDISCIP O, V58, P511, DOI 10.1007/s00158-018-1905-7
   Cheng L, 2018, COMPUT METHOD APPL M, V332, P408, DOI 10.1016/j.cma.2017.12.024
   Feng JW, 2018, COMPUT METHOD APPL M, V336, P333, DOI 10.1016/j.cma.2018.03.007
   Feng YX, 2022, MATER DESIGN, V222, DOI 10.1016/j.matdes.2022.111078
   Gao W, 2015, COMPUT AIDED DESIGN, V69, P65, DOI 10.1016/j.cad.2015.04.001
   Garner E, 2019, ADDIT MANUF, V26, P65, DOI 10.1016/j.addma.2018.12.007
   Grigorovitch M, 2017, COMPUT STRUCT, V179, P95, DOI 10.1016/j.compstruc.2016.11.001
   Grigorovitch M, 2021, COMPUT MECH, V68, P1437, DOI 10.1007/s00466-021-02077-3
   Hu JB, 2022, IEEE T VIS COMPUT GR, V28, P2615, DOI 10.1109/TVCG.2020.3037697
   Hu JB, 2019, VISUAL COMPUT, V35, P949, DOI 10.1007/s00371-019-01672-z
   Huang XD, 2010, STRUCT MULTIDISCIP O, V41, P671, DOI 10.1007/s00158-010-0487-9
   Lee M, 2018, COMPUT AIDED DESIGN, V101, P23, DOI 10.1016/j.cad.2018.03.007
   Li DW, 2019, MATERIALS, V12, DOI 10.3390/ma12132183
   Li DW, 2019, J MECH DESIGN, V141, DOI 10.1115/1.4042617
   Li DW, 2018, COMPUT AIDED DESIGN, V104, P87, DOI 10.1016/j.cad.2018.06.003
   Li DW, 2016, INT J ADV MANUF TECH, V83, P1627, DOI 10.1007/s00170-015-7704-z
   Liu K, 2014, STRUCT MULTIDISCIP O, V50, P1175, DOI 10.1007/s00158-014-1107-x
   Liu L., 2014, P SIGGRAPH ASIA COUR, P1
   Liu PQ, 2021, COMPUT GRAPH-UK, V100, P106, DOI 10.1016/j.cag.2021.07.021
   Liu XL, 2022, COMPUT METHOD APPL M, V390, DOI 10.1016/j.cma.2021.114466
   Liu XP, 2018, GRAPH MODELS, V98, P14, DOI 10.1016/j.gmod.2018.05.001
   Livesu M, 2017, COMPUT GRAPH FORUM, V36, P537, DOI 10.1111/cgf.13147
   Loh GH, 2018, ADDIT MANUF, V23, P34, DOI 10.1016/j.addma.2018.06.023
   Lu L, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601168
   Osanov M, 2016, ANNU REV MATER RES, V46, P211, DOI 10.1146/annurev-matsci-070115-031826
   Plocher J, 2019, MATER DESIGN, V183, DOI 10.1016/j.matdes.2019.108164
   Rajon DA, 2003, COMPUT MED IMAG GRAP, V27, P411, DOI 10.1016/S0895-6111(03)00032-6
   SIGMUND O, 1994, INT J SOLIDS STRUCT, V31, P2313, DOI 10.1016/0020-7683(94)90154-6
   Sigmund O, 1997, J MECH PHYS SOLIDS, V45, P1037, DOI 10.1016/S0022-5096(96)00114-7
   Sigmund O, 2001, STRUCT MULTIDISCIP O, V21, P120, DOI 10.1007/s001580050176
   Sigmund O, 2013, STRUCT MULTIDISCIP O, V48, P1031, DOI 10.1007/s00158-013-0978-6
   SVANBERG K, 1987, INT J NUMER METH ENG, V24, P359, DOI 10.1002/nme.1620240207
   Vijayavenkataraman S, 2018, ACS APPL BIO MATER, V1, P259, DOI 10.1021/acsabm.8b00052
   Wang MY, 2019, COMPUT METHOD APPL M, V349, P378, DOI 10.1016/j.cma.2019.02.026
   Wang MY, 2003, COMPUT METHOD APPL M, V192, P227, DOI 10.1016/S0045-7825(02)00559-5
   Wang SF, 2022, COMPUT AIDED DESIGN, V142, DOI 10.1016/j.cad.2021.103123
   Wang WM, 2023, ADDIT MANUF, V67, DOI 10.1016/j.addma.2023.103507
   Wang WM, 2022, ADDIT MANUF, V54, DOI 10.1016/j.addma.2022.102717
   Wang WM, 2018, IEEE T VIS COMPUT GR, V24, P2787, DOI 10.1109/TVCG.2017.2764462
   Wang WM, 2017, COMPUT GRAPH-UK, V66, P154, DOI 10.1016/j.cag.2017.05.022
   Wang WM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508382
   Wang X, 2018, ADDIT MANUF, V20, P189, DOI 10.1016/j.addma.2017.10.001
   Wu J, 2021, STRUCT MULTIDISCIP O, V63, P1455, DOI 10.1007/s00158-021-02881-8
   Wu J, 2021, IEEE T VIS COMPUT GR, V27, P43, DOI 10.1109/TVCG.2019.2938946
   Wu J, 2018, IEEE T VIS COMPUT GR, V24, P1127, DOI 10.1109/TVCG.2017.2655523
   Wu J, 2016, COMPUT AIDED DESIGN, V80, P32, DOI 10.1016/j.cad.2016.07.006
   Xie Y, 2017, VIS INFORM, V1, P9, DOI 10.1016/j.visinf.2017.01.002
   Xu SQ, 2016, MATER DESIGN, V93, P443, DOI 10.1016/j.matdes.2016.01.007
   Xu WP, 2022, IEEE T VIS COMPUT GR, V28, P4462, DOI 10.1109/TVCG.2021.3091509
   Yan CZ, 2014, MATER DESIGN, V55, P533, DOI 10.1016/j.matdes.2013.10.027
   Yan X, 2020, IEEE T VIS COMPUT GR, V26, P3037, DOI 10.1109/TVCG.2019.2914044
   Yang N, 2014, COMPUT AIDED DESIGN, V56, P11, DOI 10.1016/j.cad.2014.06.006
   Yang Y, 2018, COMPUT GRAPH-UK, V70, P148, DOI 10.1016/j.cag.2017.07.005
   Yoo DJ, 2015, INT J PRECIS ENG MAN, V16, P2021, DOI 10.1007/s12541-015-0263-2
   Yoo DJ, 2011, INT J PRECIS ENG MAN, V12, P61, DOI 10.1007/s12541-011-0008-9
   Yu SX, 2019, MATER DESIGN, V182, DOI 10.1016/j.matdes.2019.108021
   Zadpoor AA, 2016, MATER HORIZ, V3, P371, DOI 10.1039/c6mh00065g
   Zhang SN, 2022, INT J MECH SCI, V235, DOI 10.1016/j.ijmecsci.2022.107713
   Zhang XL, 2015, COMPUT AIDED GEOM D, V35-36, P149, DOI 10.1016/j.cagd.2015.03.012
   Zheng XY, 2014, SCIENCE, V344, P1373, DOI 10.1126/science.1252291
   Zhou H, 2021, J MECH PHYS SOLIDS, V148, DOI 10.1016/j.jmps.2021.104298
   Zong HM, 2019, COMPUT METHOD APPL M, V354, P487, DOI 10.1016/j.cma.2019.05.029
NR 71
TC 4
Z9 4
U1 18
U2 45
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4570
EP 4587
DI 10.1109/TVCG.2023.3268068
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400036
PM 37074903
DA 2024-11-06
ER

PT J
AU Peck, TC
   Good, JJ
AF Peck, Tabitha C.
   Good, Jessica J.
TI Measuring Embodiment: Movement Complexity and the Impact of Personal
   Characteristics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Behavioral sciences; Avatars; Particle measurements; Atmospheric
   measurements; Mathematical models; STEM; Virtual environments; Age;
   avatars; behavior; embodiment; gender; structural equation modeling
ID IMMERSIVE VIRTUAL-REALITY; OVERT HEAD MOVEMENTS; ILLUSORY OWNERSHIP;
   SELF-AVATAR; GENDER; BODY; REPRESENTATION; MASCULINITY; PERCEPTION;
   PERSUASION
AB A user's personal experiences and characteristics may impact the strength of an embodiment illusion and affect resulting behavioral changes in unknown ways. This paper presents a novel re-analysis of two fully-immersive embodiment user-studies (n = 189 and n = 99) using structural equation modeling, to test the effects of personal characteristics on subjective embodiment. Results demonstrate that individual characteristics (gender, participation in science, technology, engineering or math - Experiment 1, age, video gaming experience - Experiment 2) predicted differing self-reported experiences of embodiment Results also indicate that increased self-reported embodiment predicts environmental response, in this case faster and more accurate responses within the virtual environment. Importantly, head-tracking data is shown to be an effective objective measure for predicting embodiment, without requiring researchers to utilize additional equipment.
C1 [Peck, Tabitha C.] Davidson Coll, Davidson, NC 28035 USA.
C3 Davidson College
RP Peck, TC (corresponding author), Davidson Coll, Davidson, NC 28035 USA.
EM tapeck@davidson.edu; jegood@davidson.edu
RI Peck, Tabitha/AAH-2032-2021
OI Peck, Tabitha/0000-0002-3667-7713; Good, Jessica/0000-0003-3009-1903
FU National Science Foundation [1942146]
FX No Statement Available
CR Ahn SJG, 2016, J COMPUT-MEDIAT COMM, V21, P399, DOI 10.1111/jcc4.12173
   Banakou D, 2020, ROY SOC OPEN SCI, V7, DOI 10.1098/rsos.201848
   Banakou D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00917
   Banakou D, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00601
   Banakou D, 2013, P NATL ACAD SCI USA, V110, P12846, DOI 10.1073/pnas.1306779110
   Bianchi-Berthouze N, 2007, LECT NOTES COMPUT SC, V4738, P102
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Briñol P, 2003, J PERS SOC PSYCHOL, V84, P1123, DOI 10.1037/0022-3514.84.6.1123
   Bugg JM, 2006, BRAIN COGNITION, V62, P9, DOI 10.1016/j.bandc.2006.02.006
   Carrasco R., 2017, Negotiating Stereotypes of Older Adults Through Avatars, P218
   Cheng LK, 2014, VIRTUAL REAL-LONDON, V18, P173, DOI 10.1007/s10055-014-0244-2
   Cheung GW, 2002, STRUCT EQU MODELING, V9, P233, DOI 10.1207/S15328007SEM0902_5
   COHEN H, 1986, PERCEPT MOTOR SKILL, V63, P83, DOI 10.2466/pms.1986.63.1.83
   Cohn A., 2006, PSYCHOL MEN MASCULIN, V7, P179, DOI DOI 10.1037/1524-9220.7.4.179
   Dewez D, 2019, INT SYM MIX AUGMENT, P123, DOI 10.1109/ISMAR.2019.00-12
   Fausto-Sterling A., 2000, Sexing the Body: Gender Politics and the Construction of Sexuality
   Fazio RH, 2003, ANNU REV PSYCHOL, V54, P297, DOI 10.1146/annurev.psych.54.101601.145225
   Fribourg R, 2020, IEEE T VIS COMPUT GR, V26, P2062, DOI 10.1109/TVCG.2020.2973077
   Garbarini F, 2014, CURR BIOL, V24, pR738, DOI 10.1016/j.cub.2014.07.023
   Gonzalez R, 2001, PSYCHOL METHODS, V6, P258, DOI 10.1037/1082-989X.6.3.258
   Gonzalez-Franco M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P941, DOI [10.1109/VR.2019.8798348, 10.1109/vr.2019.8798348]
   Gonzalez-Franco M, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00074
   González-Franco M, 2014, EXP BRAIN RES, V232, P875, DOI 10.1007/s00221-013-3800-1
   Hamilton-Giachritsis C, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21036-2
   Herrera F, 2021, NEW MEDIA SOC, V23, P2189, DOI 10.1177/1461444821993121
   Juliano JM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041204
   Kalckert A, 2014, CONSCIOUS COGN, V26, P117, DOI 10.1016/j.concog.2014.02.003
   Kilteni K, 2013, IEEE T VIS COMPUT GR, V19, P597, DOI 10.1109/TVCG.2013.29
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kilteni K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040867
   Kim SY, 2014, COMPUT HUM BEHAV, V36, P376, DOI 10.1016/j.chb.2014.03.067
   Kubota JT, 2017, SOC NEUROSCI-UK, V12, P468, DOI 10.1080/17470919.2016.1182585
   Lehdonvirta M, 2012, GAMES CULT, V7, P29, DOI 10.1177/1555412012440307
   Little T. D., 2013, LONGITUDINAL STRUCTU, DOI 10.2/JQUERY.MIN.JS
   Makransky G, 2019, COMPUT EDUC, V134, P15, DOI 10.1016/j.compedu.2019.02.002
   Maselli A, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00083
   McManus ErinA., 2011, Proceedings of the ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization, P37
   Moakler MW, 2014, CAREER DEV Q, V62, P128, DOI 10.1002/j.2161-0045.2014.00075.x
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Ogawa N, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376562
   Oransky M, 2009, J ADOLESCENT RES, V24, P218, DOI 10.1177/0743558408329951
   Pan Y, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364270
   Pan Y, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0189078
   Peck J. J., 2020, P CHI C HUM FACT COM, P1
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Peck TC, 2021, IEEE T VIS COMPUT GR, V27, P2502, DOI 10.1109/TVCG.2021.3067767
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1964, DOI 10.1109/TVCG.2020.2973061
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1945, DOI 10.1109/TVCG.2020.2973498
   Peck TC, 2018, IEEE T VIS COMPUT GR, V24, P1604, DOI 10.1109/TVCG.2018.2793598
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Phillips L, 2010, P IEEE VIRT REAL ANN, P115, DOI 10.1109/VR.2010.5444802
   Pollack W., 2000, Real Boys Voices: Boys Speak Out About Drugs, Sex, Violence, Bullying, Sports, School, Parents, and So Much More
   Puri A., 2017, P 29 AUSTR C COMP HU, P503, DOI [10.1145/3152771.3156166, DOI 10.1145/3152771.3156166]
   Quinn PC, 2002, PERCEPTION, V31, P1109, DOI 10.1068/p3331
   Reimann M, 2012, J NEUROSCI PSYCHOL E, V5, P104, DOI 10.1037/a0026855
   Ries V., 2009, P 16 ACM S VIRT REAL, P59, DOI [10.1145/1643928.16439433, DOI 10.1145/1643928.16439433, DOI 10.1145/1643928.1643943]
   Schmader T, 2002, J EXP SOC PSYCHOL, V38, P194, DOI 10.1006/jesp.2001.1500
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Simon RM, 2017, J RES SCI TEACH, V54, P299, DOI 10.1002/tea.21345
   Slater M, 2004, PRESENCE-VIRTUAL AUG, V13, P484, DOI 10.1162/1054746041944849
   Slater M., 1993, VR93 Virtual Reality International 93. Proceedings of the Third Annual Conference on Virtual Reality, P34
   Slater M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-46877-3
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Steed A, 2016, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2016.7504689
   STETS JE, 1995, SOCIOL PERSPECT, V38, P129, DOI 10.2307/1389287
   Tieri G, 2015, SCI REP-UK, V5, DOI 10.1038/srep17139
   TOM G, 1991, BASIC APPL SOC PSYCH, V12, P281, DOI 10.1207/s15324834basp1203_3
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P682, DOI [10.1109/VR46266.2020.00-13, 10.1109/VR46266.2020.1581195115265]
   Wang X., 2019, Structural Equation Modeling: Applications UsingMplus
   WEISS R, 1995, IEEE MULTIMEDIA, V2, P12, DOI 10.1109/93.368596
   Wells GL, 1980, BASIC APPL SOC PSYCH, V1, P219, DOI 10.1207/s15324834basp0103_2
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
   Yee N, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P773
NR 73
TC 2
Z9 2
U1 3
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4588
EP 4600
DI 10.1109/TVCG.2023.3270725
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400063
PM 37099457
OA hybrid
DA 2024-11-06
ER

PT J
AU Lu, ZX
   He, XW
   Guo, YZ
   Liu, XH
   Wang, HM
AF Lu, Zixuan
   He, Xiaowei
   Guo, Yuzhong
   Liu, Xuehui
   Wang, Huamin
TI Projective Peridynamic Modeling of Hyperelastic Membranes With Contact
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mathematical models; Convergence; Computational modeling; Real-time
   systems; Graphics processing units; Deformation; Software; Hyperelastic
   membranes; projective peridynamics; semi-implicit successive
   substitution method (SISSM); contact handling
AB Real-time simulation of hyperelastic membranes like cloth still faces a lot of challenges, such as hyperplasticity modeling and contact handling. In this study, we propose projective peridynamics that uses a local-global strategy to enable fast and robust simulation of hyperelastic membranes with contact. In the global step, we propose a semi-implicit strategy to linearize the governing equation for hyperelastic materials that are modeled with peridynamics. By decomposing the first Piola-Kirchhoff stress tensor into a positive and a negative part, successive substitutions can be taken to solve the nonlinear problems. Convergence is guaranteed by further addressing the overshooting problem. Since our global step solve requires no energy summation and dot product operations over the entire problem, it fits into GPU implementation perfectly. In the local step, we further present a GPU-friendly gradient descent method to prevent interpenetration by solving an optimization problem independently. Putting the global and local solves together, experiments show that our method is robust and efficient in simulating complex models of membranes involving hyperelastic materials and contact.
C1 [Lu, Zixuan; He, Xiaowei; Guo, Yuzhong; Liu, Xuehui] Chinese Acad Sci, Inst Software, Beijing 100045, Peoples R China.
   [Lu, Zixuan] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Wang, Huamin] Style3D, Hangzhou 310012, Zhejiang, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP He, XW; Liu, XH (corresponding author), Chinese Acad Sci, Inst Software, Beijing 100045, Peoples R China.
EM luzx@ios.ac.cn; xiaowei@iscas.ac.cn; guoyuzhong@iscas.ac.cn;
   lxh@ios.ac.cn; wanghmin@gmail.com
RI Wang, Huamin/D-2600-2012; Liu, Xuehui/AAA-3050-2020
OI he, xiao wei/0000-0002-8870-2482; Wang, Huamin/0000-0002-8153-2337
FU National Key R & D Program of China [2021YFB1715800]; National Natural
   Science Foundation of China [61872345]; Youth Innovation Pro-motion
   Association, CAS [2019109]
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2021YFB1715800, in part by the National Natural
   Science Foundation of China under Grant 61872345, in part by Youth
   Innovation Pro-motion Association, CAS under Grant 2019109.
CR [Anonymous], 1966, Pacific J. Math., V16, P1
   [Anonymous], 1997, P EUR WORKSH COMP AN
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bender M., 2015, Eurographics Tut. Notes
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Gerstle W, 2007, NUCL ENG DES, V237, P1250, DOI 10.1016/j.nucengdes.2006.10.002
   Gibson B., 1997, Tech. Rep. TR-97-19
   Grinspun E., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P62
   He XW, 2018, IEEE T VIS COMPUT GR, V24, P2589, DOI 10.1109/TVCG.2017.2755646
   Irving G., 2004, P ACM SIGGRAPH EUR S, P131, DOI DOI 10.1145/1028523.1028541
   Javili A, 2019, MATH MECH SOLIDS, V24, P3714, DOI 10.1177/1081286518803411
   Jiang CFF, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073623
   Lan L, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530069
   Lehoucq RB, 2008, J MECH PHYS SOLIDS, V56, P1566, DOI 10.1016/j.jmps.2007.08.004
   Li MC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459767
   Li MC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392425
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Liu TT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508406
   Macklin M, 2021, PROCEEDINGS OF THE 14TH ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2021, DOI 10.1145/3487983.3488289
   McAdams A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964932
   Mott PH, 2013, PHYS SCRIPTA, V87, DOI 10.1088/0031-8949/87/05/055404
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller Matthias., 2014, P ACM SIGGRAPHEUROGR, P149, DOI DOI 10.1145/2343483.2343501
   Narain Rahul, 2016, P ACM SIGGRAPH EUR S, P21
   Nocedal J., 1999, Numerical optimization
   Ogden R.W., 2013, NONLINEAR ELASTIC DE
   Otaduy MA, 2009, COMPUT GRAPH FORUM, V28, P559, DOI 10.1111/j.1467-8659.2009.01396.x
   Oterkus E, 2014, Peridynamic Theory and Its Applications, DOI 10.1007/978-1-4614-8465-3
   Peng Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201290
   Poisson S.-D, 1828, Ann Chimie et Physique, V37, P337
   Reddy JN, 2006, Theory and Analysis of Elastic Plates and Shells.
   Silling SA, 2008, J ELASTICITY, V93, P13, DOI 10.1007/s10659-008-9163-3
   Silling SA, 2007, J ELASTICITY, V88, P151, DOI 10.1007/s10659-007-9125-1
   Silling SA, 2005, COMPUT STRUCT, V83, P1526, DOI 10.1016/j.compstruc.2004.11.026
   Silling SA, 2000, J MECH PHYS SOLIDS, V48, P175, DOI 10.1016/S0022-5096(99)00029-0
   Tamstorf R, 2013, GRAPH MODELS, V75, P362, DOI 10.1016/j.gmod.2013.07.001
   Tang M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275005
   Teran E., 2005, P ACM SIGGRAPH EUR S, P181, DOI [DOI 10.1145/1073368.1073394EVENT-PLACE, 10.1145/1073368, DOI 10.1145/1073368, DOI 10.1145/1073368.1073394]
   Tupek MR, 2013, COMPUT METHOD APPL M, V263, P20, DOI 10.1016/j.cma.2013.04.012
   Wang BL, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3460775
   Wang HM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980236
   Wu LH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3430025
   Wu XL, 2001, COMPUT GRAPH FORUM, V20, pC349
   Xu HY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766917
   Xu LY, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13553
NR 47
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4601
EP 4614
DI 10.1109/TVCG.2023.3271511
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400066
PM 37115842
DA 2024-11-06
ER

PT J
AU Wang, M
   Li, YJ
   Shi, JC
   Steinicke, F
AF Wang, Miao
   Li, Yi-Jun
   Shi, Jinchuan
   Steinicke, Frank
TI SceneFusion: Room-Scale Environmental Fusion for Efficient Traveling
   Between Separate Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Teleportation; Visualization; Collaboration; Task analysis; Portals;
   Legged locomotion; Three-dimensional displays; Virtual reality;
   collaborative virtual environments; scene transition
ID GROUP NAVIGATION; VISUAL COMFORT; REALITY; TRANSITION
AB Traveling between scenes has become a major requirement for navigation in numerous virtual reality (VR) social platforms and game applications, allowing users to efficiently explore multiple virtual environments (VEs). To facilitate scene transition, prevalent techniques such as instant teleportation and virtual portals have been extensively adopted. However, these techniques exhibit limitations when there is a need for frequent travel between separate VEs, particularly within indoor environments, resulting in low efficiency. In this article, we first analyze the design rationale for a novel navigation method supporting efficient travel between virtual indoor scenes. Based on the analysis, we introduce the SceneFusion technique that fuses separate virtual rooms into an integrated environment. SceneFusion enables users to perceive rich visual information from both rooms simultaneously, achieving high visual continuity and spatial awareness. While existing teleportation techniques passively transport users, SceneFusion allows users to actively access the fused environment using short-range locomotion techniques. User experiments confirmed that SceneFusion outperforms instant teleportation and virtual portal techniques in terms of efficiency, workload, and preference for both single-user exploration and multi-user collaboration tasks in separate VEs. Thus, SceneFusion presents an effective solution for seamless traveling between virtual indoor scenes.
C1 [Wang, Miao; Li, Yi-Jun; Shi, Jinchuan] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Miao] Zhongguancun Lab, Beijing 100094, Peoples R China.
   [Steinicke, Frank] Univ Hamburg, Dept Informat, D-22605 Hamburg, Germany.
C3 Beihang University; Zhongguancun Laboratory; University of Hamburg
RP Wang, M (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM miaowang.me@gmail.com; yaoling@buaa.edu.cn; buaashijinchuan@gmail.com;
   frank.steinicke@uni-hamburg.de
RI ; Steinicke, Frank/AAC-2976-2020
OI Li, Yi-Jun/0000-0003-2733-2913; Shi, Jin-Chuan/0009-0003-4899-2205;
   Steinicke, Frank/0000-0001-9879-7414
FU National Natural Science Foundation of China [61932003]; Fundamental
   Research Funds for the Central Universities; BMBF; BMWi; DFG; EU
FX No Statement Available
CR Abowd GD, 1999, LECT NOTES COMPUT SC, V1707, P304
   Agrawala M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P327, DOI 10.1145/258734.258875
   Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Bai HD, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376550
   Benford S, 2001, COMMUN ACM, V44, P79, DOI 10.1145/379300.379322
   Boletsis Costas, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040024
   Bolte Benjamin, 2011, P VIRT REAL INT C
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Bruder G, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P75, DOI 10.1109/3DUI.2009.4811208
   Burdea G.C., 1999, VIRTUAL REALITY PROT, V2, P17, DOI 10.1.1.135.6358
   Cheng LP, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P359, DOI [10.1109/vr.2019.8798074, 10.1109/VR.2019.8798074]
   Chow Kevin, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359142
   Churchill E. F., 1998, Virtual Reality, V3, P3, DOI 10.1007/BF01409793
   Di Luca M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445319
   Dong ZC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130893
   Feasel J, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P97
   Fitter HN, 2014, PROCEDIA ENGINEER, V97, P1155, DOI 10.1016/j.proeng.2014.12.394
   Freina L, 2015, ELEARN SOFTW EDUC, P133, DOI 10.12753/2066-026X-15-020
   Freitag S, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P119, DOI 10.1109/3DUI.2014.6798852
   Games V., 2023, Pavlov VR
   Grandi JG, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P127, DOI [10.1109/vr.2019.8798080, 10.1109/VR.2019.8798080]
   Grubert J, 2017, IEEE T VIS COMPUT GR, V23, P1706, DOI 10.1109/TVCG.2016.2543720
   Gugenheimer J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4021, DOI 10.1145/3025453.3025683
   Guo JJ, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2996296
   Han P-H., 2017, P ACM SIGGRAPH EM TE, P1
   Hanson S, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P367, DOI [10.1109/vr.2019.8797751, 10.1109/VR.2019.8797751]
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Hartmann J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300577
   He ZY, 2020, INT SYM MIX AUGMENT, P542, DOI 10.1109/ISMAR50242.2020.00082
   Hsu TW, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P363, DOI [10.1109/VR46266.2020.1581231362069, 10.1109/VR46266.2020.00-48]
   Husung M., 2019, P MENSCH COMP, P245, DOI 10.1145/3340764.3340779
   Insko Brent Edward, 2001, Passive haptics significantly enhances virtual environments
   Irlitti A, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P31, DOI [10.1109/ISMAR-Adjunct.2016.25, 10.1109/ISMAR-Adjunct.2016.0032]
   Jung YJ, 2014, IEEE T CONSUM ELECTR, V60, P1, DOI 10.1109/TCE.2014.6780918
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Keshavarzi M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P353, DOI [10.1109/VR46266.2020.1581131119600, 10.1109/VR46266.2020.00-49]
   Kim D, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P379, DOI 10.1109/VR51125.2022.00057
   Kiyokawa K., 2005, P INT C HUM COMP INT
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Kruse L, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P305, DOI 10.1109/VR.2018.8446216
   Kunert A, 2020, IEEE T VIS COMPUT GR, V26, P3271, DOI 10.1109/TVCG.2019.2914677
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Lee GA, 2020, INT SYM MIX AUGMENT, P498, DOI 10.1109/ISMAR50242.2020.00078
   Lee GA, 2018, INT SYM MIX AUGMENT, P153, DOI 10.1109/ISMAR.2018.00051
   Li CY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480478
   Li JB, 2017, EXTENDED ABSTRACTS PUBLICATION OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY'17 EXTENDED ABSTRACTS), P585, DOI 10.1145/3130859.3130860
   Li YJ, 2022, J COMPUT SCI TECH-CH, V37, P561, DOI 10.1007/s11390-022-2266-7
   Liao MY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P163, DOI [10.1109/VR.2019.8797708, 10.1109/vr.2019.8797708]
   Lili Wang, 2022, 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), P628, DOI 10.1109/VRW55335.2022.00165
   Liu J, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P521, DOI 10.1145/3242587.3242601
   Lokki T, 2005, IEEE MULTIMEDIA, V12, P80, DOI 10.1109/MMUL.2005.33
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   MacQuarrie A, 2018, IEEE T VIS COMPUT GR, V24, P1564, DOI 10.1109/TVCG.2018.2793561
   Malinov YD, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P501, DOI 10.1109/VR50410.2021.00074
   Men L, 2017, P IEEE VIRT REAL ANN, P285, DOI 10.1109/VR.2017.7892288
   Moghadam K, 2020, IEEE T VIS COMPUT GR, V26, P2273, DOI 10.1109/TVCG.2018.2884468
   Moghadam KR, 2017, P IEEE VIRT REAL ANN, P375, DOI 10.1109/VR.2017.7892333
   Nehmé Y, 2021, IEEE T VIS COMPUT GR, V27, P2202, DOI 10.1109/TVCG.2020.3036153
   Nguyen T.T.H., 2013, P GRAPP INT C COMP G, P327
   Nguyen T.T. H., 2014, P INT WORKSH COLL VI, P1
   Nilsson NC, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P31, DOI 10.1109/3DUI.2013.6550193
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Piumsomboon T, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173620
   Piumsomboon T, 2017, SA'17: SIGGRAPH ASIA 2017 EMERGING TECHNOLOGIES, DOI 10.1145/3132818.3132822
   Prithul A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.730792
   Razzaque S., 2005, Redirected Walking
   Rhee T, 2020, IEEE T VIS COMPUT GR, V26, P1923, DOI 10.1109/TVCG.2020.2973065
   Sayyad E, 2020, INT SYM MIX AUGMENT, P608, DOI 10.1109/ISMAR50242.2020.00088
   Schott D, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P296, DOI 10.1109/VR50410.2021.00052
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Sereno M, 2022, IEEE T VIS COMPUT GR, V28, P2530, DOI 10.1109/TVCG.2020.3032761
   Slater M, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00074
   Soret R, 2021, APPL ERGON, V97, DOI 10.1016/j.apergo.2021.103535
   Sra M, 2018, IEEE T VIS COMPUT GR, V24, P3174, DOI 10.1109/TVCG.2017.2762691
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Stotko P, 2019, IEEE T VIS COMPUT GR, V25, P2102, DOI 10.1109/TVCG.2019.2899231
   Summers C, 2017, 2017 IEEE 3RD VR WORKSHOP ON SONIC INTERACTIONS FOR VIRTUAL ENVIRONMENTS (SIVE)
   Sun Q, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925883
   Teo T, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364238
   Teo T, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300431
   Thanyadit Santawat, 2018, Proceedings of the ACM on Human-Computer Interaction, V2, DOI 10.1145/3274441
   Thanyadit S., 2020, P S SPAT US INT, P1
   V. Inc, 2023, VRChat
   Vasylevska K., 2014, CHALLENGING PRESENCE, P205
   Walton DJ, 1999, COMPUT AIDED DESIGN, V31, P857, DOI 10.1016/S0010-4485(99)00073-1
   Wang LL, 2021, INT SYM MIX AUGMENT, P60, DOI 10.1109/ISMAR52148.2021.00020
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Weissker Tim, 2021, IEEE Transactions on Visualization and Computer Graphics, V27, P2524, DOI 10.1109/TVCG.2021.3067756
   Weissker T, 2020, IEEE T VIS COMPUT GR, V26, P1860, DOI 10.1109/TVCG.2020.2973474
   Weissker T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P136, DOI [10.1109/vr.2019.8797807, 10.1109/VR.2019.8797807]
   Wells T, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376541
   Wendt JD, 2010, P IEEE VIRT REAL ANN, P51
   Xu SZ, 2024, IEEE T VIS COMPUT GR, V30, P1916, DOI 10.1109/TVCG.2023.3251648
   Yanxiang Zhang, 2018, Augmented Reality, Virtual Reality, and Computer Graphics. 5th International Conference, AVR 2018. Proceedings: LNCS 10850, P190, DOI 10.1007/978-3-319-95270-3_14
   Yoon B, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P547, DOI [10.1109/VR.2019.8797719, 10.1109/vr.2019.8797719]
   Yoshida N, 2006, VISUAL COMPUT, V22, P896, DOI 10.1007/s00371-006-0076-5
   Young J, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3397331
   Young J, 2019, IEEE T VIS COMPUT GR, V25, P1908, DOI 10.1109/TVCG.2019.2898737
   Yu DF, 2020, IEEE T VIS COMPUT GR, V26, P3402, DOI [10.1109/TVCG.2020.3023606, 10.1109/TCVG.2020.3023606]
   Zeltzer D., 1992, Presence: Teleoperators and Virtual Environments, V1, P127, DOI [DOI 10.1162/PRES.1992.1.1.127, 10.1162/pres.1992.1.1.127]
   Zhang YZ, 2022, IEEE T VIS COMPUT GR, V28, P2146, DOI 10.1109/TVCG.2022.3150512
   Zyda M, 2005, COMPUTER, V38, P25, DOI 10.1109/MC.2005.297
NR 102
TC 1
Z9 1
U1 8
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4615
EP 4630
DI 10.1109/TVCG.2023.3271709
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400021
PM 37126613
DA 2024-11-06
ER

PT J
AU Fung, KLT
   Perrault, ST
   Gastner, MT
AF Fung, Kelvin L. T.
   Perrault, Simon T.
   Gastner, Michael T.
TI Effectiveness of Area-to-Value Legends and Grid Lines in Contiguous Area
   Cartograms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cartogram; geovisualization; interactive data exploration; quantitative
   evaluation
ID ALGORITHM
AB A contiguous area cartogram is a geographic map in which the area of each region is proportional to numerical data (e.g., population size) while keeping neighboring regions connected. In this study, we investigated whether value-to-area legends (square symbols next to the values represented by the squares' areas) and grid lines aid map readers in making better area judgments. We conducted an experiment to determine the accuracy, speed, and confidence with which readers infer numerical data values for the mapped regions. We found that, when only informed about the total numerical value represented by the whole cartogram without any legend, the distribution of estimates for individual regions was centered near the true value with substantial spread. Legends with grid lines significantly reduced the spread but led to a tendency to underestimate the values. Comparing differences between regions or between cartograms revealed that legends and grid lines slowed the estimation without improving accuracy. However, participants were more likely to complete the tasks when legends and grid lines were present, particularly when the area units represented by these features could be interactively selected. We recommend considering the cartogram's use case and purpose before deciding whether to include grid lines or an interactive legend.
C1 [Fung, Kelvin L. T.] Yale NUS Coll, Singapore 138527, Singapore.
   [Fung, Kelvin L. T.] UCL, London WC1E 6BT, England.
   [Gastner, Michael T.] Singapore Inst Technol, Singapore 138683, Singapore.
   [Perrault, Simon T.] Singapore Univ Technol & Design, Singapore 487372, Singapore.
C3 Yale NUS College; University of London; University College London;
   Singapore Institute of Technology; Singapore University of Technology &
   Design
RP Gastner, MT (corresponding author), Singapore Inst Technol, Singapore 138683, Singapore.
EM fltkelvin@u.yale-nus.edu.sg; simon_perrault@sutd.edu.sg;
   michael.gastner@singaporetech.edu.sg
RI Gastner, Michael/AAK-5088-2021
OI Perrault, Simon/0000-0002-3105-9350; Fung, Kelvin/0009-0009-1694-3234;
   Gastner, Michael/0000-0002-1097-8833
FU Ministry of Education - Singapore [MOE-T2EP20221-0007]; Academic
   Research Fund Tier 1 programme [IG18-PRB104, R-607-000-401-114];
   Yale-NUSunder its Summer Research Programme
FX No Statement Available
CR Abdi H., 2010, Coefficient of Variation, V1, P169
   Allen Mike., 2017, SAGE ENCY COMMUNICAT, DOI DOI 10.4135/9781483381411
   Aschwanden C., 1998, Kartographische Nachrichten, V48, P221
   Bartram L, 2011, IEEE T VIS COMPUT GR, V17, P1444, DOI 10.1109/TVCG.2010.237
   Bestgen A.-K., 2013, P COGN SCI SOC, P193
   Boers M, 2018, ANN RHEUM DIS, V77, P833, DOI 10.1136/annrheumdis-2018-213396
   Brewer C.A., 2013, ColorBrewer: Color Advice for Maps
   Cano RG, 2015, COMPUT GRAPH FORUM, V34, P361, DOI 10.1111/cgf.12648
   Clark R., 2012, The carbon map
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P531, DOI 10.2307/2288400
   Cox C.W., 1976, The American Cartographer, V3, P65
   Cruz P, 2017, PROCEEDINGS OF THE 2017 IEEE VIS ARTS PROGRAM (VISAP)
   Cumming G., 2011, Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analyses
   Dent B. D., 1975, American Cartographer, V2, P154, DOI [10.1559/152304075784313278, DOI 10.1559/152304075784313278]
   Dorling D., 1996, Concepts and Techniques in Modern Geography, V59
   Dukaczewski D, 2020, INT CONF CARTOGR GIS, P15
   Duncan IK, 2021, IEEE T VIS COMPUT GR, V27, P2136, DOI 10.1109/TVCG.2020.3041745
   Fay MP, 2010, R J, V2, P53
   Few S., 2005, DM Review
   Flannery JJ., 1971, CANADIAN CARTOGRAPHE, V8, P96, DOI [DOI 10.3138/J647-1776-745H-3667, 10.3138/J647-1776-745H-3667]
   Gastner M. T., 2022, Encyclopedia of Mathematical Geosciences, P1
   Gastner M. T, 2021, Creating cartograms online
   Gastner MT, 2018, P NATL ACAD SCI USA, V115, pE2156, DOI 10.1073/pnas.1712674115
   GOODCHILD MF, 1988, AM CARTOGRAPHER, V15, P311, DOI 10.1559/152304088783886973
   Gotthard B., 2022, Worldmapper rediscover the world as you'venever seen it before
   GRANT DA, 1948, PSYCHOL BULL, V45, P427, DOI 10.1037/h0053912
   GRIFFIN TLC, 1983, AM CARTOGRAPHER, V10, P17, DOI 10.1559/152304083783948258
   Han R, 2017, ISPRS INT J GEO-INF, V6, DOI 10.3390/ijgi6060180
   Heer J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P203
   Hennig J., 2010, ArcUser, P66
   Kaspar S., 2011, P 25 INT CART C, P112
   Lachenbruch P., 2005, Encyclopedia of Biostatistics, P1
   McCrum-Gardner E, 2008, BRIT J ORAL MAX SURG, V46, P38, DOI 10.1016/j.bjoms.2007.09.002
   McKinlay J, 2010, CHANDOS INF PROF SER, P1, DOI 10.1533/9781780630243
   Mcknight P. E., 2010, CORSINI ENCY PSYCHOL, P1, DOI [10.3109/9780203450307-26, DOI 10.3109/9780203450307-26]
   Nusrat S., 2015, P EUR C VIS SHORT PA, P61
   Nusrat S, 2020, PROCEEDINGS OF THE WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES AVI 2020, DOI 10.1145/3399715.3399873
   Nusrat S, 2018, IEEE T VIS COMPUT GR, V24, P1100, DOI 10.1109/TVCG.2016.2642109
   OLSON JM, 1976, PROF GEOGR, V28, P371, DOI 10.1111/j.0033-0124.1976.00371.x
   Ordnance Survey, 2016, Using the national grid
   Peterson MP, 1999, INT J GEOGR INF SCI, V13, P375, DOI 10.1080/136588199241256
   Riedmann M., 2021, Die Erde in karten: So haben Sie die Welt nochnicht gesehen
   Rittschof KA, 1996, J GEOGR, V95, P50, DOI 10.1080/00221349608978925
   Schwabish JA, 2014, J ECON PERSPECT, V28, P209, DOI 10.1257/jep.28.1.209
   Shimizu E, 2009, INT J GEOGR INF SCI, V23, P1453, DOI 10.1080/13658810802186882
   Sischka PE, 2022, SOC SCI COMPUT REV, V40, P405, DOI 10.1177/0894439320907067
   Snyder J. P., 1987, Map projections: A working manual
   Stone M., 2008, P 16 IST SID COL IM, P355
   Sun H, 2010, CARTOGR J, V47, P12, DOI 10.1179/000870409X12525737905169
   Tingsheng I. K., P 8 INT C CART
   Tingsheng S., 2019, Abstr. Int. Cartographic Assoc., V1
   Tobler W R, 1973, Ann N Y Acad Sci, V219, P215, DOI 10.1111/j.1749-6632.1973.tb41401.x
   van Kreveld M, 2004, LECT NOTES COMPUT SC, V3221, P724
   Veaux De, 2015, Stats: Data and Models, V4th
   Wickham H., 2021, ggplot2: Elegant graphics for data analysis 2016
   Wilkinson L., 2012, Handbook of Computational Statistics, P375, DOI [10.1007/978-3-642-21551-3_13, DOI 10.1007/978-3-642-21551-3_13]
   Woytinsky WladimirS., 1953, World Population and Production: Trends and Outlook
   Yau YC, 2021, ENVIRON PLANN A, V53, P1249, DOI 10.1177/0308518X21998356
NR 59
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4631
EP 4647
DI 10.1109/TVCG.2023.3275925
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400091
PM 37186538
OA Green Accepted, Green Published, hybrid
DA 2024-11-06
ER

PT J
AU Machuca, MDB
   Israel, JH
   Keefe, DF
   Stuerzlinger, W
AF Machuca, Mayra Donaji Barrera
   Israel, Johann Habakuk
   Keefe, Daniel F.
   Stuerzlinger, Wolfgang
TI Toward More Comprehensive Evaluations of 3D Immersive Sketching,
   Drawing, and Painting
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Bibliographies; Task analysis; Art;
   Visualization; Creativity; Painting; Information interfaces and
   representation (HCI); artificial; augmented; and virtual realities;
   evaluation/ methodology
ID CONCEPTUAL DESIGN; CREATIVITY; USABILITY; SURFACES; THINKING; SHAPE
AB To understand current practice and explore the potential for more comprehensive evaluations of 3D immersive sketching, drawing, and painting, we present a survey of evaluation methodologies used in existing 3D sketching research, a breakdown and discussion of important phases (sub-tasks) in the 3D sketching process, and a framework that suggests how these factors can inform evaluation strategies in future 3D sketching research. Existing evaluations identified in the survey are organized and discussed within three high-level categories: 1) evaluating the 3D sketching activity, 2) evaluating 3D sketching tools, and 3) evaluating 3D sketching artifacts. The new framework suggests targeting evaluations to one or more of these categories and identifying relevant user populations. In addition, building upon the discussion of the different phases of the 3D sketching process, the framework suggests to evaluate relevant sketching tasks, which may range from low-level perception and hand movements to high-level conceptual design. Finally, we discuss limitations and challenges that arise when evaluating 3D sketching, including a lack of standardization of evaluation methods and multiple, potentially conflicting, ways to evaluate the same task and user interface usability; we also identify opportunities for more holistic evaluations. We hope the results can contribute to accelerating research in this domain and, ultimately, broad adoption of immersive sketching systems.
C1 [Machuca, Mayra Donaji Barrera] Dalhousie Univ, Halifax, NS B3H 4R2, Canada.
   [Israel, Johann Habakuk] HTW Berlin, D-10318 Berlin, Germany.
   [Keefe, Daniel F.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Stuerzlinger, Wolfgang] Simon Fraser Univ, Vancouver, BC V5A 1S6, Canada.
C3 Dalhousie University; University of Minnesota System; University of
   Minnesota Twin Cities; Simon Fraser University
RP Machuca, MDB (corresponding author), Dalhousie Univ, Halifax, NS B3H 4R2, Canada.
EM mbarrera@dal.ca; johannhabakuk.israel@htw-berlin.de; dfk@umn.edu;
   w.s@sfu.ca
OI Keefe, Daniel/0000-0002-7039-2340; Stuerzlinger,
   Wolfgang/0000-0002-7110-5024; Israel, Johann
   Habakuk/0000-0002-8513-6892; Barrera Machuca, Mayra
   Donaji/0000-0002-8057-7103
CR ACM, 2021, ACM digital library.
   An SG, 2017, AUTOMOTIVEUI 2017: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P183, DOI 10.1145/3122986.3123002
   [Anonymous], 2021, P CHI C HUM FACT COM
   Arora R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3459090
   Arora R, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173759
   Arora R, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5643, DOI 10.1145/3025453.3025474
   Auda J, 2021, LECT NOTES COMPUT SC, V12936, P195, DOI 10.1007/978-3-030-85607-6_14
   Bae SH, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P151
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Barnet Sylvan., 2015, A Short Guide to Writing About ART
   Barrera Machuca M.D, P CREAT COGN NEW YOR
   Baxter B, 2001, COMP GRAPH, P461, DOI 10.1145/383259.383313
   Boddien C, 2017, PROCEEDINGS OF THE 2017 ACM SYMPOSIUM ON DOCUMENT ENGINEERING (DOCENG 17), P101, DOI 10.1145/3103010.3121029
   Bohari U, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P177, DOI 10.1145/3172944.3172985
   Bruno M. L., 2002, P 14 C INT ING GRAF
   Buxton W., 2007, SketchingUserExperiences:GettingtheDesignRightandtheRight Design
   Cannavo Alberto, 2021, Interactivity and Game Creation. 9th EAI International Conference, ArtsIT 2020. Proceedings. Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering (LNICST 367), P291, DOI 10.1007/978-3-030-73426-8_17
   Chellali A.Chellali, P 5 JOINT VIRT REAL
   Cherry E, 2014, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2617588
   Chittenden T, 2018, J CONTEMP PAINTING, V4, P381, DOI 10.1386/jcp.4.2.381_1
   Cohen DJ, 1997, J EXP PSYCHOL HUMAN, V23, P609, DOI 10.1037/0096-1523.23.3.609
   Cordier F, 2011, IEEE T VIS COMPUT GR, V17, P1650, DOI 10.1109/TVCG.2010.258
   Dey S., 2003, J. Comput. Inf. Sci. Eng., V3, P302, DOI DOI 10.1145/781606.781627
   Donath D., 1996, Design Computation, Collaboration, Reasoning, Pedagogy. Proceedings of the ACADIA 1996 Conference, P201
   Dorta T, 2016, INT J ARCHIT COMPUT, V14, P87, DOI 10.1177/1478077116638921
   Drey J., 2020, P CHI C HUM FACT COM, P1
   Dudley JJ, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P241, DOI 10.1145/3196709.3196737
   Ekstromer J., 2018, Virtual reality sketchingfor design ideation, P9
   Elsayed H., 2020, P 26 ACM S VIRT REAL
   Elsevier, 2021, ScienceDirect.
   Eroglu S, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P475, DOI 10.1109/VR.2018.8446595
   Fehling P., inKultur und Informatik.
   Fiorentino M., 2005, 13 INT C CENTR EUR C, P131
   Fleisch T, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P275, DOI 10.1109/SMI.2004.1314514
   Fleury S, 2021, THINK SKILLS CREAT, V40, DOI 10.1016/j.tsc.2021.100828
   FRONT, 2007, Sketch furniture by front.
   Fuge M, 2012, COMPUT AIDED DESIGN, V44, P1020, DOI 10.1016/j.cad.2011.05.009
   Fuhrmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601163
   Gardner D., 2006, P 7 AUSTR US INT C, P177
   Giunchi D, 2021, PROCEEDINGS OF THE 2021 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE MEDIA EXPERIENCES, IMX 2021, P144, DOI 10.1145/3452918.3458806
   Giunchi S., 2019, P 17 INT C VIRT REAL
   Google, 2020, Tiltbrush.
   Google, 2021, Google scholar.
   GOULD JD, 1985, COMMUN ACM, V28, P300, DOI 10.1145/3166.3170
   Grey J, 2002, SIXTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P761, DOI 10.1109/IV.2002.1028866
   Grossman T., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P121, DOI 10.1145/503376.503398
   Hacker W, 1999, SPRACHE KOGNIT, V18, P88, DOI 10.1024//0253-4533.18.34.88
   Hagbi N, 2010, P IEEE VIRT REAL ANN, P91, DOI 10.1109/VR.2010.5444806
   Haggvik A., 2017, Ph.D. dissertation
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Hassenzahl Marc, 2003, Mensch & Computer, P187, DOI [DOI 10.1007/978-3-322-80058-9_19, 10.1007/978-3-322-80058-919]
   Herman LM, 2019, PROCEEDINGS OF THE 2019 ON CREATIVITY AND COGNITION - C&C 19, P612, DOI 10.1145/3325480.3326579
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Houzangbe S, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.958223
   Huang R., 2018, J. Comput. Inf. Sci. Eng., V18
   Huo K, 2017, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION (TEI'17), P37, DOI 10.1145/3024969.3024995
   IEEE, 2021, IEEEXplore
   Israel JH, 2009, COMPUT GRAPH-UK, V33, P462, DOI 10.1016/j.cag.2009.05.005
   Jackson B, 2016, IEEE T VIS COMPUT GR, V22, P1442, DOI 10.1109/TVCG.2016.2518099
   Jackson D., 2011, P SKETCH REC WORKSH
   Jetter R., 2020, P CHI C HUM FACT COM, P1
   Ji-Young Oh, 2006, Designing Interactive Systems. DIS2006, P80
   Johnson Seth., 2016, Proceedings of the 2016 ACM Companion on Interactive Surfaces and Spaces, P107
   Joundi J., 2020, Proceedings of the Design Society: DESIGN Conference, V1, P225, DOI 10.1017/dsd.2020.61
   Jung A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2749458
   Kang S., 2021, P 27 ACM S VIRT REAL
   Keefe D.F., 2001, P S INT 3D GRAPH NEW, P85
   Keefe DF, 2008, IEEE T VIS COMPUT GR, V14, P835, DOI 10.1109/TVCG.2008.31
   Keefe DF, 2007, IEEE T VIS COMPUT GR, V13, P1067, DOI 10.1109/TVCG.2007.1060
   Keefe DF, 2005, IEEE COMPUT GRAPH, V25, P18, DOI 10.1109/MCG.2005.34
   Keefe R. C., 2008, P IEEE S 3D US INT, P51
   Keeley D., 2018, Masters thesis
   Kennedy S., 2018, Refractory: J.Entertainment Media, V30
   Kim KH, 2006, CREATIVITY RES J, V18, P3, DOI 10.1207/s15326934crj1801_2
   Kim S.-G., 2018, P CHI C HUM FACT COM
   Kim Y, 2017, Arxiv, DOI arXiv:1704.02724
   Kim Y, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P797, DOI 10.1145/2984511.2984567
   Krs V, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073603
   Kwan H., 2019, P CHI C HUM FACT COM, P1
   Kwon J, 2010, IEICE T INF SYST, VE93D, P167, DOI 10.1587/transinf.E93.D.167
   Laing S, 2020, DESIGN STUD, V71, DOI 10.1016/j.destud.2020.100965
   Leal A., 2011, P GRAPHICS INTERFACE, P49
   Lee J.H, 2020, P CHI C HUM FACT COM, P1
   Leiva C., 2020, P CHI C HUM FACT COM, P1
   LEWIS JR, 1992, PROCEEDINGS OF THE HUMAN FACTORS SOCIETY, 36TH ANNUAL MEETING, VOLS 1 AND 2, P1259, DOI 10.1177/154193129203601617
   Lorusso M., 2021, Computer-Aided Design Applications, V19, P256, DOI [10.14733/cadaps.2022.256-268, DOI 10.14733/CADAPS.2022.256-268]
   Lorusso M., 2020, Comput.-Aided Des.Appl., V18, P383
   MacDonald C.M., 2013, Proc. ExtendedAbstr. Hum. Factors Comput. Syst, P1969
   Machuca MDB, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364254
   Machuca MDB, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P36, DOI 10.1145/3267782.3267786
   Makela W., 2005, P IEEE VR WORKSH, P70
   Maurya S, 2021, J ENG DESIGN, V32, P1, DOI 10.1080/09544828.2020.1851662
   Mc Kim R. H., 1980, Experiences in Visual Thinking
   McGraw E., 2017, P S SKETCH BAS INT M
   Merriam-Webster, 2020, Ideation.
   Mohanty RR, 2018, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2018, VOL 1B
   Mu M., 2022, P IEEE 28 INT C ENG, P1
   Mu M, 2022, MULTIMED TOOLS APPL, DOI 10.1007/s11042-022-13365-2
   Nam J, 2017, LEONARDO, V50, P94, DOI 10.1162/LEON_a_01226
   Norman D.A., 2002, DESIGN EVERYDAY THIN, DOI [10.1002/hfm.20127, DOI 10.1002/HFM.20127]
   Novotny J, 2019, IEEE T VIS COMPUT GR, V25, P2145, DOI 10.1109/TVCG.2019.2898796
   Oh W., 2005, Sketch-BasedInterfaces Model, P81
   Olivier P, 2019, COMPUT GRAPH-UK, V82, P203, DOI 10.1016/j.cag.2019.05.027
   Oti A, 2021, COMPUT GRAPH-UK, V94, P111, DOI 10.1016/j.cag.2020.10.007
   Page MJ, 2021, BMJ-BRIT MED J, V372, DOI [10.1136/bmj.n71, 10.1016/j.ijsu.2021.105906, 10.1136/bmj.n160]
   Pakanen M, 2017, 16TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2017), P393, DOI 10.1145/3152832.3156613
   Panda P., 2021, Proc. Creativity Cogn.
   Peng H, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174153
   Perkunder Helen., 2010, Proceedings of the Seventh Sketch-Based Interfaces and Modeling Symposium, SBIM '10, P127
   Prats M, 2009, DESIGN STUD, V30, P503, DOI 10.1016/j.destud.2009.04.002
   Qian J, 2021, PROCEEDINGS OF THE 2021 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2021), P205, DOI 10.1145/3461778.3462098
   Rasmussen J., Information Processing and Human-Machine Interaction:An Approach to Cognitive Engineering, P198
   rkmen R., 2022, P CHI C HUM FACT COM
   Romat H, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P306, DOI 10.1109/VR50410.2021.00053
   Rosales E, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480511
   Rosales E, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322970
   Rowe P. G., 1991, Design Thinking
   Rubin C.B., 2002, P ACM SIGGRAPH C ABS
   Saalfeld A., 2016, P EUR WROKSH VIS COM, P123, DOI [10.2312/vcbm.20161280, DOI 10.2312/VCBM.20161280]
   Satter K, 2015, J COMPUT INF SCI ENG, V15, DOI 10.1115/1.4029750
   Schkolne S., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P261, DOI 10.1145/365024.365114
   Schon D. A., 1983, REFLECTIVE PRACTITIO
   Schott M, 2009, COMPUT GRAPH FORUM, V28, P855, DOI 10.1111/j.1467-8659.2009.01464.x
   Schubert G., 2012, P 32 ANN C ASS COMP, P409
   Schurch R., 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P409, DOI 10.1109/IST.2012.6295555
   Seichter H., 2003, P 8 INT C COMP AID A, P209
   Semmo Amir., 2017, Proceedings of International Symposium on Non-Photorealistic Animation and Rendering, V5, P1, DOI DOI 10.1145/3092919.3092920
   Seo M., 2018, P C HUM FACT COMP SY, P1
   Seybold Carsten, 2020, Series Title: IFIP Advances in Information and Communication Technology, V594, P297
   Shankar S, 2017, AI EDAM, V31, P376, DOI 10.1017/S0890060416000512
   Sketch G., 2020, Gravity sketch.
   Snibbe S. Anderson, 1998, P 3 PHANT US GROUP
   Stadler H., 2020, P DES SOC INT C ENG, P1375
   Sung-A Jang, 2014, Distributed, Ambient, and Pervasive Interactions. Second International Conference (DAPI 2014) Held as Part of HCI International 2014. Proceedings: LNCS 8530, P130, DOI 10.1007/978-3-319-07788-8_13
   Tano S, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P245, DOI 10.1109/ISM.2014.54
   Tchalenko J, 2009, VISION RES, V49, P791, DOI 10.1016/j.visres.2009.02.012
   Thoravi Kumaravel B., P CHI C HUM FACT COM, P1
   Tversky B., 2002, 2002 AAAI SPRING S S, P148
   van der Horst S, 2021, PROCEEDINGS OF THE FIFTEENTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION, TEI 2021, DOI 10.1145/3430524.3446068
   Van Goethem S., 2020, Advances in Usability, User Experience, Wearable and As-sistive Technology, P69
   Van Goethem Sander, 2021, PROCEED DES SOC, V1, P3339, DOI [10.1017/pds.2021.595, DOI 10.1017/PDS.2021.59]
   Vermeeren A.P., 2010, P 6 NORD C HUM COMP, P521, DOI [10.1145/1868914.1868973, DOI 10.1145/1868914.1868973]
   Vistisen P., 2019, P 7 ECAADE REG INT S, P25
   Wacker P, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P25, DOI 10.1145/3267782.3267788
   Wacker P, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300849
   Wang SX, 2021, INT SYM MIX AUGMENT, P362, DOI 10.1109/ISMAR-Adjunct54149.2021.00082
   Wang SX, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P156, DOI 10.1109/ISMAR-Adjunct.2019.00-58
   Wesche Gerold, 2001, P ACM S VIRT REAL SO, P167, DOI DOI 10.1145/505008.505041
   Wiese J. H., 2010, P 7 SKETCH BAS INT, P135
   Xin Min, 2008, P 2008 ACM S VIRT RE, P223, DOI DOI 10.1145/1450579.1450627
   Xu HN, 2022, IEEE T HUM-MACH SYST, V52, P1352, DOI 10.1109/THMS.2022.3186592
   Xu PF, 2019, IEEE T VIS COMPUT GR, V25, P2927, DOI 10.1109/TVCG.2018.2860016
   Xu WJ, 2018, LECT NOTES COMPUT SC, V10922, P240, DOI 10.1007/978-3-319-91131-1_19
   Xue Yu, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P372, DOI 10.1145/3472749.3474756
   Yang EK, 2020, DIGIT CREAT, V31, P82, DOI 10.1080/14626268.2020.1726964
   Yang XZ, 2018, ETR&D-EDUC TECH RES, V66, P1231, DOI 10.1007/s11423-018-9604-z
   Ye H, 2022, IEEE T VIS COMPUT GR, V28, P2809, DOI 10.1109/TVCG.2020.3049006
   Yee Y., 2009, P INT UI WORKSH SKET, P1
   Yilmaz S., 2008, P KOR JPN DES ENG WO
   Yu R., 2021, P CHI C HUM FACT COM
   Yuan Q, 2021, COMPUT GRAPH-UK, V94, P132, DOI 10.1016/j.cag.2020.12.001
   Yue YT, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P3693, DOI 10.1145/3025453.3025792
   Zhilyaeva A., 2022, annadreambrush.
   Zhu XQ, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1764
   Zou QY, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P968, DOI 10.1109/VRW55335.2022.00335
NR 166
TC 6
Z9 6
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4648
EP 4664
DI 10.1109/TVCG.2023.3276291
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400085
PM 37186537
DA 2024-11-06
ER

PT J
AU Hashemian, AM
   Adhikari, A
   Aguilar, IA
   Kruijff, E
   von der Heyde, M
   Riecke, BE
AF Hashemian, Abraham M.
   Adhikari, Ashu
   Aguilar, Ivan A.
   Kruijff, Ernst
   von der Heyde, Markus
   Riecke, Bernhard E.
TI Leaning-Based Interfaces Improve Simultaneous Locomotion and Object
   Interaction in VR Compared to the Handheld Controller
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D user interface; dual task; continuous interaction; motion sickness;
   cybersickness; locomotion; travel techniques; virtual reality
ID VIRTUAL ENVIRONMENTS; PERFORMANCE; ORIENTATION; TRAVEL; GAMES
AB Physical walking is often considered the gold standard for VR travel whenever feasible. However, limited free-space walking areas in the real-world do not allow exploring larger-scale virtual environments by actual walking. Therefore, users often require handheld controllers for navigation, which can reduce believability, interfere with simultaneous interaction tasks, and exacerbate adverse effects such as motion sickness and disorientation. To investigate alternative locomotion options, we compared handheld Controller (thumbstick-based) and physical walking versus a seated (HeadJoystick) and standing/stepping (NaviBoard) leaning-based locomotion interface, where seated/standing users travel by moving their head toward the target direction. Rotations were always physically performed. To compare these interfaces, we designed a novel simultaneous locomotion and object interaction task, where users needed to keep touching the center of upward moving target balloons with their virtual lightsaber, while simultaneously staying inside a horizontally moving enclosure. Walking resulted in the best locomotion, interaction, and combined performances while the controller performed worst. Leaning-based interfaces improved user experience and performance compared to Controller, especially when standing/stepping using NaviBoard, but did not reach walking performance. That is, leaning-based interfaces HeadJoystick (sitting) and NaviBoard (standing) that provided additional physical self-motion cues compared to controller improved enjoyment, preference, spatial presence, vection intensity, motion sickness, as well as performance for locomotion, object interaction, and combined locomotion and object interaction. Our results also showed that less embodied interfaces (and in particular the controller) caused a more pronounced performance deterioration when increasing locomotion speed. Moreover, observed differences between our interfaces were not affected by repeated interface usage.
C1 [Hashemian, Abraham M.; Adhikari, Ashu; Aguilar, Ivan A.; Kruijff, Ernst; von der Heyde, Markus; Riecke, Bernhard E.] Simon Fraser Univ, Sch Interact Arts & Technol, Burnaby, BC V5A 1S6, Canada.
   [Kruijff, Ernst] Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, D-53757 St Augustin, Germany.
   [von der Heyde, Markus] Simon Fraser Univ, VdH IT, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University; Hochschule Bonn Rhein Sieg; Simon Fraser
   University
RP Hashemian, AM (corresponding author), Simon Fraser Univ, Sch Interact Arts & Technol, Burnaby, BC V5A 1S6, Canada.
EM hashemia@sfu.ca; ashua@sfu.ca; ivan_aguilar@sfu.ca;
   ernst.kruijff@h-brs.de; info@vdh-it.de; ber1@sfu.ca
RI Aguilar, Ivan/AAO-4316-2020; von der Heyde, Markus/HJA-0319-2022;
   Riecke, Bernhard/C-6399-2011
OI Hashemian, Abraham M./0000-0001-8385-4332; Riecke,
   Bernhard/0000-0001-7974-0850; Kruijff, Ernst/0000-0003-1625-0955; von
   der Heyde, Markus/0000-0002-6026-082X; Aguilar,
   Ivan/0000-0002-8735-4041; Adhikari, Ashu/0000-0002-2540-6344
FU NSERC [R611547]
FX This work was supported by NSERC under Grant R611547.
CR Adhikari A, 2023, IEEE T VIS COMPUT GR, V29, P5265, DOI 10.1109/TVCG.2022.3207157
   Adhikari A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.730334
   Badcock DR, 2015, HUM FACTORS ERGON, P39
   Beckhaus S., 2005, Proc. Comput. Sci. Magic, P1
   Beckhaus Steffi., 2005, New Directions in 3D User Interfaces Workshop of IEEE VR, P57
   Bowman D. A., 1998, Virtual Reality, V3, P120, DOI 10.1007/BF01417673
   Bowman D. A., 1999, Ph.D. dissertation
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Bowman DA, 1999, PRESENCE-TELEOP VIRT, V8, P618, DOI 10.1162/105474699566521
   Brument H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P680, DOI [10.1109/VR.2019.8797721, 10.1109/vr.2019.8797721]
   Buttussi F, 2021, IEEE T VIS COMPUT GR, V27, P125, DOI 10.1109/TVCG.2019.2928304
   Cherni H., 2020, International Journal of Virtual Reality, V20, P1, DOI [DOI 10.20870/IJVR.2020.20.1.3183, 10.20870/IJVR.2020.20.1.3183, 10.20870/ijvr.2020.20.1, DOI 10.20870/IJVR.2020.20.1]
   Chester MR, 2002, INT J IND ERGONOM, V29, P289, DOI 10.1016/S0169-8141(01)00069-5
   Cunningham DW, 2001, J VISION, V1, P88, DOI 10.1167/1.2.3
   de Winter JCF, 2009, ERGONOMICS, V52, P137, DOI 10.1080/00140130802277521
   Farrell MJ, 1998, J EXP PSYCHOL LEARN, V24, P227, DOI 10.1037/0278-7393.24.1.227
   Field A., 2013, Discovering statistics using IBM SPSS statistics, V4th ed.
   Freiberg J., 2015, Master's thesis,
   Games B., 2019, Beat saber
   Griffin NN, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY 2018), P211, DOI 10.1145/3242671.3242707
   Ha C, 2015, IEEE INT CONF ROBOT, P164, DOI 10.1109/ICRA.2015.7138995
   Harris A., 2014, P 13 ACM SIGGRAPH IN, P231
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Hashemian A. M., 2017, P SOFTW AUGM VIRT RE, P1
   Hashemian AM, 2022, IEEE T VIS COMPUT GR, V28, P1792, DOI 10.1109/TVCG.2020.3025084
   Hashemian AM, 2017, LECT NOTES COMPUT SC, V10280, P15, DOI 10.1007/978-3-319-57987-0_2
   Hashemian AM, 2023, IEEE T VIS COMPUT GR, V29, P1748, DOI 10.1109/TVCG.2021.3131422
   Jerald J., 2016, VR BOOK HUMAN CENTER, VFirst
   Jong-Won Yoon, 2010, 2010 IEEE Information Theory Workshop (ITW 2010), P69, DOI 10.1109/ITW.2010.5593369
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kitson A, 2017, IEEE SYMP 3D USER, P73, DOI 10.1109/3DUI.2017.7893320
   Klatzky RL, 1998, PSYCHOL SCI, V9, P293, DOI 10.1111/1467-9280.00058
   Krompiec P, 2019, IEEE ACCESS, V7, P124548, DOI 10.1109/ACCESS.2019.2937937
   Kruijff E., 2015, P 3 ACM S SPAT US IN, P103, DOI 10.1145/2788940.2788943
   Kruijff E, 2016, SUI'16: PROCEEDINGS OF THE 2016 SYMPOSIUM ON SPATIAL USER INTERACTION, P149, DOI 10.1145/2983310.2985759
   Langbehn E., 2015, P GI WORKSH VIRT AUG, P149
   LaViola JosephJ., 2001, Proceedings Symposium on Interactive 3D Graphics, P9
   LaViola Jr J. J., 2017, 3D user interfaces: theory and practice
   Le Renard Marc, 2013, P ACM INT WORKSH IMM, P7, DOI [10.1145/2512142, DOI 10.1145/2512142]
   Marchal M, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P19, DOI 10.1109/3DUI.2010.5446238
   Martel E., 2015, P INT C FDN DIG GAM, P1
   Martel E, 2017, ENTERTAIN COMPUT, V21, P19, DOI 10.1016/j.entcom.2017.04.004
   Mayor J, 2021, IEEE T EMERG TOP COM, V9, P1542, DOI 10.1109/TETC.2019.2915287
   McMahan R. P., 2011, Exploring the effects of higher-fidelity display and interaction for virtual reality games
   McMahan RP, 2015, HUM FACTORS ERGON, P285
   McMahan RP, 2012, IEEE T VIS COMPUT GR, V18, P626, DOI 10.1109/TVCG.2012.43
   Merhi O, 2007, HUM FACTORS, V49, P920, DOI 10.1518/001872007X230262
   Müller T, 2019, NEUROPSYCHOLOGIA, V123, P141, DOI 10.1016/j.neuropsychologia.2018.04.030
   Natapov D., 2009, Proceedings of Graphics Interface 2009, P223
   Nguyen-Vo T, 2021, IEEE T VIS COMPUT GR, V27, P165, DOI 10.1109/TVCG.2019.2935730
   PRESSON CC, 1994, PERCEPTION, V23, P1447, DOI 10.1068/p231447
   Prithul A., 2021, Proc. Graph. Interface, P1
   RICCIO G E, 1991, Ecological Psychology, V3, P195, DOI 10.1207/s15326969eco0303_2
   Riecke B. E., 2015, P 3 S SPAT US INT LO, P123, DOI DOI 10.1145/2788940.2788956
   Riecke B.E., 2006, P ACM S VIRTUAL REAL, P104, DOI [10.1145/1180495.1180517, DOI 10.1145/1180495.1180517, 10.1145/ 1180495.1180517]
   Riecke BE, 2010, LECT NOTES ARTIF INT, V6222, P234, DOI 10.1007/978-3-642-14749-4_21
   Riecke BE, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P147
   Riecke BernhardE., 2012, P ACM S APPL PERCEPT, P17, DOI [10.1145/2338676.2338680, DOI 10.1145/2338676.2338680]
   RIESER JJ, 1989, J EXP PSYCHOL LEARN, V15, P1157, DOI 10.1037/0278-7393.15.6.1157
   Rogers K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300644
   Rognon C, 2018, IEEE ROBOT AUTOM LET, V3, P2362, DOI 10.1109/LRA.2018.2810955
   Ruddle R.A., 2013, HUMAN WALKING VIRTUA, P99, DOI [10.1007/978-1-4419-8432-6_5, DOI 10.1007/978-1-4419-8432-6_5]
   Ruddle RA, 1999, PRESENCE-TELEOP VIRT, V8, P157, DOI 10.1162/105474699566143
   Sait MSMY, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON INTERACTIVE AND SPATIAL COMPUTING (IWISC 18), P64, DOI 10.1145/3191801.3191814
   Schmider E, 2010, METHODOLOGY-EUR, V6, P147, DOI 10.1027/1614-2241/a000016
   Sigurdarson S., 2014, Master's thesis
   Sigurdarson S, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P31, DOI 10.1109/VR.2012.6180874
   Slater M, 1998, HUM FACTORS, V40, P469, DOI 10.1518/001872098779591368
   Steinicke F, 2013, Human Walking in Virtual Environments
   Tedjokusumo J, 2010, IEEE T SYST MAN CY A, V40, P147, DOI 10.1109/TSMCA.2009.2028432
   van Mier H, 2006, HUM MOVEMENT SCI, V25, P657, DOI 10.1016/j.humov.2006.06.004
   Wells M., 1996, P IEEE C VIRT REAL 3, V97, P1
   Wiedemann D., 2020, SCIFI IT 2020 4 INT, P49
   Wilson G, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173673
   Zhai SM, 2004, PRESENCE-TELEOP VIRT, V13, P113, DOI 10.1162/1054746041382393
   Zielasko D, 2021, COMPUTERS, V10, DOI 10.3390/computers10060073
   Zielasko D, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P297, DOI [10.1109/VRW50115.2020.0-209, 10.1109/VRW50115.2020.00067]
   Zielasko D, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P398, DOI [10.1109/VR46266.2020.1581426770550, 10.1109/VR46266.2020.00-44]
   Zielasko D, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P113, DOI 10.1109/3DUI.2016.7460040
NR 79
TC 3
Z9 3
U1 7
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4665
EP 4682
DI 10.1109/TVCG.2023.3275111
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400081
PM 37200130
DA 2024-11-06
ER

PT J
AU Feng, YF
   Shen, LY
   Li, X
   Yuan, CM
   Jiang, X
AF Feng, Yi-Fei
   Shen, Li-Yong
   Li, Xin
   Yuan, Chun-Ming
   Jiang, Xing
TI Patching Non-Uniform Extraordinary Points
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE NURBS; T-splines; extraordinary points; capping
ID SUBDIVISION SURFACES; ISOGEOMETRIC ANALYSIS; POLYNOMIAL SPLINES; NURBS;
   CAD
AB Smooth surfaces from an arbitrary topological control grid have been widely studied, which are mostly generalized from splines with uniform knot intervals. These methods fail to work well on extraordinary points (EPs) whose edges have varying knot intervals. This article presents a patching solution for arbitrary topological 2-manifold control grid with non-uniform knots that defines one bi-cubic Bezier patch per control grid face except those faces with EPs. Experimental results demonstrate that the new solution can improve the surface quality for non-uniform parameterization. Applications in surface reconstruction, arbitrary sharp features on the complex surface and tool path planning for the new surface representation are also provided in the paper.
C1 [Feng, Yi-Fei] UCAS, Beijing 101408, Peoples R China.
   [Shen, Li-Yong; Li, Xin] UCAS, Beijing 230026, Anhui, Peoples R China.
   Univ Sci & Technol China, AMSS, Hefei 100045, Peoples R China.
   [Yuan, Chun-Ming] AMSS CAS, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Academy of Mathematics & System
   Sciences, CAS
RP Li, X (corresponding author), UCAS, Beijing 230026, Anhui, Peoples R China.
EM fengyifei15@mails.ucas.ac.cn; lyshen@ucas.ac.cn; lixustc@ustc.edu.cn;
   cmyuan@mmrc.iss.ac.cn; jiangxin@buaa.edu.cn
RI Shen, Li-Yong/P-9693-2015; Li, Xin/B-1324-2012
OI Shen, Li-Yong/0000-0001-5769-4814; Li, Xin/0000-0003-0477-7098; ?,
   ??/0000-0003-3349-533X
FU National Key Research and Development Program of China [2020YFA0713703];
   Beijing Natural Science Foundation [Z190004]; NSFC [61872332]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFA0713703, in part by
   Beijing Natural Science Foundation under Grant Z190004, and in part by
   NSFC under Grant 61872332.
CR Biermann H, 2000, COMP GRAPH, P113, DOI 10.1145/344779.344841
   Cashman TJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531352
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Deng JS, 2008, GRAPH MODELS, V70, P76, DOI 10.1016/j.gmod.2008.03.001
   DeRose T., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P85, DOI 10.1145/280814.280826
   Dokken T, 2013, COMPUT AIDED GEOM D, V30, P331, DOI 10.1016/j.cagd.2012.12.005
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   Farin G. E., 2002, NURBS Curves and Surfaces: From Projective Geometry toPractical Use
   Forsey D. R., 1988, Computer Graphics, V22, P205, DOI 10.1145/378456.378512
   Huang JW, 2018, COMPUT GRAPH FORUM, V37, P147, DOI 10.1111/cgf.13498
   Hughes TJR, 2005, COMPUT METHOD APPL M, V194, P4135, DOI 10.1016/j.cma.2004.10.008
   Jakob W, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818078
   Karciauskas K, 2022, COMPUT AIDED DESIGN, V150, DOI 10.1016/j.cad.2022.103310
   Karciauskas K, 2022, COMPUT GRAPH-UK, V102, P370, DOI 10.1016/j.cag.2021.10.008
   Karciauskas K, 2021, COMPUT AIDED GEOM D, V86, DOI 10.1016/j.cagd.2021.101978
   Karciauskas K, 2020, COMPUT AIDED GEOM D, V83, DOI 10.1016/j.cagd.2020.101934
   Karciauskas K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3136954
   Kosinka J, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12258
   Kovacs D, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766972
   Kruskal J. B., 1956, Proceedings of the American Mathematical Society, V7, P48, DOI [10.1090/S0002-9939-1956-0078686-7, DOI 10.1090/S0002-9939-1956-0078686-7, http://dx.doi.org/10.1090/S0002-9939-1956-0078686-7]
   Lai YK, 2006, COMPUT AIDED DESIGN, V38, P800, DOI 10.1016/j.cad.2006.04.007
   Li XF, 2016, ACTA OCEANOL SIN, V35, P1
   Li X, 2019, COMPUT METHOD APPL M, V352, P606, DOI 10.1016/j.cma.2019.04.036
   Li X, 2012, COMPUT AIDED GEOM D, V29, P63, DOI 10.1016/j.cagd.2011.08.005
   Ma WY, 2005, COMPUT AIDED DESIGN, V37, P693, DOI 10.1016/j.cad.2004.08.008
   Majeed M, 2017, COMPUT METHOD APPL M, V316, P547, DOI 10.1016/j.cma.2016.08.013
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   Müller K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805969
   Müller K, 2006, ACM T GRAPHIC, V25, P268, DOI 10.1145/1138450.1138455
   Peters J, 2000, COMP GRAPH, P255, DOI 10.1145/344779.344908
   Peters J., 2019, SMAI J. Comput. Math., VS5, P183
   Peters U., 2008, Subdivision Methods for Geometric Design
   Pottmann H, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P8, DOI 10.1109/PCCGA.2002.1167835
   Scott MA, 2013, COMPUT METHOD APPL M, V254, P197, DOI 10.1016/j.cma.2012.11.001
   Sederberg T. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P387, DOI 10.1145/280814.280942
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Sederberg TW, 2004, ACM T GRAPHIC, V23, P276, DOI 10.1145/1015706.1015715
   Shi XQ, 2004, COMPUT AIDED DESIGN, V36, P413, DOI 10.1016/S0010-4485(03)00111-8
   Tian YF, 2020, COMPUT GRAPH FORUM, V39, P232, DOI 10.1111/cgf.14014
   Toshniwal D, 2017, COMPUT METHOD APPL M, V327, P411, DOI 10.1016/j.cma.2017.06.008
   Wei XD, 2021, INT J NUMER METH ENG, V122, P2117, DOI 10.1002/nme.6608
   Wei XD, 2015, COMPUT METHOD APPL M, V291, P1, DOI 10.1016/j.cma.2015.03.019
   Zhao HS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201338
NR 44
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4683
EP 4693
DI 10.1109/TVCG.2023.3271669
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400024
PM 37126614
DA 2024-11-06
ER

PT J
AU Li, J
   Gao, Y
   Dai, J
   Li, S
   Hao, AM
   Qin, H
AF Li, Jin
   Gao, Yang
   Dai, Ju
   Li, Shuai
   Hao, Aimin
   Qin, Hong
TI MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Numerical models; Solid modeling; Computational modeling; Solids; Neural
   networks; Couplings; Mathematical models; Data-driven simulation; neural
   networks; physics-based simulation; fluid-solid interaction
ID EULER
AB High-accuracy, high-efficiency physics-based fluid-solid interaction is essential for reality modeling and computer animation in online games or real-time Virtual Reality (VR) systems. However, the large-scale simulation of incompressible fluid and its interaction with the surrounding solid environment is either time-consuming or suffering from the reduced time/space resolution due to the complicated iterative nature pertinent to numerical computations of involved Partial Differential Equations (PDEs). In recent years, we have witnessed significant growth in exploring a different, alternative data-driven approach to addressing some of the existing technical challenges in conventional model-centric graphics and animation methods. This article showcases some of our exploratory efforts in this direction. One technical concern of our research is to address the central key challenge of how to best construct the numerical solver effectively and how to best integrate spatiotemporal/dimensional neural networks with the available MPM's pressure solvers. In particular, we devise the MPMNet, a hybrid data-driven framework supporting the popular and powerful MPM, to combine the comprehensive properties of MPM in numerically handling physical behaviors ranging from fluid to deformable solids and the high efficiency of data-driven models. At the architectural level, our MPMNet comprises three primary components: A data processing module to describe the physical properties by way of the input fields; A deep neural network group to learn the spatiotemporal features; And an iterative refinement process to continue to reduce possible numerical errors. The goal of these special technical developments is to aim at involved numerical acceleration while preserving physical accuracy, realizing efficient and accurate fluid-solid interactions in a data-driven fashion. The extensive experimental results verify that our MPMNet can tremendously speed up the computation compared with the popular numerical methods as the complexity of interaction scenes increases while better retaining the numerical accuracy.
C1 [Li, Jin; Gao, Yang; Li, Shuai] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Dai, Ju; Hao, Aimin] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Hao, Aimin] Chinese Acad Med Sci, Res Unit Virtual Body & Virtual Surg 2019RU004, Beijing 100050, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Peng Cheng Laboratory; Chinese Academy of Medical
   Sciences - Peking Union Medical College; State University of New York
   (SUNY) System; Stony Brook University
RP Hao, AM (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.; Hao, AM (corresponding author), Chinese Acad Med Sci, Res Unit Virtual Body & Virtual Surg 2019RU004, Beijing 100050, Peoples R China.; Qin, H (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM lijin2019@buaa.edu.cn; gaoyangvr@buaa.edu.cn; daij@pcl.ac.cn;
   lishuaiouc@126.com; ham@buaa.edu.cn; qin@cs.stonybrook.edu
RI Gao, Yang/JQV-9627-2023; Zhao, Mingyu/HHS-0141-2022
OI Li, Jin/0009-0009-3097-8193; Gao, Yang/0000-0002-9149-3554; QIN,
   HONG/0000-0001-7699-1355; Dai, Ju/0000-0002-9397-8539
CR Barrett R., 1994, Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, DOI DOI 10.1137/1.9781611971538.CH2
   Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446
   Cao Y, 2022, Arxiv, DOI [arXiv:2210.02573, 10.48550/arXiv.2210.02573, 10.48550/arxiv.2210.02573, DOI 10.48550/ARXIV.2210.02573]
   Chen LW, 2023, COMPUT FLUIDS, V250, DOI 10.1016/j.compfluid.2022.105707
   Chentanez N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964977
   Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643
   Daviet G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925877
   Demidov D, 2019, LOBACHEVSKII J MATH, V40, P535, DOI 10.1134/S1995080219050056
   Fang Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392438
   Fang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322968
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Fei Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201392
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Gagniere S, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.14096
   Gao Y, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095151
   Gao Y, 2021, IEEE T VIS COMPUT GR, V27, P4483, DOI 10.1109/TVCG.2021.3107597
   Gao Y, 2020, COMPUT GRAPH FORUM, V39, P180, DOI 10.1111/cgf.14010
   Gao Y, 2019, VISUAL COMPUT, V35, P1741, DOI 10.1007/s00371-018-1569-8
   Gissler C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3284980
   Golub G.H., 2013, Matrix Computations, V4th, DOI 10.56021/9781421407944
   Guo Q, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201346
   Hochreiter S., 2001, Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, DOI [DOI 10.1109/9780470544037.CH14, 10.1109/9780470544037.ch14]
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201293
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Jiang C., 2016, ACM SIGGRAPH 2016 CO, P1
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Kim B, 2019, COMPUT GRAPH FORUM, V38, P59, DOI 10.1111/cgf.13619
   Kingma D.P., 2014, P INT C LEARNING REP
   Koschier D, 2022, COMPUT GRAPH FORUM, V41, P737, DOI 10.1111/cgf.14508
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Li C, 2021, IEEE T VIS COMPUT GR, V27, P3867, DOI 10.1109/TVCG.2020.2991217
   Li X., 2021, Comput. Methods Appl. Mechanics Eng, V390
   Li Y., 2020, International Conference on Machine Learning, P5927
   Li Yunzhu, 2018, P INT C LEARN REPR
   Peng C, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1901
   Prantl L., 2022, P C NEUR INF PROC SY, P1
   Ram D., 2015, ACM SIGGRAPH / Eurographics Symposium on Computer Animation, P157
   RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623
   Robinson-Mosher A, 2011, J COMPUT PHYS, V230, P1547, DOI 10.1016/j.jcp.2010.11.021
   Sanchez-Gonzalez A., 2020, PMLR, P8459
   Stam J., 2003, P GAM DEV C, VVolume 18, P25
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Su HZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459820
   SULSKY D, 1994, COMPUT METHOD APPL M, V118, P179, DOI 10.1016/0045-7825(94)00033-6
   SULSKY D, 1995, COMPUT PHYS COMMUN, V87, P236, DOI 10.1016/0010-4655(94)00170-7
   Takahashi T, 2021, AAAI CONF ARTIF INTE, V35, P6138
   Tang JW, 2022, COMPUT GRAPH-UK, V107, P186, DOI 10.1016/j.cag.2022.07.016
   Teng Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980229
   Tompson J, 2017, PR MACH LEARN RES, V70
   Tumanov E, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451261
   Um K, 2020, ADV NEUR IN, V33
   Um K, 2018, COMPUT GRAPH FORUM, V37, P171, DOI 10.1111/cgf.13522
   Ummenhofer Benjamin, 2019, INT C LEARN REPR
   Wang S, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3340259
   Wiewel S, 2020, COMPUT GRAPH FORUM, V39, P15, DOI 10.1111/cgf.14097
   Wiewel S, 2019, COMPUT GRAPH FORUM, V38, P71, DOI 10.1111/cgf.13620
   Wolper J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392428
   Wolper J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322949
   Xiao XY, 2019, COMPUT GRAPH FORUM, V38, P431, DOI 10.1111/cgf.13649
   Xiao XY, 2020, IEEE T VIS COMPUT GR, V26, P1454, DOI 10.1109/TVCG.2018.2873375
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Yang C, 2016, COMPUT ANIMAT VIRT W, V27, P415, DOI 10.1002/cav.1695
   YOON S, 1988, AIAA J, V26, P1025, DOI 10.2514/3.10007
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 64
TC 0
Z9 0
U1 3
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4694
EP 4708
DI 10.1109/TVCG.2023.3272156
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400047
PM 37126612
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Lin, ZH
   Hu, CX
   Jia, JZ
   Li, S
AF Lin, Zehui
   Hu, Chenxiao
   Jia, Jinzhu
   Li, Sheng
TI Hypothesis Testing for Progressive Kernel Estimation and VCM Framework
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Photonics; Kernel; Estimation; Testing; Analysis of variance; Lighting;
   Merging; bidirectional path sampling; f-test; hypothesis testing; kernel
   estimation; progressive photon mapping; radius; statistical model;
   vertex connection and merging
ID STILL
AB Identifying an appropriate radius for unbiased kernel estimation is crucial for the efficiency of radiance estimation. However, determining both the radius and unbiasedness still faces big challenges. In this paper, we first propose a statistical model of photon samples and associated contributions for progressive kernel estimation, under which the kernel estimation is unbiased if the null hypothesis of this statistical model stands. Then, we present a method to decide whether to reject the null hypothesis about the statistical population (i.e., photon samples) by the F-test in the Analysis of Variance. Hereby, we implement a progressive photon mapping (PPM) algorithm, wherein the kernel radius is determined by this hypothesis test for unbiased radiance estimation. Second, we propose VCM+, a reinforcement of Vertex Connection and Merging (VCM), and derive its theoretically unbiased formulation. VCM+ combines hypothesis testing-based PPM with bidirectional path tracing (BDPT) via multiple importance sampling (MIS), wherein our kernel radius can leverage the contributions from PPM and BDPT. We test our new algorithms, improved PPM and VCM+, on diverse scenarios with different lighting settings. The experimental results demonstrate that our method can alleviate light leaks and visual blur artifacts of prior radiance estimate algorithms. We also evaluate the asymptotic performance of our approach and observe an overall improvement over the baseline in all testing scenarios.
C1 [Lin, Zehui; Hu, Chenxiao; Li, Sheng] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Li, Sheng] Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
   [Jia, Jinzhu] Peking Univ, Dept Biostat, Beijing, Peoples R China.
   [Jia, Jinzhu] Peking Univ, Ctr Stat Sci, Beijing, Peoples R China.
C3 Peking University; Peking University; Peking University; Peking
   University
RP Li, S (corresponding author), Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
EM zehui@pku.edu.cn; hinevenwob@qq.com; jzjia@math.pku.edu.cn;
   lisheng@pku.edu.cn
RI chen, haoran/HJG-8589-2022
OI Hu, Chenxiao/0009-0000-3306-6653; Li, Sheng/0000-0002-8901-2184; Jia,
   Jinzhu/0000-0002-6554-2463
FU National Key Research and Development Program of China [2022ZD0160805];
   NSFC of China [62172013]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2022ZD0160805, and in par tby
   NSFC of China under Grant 62172013.
CR Bathke A, 2004, J STAT PLAN INFER, V126, P413, DOI 10.1016/j.jspi.2003.09.010
   Blanca MJ, 2017, PSICOTHEMA, V29, P552, DOI 10.7334/psicothema2016.383
   Calder K., 1953, Statistical inference
   COCHRAN WG, 1952, ANN MATH STAT, V23, P315, DOI 10.1214/aoms/1177729380
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Everitt B, 1998, The Cambridge dictionary of statistics
   FEIRWALS.BJ, 1974, EDUC PSYCHOL MEAS, V34, P789, DOI 10.1177/001316447403400406
   Fisher R., 1921, METRO, V1, P3
   Georgiev I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366211
   Grittmann P, 2021, COMPUT GRAPH FORUM, V40, P231, DOI 10.1111/cgf.142628
   Hachisuka T., 2012, P SIGGRAPH AS COURS
   Hachisuka T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366210
   Hachisuka T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019633
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Hachisuka T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866170
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Jakob Wenzel, 2010, Mitsuba Renderer
   Jan SL, 2014, BRIT J MATH STAT PSY, V67, P72, DOI 10.1111/bmsp.12006
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kaplanyan AS, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451242
   Knaus C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966404
   Kondapaneni I, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323009
   KRUSKAL WH, 1952, J AM STAT ASSOC, V47, P583, DOI 10.1080/01621459.1952.10483441
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Lin ZH, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417822
   Miller Jr R. G., 1997, ANOVA Beyond Basics of Applied Statistics
   Moore D.S., 2016, Introduction to the Practice of Statistics, V9th
   Müller T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3341156
   Müller T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417804
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Qin H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818119
   Rigau J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P260
   Sik M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982411
   Su FJ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530183
   Veach E., 1995, Photorealistic Rendering Techniques, P145
   Veach Eric, 1997, Ph. D. Dissertation, V1610
   Vorba J., 2011, P CENTR EUR SEM COMP, P25
   Vorba J, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601203, 10.1145/2801097.2801203]
   Zhu SL, 2020, COMPUT GRAPH FORUM, V39, P35, DOI 10.1111/cgf.14052
NR 40
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4709
EP 4723
DI 10.1109/TVCG.2023.3274595
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400010
PM 37159325
DA 2024-11-06
ER

PT J
AU Tehrani, MA
   Ibrahim, MT
   Majumder, A
   Gopi, M
AF Tehrani, Mahdi Abbaspour
   Ibrahim, Muhammad Twaha
   Majumder, Aditi
   Gopi, M.
TI 3D Gamut Morphing for Non-Rectangular Multi-Projector Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Three-dimensional displays; Real-time systems;
   Transfer functions; Geometry; Dynamic range; Brightness;
   Color-correction; gamut morphing; multi-projector displays; photometric
   correction
ID SEAMLESSNESS
AB In a spatially augmented reality system, multiple projectors are tiled on a complex shaped surface to create a seamless display on it. This has several applications in visualization, gaming, education and entertainment. The main challenges in creating seamless and undistorted imagery on such complex shaped surfaces are geometric registration and color correction. Prior methods that provide solutions for the spatial color variation in multi-projector displays assume rectangular overlap regions across the projectors that is possible only on flat surfaces with extremely constrained projector placement. In this article, we present a novel and fully automated method for removing color variations in a multi-projector display on arbitrary shaped smooth surfaces using a general color gamut morphing algorithm that can handle any arbitrarily shaped overlap between the projectors and assures imperceptible color variations across the display surface.
C1 [Tehrani, Mahdi Abbaspour] Genentech Inc, South San Francisco, CA 94080 USA.
   [Ibrahim, Muhammad Twaha; Majumder, Aditi; Gopi, M.] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 Roche Holding; Genentech; University of California System; University of
   California Irvine
RP Ibrahim, MT (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.
EM ma.tehrani86@gmail.com; muhammti@uci.edu; majumder@ics.uci.edu;
   gopi@ics.uci.edu
RI Ibrahim, Muhammad Twaha/ADD-8310-2022
OI Meenakshisundaram, Gopi/0000-0003-2326-8029; Ibrahim, Muhammad
   Twaha/0000-0001-9286-6124
CR Aliaga DG, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P53
   Aliaga DanielG., 2008, COMPUTER VISION PATT, P1
   Aliaga Y. H., 2012, inACM Trans. Graph., V31, P1
   Bern M, 2003, P 19 ANN S COMP GEOM, P274, DOI 10.1145/777792.777834
   Giorgianni T. E. M., 2009, Digital Color Management:Encoding Solutions, V2nd
   Goldstein J., 2016, inCengageLearning
   Hasker ES, 2006, IEEE T VIS COMPUT GR, V12, P1101, DOI 10.1109/TVCG.2006.121
   Hereld M, 2000, IEEE COMPUT GRAPH, V20, P22, DOI 10.1109/38.851746
   Ibrahim M., 2020, P 26 ACM S VIRT REAL, P1, DOI [10.1145/33859563418970.10M.T., DOI 10.1145/33859563418970.10M.T]
   Ibrahim MT, 2022, COMPUT GRAPH-UK, V103, P61, DOI 10.1016/j.cag.2022.01.004
   Kurth Philipp, 2020, 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), P174, DOI 10.1109/ISMAR50242.2020.00039
   Law AJ, 2011, COMPUT GRAPH FORUM, V30, P2288, DOI 10.1111/j.1467-8659.2011.02035.x
   Majumder A, 2005, ACM T GRAPHIC, V24, P118, DOI 10.1145/1037957.1037964
   Majumder A, 2000, IEEE VISUAL, P117, DOI 10.1109/VISUAL.2000.885684
   Majumder A., 2002, Proc. SID Eurodisplay, P1
   Majumder M., 2018, Introduction to Visual Computing: CoreConcepts in Computer Vision, Graphics, and Image Processing
   Majumder R., 2004, Trans. Vis Comput. Graph., V10
   Nomoto T, 2022, IEEE T VIS COMPUT GR, V28, P2125, DOI 10.1109/TVCG.2022.3150488
   Pailthorpe B., 2001, Proceedings Asia Display IDW, P1295
   Pjanic P, 2018, IEEE T VIS COMPUT GR, V24, P2963, DOI 10.1109/TVCG.2018.2868597
   Raij A, 2004, INT C PATT RECOG, P14, DOI 10.1109/ICPR.2004.1333994
   Raij A., 2003, IEEE INT WORKSH PROJ, P203
   Raskar G., 1998, P 25 ANN C COMP GRAP, P179
   Raskar R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P161, DOI 10.1109/VISUAL.1999.809883
   Sajadi B., 2015, P 3D TV C IMM INT 3D, P1
   Sajadi B, 2009, IEEE T VIS COMPUT GR, V15, P1317, DOI 10.1109/TVCG.2009.124
   Tehrani MA, 2021, IEEE T VIS COMPUT GR, V27, P2265, DOI 10.1109/TVCG.2019.2950942
   Wallace H. Chen, 2003, P IMM PROJ TECHN WOR
   Yang RG, 2001, IEEE VISUAL, P167, DOI 10.1109/VISUAL.2001.964508
NR 29
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4724
EP 4738
DI 10.1109/TVCG.2023.3277436
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400086
PM 37200131
DA 2024-11-06
ER

PT J
AU Fu, Q
   He, SH
   Li, XM
   Fu, HB
AF Fu, Qiang
   He, Shuhan
   Li, Xueming
   Fu, Hongbo
TI PlanNet: A Generative Model for Component-Based Plan Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Floors; Layout; Semantics; Three-dimensional displays; Wave functions;
   Buildings; Task analysis; Generative model; wave function collapse;
   floor plan synthesis
ID GAN
AB We propose a novel generative model named as PlanNet for component-based plan synthesis. The proposed model consists of three modules, a wave function collapse algorithm to create large-scale wireframe patterns as the embryonic forms of floor plans, and two deep neural networks to outline the plausible boundary from each squared pattern, and meanwhile estimate the potential semantic labels for the components. In this manner, we use PlanNet to generate a large-scale component-based plan dataset with 10 K examples. Given an input boundary, our method retrieves dataset plan examples with similar configurations to the input, and then transfers the space layout from a user-selected plan example to the input. Benefiting from our interactive workflow, users can recursively subdivide individual components of the plans to enrich the plan contents, thus designing more complex plans for larger scenes. Moreover, our method also adopts a random selection algorithm to make the variations on semantic labels of the plan components, aiming at enriching the 3D scenes that the output plans are suited for. To demonstrate the quality and versatility of our generative model, we conduct intensive experiments, including the analysis of plan examples and their evaluations, plan synthesis with both hard and soft boundary constraints, and 3D scenes designed with the plan subdivision on different scales. We also compare our results with the state-of-the-art floor plan synthesis methods to validate the feasibility and efficacy of the proposed generative model.
C1 [Fu, Qiang; He, Shuhan; Li, Xueming] Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing 100876, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Beijing University of Posts & Telecommunications; City University of
   Hong Kong
RP Fu, Q (corresponding author), Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing 100876, Peoples R China.
EM fu.john.qiang@gmail.com; heshuhan@bupt.edu.cn; lixm@bupt.edu.cn;
   fuplus@gmail.com
RI Li, Xueming/W-8707-2019; Fu, Qiang/AAF-2612-2021
OI Fu, Qiang/0000-0002-8944-8981; LI, xueming/0000-0003-1058-2799; FU,
   Hongbo/0000-0002-0284-726X
FU National Natural Science Foundation of China [61902032]
FX No Statement Available
CR Aliaga DG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409113
   Bao F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421644
   Chen XW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P321, DOI 10.1145/2733373.2806274
   Douglas D. H., 1973, Cartogr. Int. J. Geogr. Inf. Geovis, V10, P112, DOI DOI 10.3138/FM57-6770-U75U-7727
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Fu Q, 2024, IEEE T VIS COMPUT GR, V30, P4068, DOI 10.1109/TVCG.2023.3250488
   Fu Q, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-019-2930-x
   Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gumin M., 2016, Wavefunction collapse
   Hendrikx M, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422957
   Hu RZ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392391
   Kipf TN, 2016, arXiv:1609.02907, P1
   Li JN, 2021, IEEE T VIS COMPUT GR, V27, P4039, DOI 10.1109/TVCG.2020.2999335
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Lin JJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024217
   Ma D., 2019, Frontiers of Data and Computing, V1, P105, DOI [DOI 10.11871/JFDC.ISSN.2096.742X.2019.01.011, DOI 10.11871/JFDC]
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Nauata K.-H., 2020, P EUR C COMP VIS, P162, DOI [DOI 10.1007/978-3-030-58452-810, 10.1007/978-3-030-58452-8_10]
   Nauata N, 2021, PROC CVPR IEEE, P13627, DOI 10.1109/CVPR46437.2021.01342
   Para W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6670, DOI 10.1109/ICCV48922.2021.00662
   Peng CH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925935
   Peng CH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601164
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schulz A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601127
   Shen CH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366199
   Su XY, 2016, COMPUT GRAPH-UK, V54, P145, DOI 10.1016/j.cag.2015.06.009
   Sun JH, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530135
   Vanegas CA, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366187
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wu WM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356556
   Wu WM, 2018, COMPUT GRAPH FORUM, V37, P511, DOI 10.1111/cgf.13380
   Xiong GM, 2021, IEEE T VIS COMPUT GR, V27, P4413, DOI 10.1109/TVCG.2020.3005680
   Xu K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601109
   Xu P., 2014, P 27 ANN ACM S US IN, P243, DOI DOI 10.1145/2642918.2647398
   Xu PF, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2787, DOI 10.1145/2702123.2702198
   Yang YL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508405
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Zhang SK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P965, DOI 10.1145/3474085.3475194
   Zhang SH, 2022, IEEE T VIS COMPUT GR, V28, P3082, DOI 10.1109/TVCG.2021.3050143
   Zou CQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925887
NR 43
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4739
EP 4751
DI 10.1109/TVCG.2023.3275200
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400087
PM 37167051
DA 2024-11-06
ER

PT J
AU Li, J
   Kang, D
   Pei, WJ
   Zhe, X
   Zhang, Y
   Bao, LC
   He, ZY
AF Li, Jing
   Kang, Di
   Pei, Wenjie
   Zhe, Xuefei
   Zhang, Ying
   Bao, Linchao
   He, Zhenyu
TI Audio2Gestures: Generating Diverse Gestures From Audio
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Codes; Training; Three-dimensional displays; Dynamics; Decoding; Image
   reconstruction; Hidden Markov models; Cross-model generation; gesture;
   motion generation
ID SCALE
AB People may perform diverse gestures affected by various mental and physical factors when speaking the same sentences. This inherent one-to-many relationship makes co-speech gesture generation from audio particularly challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, easily resulting in plain/boring motions during inference. So we propose to explicitly model the one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code is expected to be responsible for the motion component that is more correlated to the audio while the motion-specific code is expected to capture diverse motion information that is more independent of the audio. However, splitting the latent code into two parts poses extra training difficulties. Several crucial training losses/strategies, including relaxed motion loss, bicycle constraint, and diversity loss, are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than previous state-of-the-art methods, quantitatively and qualitatively. Besides, our formulation is compatible with discrete cosine transformation (DCT) modeling and other popular backbones (i.e., RNN, Transformer). As for motion losses and quantitative motion evaluation, we find structured losses/metrics (e.g. STFT) that consider temporal and/or spatial context complement the most commonly used point-wise losses (e.g. PCK), resulting in better motion dynamics and more nuanced motion details. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline.
C1 [Li, Jing; He, Zhenyu] Harbin Inst Technol, Shenzhen 518055, Peoples R China.
   [Kang, Di; Bao, Linchao] Tencent AI Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Tencent
RP He, ZY (corresponding author), Harbin Inst Technol, Shenzhen 518055, Peoples R China.; Bao, LC (corresponding author), Tencent AI Lab, Shenzhen 518055, Peoples R China.
EM lijing@stu.hit.edu.cn; dkang@tencent.com; wenjiecoder@outlook.com;
   zhexuefei@outlook.com; yinggzhang@tencent.com; linchaobao@gmail.com;
   zhenyuhe@hit.edu.cn
RI Pei, Wenjie/IQT-7671-2023; Bao, Linchao/AAG-9148-2020
OI Bao, Linchao/0000-0001-9543-3754; li, jing/0000-0002-2162-1004
FU National Natural Science Foundation of China [62172126, U2013210,
   62006060]; Shenzhen Fundamental Research Program
   [JCYJ20220818102415032]; Shenzhen Research Council
   [JCYJ20210324120202006]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172126, in part by the National
   Natural Science Foundation of China under Grants U2013210, 62006060, in
   part by Shenzhen Fundamental Research Program under Grant
   JCYJ20220818102415032, and in part by Shenzhen Research Council under
   Grant JCYJ20210324120202006.
CR Agarap A.F., 2018, arXiv, DOI 10.48550/arXiv.1803.08375
   Alexanderson S., 2022, arXiv
   Alexanderson S, 2020, COMPUT GRAPH FORUM, V39, P487, DOI 10.1111/cgf.13946
   Ao TL, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555435
   Bai SJ, 2018, Arxiv, DOI arXiv:1803.01271
   Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009
   Bougares F., 2014, P C EMP METH NAT LAN, DOI DOI 10.3115/V1/D14-1179
   Brian M., 2015, P 14 PYTH SCI C AUST, P18, DOI [10. 25080/Majora-7b98e3ed-003, DOI 10.25080/MAJORA-7B98E3ED-003]
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen X, 2016, ADV NEUR IN, V29
   Choi Y, 2020, PROC CVPR IEEE, P8185, DOI 10.1109/CVPR42600.2020.00821
   Defossez A., 2020, arXiv, DOI DOI 10.48550/ARXIV.2006.12847
   Doersch C, 2021, Arxiv, DOI arXiv:1606.05908
   Donahue J., 2016, arXiv
   Ferstl Y., 2019, P 12 ACM SIGGRAPH C, P1
   Ferstl Y, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P93, DOI 10.1145/3267851.3267898
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Habibie I., 2022, P SPEC INT GROUP COM, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Huang R., 2021, P INT C LEARN REPR
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D.P., 2014, P INT C LEARNING REP
   Kingma DP, 2018, ADV NEUR IN, V31
   Kobyzev I, 2021, IEEE T PATTERN ANAL, V43, P3964, DOI 10.1109/TPAMI.2020.2992934
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kucherenko T, 2021, IUI '21 - 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P11, DOI 10.1145/3397481.3450692
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Levine S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778861
   Levine S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618518
   Li J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11273, DOI 10.1109/ICCV48922.2021.01110
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Liu HY, 2022, LECT NOTES COMPUT SC, V13667, P612, DOI 10.1007/978-3-031-20071-7_36
   Liu X, 2022, PROC CVPR IEEE, P10452, DOI 10.1109/CVPR52688.2022.01021
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Mixamo, About us
   Paszke A, 2019, Arxiv, DOI arXiv:1912.01703
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavllo D., 2018, arXiv preprint arXiv:1805.06485, DOI [DOI 10.1109/HUMANOIDS.2018.8624922, 10.1109/HUMANOIDS.2018.8624922]
   Ruiz AH, 2019, IEEE I CONF COMP VIS, P7133, DOI 10.1109/ICCV.2019.00723
   Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790
   Stevens SS, 1937, J ACOUST SOC AM, V8, P185, DOI 10.1121/1.1915893
   Sutskever I, 2014, ADV NEUR IN, V27
   Tevet G, 2022, LECT NOTES COMPUT SC, V13682, P358, DOI 10.1007/978-3-031-20047-2_21
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan XC, 2018, LECT NOTES COMPUT SC, V11209, P276, DOI 10.1007/978-3-030-01228-1_17
   Yazdian PJ, 2022, IEEE INT C INT ROBOT, P3100, DOI 10.1109/IROS47612.2022.9981117
   Yi HW, 2023, Arxiv, DOI arXiv:2212.04420
   Yoon Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417838
   Yoon Y, 2019, IEEE INT CONF ROBOT, P4303, DOI [10.1109/icra.2019.8793720, 10.1109/ICRA.2019.8793720]
   Zhang F, 2023, LECT NOTES COMPUT SC, V13833, P231, DOI 10.1007/978-3-031-27077-2_18
   Zhang JY, 2019, IEEE I CONF COMP VIS, P7113, DOI 10.1109/ICCV.2019.00721
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y, 2021, PROC CVPR IEEE, P3371, DOI 10.1109/CVPR46437.2021.00338
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu YZ, 2020, PROC CVPR IEEE, P6537, DOI 10.1109/CVPR42600.2020.00657
NR 67
TC 1
Z9 1
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4752
EP 4766
DI 10.1109/TVCG.2023.3276973
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400030
PM 37195841
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Cabaret, PA
   Howard, T
   Gicquel, G
   Pacchierotti, C
   Babel, M
   Marchal, M
AF Cabaret, Pierre-Antoine
   Howard, Thomas
   Gicquel, Guillaume
   Pacchierotti, Claudio
   Babel, Marie
   Marchal, Maud
TI Does Multi-Actuator Vibrotactile Feedback Within Tangible Objects Enrich
   VR Manipulation?
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic interfaces; Actuators; Rendering (computer graphics); Vibrations;
   Prototypes; Grasping; Wearable computers; Haptics; vibrotactile;
   tangible; VR; manipulation
ID DISCRIMINATION; PERCEPTION; DISPLAY; TEXTURE; TOUCH
AB Rich, informative and realistic haptic feedback is key to enhancing Virtual Reality (VR) manipulation. Tangible objects provide convincing grasping and manipulation interactions with haptic feedback of e.g., shape, mass and texture properties. But these properties are static, and cannot respond to interactions in the virtual environment. On the other hand, vibrotactile feedback provides the opportunity for delivering dynamic cues rendering many different contact properties, such as impacts, object vibrations or textures. Handheld objects or controllers in VR are usually restricted to vibrating in a monolithic fashion. In this article, we investigate how spatialiazing vibrotactile cues within handheld tangibles could enable a wider range of sensations and interactions. We conduct a set of perception studies, investigating the extent to which spatialization of vibrotactile feedback within tangible objects is possible as well as the benefits of proposed rendering schemes leveraging multiple actuators in VR. Results show that vibrotactile cues from localized actuators can be discriminated and are beneficial for certain rendering schemes.
C1 [Cabaret, Pierre-Antoine; Howard, Thomas; Gicquel, Guillaume; Babel, Marie; Marchal, Maud] Univ Rennes, INSA Rennes, IRISA, INRIA,Inria,CNRS Rennes, F-35000 Rennes, France.
   [Cabaret, Pierre-Antoine; Howard, Thomas; Gicquel, Guillaume; Babel, Marie; Marchal, Maud] Inst Univ France Paris, F-35000 Rennes, France.
   [Pacchierotti, Claudio] Univ Rennes, CNRS, Inria, IRISA Rennes, F-35000 Rennes, France.
C3 Universite de Rennes; Inria; Centre National de la Recherche
   Scientifique (CNRS); Institut National des Sciences Appliquees de
   Rennes; Institut Universitaire de France; Inria; Universite de Rennes;
   Centre National de la Recherche Scientifique (CNRS)
RP Cabaret, PA (corresponding author), Univ Rennes, INSA Rennes, IRISA, INRIA,Inria,CNRS Rennes, F-35000 Rennes, France.
EM pierre-antoine.cabaret@irisa.fr; thomas.howard@inria.fr;
   guillaume.gicquel@irisa.fr; claudio.pacchierotti@irisa.fr;
   marie.babel@irisa.fr; maud.marchal@irisa.fr
RI Pacchierotti, Claudio/G-7304-2011
OI Pacchierotti, Claudio/0000-0002-8006-9168; Howard,
   Thomas/0000-0003-4904-375X; Cabaret, Pierre-Antoine/0000-0003-3444-7970;
   Gicquel, Guillaume/0009-0004-5535-3900; Babel,
   Marie/0000-0001-6425-389X; Marchal, Maud/0000-0002-6080-7178
FU Inria D#x00E9;fi project; ANR project "MIMESIS"; European Union
   [801413]; Project "H-Reality"
FX No Statement Available
CR Aggravi Marco, 2018, IEEE Robotics and Automation Letters, V3, P2166, DOI 10.1109/LRA.2018.2810887
   ALLES DS, 1970, IEEE T MAN MACHINE, VMM11, P85, DOI 10.1109/TMMS.1970.299967
   Azmandian M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1968, DOI 10.1145/2858036.2858226
   Bae Y, 2020, INT J CONTROL AUTOM, V18, P1335, DOI 10.1007/s12555-018-0882-3
   Brument H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1409, DOI 10.1109/vr.2019.8797848
   Cabaret P.-A., 2022, P INT C HUM HAPT SEN, P273
   Cheng LP, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3186482
   Chinello F, 2018, IEEE T HAPTICS, V11, P39, DOI 10.1109/TOH.2017.2755015
   Choi I, 2021, IEEE T VIS COMPUT GR, V27, P4387, DOI 10.1109/TVCG.2020.3002245
   Choi I, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174228
   Choi I, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P119, DOI 10.1145/3126594.3126599
   Choi S, 2013, P IEEE, V101, P2093, DOI 10.1109/JPROC.2012.2221071
   Culbertson Heather, 2012, 2012 IEEE Haptics Symposium (HAPTICS), P385, DOI 10.1109/HAPTIC.2012.6183819
   Culbertson H., 2014, The penn haptic texture toolkit for modeling, rendering, and evaluating haptic virtual textures
   Culbertson H, 2018, ANNU REV CONTR ROBOT, V1, P385, DOI 10.1146/annurev-control-060117-105043
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Oliveira VAD, 2017, IEEE T VIS COMPUT GR, V23, P1340, DOI 10.1109/TVCG.2017.2657238
   de Tinguy X, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P580, DOI [10.1109/whc.2019.8816164, 10.1109/WHC.2019.8816164]
   Delazio A., 2018, P CHI C HUM FACT COM, P12
   Dunkelberger N, 2018, LECT NOTES COMPUT SC, V10894, P289, DOI 10.1007/978-3-319-93399-3_26
   García-Valle G, 2018, IEEE ACCESS, V6, P7224, DOI 10.1109/ACCESS.2017.2782254
   Guruswamy VL, 2011, IEEE T INSTRUM MEAS, V60, P93, DOI 10.1109/TIM.2010.2065751
   Hoffman HG, 1998, P IEEE VIRT REAL ANN, P59, DOI 10.1109/VRAIS.1998.658423
   Hollins M, 2000, PERCEPT PSYCHOPHYS, V62, P695, DOI 10.3758/BF03206916
   Hollins M, 2001, SOMATOSENS MOT RES, V18, P253
   Ito K, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3340961
   Kaul OB, 2017, LECT NOTES COMPUT SC, V10516, P289, DOI 10.1007/978-3-319-68059-0_19
   Keef CV, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000018
   Khoudja M. B., 2004, VITAL: A vibrotactile interface with thermal feedback
   Kildal Johan, 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P191, DOI 10.1007/978-3-642-31404-9_33
   Kovacs R., 2020, P 33 ANN ACM S US IN, P1059
   Kreimeier J, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P289, DOI 10.1145/3316782.3321536
   LEDERMAN SJ, 1972, PERCEPT PSYCHOPHYS, V12, P401, DOI 10.3758/BF03205850
   Li-Te Cheng, 1996, Proceedings ACM Multimedia 96, P243, DOI 10.1145/244130.244220
   Maereg AT, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00042
   McDonald CG, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P307, DOI 10.1109/WHC.2013.6548426
   Mercado VR, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P373, DOI 10.1109/WHC49131.2021.9517250
   Okamoto S, 2013, IEEE T HAPTICS, V6, P81, DOI [10.1109/TOH.2012.32, 10.1109/ToH.2012.32]
   Okamura AM, 1998, IEEE INT CONF ROBOT, P674, DOI 10.1109/ROBOT.1998.677050
   Pacchierotti C, 2017, IEEE T HAPTICS, V10, P580, DOI 10.1109/TOH.2017.2689006
   Pantera L, 2020, IEEE T HAPTICS, V13, P493, DOI 10.1109/TOH.2020.2981307
   Passalenti A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1116, DOI [10.1109/vr.2019.8798168, 10.1109/VR.2019.8798168]
   Perez CA, 1998, P ANN INT IEEE EMBS, V20, P2542, DOI 10.1109/IEMBS.1998.744968
   Perez CA, 2000, MED BIOL ENG COMPUT, V38, P74, DOI 10.1007/BF02344692
   Pezent E, 2021, IEEE T HAPTICS, V14, P225, DOI 10.1109/TOH.2020.3002696
   Pezent E, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P1, DOI [10.1109/WHC.2019.8816098, 10.1109/whc.2019.8816098]
   Popescu G. V., 2002, Handbook of Virtual Environments, P475
   Saal HP, 2014, TRENDS NEUROSCI, V37, P689, DOI 10.1016/j.tins.2014.08.012
   Sagheb S, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P751, DOI 10.1145/3332165.3347870
   SHERRICK CE, 1990, J ACOUST SOC AM, V88, P169, DOI 10.1121/1.399937
   Shigeyama J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300241
   Sinclair M, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P815, DOI 10.1145/3332165.3347891
   Sun MM, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P42, DOI 10.1109/ISMAR-Adjunct.2019.00026
   Tinguy Xavier, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P262, DOI 10.1007/978-3-030-58147-3_29
   VONBEKESY G, 1958, J ACOUST SOC AM, V30, P399
   Wang DX, 2020, IEEE T IND ELECTRON, V67, P610, DOI 10.1109/TIE.2019.2920602
   WEINSTEIN SIDNEY, 1968, P195
   Wellman Parris., 1995, PROC ASME DYNAMIC SY, V57, P713
   Wolf D, 2019, IEEE T VIS COMPUT GR, V25, P3169, DOI 10.1109/TVCG.2019.2932215
   Yang S., 2014, P HUM FACT ERG SOC A, P1720
   Yem V, 2017, P IEEE VIRT REAL ANN, P99, DOI 10.1109/VR.2017.7892236
   Zhong X., 2017, P INT C INT ROB APPL, P372
NR 62
TC 1
Z9 1
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4767
EP 4779
DI 10.1109/TVCG.2023.3279398
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400093
PM 37224347
OA Green Published
DA 2024-11-06
ER

PT J
AU Chen, YZ
   Gao, SJ
   Tu, PX
   Chen, XJ
AF Chen, Yizhou
   Gao, Shuojie
   Tu, Puxun
   Chen, Xiaojun
TI Automatic 3D Teeth Reconstruction From Five Intra-Oral Photos Using
   Parametric Teeth Model
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Teeth; Three-dimensional displays; Shape; Solid modeling; Image
   reconstruction; Dentistry; Image restoration; Statistical shape
   modeling; parametric teeth model; teeth contour extraction; teeth
   reconstruction
AB Orthodontic treatment is a lengthy process that requires regular in-person dental monitoring, making remote dental monitoring a viable alternative when face-to-face consultation is not possible. In this study, we propose an improved 3D teeth reconstruction framework that automatically restores the shape, arrangement, and dental occlusion of upper and lower teeth from five intra-oral photographs to aid orthodontists in visualizing the condition of patients in virtual consultations. The framework comprises a parametric model that leverages statistical shape modeling to describe the shape and arrangement of teeth, a modified U-net that extracts teeth contours from intra-oral images, and an iterative process that alternates between finding point correspondences and optimizing a compound loss function to fit the parametric teeth model to predicted teeth contours. We perform a five-fold cross-validation on a dataset of 95 orthodontic cases and report an average Chamfer distance of 1.0121 mm(2) and an average Dice similarity coefficient of 0.7672 on all the test samples in the cross-validation, demonstrating a significant improvement compared with the previous work. Our teeth reconstruction framework provides a feasible solution for visualizing 3D teeth models in remote orthodontic consultations.
C1 [Chen, Yizhou; Gao, Shuojie; Tu, Puxun] Shanghai Jiao Tong Univ, Inst Biomed Mfg & Life Qual Engn, Sch Mech Engn, Shanghai 200240, Peoples R China.
   [Chen, Xiaojun] Shanghai Jiao Tong Univ, Inst Med Robot, Sch Mech Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Chen, XJ (corresponding author), Shanghai Jiao Tong Univ, Inst Med Robot, Sch Mech Engn, Shanghai 200240, Peoples R China.
EM yizhou.chen@sjtu.edu.cn; gaoshuojie@sjtu.edu.cn; puxuntu@sjtu.edu.cn;
   xiaojunchen@sjtu.edu.cn
OI Gao, Shuojie/0000-0002-0186-8413; CHEN, YIZHOU/0000-0003-3693-7893; Tu,
   Puxun/0000-0003-4809-9081
FU National Natural Science Foundation of China [81971709, M-0019,
   82011530141]; Foundation of Science and Technology Commission of
   Shanghai Municipality [20490740700]; Shanghai Pudong Science and
   Technology Development Fund [PKX2021-R04]; Shanghai Jiao Tong University
   Foundation on Medical and Technological Joint Science Research
   [YG2019ZDA06, YG2021ZD21, YG2021QN72, YG2022QN056]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 81971709, M-0019, and 82011530141, in
   part by the Foundation of Science and Technology Commission of Shanghai
   Municipality under Grant 20490740700, in part by Shanghai Pudong Science
   and Technology Development Fund under Grant PKX2021-R04, in part by
   Shanghai Jiao Tong University Foundation on Medical and Technological
   Joint Science Research under Grants YG2019ZDA06, YG2021ZD21,
   YG2021QN72,and YG2022QN056.
CR Abdelrahim A.S., 2012, 2012 IEEE COMP SOC C, P64
   Abdelrehim A. A., 2013, P INT MICCAI WORKSH, P44
   Alldieck T, 2022, PROC CVPR IEEE, P1496, DOI 10.1109/CVPR52688.2022.00156
   Berlinet C., 2011, Reproducing Kernel Hilbert Spaces inProbability and Statistics
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cui ZM, 2019, PROC CVPR IEEE, P6361, DOI 10.1109/CVPR.2019.00653
   De Maesschalck R, 2000, CHEMOMETR INTELL LAB, V50, P1, DOI 10.1016/S0169-7439(99)00047-7
   Farag Aly., 2012, International MICCAI Workshop on Medical Computer Vision, P263
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Hartley R., 2003, Computer Vision
   He H, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091015
   Hong-Seok P, 2015, PROCEDIA ENGINEER, V100, P1174, DOI 10.1016/j.proeng.2015.01.481
   Jackson AS, 2019, LECT NOTES COMPUT SC, V11132, P64, DOI 10.1007/978-3-030-11018-5_6
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lüthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743
   Ma J, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102035
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Morales A, 2021, COMPUT SCI REV, V40, DOI 10.1016/j.cosrev.2021.100400
   Mostafa E, 2014, IEEE IMAGE PROC, P4285, DOI 10.1109/ICIP.2014.7025870
   Mustafa A, 2021, PROC CVPR IEEE, P14469, DOI 10.1109/CVPR46437.2021.01424
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461
   Özyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X
   Revilla-Leon M, 2021, J AM DENT ASSOC, V152, P669, DOI 10.1016/j.adaj.2021.05.018
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler J, 2002, J Orthod, V29, P158, DOI 10.1093/ortho/29.2.158
   Schoukens J, 2019, IEEE CONTR SYST MAG, V39, P28, DOI 10.1109/MCS.2019.2938121
   Song WN, 2021, AAAI CONF ARTIF INTE, V35, P566
   Styner MA, 2003, LECT NOTES COMPUT SC, V2732, P63
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wirtz A, 2021, PROC SPIE, V11596, DOI 10.1117/12.2582253
   Wu CL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980233
   Yang LC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417771
   Yuan Liang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12262), P400, DOI 10.1007/978-3-030-59713-9_39
   Zheng S. Li, 2017, Statistical Shape and DeformationAnalysis: Methods, Implementation and Applications
NR 38
TC 0
Z9 0
U1 8
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4780
EP 4791
DI 10.1109/TVCG.2023.3277914
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400098
PM 37204961
DA 2024-11-06
ER

PT J
AU Hu, L
   Zhang, ZH
   Zhong, CY
   Jiang, BY
   Xia, SH
AF Hu, Lei
   Zhang, Zihao
   Zhong, Chongyang
   Jiang, Boyuan
   Xia, Shihong
TI Pose-Aware Attention Network for Flexible Motion Retargeting by Body
   Part
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep learning; motion processing; motion retargeting
AB Motion retargeting is a fundamental problem in computer graphics and computer vision. Existing approaches usually have many strict requirements, such as the source-target skeletons needing to have the same number of joints or share the same topology. To tackle this problem, we note that skeletons with different structure may have some common body parts despite the differences in joint numbers. Following this observation, we propose a novel, flexible motion retargeting framework. The key idea of our method is to regard the body part as the basic retargeting unit rather than directly retargeting the whole body motion. To enhance the spatial modeling capability of the motion encoder, we introduce a pose-aware attention network (PAN) in the motion encoding phase. The PAN is pose-aware since it can dynamically predict the joint weights within each body part based on the input pose, and then construct a shared latent space for each body part by feature pooling. Extensive experiments show that our approach can generate better motion retargeting results both qualitatively and quantitatively than state-of-the-art methods. Moreover, we also show that our framework can generate reasonable results even for a more challenging retargeting scenario, like retargeting between bipedal and quadrupedal skeletons because of the body part retargeting strategy and PAN.
C1 [Hu, Lei; Zhang, Zihao; Zhong, Chongyang; Jiang, Boyuan; Xia, Shihong] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Hu, Lei; Zhong, Chongyang; Jiang, Boyuan; Xia, Shihong] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xia, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.; Xia, SH (corresponding author), Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
EM hulei19z@ict.ac.cn; zhangzihao@ict.ac.cn; zhongchongyang@ict.ac.cn;
   jiangboyuan20s@ict.ac.cn; xsh@ict.ac.cn
OI Zhong, Chongyang/0000-0003-0020-1892; Hu, Lei/0000-0001-8938-5071;
   Zhang, Zihao/0000-0001-6859-7518
FU National Key R#x0026;D Program #x201C;Industrial Software#x201D; Special
   Project of China [2022YFB3303202]
FX No Statement Available
CR A. S. Inc, 2021, Adobes Mixamo.
   Abdul-Massih M, 2017, COMPUT GRAPH FORUM, V36, P86, DOI 10.1111/cgf.12860
   Aberman K, 2019, Arxiv, DOI arXiv:1905.01680
   Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392462
   Baran I., 2009, ACM T GRAPHIC, P1
   Celikcan U, 2015, COMPUT GRAPH FORUM, V34, P216, DOI 10.1111/cgf.12507
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   Gao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275028
   Gleicher M., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P33, DOI 10.1145/280814.280820
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Harvey FG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392480
   Heck R, 2006, COMPUT GRAPH FORUM, V25, P459, DOI 10.1111/j.1467-8659.2006.00965.x
   Hecker C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360626
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Holden J., 2015, P SIGGRAPH AS TECH B, P1
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jang DK, 2022, Arxiv, DOI arXiv:2202.05274
   Jang WS, 2008, VISUAL COMPUT, V24, P271, DOI 10.1007/s00371-007-0200-1
   Kim Sunwoo, 2022, arXiv
   Kingma D.P., 2014, P INT C LEARNING REP
   Lee J, 1999, COMP GRAPH, P39
   Lee S., 2022, ACM Trans. Graph., V41, P1
   Liao ZYC, 2022, Arxiv, DOI arXiv:2208.00790
   Lim H. J., 2019, P BRIT MACH VIS C
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273
   MXH*10 MA W., 2010, P 2010 ACM SIGGRAPH, P21, DOI DOI 10.1109/IUCS.2010.5666642
   Petrovich M., 2021, arXiv
   Popovic Z, 1999, COMP GRAPH, P11, DOI 10.1145/311535.311536
   Rempe D., 2020, P EUR C COMP VIS, V12350, P71, DOI [10.1007/978- 3- 030-58558-7 5, DOI 10.1007/978-3-030-58558-75]
   Ren M., 2021, P IEEE CVF C COMP VI
   Seol Y., 2013, P 12 ACM SIGGRAPH EU, P213
   Shi L., 2020, P AS C COMP VIS, P38, DOI DOI 10.1007/978-3-030-69541-5_3
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Starke S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530178
   Starke S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459881
   Starke S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392450
   Starke S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356505
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tak S, 2005, ACM T GRAPHIC, V24, P98, DOI 10.1145/1037957.1037963
   Kim SU, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1947
   Vaswani A, 2017, ADV NEUR IN, V30
   Villegas D., 2021, arXiv:2109.07431, P33
   Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yamane K., 2010, P ACM SIGGRAPH EUR S, P169
   Ye YJ, 2022, Arxiv, DOI [arXiv:2209.05753, DOI 10.48550/ARXIV.2209.05753]
   Zhang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201366
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhong CY, 2022, PROC CVPR IEEE, P6437, DOI 10.1109/CVPR52688.2022.00634
NR 52
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4792
EP 4808
DI 10.1109/TVCG.2023.3277918
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400082
PM 37204962
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Kim, H
   Kim, J
   Han, Y
   Hong, H
   Kwon, OS
   Park, YW
   Elmqvist, N
   Ko, S
   Kwon, BC
AF Kim, Hwiyeon
   Kim, Joohee
   Han, Yunha
   Hong, Hwajung
   Kwon, Oh-Sang
   Park, Young-Woo
   Elmqvist, Niklas
   Ko, Sungahn
   Kwon, Bum Chul
TI Towards Visualization Thumbnail Designs That Entice Reading Data-Driven
   Articles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data journalism; data-driven storytelling; online news; visualization;
   thumbnail images; data stories
ID VISUAL EMBELLISHMENTS; PICTURE; RECOGNITION; MEMORY
AB As online news increasingly include data journalism, there is a corresponding increase in the incorporation of visualization in article thumbnail images. However, little research exists on the design rationale for visualization thumbnails, such as resizing, cropping, simplifying, and embellishing charts that appear within the body of the associated article. Therefore, in this paper we aim to understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. To this end, we first survey visualization thumbnails collected online and discuss visualization thumbnail practices with data journalists and news graphics designers. Based on the survey and discussion results, we then define a design space for visualization thumbnails and conduct a user study with four types of visualization thumbnails derived from the design space. The study results indicate that different chart components play different roles in attracting reader attention and enhancing reader understandability of the visualization thumbnails. We also find various thumbnail design strategies for effectively combining the charts' components, such as a data summary with highlights and data labels, and a visual legend with text labels and Human Recognizable Objects (HROs), into thumbnails. Ultimately, we distill our findings into design implications that allow effective visualization thumbnail designs for data-rich news articles. Our work can thus be seen as a first step toward providing structured guidance on how to design compelling thumbnails for data stories.
C1 [Kim, Hwiyeon; Kim, Joohee; Han, Yunha; Kwon, Oh-Sang; Park, Young-Woo; Ko, Sungahn] UNIST, Ulsan 44919, South Korea.
   [Hong, Hwajung] Korea Adv Inst Sci & Technol, Daejeon 34141, South Korea.
   [Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Kwon, Bum Chul] IBM Res, Cambridge, MA 10598 USA.
C3 Ulsan National Institute of Science & Technology (UNIST); Korea Advanced
   Institute of Science & Technology (KAIST); University System of
   Maryland; University of Maryland College Park; International Business
   Machines (IBM)
RP Ko, S (corresponding author), UNIST, Ulsan 44919, South Korea.
EM hwiyeon.d@gmail.com; jkim17@unist.ac.kr; diana438@unist.ac.kr;
   hwajung@kaist.ac.kr; oskwon@unist.ac.kr; ywpark@unist.ac.kr;
   elm@umd.edu; sako@unist.ac.kr; bumchul.kwon@us.ibm.com
OI Kim, Joohee/0000-0002-0745-2339; Kwon, Bum Chul/0000-0002-9391-6274;
   Elmqvist, Niklas/0000-0001-5805-5301; Ko, Sungahn/0000-0002-7410-5652;
   Park, Young-Woo/0000-0003-2257-9394
FU Korean National Research Foundation (NRF) [2021R1A2C1004542]; Korea
   Health Industry Development Institute(KHIDI) - Ministry of Health &
   Welfare, Republic of Korea [HI22C0646]; Institute of Information &
   Communications Technology Planning & Evaluation (IITP) [2020-0-01336];
   UNIST - Korea government (MSIT)
FX This work was supported in part by the Korean National Research
   Foundation (NRF) under Grant 2021R1A2C1004542, in part by a grant of the
   Korea Health Technology R&D Project through the Korea Health Industry
   Development Institute(KHIDI), funded by the Ministry of Health &
   Welfare, Republic of Korea under Grant HI22C0646, and in part by the
   Institute of Information & Communications Technology Planning &
   Evaluation (IITP) under Grant 2020-0-01336-Artificial Intelligence
   Graduate School Program, UNIST, funded by the Korea government(MSIT).
   Recommended for acceptance by R. Chang.
CR Agrawala M, 2011, COMMUN ACM, V54, P60, DOI 10.1145/1924421.1924439
   Ally B, 2009, NEUROPSYCHOLOGIA, V47, P595, DOI 10.1016/j.neuropsychologia.2008.10.010
   Amini F., 2018, Data-Driven Storytelling
   Andry T., 2021, P ACM C HUM FACT COM, P1
   [Anonymous], 2007, Interactions
   Aula A., 2010, Proceedings of the 19th international conference on World Wide Web, WWW '10, P51
   Barrett AW, 2005, POLIT RES QUART, V58, P609, DOI 10.2307/3595646
   Bateman S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2573
   Borgo R, 2012, IEEE T VIS COMPUT GR, V18, P2759, DOI 10.1109/TVCG.2012.197
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Braun K., 2006, Qualitative. Psychol. Res., V3, P77, DOI DOI 10.1191/1478088706QP063OA
   Bycoffe A., 2018, The New York Times
   Bycoffe A., 2018, Five ThirtyEight
   Bylinskii Z, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/3126594.3126653
   Byrne L, 2016, IEEE T VIS COMPUT GR, V22, P509, DOI 10.1109/TVCG.2015.2467321
   Caple H, 2012, VISUAL COMMUN-US, V11, P207, DOI 10.1177/1470357211434032
   Cawthon N, 2007, IEEE INT CONF INF VI, P637
   CHILDERS TL, 1984, J CONSUM RES, V11, P643, DOI 10.1086/209001
   Cockburn A., 2006, Proceedings of the ACM CHI Conference on Human Factors in Computing Systems, P1
   Correll M., 2020, P ACM C HUM FACT COM, P1
   Curran T, 2011, J COGNITIVE NEUROSCI, V23, P1247, DOI 10.1162/jocn.2010.21464
   Dziadosz S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P365
   Eastwood J., 2017, The Wall Street J.
   Grocer S., 2019, Five Thirty Eight
   Haroz S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1191, DOI 10.1145/2702123.2702275
   Harrison L, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1187, DOI 10.1145/2702123.2702545
   Heer J, 2008, IEEE T VIS COMPUT GR, V14, P1189, DOI 10.1109/TVCG.2008.137
   Hill S., 2017, P C INF SYST APPL RE
   Hullman J., 2013, P SIGCHI C HUM FACT, P2707
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2213, DOI 10.1109/TVCG.2011.175
   Inbar O., 2007, P ACM EUR C COGN ERG, P185, DOI DOI 10.1145/1362550.1362587
   Kaasten S, 2002, BCS CONF SERIES, P247
   Kelly J. D, 1988, P ANN M ASS ED JOURN, P1
   Kennedy H., 2020, Data Vis. Soc., V11, P169, DOI DOI 10.1515/9789048543137-015
   Kim H, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P116, DOI [10.1109/visual.2019.8933773, 10.1109/VISUAL.2019.8933773]
   Knox J.S., 2009, SOC SEMIOT, V19, P165
   Knox JS, 2009, DISCOURSE COMMUN, V3, P145, DOI 10.1177/1750481309102450
   Koerth M., 2018, Five Thirty Eight
   Kong HK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174012
   Kong N, 2012, IEEE T VIS COMPUT GR, V18, P2631, DOI 10.1109/TVCG.2012.229
   Kosara R, 2016, IEEE COMPUT GRAPH, V36, P80, DOI 10.1109/MCG.2016.2
   Kosara R, 2013, COMPUTER, V46, P44, DOI 10.1109/MC.2013.36
   Kruschke J., 2014, Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan
   Lam Heidi., 2005, P SIGCHI C HUMAN FAC, P681
   Lee B, 2015, IEEE COMPUT GRAPH, V35, P84, DOI 10.1109/MCG.2015.99
   Lee S, 2017, IEEE T VIS COMPUT GR, V23, P551, DOI 10.1109/TVCG.2016.2598920
   Li Z., 2008, Proceeding of the 17th international conference on World Wide Web, WWW '08, P21
   Matejka J., 2013, P ACM C HUM FACT COM, P1159
   McBride DM, 2002, CONSCIOUS COGN, V11, P423, DOI 10.1016/S1053-8100(02)00007-7
   Moere AV, 2011, INFORM VISUAL, V10, P356, DOI 10.1177/1473871611415996
   Nguyen PH, 2016, IEEE CONF VIS ANAL, P91, DOI 10.1109/VAST.2016.7883515
   Pandey AV, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1469, DOI 10.1145/2702123.2702608
   Phillips O., 2018, Five Thirty Eight
   Ren DH, 2017, IEEE PAC VIS SYMP, P230, DOI 10.1109/PACIFICVIS.2017.8031599
   Ritchie J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300423
   Robertson G., 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P153, DOI 10.1145/288392.288596
   Roeder O., 2018, Five ThirtyEight
   Satyanarayan A, 2014, COMPUT GRAPH FORUM, V33, P361, DOI 10.1111/cgf.12392
   SHEPARD RN, 1967, J VERB LEARN VERB BE, V6, P156, DOI 10.1016/S0022-5371(67)80067-7
   SIMKIN D, 1987, J AM STAT ASSOC, V82, P454, DOI 10.2307/2289447
   Skau D, 2015, COMPUT GRAPH FORUM, V34, P221, DOI 10.1111/cgf.12634
   Smith A, 2019, Majorities of Americans find it unacceptable to use algorithms to make decisions with real-world consequences for humans
   Song YL, 2016, Arxiv, DOI arXiv:1609.01388
   Stolper C.D., 2018, Data-Driven Storytelling
   Suh B., 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   T. E. Team, 2018, The Economist
   Teevan J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2023
   The New York Times, 2018, The New York Times
   The New York Times' Upshot, 2018, The New York Times
   Topkara M., 2012, P ACM C HUM FACT COM, P565
   Vande Moere A, 2012, IEEE T VIS COMPUT GR, V18, P2739, DOI 10.1109/TVCG.2012.221
   Whitehouse AJO, 2006, BRIT J DEV PSYCHOL, V24, P767, DOI 10.1348/026151005X74153
   Woodruff A., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P198, DOI 10.1145/365024.365098
   Yangandul C, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204571
   Yao G, 1999, BRIT J MATH STAT PSY, V52, P79, DOI 10.1348/000711099158973
   Yoghourdjian V, 2018, IEEE T VIS COMPUT GR, V24, P3081, DOI 10.1109/TVCG.2018.2790961
NR 78
TC 1
Z9 1
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4825
EP 4840
DI 10.1109/TVCG.2023.3278304
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400043
PM 37216254
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Lau, CK
   Xia, MH
   Wong, TT
AF Lau, Cheuk-Kit
   Xia, Menghan
   Wong, Tien-Tsin
TI Taming Reversible Halftoning Via Predictive Luminance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image restoration; Image color analysis; Image coding; Gray-scale;
   Color; Predictive coding; Logic gates; Blue-noise; deep learning;
   reversible halftoning
ID IMAGE COMPRESSION; ALGORITHM; COLOR; GRAYSCALE; GRAY
AB Traditional halftoning usually drops colors when dithering images with binary dots, which makes it difficult to recover the original color information. We proposed a novel halftoning technique that converts a color image into a binary halftone with full restorability to its original version. Our novel base halftoning technique consists of two convolutional neural networks (CNNs) to produce the reversible halftone patterns, and a noise incentive block (NIB) to mitigate the flatness degradation issue of CNNs. Furthermore, to tackle the conflicts between the blue-noise quality and restoration accuracy in our novel base method, we proposed a predictor-embedded approach to offload predictable information from the network, which in our case is the luminance information resembling from the halftone pattern. Such an approach allows the network to gain more flexibility to produce halftones with better blue-noise quality without compromising the restoration quality. Detailed studies on the multiple-stage training method and loss weightings have been conducted. We have compared our predictor-embedded method and our novel method regarding spectrum analysis on halftone, halftone accuracy, restoration accuracy, and the data embedding studies. Our entropy evaluation evidences our halftone contains less encoding information than our novel base method. The experiments show our predictor-embedded method gains more flexibility to improve the blue-noise quality of halftones and maintains a comparable restoration quality with a higher tolerance for disturbances.
C1 [Lau, Cheuk-Kit] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Xia, Menghan] Tencent AI Lab, Bellevue, WA 98004 USA.
C3 Chinese University of Hong Kong
RP Lau, CK (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
EM cklau21@cse.cuhk.edu.hk; menghanxyz@gmail.com; ttwong@cse.cuhk.edu.hk
OI Lau, Cheuk-Kit/0000-0003-3293-8362; Wong, Tien-Tsin/0000-0002-7792-9307
FU RGC [4055152]
FX This work was supported by RGC under Grant 4055152. Recommended
   foracceptance by T. Lee.
CR Anastassiou D., 1988, 1988 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No.88CH2458-8), P507, DOI 10.1109/ISCAS.1988.14975
   Ardizzone L, 2019, Arxiv, DOI arXiv:1907.02392
   Avinery R, 2019, PHYS REV LETT, V123, DOI 10.1103/PhysRevLett.123.178102
   Baronchelli A, 2005, EUR J PHYS, V26, pS69, DOI 10.1088/0143-0807/26/5/S08
   Bayers B., 1973, P IEEE INT C COMM, P2611
   Behrmann W., 2019, P 36 INT C MACHINE L, P573
   Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Chandu M., 2013, COLOR IMAGING
   Chang JH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618508
   Chen LM, 1997, IEEE T IMAGE PROCESS, V6, P1202, DOI 10.1109/83.605420
   CROUNSE KR, 1993, IEEE T CIRCUITS-II, V40, P267, DOI 10.1109/82.224318
   de Queiroz RL, 2006, IEEE T IMAGE PROCESS, V15, P1464, DOI 10.1109/TIP.2006.871181
   Dinh L, 2017, Arxiv, DOI arXiv:1605.08803
   ESCHBACH R, 1991, J OPT SOC AM A, V8, P1844, DOI 10.1364/JOSAA.8.001844
   Everingham M., 2012, PASCAL VISUAL OBJECT
   FLOYD RW, 1976, P SID, V17, P75
   Freitas PG, 2016, SIGNAL PROCESS-IMAGE, V49, P1, DOI 10.1016/j.image.2016.09.008
   Fung YH, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.1.013013
   Gailly M., 2004, Zlib compression library
   Gao Q., 2019, P IEEECVF INT C COMP, P4120
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   Hein Soren., 1993, Delta Modulators, V213, P133
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Huang WB, 2008, EXPERT SYST APPL, V34, P2491, DOI 10.1016/j.eswa.2007.04.013
   Jacobsen JH, 2018, Arxiv, DOI [arXiv:1802.07088, 10.48550/ARXIV.1802.07088, DOI 10.48550/ARXIV.1802.07088]
   Kim TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201377
   KIM YT, 1995, IEEE T IMAGE PROCESS, V4, P1296, DOI 10.1109/83.413173
   Kingma DP, 2018, ADV NEUR IN, V31
   Kite TD, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P59, DOI 10.1109/ICIP.1998.723317
   KNUTH DE, 1987, ACM T GRAPHIC, V6, P245, DOI 10.1145/35039.35040
   Kumar D., 2021, P INT C LEARN REPR
   Lagae A., 2008, P SIGGRAPH C CLASS, P93
   Lai JZC, 2003, J VIS COMMUN IMAGE R, V14, P389, DOI 10.1016/S1047-3203(03)00041-5
   Lee JH, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/430235
   Lee JH, 2009, SIGMAP 2009: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA APPLICATIONS, P86
   Li X, 2006, IEEE SIGNAL PROC LET, V13, P688, DOI 10.1109/LSP.2006.879465
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   LIMB JO, 1969, AT&T TECH J, V48, P2555, DOI 10.1002/j.1538-7305.1969.tb01187.x
   LIPPEL B, 1971, IEEE T COMMUN TECHN, VCO19, P879, DOI 10.1109/TCOM.1971.1090773
   Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316
   Luo T, 2014, IEEE IMAGE PROC, P5492, DOI 10.1109/ICIP.2014.7026111
   Mese M, 2001, IEEE T IMAGE PROCESS, V10, P1566, DOI 10.1109/83.951541
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   MITSA T, 1992, J OPT SOC AM A, V9, P1920, DOI 10.1364/JOSAA.9.001920
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ostromoukhov V, 2001, COMP GRAPH, P567, DOI 10.1145/383259.383326
   Pang Y. Qu, 2008, P ACM SIGGRAPH C PAP, P1
   R. I.-R. Bt, 2011, Tech. Rep. ITU-R BT.601-6
   ROETLING PG, 1976, J OPT SOC AM, V66, P985, DOI 10.1364/JOSA.66.000985
   SAID A, 1993, P SOC PHOTO-OPT INS, V2094, P664, DOI 10.1117/12.157984
   Shannon C. E., 2005, Eur. J. Phys., V26
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Son CH, 2014, IEEE T IMAGE PROCESS, V23, P2542, DOI 10.1109/TIP.2014.2319732
   Son CH, 2012, OPT LETT, V37, P2352, DOI 10.1364/OL.37.002352
   Spratling MW, 2008, VISION RES, V48, P1391, DOI 10.1016/j.visres.2008.03.009
   Spratling MW, 2017, BRAIN COGNITION, V112, P92, DOI 10.1016/j.bandc.2015.11.003
   Tan WM, 2020, IEEE T MULTIMEDIA, V22, P1730, DOI 10.1109/TMM.2019.2959925
   TING MY, 1994, IEEE T IMAGE PROCESS, V3, P854, DOI 10.1109/83.336256
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   Ulichney Robert, 1987, Digital Halftoning
   Unal GB, 2001, IEEE T IMAGE PROCESS, V10, P1836, DOI 10.1109/83.974568
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WONG PW, 1995, IEEE T IMAGE PROCESS, V4, P486, DOI 10.1109/83.370677
   Xia M., 2018, ACM Trans. Graphics, V37, P1
   Xia MH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275080
   Xia W., 2021, P IEEE CVF INT C COM, P14000
   Xiao Y, 2017, I C VIRTUAL REALITY, P213, DOI 10.1109/ICVRV.2017.00051
   Xing JB, 2022, Arxiv, DOI arXiv:2201.12576
   Xiong ZX, 1999, IEEE T IMAGE PROCESS, V8, P1479, DOI 10.1109/83.791977
   Xu ZX, 2017, SIGNAL PROCESS-IMAGE, V52, P111, DOI 10.1016/j.image.2016.12.005
   Yang WC, 2015, MULTIMED TOOLS APPL, V74, P7181, DOI 10.1007/s11042-014-1958-6
   Ye TZ, 2020, IEEE ACCESS, V8, P89670, DOI 10.1109/ACCESS.2020.2994148
   Yen YT, 2021, IEEE IMAGE PROC, P1734, DOI 10.1109/ICIP42928.2021.9506307
   Yu K. J., 1997, COLOR IMAGING DEVICE, P272
   Yue TW, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1450, DOI 10.1109/ICNN.1995.487373
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y, 2018, ENG APPL ARTIF INTEL, V72, P43, DOI 10.1016/j.engappai.2018.03.012
   Zhao R, 2021, IEEE T IMAGE PROCESS, V30, P6081, DOI 10.1109/TIP.2021.3091902
   Zhou BF, 2003, ACM T GRAPHIC, V22, P437, DOI 10.1145/882262.882289
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 82
TC 0
Z9 0
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4841
EP 4852
DI 10.1109/TVCG.2023.3278691
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400068
PM 37220038
OA Green Submitted, hybrid
DA 2024-11-06
ER

PT J
AU Lan, J
   Zhou, Z
   Xie, X
   Wu, YH
   Zhang, H
   Wu, YC
AF Lan, Ji
   Zhou, Zheng
   Xie, Xiao
   Wu, Yanhong
   Zhang, Hui
   Wu, Yingcai
TI MediVizor: Visual Mediation Analysis of Nominal Variables
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Exploratory causal analysis; information visualization; visual
   analytics; mediation analysis
ID ANALYTICS; MODEL; USER
AB Mediation analysis is crucial for diagnosing indirect causal relations in many scientific fields. However, mediation analysis of nominal variables requires examining and comparing multiple total effects and their corresponding direct/indirect causal effects derived from mediation models. This process is tedious and challenging to achieve with classical analysis tools such as Excel tables. In this study, we worked closely with experts from two scientific domains to design MediVizor, a visualization system that enables experts to conduct visual mediation analysis of nominal variables. The visualization design allows users to browse and compare multiple total effects together with the direct/indirect effects that compose them. The design also allows users to examine to what extent the positive and negative direct/indirect effects contribute to and reduce the total effects, respectively. We conducted two case studies separately with the experts from the two domains, sports and communication science, and a user study with common users to evaluate the system and design. The positive feedback from experts and common users demonstrates the effectiveness and generalizability of the system.
C1 [Lan, Ji; Wu, Yanhong; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhou, Zheng; Xie, Xiao; Zhang, Hui] Zhejiang Univ, Dept Sport Sci, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhou, Z (corresponding author), Zhejiang Univ, Dept Sport Sci, Hangzhou 310027, Zhejiang, Peoples R China.
EM lanjizju@zju.edu.cn; zhouzhengzju@zju.edu.cn; xxie@zju.edu.cn;
   yanhongwu@zju.edu.cn; zhang_hui@zju.edu.cn; ycwu@zju.edu.cn
RI zhang, hui/GXH-6098-2022; LAN, JI/M-2006-2018
OI , Hui/0000-0003-0601-3905; Wu, Yanhong/0009-0008-8762-5041; LAN,
   JI/0000-0002-8658-8620; Zhou, Zheng/0000-0002-6319-942X
FU NSFC [62072400, 62202424]; Collaborative Innovation Center of Artificial
   Intelligence
FX This work was supported in part by NSFC under Grants 62072400 and
   62202424 and in part by the Collaborative Innovation Center of
   Artificial Intelligence by MOE and Zhejiang Provincial Government (ZJU).
CR Bae J, 2017, COMPUT GRAPH FORUM, V36, P411, DOI 10.1111/cgf.13198
   BARON RM, 1986, J PERS SOC PSYCHOL, V51, P1173, DOI 10.1037/0022-3514.51.6.1173
   Barth L, 2019, INFORM VISUAL, V18, P110, DOI 10.1177/1473871618799500
   Bekos MA, 2010, ALGORITHMICA, V57, P436, DOI 10.1007/s00453-009-9283-6
   Dang Tuan Nhon, 2015, BMC Proc, V9, pS6, DOI 10.1186/1753-6561-9-S6-S6
   Deng ZK, 2022, IEEE T VIS COMPUT GR, V28, P1051, DOI 10.1109/TVCG.2021.3114875
   Dutilh G, 2009, PSYCHON B REV, V16, P1026, DOI 10.3758/16.6.1026
   Eells WC, 1926, J AM STAT ASSOC, V21, P119, DOI 10.2307/2277140
   Elmqvist N, 2003, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2003, PROCEEDINGS, P189, DOI 10.1109/INFVIS.2003.1249025
   Gershuny J., 2017, United Kingdom Time Use Survey, 2014-2015
   de Zúñiga HG, 2021, INFORM COMMUN SOC, V24, P201, DOI 10.1080/1369118X.2019.1642933
   Hoque MN, 2022, IEEE T VIS COMPUT GR, V28, P4728, DOI 10.1109/TVCG.2021.3102051
   Husain F, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P121, DOI [10.1109/VIS49827.2021.9623318, 10.1109/VIS49827.2021.00032]
   Imai K, 2010, PSYCHOL METHODS, V15, P309, DOI 10.1037/a0020761
   Ji XN, 2021, VIS INFORM, V5, P1, DOI 10.1016/j.visinf.2021.03.003
   Jin ZC, 2021, IEEE T VIS COMPUT GR, V27, P1343, DOI 10.1109/TVCG.2020.3030465
   Kadaba N.R., 2009, P 6 S APPL PERC GRAP, P77, DOI 10.1145/1620993.1621009
   Kadaba NR, 2007, IEEE T VIS COMPUT GR, V13, P1254, DOI 10.1109/TVCG.2007.70528
   Kale A, 2022, IEEE T VIS COMPUT GR, V28, P1150, DOI 10.1109/TVCG.2021.3114824
   Kim JJ, 2022, J VISUAL-JAPAN, V25, P741, DOI 10.1007/s12650-021-00825-4
   Kosara R., 2010, P BELIV 10 WORKSH TI, P63, DOI [10.1145/2110192.2110202, DOI 10.1145/2110192.2110202]
   Lam T., 2010, Synth. Lectures Visual., V1, P35
   Li CL, 2022, VIS INFORM, V6, P1, DOI 10.1016/j.visinf.2022.06.001
   Li H, 2021, J COMPUT-MEDIAT COMM, V26, P403, DOI 10.1093/jcmc/zmab016
   Lin JH, 2013, J COMMUN, V63, P682, DOI 10.1111/jcom.12044
   Lu JH, 2019, IEEE PAC VIS SYMP, P112, DOI 10.1109/PacificVis.2019.00021
   Mansoor H, 2021, VIS INFORM, V5, P39, DOI 10.1016/j.visinf.2021.07.001
   Mühlbacher T, 2013, IEEE T VIS COMPUT GR, V19, P1962, DOI 10.1109/TVCG.2013.125
   Osborne T, 2004, ECON SOC, V33, P430, DOI 10.1080/0308514042000285224
   Rifon NJ, 2004, J ADVERTISING, V33, P29, DOI 10.1080/00913367.2004.10639151
   Salanova M, 2005, J APPL PSYCHOL, V90, P1217, DOI 10.1037/0021-9010.90.6.1217
   Siirtola H, 2019, IEEE INT CON INF VIS, P151, DOI 10.1109/IV.2019.00034
   SIMKIN D, 1987, J AM STAT ASSOC, V82, P454, DOI 10.2307/2289447
   Sun GD, 2014, IEEE T VIS COMPUT GR, V20, P1753, DOI 10.1109/TVCG.2014.2346919
   Tingley D, 2014, J STAT SOFTW, V59
   Tominski C, 2021, VIS INFORM, V5, P28, DOI 10.1016/j.visinf.2021.06.004
   Wang HX, 2021, VIS INFORM, V5, P82, DOI 10.1016/j.visinf.2021.09.001
   Wang J, 2023, IEEE T VIS COMPUT GR, V29, P5342, DOI 10.1109/TVCG.2022.3207929
   Wang J, 2017, IEEE CONF VIS ANAL, P151, DOI 10.1109/VAST.2017.8585647
   Wang J, 2016, IEEE T VIS COMPUT GR, V22, P230, DOI 10.1109/TVCG.2015.2467931
   Xie X, 2021, IEEE T VIS COMPUT GR, V27, P1448, DOI 10.1109/TVCG.2020.3028957
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P853, DOI 10.1109/TVCG.2019.2934399
   Xu PP, 2013, IEEE T VIS COMPUT GR, V19, P2012, DOI 10.1109/TVCG.2013.221
   Yang YL, 2017, IEEE T VIS COMPUT GR, V23, P411, DOI 10.1109/TVCG.2016.2598885
   Yen CHE, 2019, COMPUT GRAPH FORUM, V38, P173, DOI 10.1111/cgf.13680
   Yoon GY, 2023, J VISUAL-JAPAN, V26, P289, DOI 10.1007/s12650-022-00876-1
   Zhang HJ, 2023, J VISUAL-JAPAN, V26, P723, DOI 10.1007/s12650-022-00899-8
   Zhao J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P259, DOI 10.1145/2702123.2702419
NR 48
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4853
EP 4866
DI 10.1109/TVCG.2023.3282801
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400029
PM 37276102
DA 2024-11-06
ER

PT J
AU Mao, AH
   Yang, Z
   Chen, WX
   Yi, R
   Liu, YJ
AF Mao, Aihua
   Yang, Zhi
   Chen, Wanxin
   Yi, Ran
   Liu, Yong-Jin
TI Complete 3D Relationships Extraction Modality Alignment Network for 3D
   Dense Captioning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D dense captioning; multi-modal learning; 3D spatial relationship;
   modality alignment
AB 3D dense captioning aims to semantically describe each object detected in a 3D scene, which plays a significant role in 3D scene understanding. Previous works lack a complete definition of 3D spatial relationships and the directly integrate visual and language modalities, thus ignoring the discrepancies between the two modalities. To address these issues, we propose a novel complete 3D relationship extraction modality alignment network, which consists of three steps: 3D object detection, complete 3D relationships extraction, and modality alignment caption. To comprehensively capture the 3D spatial relationship features, we define a complete set of 3D spatial relationships, including the local spatial relationship between objects and the global spatial relationship between each object and the entire scene. To this end, we propose a complete 3D relationships extraction module based on message passing and self-attention to mine multi-scale spatial relationship features and inspect the transformation to obtain features in different views. In addition, we propose the modality alignment caption module to fuse multi-scale relationship features and generate descriptions to bridge the semantic gap from the visual space to the language space with the prior information in the word embedding, and help generate improved descriptions for the 3D scene. Extensive experiments demonstrate that the proposed model outperforms the state-of-the-art methods on the ScanRefer and Nr3D datasets.
C1 [Mao, Aihua; Yang, Zhi; Chen, Wanxin] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Yi, Ran] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100190, Peoples R China.
C3 South China University of Technology; Shanghai Jiao Tong University;
   Tsinghua University
RP Mao, AH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.; Yi, R (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM ahmao@scut.edu.cn; 202021044575@mail.scut.edu.cn;
   adrien.chenwx@gmail.com; ranyi@sjtu.edu.cn; liuyongjin@tsinghua.edu.cn
RI Yi, Ran/AAU-6636-2021
OI Yang, Zhi/0000-0002-9768-2356; Yi, Ran/0000-0003-1858-3358; Aihua,
   Mao/0000-0001-6861-9414
FU NSF of Guangdong Province [2022A1515011573]; Beijing Natural Science
   Foundation [L222008]; Shanghai Sailing Program [22YF1420300];
   CCF-Tencent Open Research Fund [RAGR20220121]; Young Elite Scientists
   Sponsorship Program by CAST [2022QNRC001]; National Natural Science
   Foundation of China [62272447]
FX This work was supported in part by the NSF of Guangdong Province under
   Grant 2022A1515011573, in part by the Beijing Natural Science Foundation
   under Grant L222008, in part by the Shanghai Sailing Program under Grant
   22YF1420300, in part by CCF-Tencent Open Research Fund under Grant
   RAGR20220121, in part by the Young Elite Scientists Sponsorship Program
   by CAST under Grant 2022QNRC001, and in part by the National Natural
   Science Foundation of China under Grant 62272447.
CR Achlioptas Panos, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P422, DOI 10.1007/978-3-030-58452-8_25
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Azuma D., 2021, arXiv
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65, DOI DOI 10.3115/1626355.1626389
   Chen D.Z., 2020, COMP VIS ECCV 202 20, P202
   Chen DZ, 2021, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR46437.2021.00321
   Chen K, 2019, LECT NOTES COMPUT SC, V11363, P100, DOI 10.1007/978-3-030-20893-6_7
   Cornia M, 2020, PROC CVPR IEEE, P10575, DOI 10.1109/CVPR42600.2020.01059
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong XZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2615, DOI 10.1145/3474085.3475439
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu J., 2018, P AAAI C ART INT
   Han ZZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1018, DOI 10.1145/3394171.3413889
   He DL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2344, DOI 10.1145/3474085.3475397
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang PH, 2021, AAAI CONF ARTIF INTE, V35, P1610
   Kant Yash, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P715, DOI 10.1007/978-3-030-58545-7_41
   Kinghorn P, 2018, NEUROCOMPUTING, V272, P416, DOI 10.1016/j.neucom.2017.07.014
   Li LH, 2017, AAAI CONF ARTIF INTE, P4133
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li PG, 2021, IEEE T MULTIMEDIA, V24, P3455, DOI 10.1109/TMM.2021.3098988
   Li X., 2020, EUR C COMP VIS, P121
   Li Y., 2021, IEEE Trans.Multimedia, V25, P543
   Lin CY, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P150
   Lu JS, 2019, ADV NEUR IN, V32
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Kipf TN, 2017, Arxiv, DOI [arXiv:1609.02907, 10.48550/arXiv.1609.02907]
   Pan YW, 2020, PROC CVPR IEEE, P10968, DOI 10.1109/CVPR42600.2020.01098
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140
   Pennington J, 2014, PROCEEDING 2014 C EM, P1532, DOI DOI 10.3115/V1/D14-1162
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Su WJ, 2019, ANN NUTR METAB, V75, P31, DOI 10.1159/000501710
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang J, 2021, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR46437.2021.00136
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan X, 2023, Arxiv, DOI arXiv:2112.11691
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Ye SQ, 2022, Arxiv, DOI arXiv:2112.08359
   Yuan Z., 2022, arXiv
   Yuan ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1771, DOI 10.1109/ICCV48922.2021.00181
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
NR 53
TC 1
Z9 1
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4867
EP 4880
DI 10.1109/TVCG.2023.3279204
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400050
PM 37220037
DA 2024-11-06
ER

PT J
AU Tan, BY
   Qin, HX
   Zhang, XX
   Wang, YQ
   Xiang, T
   Chen, BQ
AF Tan, Boyuan
   Qin, Hongxing
   Zhang, Xiaoxi
   Wang, Yiqun
   Xiang, Tao
   Chen, Baoquan
TI Using Multi-Level Consistency Learning for Partial-to-Partial Point
   Cloud Registration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud compression; Feature extraction; Task analysis;
   Three-dimensional displays; Pipelines; Learning systems; Visualization;
   Attention; consistency; partial overlap; point cloud registration
ID CONSENSUS
AB Point cloud registration is a basic task in computer vision and computer graphics. Recently, deep learning-based end-to-end methods have made great progress in this field. One of the challenges of these methods is to deal with partial-to-partial registration tasks. In this work, we propose a novel end-to-end framework called MCLNet that makes full use of multi-level consistency for point cloud registration. First, the point-level consistency is exploited to prune points located outside overlapping regions. Second, we propose a multi-scale attention module to perform consistency learning at the correspondence-level for obtaining reliable correspondences. To further improve the accuracy of our method, we propose a novel scheme to estimate the transformation based on geometric consistency between correspondences. Compared to baseline methods, experimental results show that our method performs well on smaller-scale data, especially with exact matches. The reference time and memory footprint of our method are relatively balanced, which is more beneficial for practical applications.
C1 [Tan, Boyuan] Chongqing Univ Posts & Telecommun, Key Lab Data Engn & Visual Comp, Chongqing 400065, Peoples R China.
   [Qin, Hongxing] Chongqing Univ, Chongqing 400065, Peoples R China.
   [Qin, Hongxing] Chongqing Univ Posts & Telecommun, Chongqing 400065, Peoples R China.
   [Zhang, Xiaoxi] Sichuan Int Studies Univ, Chongqing 400031, Peoples R China.
   [Wang, Yiqun; Xiang, Tao] Chongqing Univ, Chongqing 400044, Peoples R China.
   [Chen, Baoquan] Peking Univ, Beijing 100871, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Chongqing
   University; Chongqing University of Posts & Telecommunications; Sichuan
   International Studies University; Chongqing University; Peking
   University
RP Qin, HX (corresponding author), Chongqing Univ, Chongqing 400065, Peoples R China.
EM tanboyuan20@163.com; qinhx@cqu.edu.cn; xiaoxizhang@sisu.edu.cn;
   yiqun.wang@cqu.edu.cn; txiang@cqu.edu.cn; baoquan@pku.edu.cn
RI 张, 啸西/JZD-5139-2024
OI Wang, Yiqun/0000-0003-1942-5597
FU National Key R&D Program of China [2022ZD0160804]; NSFC [62272071]; NSFC
   of Chongqing [CSTB2022NSCQ-MSX0924]; Sichuan Science and Technology
   Program [2021YFQ0056]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2022ZD0160804, in part by NSFC under Grant 62272071, in part
   by NSFC of Chongqing CSTB2022NSCQ-MSX0924, and in part by Sichuan
   Science and Technology Program under Grant 2021YFQ0056.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Bai XY, 2021, PROC CVPR IEEE, P15854, DOI 10.1109/CVPR46437.2021.01560
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Cao AQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13209, DOI 10.1109/ICCV48922.2021.01298
   Chen H., 2022, P ACM SIGGRAPH C P, P1
   Chen Z, 2022, PROC CVPR IEEE, P13211, DOI 10.1109/CVPR52688.2022.01287
   Choi S, 2016, Arxiv, DOI [arXiv:1602.02481, 10.48550/arXiv.1602.02481, DOI 10.48550/ARXIV.1602.02481]
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu KX, 2021, PROC CVPR IEEE, P8889, DOI [10.1109/CVPR46437.2021.00878, 10.1109/TPAMI.2022.3204713]
   Glorot X., 2011, JMLR WORKSHOP C P, P315, DOI DOI 10.1002/ECS2.1832
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hezroni I, 2021, Arxiv, DOI arXiv:2110.03016
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kingma D.P., 2014, P INT C LEARNING REP
   Li J., 2020, Lecture Notes in Computer Science), P378, DOI 10.1007/978-3-030-58586-0_23
   Paszke A, 2019, ADV NEUR IN, V32
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Quan SW, 2020, IEEE T GEOSCI REMOTE, V58, P7380, DOI 10.1109/TGRS.2020.2982221
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusinkiewicz S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323037
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sarode V, 2019, Arxiv, DOI [arXiv:1908.07906, DOI 10.48550/ARXIV.1908.07906]
   Sarode V, 2020, INT CONF 3D VISION, P1029, DOI 10.1109/3DV50981.2020.00113
   Segal A., 2009, P ROBOTICS SCI SYSTE, VVolume 5, P168
   SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2019, P IEEE C NEUR INF PR, P8814, DOI DOI 10.48550/ARXIV.1910.12240
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu H, 2022, AAAI CONF ARTIF INTE, P2848
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Yan ZH, 2022, IEEE T VIS COMPUT GR, V28, P4304, DOI 10.1109/TVCG.2021.3086113
   Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184
   Yang JQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3058552
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Yew ZJ, 2020, PROC CVPR IEEE, P11821, DOI 10.1109/CVPR42600.2020.01184
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhao C., 2021, arXiv
   Zhou QY, 2018, Arxiv, DOI [arXiv:1801.09847, 10.48550/arXiv.1801.09847, DOI 10.48550/ARXIV.1801.09847]
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
NR 49
TC 3
Z9 3
U1 8
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4881
EP 4894
DI 10.1109/TVCG.2023.3280171
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400003
PM 37235469
DA 2024-11-06
ER

PT J
AU van den Brandt, A
   Jonkheer, EM
   van Workum, DJM
   van de Wetering, H
   Smit, S
   Vilanova, A
AF van den Brandt, Astrid
   Jonkheer, Eef M.
   van Workum, Dirk-Jan M.
   van de Wetering, Huub
   Smit, Sandra
   Vilanova, Anna
TI PanVA: Pangenomic Variant Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; design study; pangenomics; comparative genomics;
   variant analysis
ID SEQUENCE; ALIGNMENT; DESIGN
AB Genomics researchers increasingly use multiple reference genomes to comprehensively explore genetic variants underlying differences in detectable characteristics between organisms. Pangenomes allow for an efficient data representation of multiple related genomes and their associated metadata. However, current visual analysis approaches for exploring these complex genotype-phenotype relationships are often based on single reference approaches or lack adequate support for interpreting the variants in the genomic context with heterogeneous (meta)data. This design study introduces PanVA, a visual analytics design for pangenomic variant analysis developed with the active participation of genomics researchers. The design uniquely combines tailored visual representations with interactions such as sorting, grouping, and aggregation, allowing users to navigate and explore different perspectives on complex genotype-phenotype relations. Through evaluation in the context of plants and pathogen research, we show that PanVA helps researchers explore variants in genes and generate hypotheses about their role in phenotypic variation.
C1 [van den Brandt, Astrid; van de Wetering, Huub; Vilanova, Anna] Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
   [Jonkheer, Eef M.; van Workum, Dirk-Jan M.; Smit, Sandra] Wageningen Univ & Res, Bioinformat Grp, NL-6708 PB Wageningen, Netherlands.
C3 Eindhoven University of Technology; Wageningen University & Research
RP van den Brandt, A (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
EM a.v.d.brandt@tue.nl; eef.jonkheer@wur.nl; dirk-jan.vanworkum@wur.nl;
   h.v.d.wetering@tue.nl; sandra.smit@wur.nl; a.vilanova@tue.nl
RI Smit, Sandra/E-6787-2010
OI van den Brandt, Astrid/0000-0002-3676-1341; van Workum,
   Dirk-Jan/0000-0001-6247-5499; Jonkheer, Eef/0000-0002-2608-6311;
   Vilanova, Anna/0000-0002-1034-737X; van de Wetering,
   Huub/0000-0002-0517-1322
FU Dutch Top Consortium for Knowledge and Innovation (TKI) Agri Food
   [TU18034]
FX This work was supported in part by the Dutch Top Consortium for
   Knowledge and Innovation (TKI) Agri & Food under Grant TU18034.
CR Alonso-Blanco C, 2016, CELL, V166, P481, DOI 10.1016/j.cell.2016.05.063
   Bandeira V, 2020, IEEE INT NEW CIRC, P74, DOI [10.1109/newcas49341.2020.9159791, 10.1109/NEWCAS49341.2020.9159791]
   Bayer PE, 2020, NAT PLANTS, V6, P914, DOI 10.1038/s41477-020-0733-0
   Bederson B. B., 1999, INFOVIS 99
   Carriço JA, 2018, ALGORITHM MOL BIOL, V13, DOI 10.1186/s13015-017-0119-7
   Chen YA, 2010, BMC GENOMICS, V11, DOI 10.1186/1471-2164-11-293
   Cigna J, 2017, PLANT DIS, V101, P1278, DOI 10.1094/PDIS-12-16-1810-RE
   Computomics, 2022, Computomics GmbH
   Diesh C, 2023, GENOME BIOL, V24, DOI 10.1186/s13059-023-02914-z
   Ding W, 2018, NUCLEIC ACIDS RES, V46, DOI 10.1093/nar/gkx977
   Donlin M. J., 2009, Curr. Protoc. Bioinf., V28, P1
   Duda R. O., 2001, PATTERN CLASSIFICATI
   Eades P., 1991, Tech. Rep. IIAS-RR-91-16E
   Fernandez NF, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.151
   Ferstay JA, 2013, IEEE T VIS COMPUT GR, V19, P2546, DOI 10.1109/TVCG.2013.214
   Gautreau G, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1007732
   Gleicher M, 2018, IEEE T VIS COMPUT GR, V24, P413, DOI 10.1109/TVCG.2017.2744199
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Glueck M, 2017, IEEE T VIS COMPUT GR, V23, P191, DOI 10.1109/TVCG.2016.2598469
   Golicz AA, 2020, TRENDS GENET, V36, P132, DOI 10.1016/j.tig.2019.11.006
   Guarracino A, 2022, BIOINFORMATICS, V38, P3319, DOI 10.1093/bioinformatics/btac308
   Guo Y, 2022, IEEE T VIS COMPUT GR, V28, P5091, DOI 10.1109/TVCG.2021.3100413
   Heinrich J, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-S8-S2
   Hurgobin B, 2017, BIOLOGY-BASEL, V6, DOI 10.3390/biology6010021
   Huson D.H., 2010, Phylogenetic Networks: Concepts, Algorithms and Applications, V1st, DOI DOI 10.1080/10635150701313830
   Jiao WB, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-14779-y
   Jonkheer EM, 2022, BIOINFORMATICS, V38, P4403, DOI 10.1093/bioinformatics/btac506
   Jonkheer EM, 2021, BMC GENOMICS, V22, DOI 10.1186/s12864-021-07583-5
   Kearse M, 2012, BIOINFORMATICS, V28, P1647, DOI 10.1093/bioinformatics/bts199
   Kerzner E, 2019, IEEE T VIS COMPUT GR, V25, P748, DOI 10.1109/TVCG.2018.2865241
   Kultys M., 2014, BMC Proc., V8, P1
   L'Yi S, 2022, IEEE T VIS COMPUT GR, V28, P140, DOI 10.1109/TVCG.2021.3114876
   Lam HM, 2010, NAT GENET, V42, P1053, DOI 10.1038/ng.715
   Letunic I, 2021, NUCLEIC ACIDS RES, V49, pW293, DOI 10.1093/nar/gkab301
   Magallanes J, 2022, IEEE T VIS COMPUT GR, V28, P901, DOI 10.1109/TVCG.2021.3114868
   Malik S, 2016, ACM T INTERACT INTEL, V6, DOI 10.1145/2890478
   Marschall T, 2018, BRIEF BIOINFORM, V19, P118, DOI 10.1093/bib/bbw089
   Mayor C, 2000, BIOINFORMATICS, V16, P1046, DOI 10.1093/bioinformatics/16.11.1046
   Meyer M, 2010, COMPUT GRAPH FORUM, V29, P1043, DOI 10.1111/j.1467-8659.2009.01710.x
   Meyer M, 2009, IEEE T VIS COMPUT GR, V15, P897, DOI 10.1109/TVCG.2009.167
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Nobre C, 2019, COMPUT GRAPH FORUM, V38, P807, DOI 10.1111/cgf.13728
   Nobre C, 2019, IEEE T VIS COMPUT GR, V25, P1543, DOI 10.1109/TVCG.2018.2811488
   Nusrat S, 2019, COMPUT GRAPH FORUM, V38, P781, DOI 10.1111/cgf.13727
   Paten B, 2017, GENOME RES, V27, P665, DOI 10.1101/gr.214155.116
   Pendergrass SA, 2012, BIODATA MIN, V5, DOI 10.1186/1756-0381-5-5
   Roberts JC, 2016, IEEE T VIS COMPUT GR, V22, P419, DOI 10.1109/TVCG.2015.2467271
   Rogers J, 2021, IEEE T VIS COMPUT GR, V27, P1106, DOI 10.1109/TVCG.2020.3030405
   Ruddle RA, 2022, IEEE T VIS COMPUT GR, V28, P3070, DOI 10.1109/TVCG.2021.3050497
   SCHNEIDER TD, 1990, NUCLEIC ACIDS RES, V18, P6097, DOI 10.1093/nar/18.20.6097
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Seo J, 2002, COMPUTER, V35, P80
   Sheikhizadeh S, 2016, BIOINFORMATICS, V32, P487, DOI 10.1093/bioinformatics/btw455
   Sievers F, 2011, MOL SYST BIOL, V7, DOI 10.1038/msb.2011.75
   Slack J., 2004, German Conference on Bioinformatics, P37
   Smit S., 2022, Visual analytics for plant pangenomes
   Sun C, 2017, NUCLEIC ACIDS RES, V45, P597, DOI 10.1093/nar/gkw958
   Thorvaldsdóttir H, 2013, BRIEF BIOINFORM, V14, P178, DOI 10.1093/bib/bbs017
   Treangen TJ, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0524-x
   Venkatachalam B, 2010, IEEE ACM T COMPUT BI, V7, P588, DOI 10.1109/TCBB.2010.57
   Walkowiak S, 2020, NATURE, V588, DOI 10.1038/s41586-020-2961-x
   Waterhouse AM, 2009, BIOINFORMATICS, V25, P1189, DOI 10.1093/bioinformatics/btp033
   Wilkinson L, 2009, AM STAT, V63, P179, DOI 10.1198/tas.2009.0033
   Paz MW, 2022, BIOINFORM ADV, V2, DOI 10.1093/bioadv/vbac075
NR 64
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4895
EP 4909
DI 10.1109/TVCG.2023.3282364
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400084
PM 37267130
OA Green Published
DA 2024-11-06
ER

PT J
AU Tajdari, F
   Huysmans, T
   Song, Y
AF Tajdari, Farzam
   Huysmans, Toon
   Song, Yu
TI Non-Rigid Registration Via Intelligent Adaptive Feedback Control
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deformation; Shape; Surface treatment; Cost function; Geometry;
   Three-dimensional displays; Iterative methods; Adaptive control; global
   asymptotic stability; ANFIS predictor; mesh quality; shape descriptor;
   non-rigid registration
AB Preserving features or local shape characteristics of a mesh using conventional non-rigid registration methods is always difficult, as the preservation and deformation are competing with each other. The challenge is to find a balance between these two terms in the process of the registration, especially in presence of artefacts in the mesh. We present a non-rigid Iterative Closest Points (ICP) algorithm which addresses the challenge as a control problem. An adaptive feedback control scheme with global asymptotic stability is derived to control the stiffness ratio for maximum feature preservation and minimum mesh quality loss during the registration process. A cost function is formulated with the distance term and the stiffness term where the initial stiffness ratio value is defined by an Adaptive Neuro-Fuzzy Inference System (ANFIS)-based predictor regarding the source mesh and the target mesh topology, and the distance between the correspondences. During the registration process, the stiffness ratio of each vertex is continuously adjusted by the intrinsic information, represented by shape descriptors, of the surrounding surface as well as the steps in the registration process. Besides, the estimated process-dependent stiffness ratios are used as dynamic weights for establishing the correspondences in each step of the registration. Experiments on simple geometric shapes as well as 3D scanning datasets indicated that the proposed approach outperforms current methodologies, especially for the regions where features are not eminent and/or there exist interferences between/among features, due to its ability to embed the inherent properties of the surface in the process of the mesh registration.
C1 [Tajdari, Farzam; Huysmans, Toon; Song, Yu] Delft Univ Technol, Fac Ind Design Engn, NL-2628 Delft, Netherlands.
   [Tajdari, Farzam] Tech Univ Eindhoven, Dept Mech Engn, Dynam & Control D&C Grp, NL-5612 AZ Eindhoven, Netherlands.
   [Huysmans, Toon] Univ Antwerp, Imec Vis Lab, Dept Phys, B-2000 Antwerp, Belgium.
C3 Delft University of Technology; Eindhoven University of Technology;
   University of Antwerp; Interuniversity Microelectronics Centre
RP Tajdari, F (corresponding author), Delft Univ Technol, Fac Ind Design Engn, NL-2628 Delft, Netherlands.
EM f.tajdari@tudelft.nl; t.huysmans@tudelft.nl; y.song@tudelft.nl
RI ; Song, Yu/M-6102-2017
OI Huysmans, Toon/0000-0001-7053-6458; Song, Yu/0000-0002-9542-1312
FU Dutch NWO Next UPPS - Integrated design methodology for Ultra
   Personalised Products and Services project [15470]
FX No Statement Available
CR Aguilar WG, 2017, LECT NOTES COMPUT SC, V10306, P585, DOI 10.1007/978-3-319-59147-6_50
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   Antepara O, 2021, INT J NUMER METH FL, V93, P481, DOI 10.1002/fld.4893
   Astrom K J., 1995, PID Controllers: Theory, Design, and Tuning, V2
   Blackwell S., 2002, Tech. Rep.2002-06-01, V1
   Charlie N, 2020, nricp - Non-rigid iterative closest point
   Chaudhury A, 2020, IEEE T IMAGE PROCESS, V29, P8735, DOI 10.1109/TIP.2020.3019649
   Cirrottola L, 2021, J COMPUT PHYS, V433, DOI 10.1016/j.jcp.2021.110177
   Dai H, 2018, ICEBT 2018: PROCEEDINGS OF THE 2018 2ND INTERNATIONAL CONFERENCE ON E-EDUCATION, E-BUSINESS AND E-TECHNOLOGY, P48, DOI 10.1145/3241748.3241773
   Davis TA, 2004, ACM T MATH SOFTWARE, V30, P196, DOI 10.1145/992200.992206
   Dekker M, 1986, Mathematical Programming, V4
   Dryden I.L., 2016, Statistical Shape Analysis: with Applications in R, V995
   Fan JF, 2019, MED IMAGE ANAL, V54, P193, DOI 10.1016/j.media.2019.03.006
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Gaudio JE, 2022, IEEE T AUTOMAT CONTR, V67, P5440, DOI 10.1109/TAC.2021.3126243
   Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722
   Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004
   Hirose O, 2020, Executable source code
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Ibragimov B., 2017, Statistical Shape and Deformation Analysis, P89
   Jiang T, 2017, VISUAL COMPUT, V33, P891, DOI 10.1007/s00371-017-1390-9
   Jin C, 2020, J COMPUT CIVIL ENG, V34, DOI 10.1061/(ASCE)CP.1943-5487.0000902
   Jin HZ, 2018, IEEE-ASME T MECH, V23, P2896, DOI 10.1109/TMECH.2018.2873232
   Khalil HK, 1996, Noninear Systems
   Kim D, 2010, IEEE SIGNAL PROC LET, V17, P402, DOI 10.1109/LSP.2009.2039888
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Lee WS, 2000, IMAGE VISION COMPUT, V18, P355, DOI 10.1016/S0262-8856(99)00057-8
   Lewis D. Vrabie, 2012, Optimal Control
   Li K, 2019, IEEE T VIS COMPUT GR, V25, P2255, DOI 10.1109/TVCG.2018.2832136
   Liang LM, 2018, OPT LASER ENG, V100, P141, DOI 10.1016/j.optlaseng.2017.08.005
   LIU A, 1994, BIT, V34, P268, DOI 10.1007/BF01955874
   MATLAB, 2019, Fuzzy Membership Function in MATLAB
   Matsopoulos GK, 2005, MED IMAGE ANAL, V9, P237, DOI 10.1016/j.media.2004.09.002
   Min CB, 2021, IET IMAGE PROCESS, V15, P1144, DOI 10.1049/ipr2.12093
   Minnoye ALM, 2022, PROCEEDINGS OF ASME 2022 INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, IDETC-CIE2022, VOL 2
   Myronenko A., 2010, Executable source code
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Oktay O, 2017, IEEE T MED IMAGING, V36, P332, DOI 10.1109/TMI.2016.2597270
   Peters JR, 2015, SPINE J, V15, P1000, DOI 10.1016/j.spinee.2015.01.016
   Qingqiong Deng, 2010, 2010 IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS 2010), P888, DOI 10.1109/ICICISYS.2010.5658814
   RUSSIAN3DSCANNER, 2019, Wrap 3.4
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Shirani S, 2006, IEEE T MULTIMEDIA, V8, P411, DOI 10.1109/TMM.2005.864349
   Sketchfab, 2020, Thoracic-vertebrae 3D models
   Slotine J.-J. E., 1991, Applied nonlinear control
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tajdari Farzam, 2020, Proceedings of the 10th International Conference on Computer and Knowledge Engineering (ICCKE 2020), P579, DOI 10.1109/ICCKE50421.2020.9303652
   Tajdari F, 2022, PROCEEDINGS OF ASME 2022 INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, IDETC-CIE2022, VOL 2
   Tajdari F, 2022, PROCEEDINGS OF ASME 2022 INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, IDETC-CIE2022, VOL 2
   Tajdari F, 2023, J VIB CONTROL, V29, P5511, DOI 10.1177/10775463221137141
   Tajdari F, 2021, IEEE INT CONF ROBOT, P12701, DOI 10.1109/ICRA48506.2021.9561010
   Tajdari F, 2022, IEEE T IMAGE PROCESS, V31, P1841, DOI 10.1109/TIP.2022.3148822
   Tajdari F, 2021, IEEE T VEH TECHNOL, V70, P8567, DOI 10.1109/TVT.2021.3099736
   Tajdari F, 2021, IFAC PAPERSONLINE, V54, P271, DOI 10.1016/j.ifacol.2021.06.051
   Tajdari F, 2022, J VIB CONTROL, V28, P2678, DOI 10.1177/10775463211019177
   Tajdari F, 2022, IEEE T INTELL TRANSP, V23, P939, DOI 10.1109/TITS.2020.3018873
   Tajdari F, 2019, RSI INT CONF ROBOT M, P248, DOI [10.1109/icrom48714.2019.9071883, 10.1109/ICRoM48714.2019.9071883]
   Tajdari M, 2022, ENG COMPUT-GERMANY, V38, P4061, DOI 10.1007/s00366-022-01742-2
   Tajdari M, 2021, COMPUT METHOD APPL M, V374, DOI 10.1016/j.cma.2020.113590
   Tang S, 2012, PROC SPIE, V8290, DOI 10.1117/12.912153
   Tazir M. L., 2018, P INT C INT AUT SYST, P730
   Toschi I, 2021, ISPRS J PHOTOGRAMM, V172, P160, DOI 10.1016/j.isprsjprs.2020.12.005
   Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430
   Visioli A, 2006, ADV IND CONTROL, P1
   Wang SB, 2020, IEEE T IND INFORM, V16, P6816, DOI 10.1109/TII.2020.2971056
   Wilke HJ, 2021, J ANAT, V238, P626, DOI 10.1111/joa.13323
   Williams R. L, 2007, Linear Stat.-Space Control Systems
   Xi P., 2007, Graphics Interface Conference 2007, P19
   Xi P, 2007, Anal. Segmented Hum.Body Scans, P19
   Yang JY, 2015, COMPUT GRAPH FORUM, V34, P89, DOI 10.1111/cgf.12699
   Yao ZW, 2021, MEASUREMENT, V177, DOI 10.1016/j.measurement.2021.109274
   Zhang J, 2018, CHINA PERSPECTIVE, P1, DOI 10.1080/10255842.2018.1484914
NR 72
TC 1
Z9 1
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4910
EP 4926
DI 10.1109/TVCG.2023.3283990
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400074
PM 37289615
DA 2024-11-06
ER

PT J
AU Zhang, J
   Zhou, KN
   Luximon, Y
   Lee, TY
   Li, P
AF Zhang, Jie
   Zhou, Kangneng
   Luximon, Yan
   Lee, Tong-Yee
   Li, Ping
TI MeshWGAN: Mesh-to-Mesh Wasserstein GAN With Multi-Task Gradient Penalty
   for 3D Facial Geometric Age Transformation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Age transformation; 3D face geometry; MeshWGAN; mesh generative
   adversarial networks; multi-task gradient penalty
ID FACE; IMAGE
AB As the metaverse develops rapidly, 3D facial age transformation is attracting increasing attention, which may bring many potential benefits to a wide variety of users, e.g., 3D aging figures creation, 3D facial data augmentation and editing. Compared with 2D methods, 3D face aging is an underexplored problem. To fill this gap, we propose a new mesh-to-mesh Wasserstein generative adversarial network (MeshWGAN) with a multi-task gradient penalty to model a continuous bi-directional 3D facial geometric aging process. To the best of our knowledge, this is the first architecture to achieve 3D facial geometric age transformation via real 3D scans. As previous image-to-image translation methods cannot be directly applied to the 3D facial mesh, which is totally different from 2D images, we built a mesh encoder, decoder, and multi-task discriminator to facilitate mesh-to-mesh transformations. To mitigate the lack of 3D datasets containing children's faces, we collected scans from 765 subjects aged 5-17 in combination with existing 3D face databases, which provided a large training dataset. Experiments have shown that our architecture can predict 3D facial aging geometries with better identity preservation and age closeness compared to 3D trivial baselines. We also demonstrated the advantages of our approach via various 3D face-related graphics applications.
C1 [Zhang, Jie; Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Zhang, Jie; Luximon, Yan; Li, Ping] Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
   [Zhou, Kangneng] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
   [Luximon, Yan] Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University;
   University of Science & Technology Beijing; National Cheng Kung
   University
RP Li, P (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.; Luximon, Y; Li, P (corresponding author), Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
EM peterzhang1130@163.com; elliszkn@163.com; yan.luximon@polyu.edu.hk;
   tonylee@ncku.edu.tw; p.li@polyu.edu.hk
RI Li, Ping/AAO-2019-2020; Luximon, Yan/A-7946-2010
OI Li, Ping/0000-0002-1503-0240; Zhang, Jie/0000-0001-8219-5590; Zhou,
   Kangneng/0000-0001-9102-9527; Luximon, Yan/0000-0003-2843-847X
FU Research Grants Council of Hong Kong [PolyU 15603419]; National Science
   and Technology Council, Taiwan [110-2221-E-006-135-MY3]; Hong Kong
   Polytechnic University [P0042740, P0030419, P0043906, P0044520]
FX This work was supported in part by the Research Grants Council of Hong
   Kong under Grant PolyU 15603419, in part by the National Science and
   Technology Council under Grant 110-2221-E-006-135-MY3, Taiwan, and in
   part by The Hong Kong Polytechnic University under Grants P0042740,
   P0030419, P0043906, and P0044520.
CR Alaluf Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459805
   Albert AM, 2007, FORENSIC SCI INT, V172, P1, DOI 10.1016/j.forsciint.2007.03.015
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bagdanov A. D., 2011, P JOINT ACM WORKSH H, P79
   Bao LC, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3472954
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Bouritsas G, 2019, IEEE I CONF COMP VIS, P7212, DOI 10.1109/ICCV.2019.00731
   Bruna J., 2014, P INT C LEARN REPR, P14
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Choi Y, 2020, PROC CVPR IEEE, P8185, DOI 10.1109/CVPR42600.2020.00821
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dai H, 2020, INT J COMPUT VISION, V128, P547, DOI 10.1007/s11263-019-01260-7
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Duong C. N., 2018, ARXIV
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Fang H, 2020, IEEE COMPUT SOC CONF, P3500, DOI 10.1109/CVPRW50498.2020.00410
   Fey M., 2019, ICLR WORKSH REPR LEA
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Gecer Baris, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P415, DOI 10.1007/978-3-030-58526-6_25
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Georgopoulos M, 2020, IEEE COMPUT SOC CONF, P66, DOI 10.1109/CVPRW50498.2020.00015
   Gong SW, 2019, IEEE INT CONF COMP V, P4141, DOI 10.1109/ICCVW.2019.00509
   He S., 2021, P IEEE INT C COMP VI, P3877
   He ZL, 2019, IEEE I CONF COMP VIS, P9439, DOI 10.1109/ICCV.2019.00953
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang ZH, 2019, PROC CVPR IEEE, P11949, DOI 10.1109/CVPR.2019.01223
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kingma D. P., 2015, INT C LEARN REPR ICL, P15
   Li P, 2022, IEEE T NEUR NET LEAR, V33, P5346, DOI 10.1109/TNNLS.2021.3070463
   Lim I, 2019, LECT NOTES COMPUT SC, V11131, P349, DOI 10.1007/978-3-030-11015-4_26
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Matthews HS, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91465-z
   Moschoglou S, 2020, INT J COMPUT VISION, V128, P2534, DOI 10.1007/s11263-020-01329-8
   Olivier N, 2023, COMPUT GRAPH-UK, V110, P69, DOI 10.1016/j.cag.2022.12.004
   Or-El Roy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P739, DOI 10.1007/978-3-030-58539-6_44
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Paszke A, 2019, ADV NEUR IN, V32
   Peipei Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P86, DOI 10.1007/978-3-030-58580-8_6
   Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Reddi S. J., 2018, P INT C LEARN REPR, P23
   Sheng B, 2019, IEEE T VIS COMPUT GR, V25, P3216, DOI 10.1109/TVCG.2018.2866090
   Smith WAP, 2020, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR42600.2020.00506
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wu YQ, 2022, IEEE T CIRC SYST VID, V32, P4338, DOI 10.1109/TCSVT.2021.3133313
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yao X, 2021, INT C PATT RECOG, P8624, DOI 10.1109/ICPR48806.2021.9412383
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhang J, 2022, INT J IND ERGONOM, V90, DOI 10.1016/j.ergon.2022.103321
   Zhang J, 2022, COMPUT AIDED DESIGN, V150, DOI 10.1016/j.cad.2022.103271
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 56
TC 2
Z9 2
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4927
EP 4940
DI 10.1109/TVCG.2023.3284500
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400023
PM 37307186
DA 2024-11-06
ER

PT J
AU Wu, HS
   Ma, ZH
   Wu, WL
   Liu, XT
   Li, CZ
   Wen, ZK
AF Wu, Huisi
   Ma, Ziheng
   Wu, Wenliang
   Liu, Xueting
   Li, Chengze
   Wen, Zhenkun
TI Shading-Guided Manga Screening From Reference
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Line drawing shading; manga screening; screentone generation
ID IMAGE; QUALITY
AB Manga screening is a critical process in manga production, which still requires intensive labor and cost. Existing manga screening methods either generate simple dotted screentones only or rely on color information and manual hints during screentone selection. Due to the large domain gap between line drawings and screened manga, and the difficulties in generating high-quality, properly selected and shaded screentones, even state-of-the-art deep learning methods cannot convert line drawings to screened manga well. Besides, ambiguity exists in the screening process since different artists may screen differently for the same line drawing. In this article, we propose to introduce shaded line drawing as the intermediate counterpart of the screened manga so that the manga screening task can be decomposed into two sub-tasks, generating shading from a line drawing and replacing shading with proper screentones. The reference image is adopted to resolve the ambiguity issue and provides options and controls on the generated screened manga. We proposed a reference-based shading generation network and a reference-based screentone generation module to achieve the two sub-tasks individually. We conduct extensive visual and quantitative experiments to verify the effectiveness of our system. Results and statistics show that our method outperforms existing methods on the manga screening task.
C1 [Wu, Huisi; Ma, Ziheng; Wu, Wenliang; Liu, Xueting; Wen, Zhenkun] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Li, Chengze] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong 999077, Peoples R China.
C3 Shenzhen University; Saint Francis University Hong Kong
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
EM hswu@szu.edu.cn; 2070276193@email.szu.edu.cn;
   wenliangwu2019@email.szu.edu.cn; xtliu@szu.edu.cn; czli@cihe.edu.hk;
   wenzk@szu.edu.cn
RI Liu, Xueting/AAG-9648-2019; Li, Chengze/AAU-7168-2021
OI Ma, Ziheng/0000-0002-2628-553X; Wu, Huisi/0000-0002-0399-9089; Li,
   Chengze/0000-0002-1519-750X
FU National Natural Science Foundation of China [61973221, 62002232,
   62273241]; Natural Science Foundation of Guangdong Province, China
   [2019A1515011165]; Major Project of the New Generation of Artificial
   Intelligence [2018AAA0102900]
FX No Statement Available
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Aramaki Y, 2016, IEEE IMAGE PROC, P2901, DOI 10.1109/ICIP.2016.7532890
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Baqai FA, 2003, IEEE T IMAGE PROCESS, V12, P1, DOI 10.1109/TIP.2002.806244
   Bayer B. E., 1973, P IEEE INT C COMM, P69
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Cheng JX, 2021, PROC CVPR IEEE, P134, DOI 10.1109/CVPR46437.2021.00020
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   ESCHBACH R, 1991, J OPT SOC AM A, V8, P1844, DOI 10.1364/JOSAA.8.001844
   Fujimoto A, 2016, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON COMICS ANALYSIS, PROCESSING AND UNDERSTANDING (MANPU 2016), DOI 10.1145/3011549.3011551
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456
   Graphics Gems V, 1995, P 14 ANN C COMP GRAP, P65
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hwang BW, 2004, LECT NOTES COMPUT SC, V3029, P473
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ito Kota., 2015, EUROGRAPHICS SHORT P
   Jayaraman PK, 2018, IEEE T VIS COMPUT GR, V24, P2103, DOI 10.1109/TVCG.2017.2705182
   Jing YC, 2020, AAAI CONF ARTIF INTE, V34, P4369
   Kalischek N, 2021, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR46437.2021.00926
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D.P., 2014, P INT C LEARNING REP
   KNUTH DE, 1987, ACM T GRAPHIC, V6, P245, DOI 10.1145/35039.35040
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Kwak NJ, 2006, 2006 International Conference on Hybrid Information Technology, Vol 1, Proceedings, P499
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Li CL, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3091108
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Lin T., 2021, P IEEE CVF C COMP VI, P5141
   Liu C. Li, Comput. Vis. Media, V3, P61
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mitchell Don P., 1987, P 14 ANN C COMP GRAP, P65, DOI [DOI 10.1145/37402.37410, 10.1145/37401.37410, DOI 10.1145/37401.37410]
   Pang Y., 2008, P ACM SIGGRAPH C, P1
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Qu YG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409108
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Shi M, 2023, IEEE T VIS COMPUT GR, V29, P2965, DOI 10.1109/TVCG.2022.3146000
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Svoboda J, 2020, PROC CVPR IEEE, P13813, DOI 10.1109/CVPR42600.2020.01383
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie MS, 2021, Arxiv, DOI arXiv:2105.06830
   Xie MS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459822
   Xie MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417873
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang LM, 2021, PROC CVPR IEEE, P5638, DOI 10.1109/CVPR46437.2021.00559
   Zhang LM, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P506, DOI 10.1109/ACPR.2017.61
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zhou BF, 2003, ACM T GRAPHIC, V22, P437, DOI 10.1145/882262.882289
   Zhou XR, 2021, PROC CVPR IEEE, P11460, DOI 10.1109/CVPR46437.2021.01130
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 62
TC 0
Z9 0
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4941
EP 4954
DI 10.1109/TVCG.2023.3282223
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400012
PM 37267131
DA 2024-11-06
ER

PT J
AU Shan, ZY
   Yang, Q
   Ye, R
   Zhang, YJ
   Xu, YL
   Xu, XZ
   Liu, S
AF Shan, Ziyu
   Yang, Qi
   Ye, Rui
   Zhang, Yujie
   Xu, Yiling
   Xu, Xiaozhong
   Liu, Shan
TI GPA-Net:No-Reference Point Cloud Quality Assessment With Multi-Task
   Graph Convolutional Network
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud compression; Measurement; Feature extraction; Distortion;
   Convolution; Task analysis; Multitasking; Graph convolutional network;
   multi-task learning; point cloud; quality assessment
ID MODEL
AB With the rapid development of 3D vision, point cloud has become an increasingly popular 3D visual media content. Due to the irregular structure, point cloud has posed novel challenges to the related research, such as compression, transmission, rendering and quality assessment. In these latest researches, point cloud quality assessment (PCQA) has attracted wide attention due to its significant role in guiding practical applications, especially in many cases where the reference point cloud is unavailable. However, current no-reference metrics which based on prevalent deep neural network have apparent disadvantages. For example, to adapt to the irregular structure of point cloud, they require preprocessing such as voxelization and projection that introduce extra distortions, and the applied grid-kernel networks, such as Convolutional Neural Networks, fail to extract effective distortion-related features. Besides, they rarely consider the various distortion patterns and the philosophy that PCQA should exhibit shift, scaling, and rotation invariance. In this paper, we propose a novel no-reference PCQA metric named the Graph convolutional PCQA network (GPA-Net). To extract effective features for PCQA, we propose a new graph convolution kernel, i.e., GPAConv, which attentively captures the perturbation of structure and texture. Then, we propose the multi-task framework consisting of one main task (quality regression) and two auxiliary tasks (distortion type and degree predictions). Finally, we propose a coordinate normalization module to stabilize the results of GPAConv under shift, scale and rotation transformations. Experimental results on two independent databases show that GPA-Net achieves the best performance compared to the state-of-the-art no-reference PCQA metrics, even better than some full-reference metrics in some cases.
C1 [Shan, Ziyu; Ye, Rui; Zhang, Yujie; Xu, Yiling] Shanghai Jiao Tong Univ, Cooperat Media Innovat Ctr, Shanghai 200240, Peoples R China.
   [Yang, Qi; Xu, Xiaozhong; Liu, Shan] Tencent Media Lab, Shenzhen 518054, Peoples R China.
C3 Shanghai Jiao Tong University
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Cooperat Media Innovat Ctr, Shanghai 200240, Peoples R China.
EM shanziyu@sjtu.edu.cn; chinoyang@tencent.com; yr991129@sjtu.edu.cn;
   yujie19981026@sjtu.edu.cn; yl.xu@sjtu.edu.cn; xiaozhongxu@tencent.com;
   shanl@tencent.com
OI Ye, Rui/0009-0007-5998-8200; , Shan/0000-0002-1442-1207; Zhang,
   Yujie/0000-0002-5534-0198; Yang, Qi/0000-0002-4274-3457; Shan,
   Ziyu/0000-0002-3346-4261
FU National Key R#x0026;D Program of China [2018YFE0206700]; National
   Natural Science Foundation of China [61971282, U20A20185]
FX No Statement Available
CR Alexiou E, 2018, IEEE INT CON MULTI
   Alexiou E, 2017, IEEE INT WORKSH MULT
   Golestaneh SA, 2020, Arxiv, DOI [arXiv:2006.03783, DOI 10.48550/ARXIV.2006.03783, 10.48550/arXiv.2006.03783]
   Antkowiak J., 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Ao S, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107186
   Bletterer A, 2022, IEEE T VIS COMPUT GR, V28, P2822, DOI 10.1109/TVCG.2020.3042588
   Bro R, 2008, J CHEMOMETR, V22, P135, DOI 10.1002/cem.1122
   Chetouani A, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455967
   Fan Y, 2022, Arxiv, DOI arXiv:2206.05054
   Hou JW, 2020, IEEE IMAGE PROC, P3463, DOI 10.1109/ICIP40778.2020.9191241
   Kovács E, 2012, ANN MATH INFORM, V40, P175
   Li AB, 2022, NEUROCOMPUTING, V500, P307, DOI 10.1016/j.neucom.2022.05.043
   Li XZ, 2022, IEEE T VIS COMPUT GR, V28, P4503, DOI 10.1109/TVCG.2021.3092570
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Liu Q, 2023, IEEE T VIS COMPUT GR, V29, P3642, DOI 10.1109/TVCG.2022.3167151
   Liu Q, 2021, IEEE T CIRC SYST VID, V31, P4645, DOI 10.1109/TCSVT.2021.3100282
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197
   Liu YX, 2022, APPL GEOPHYS, DOI 10.1007/s11770-022-0970-2
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Mekuria R., 2016, ISO/IEC MPEG 16332
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Paszke A, 2019, ADV NEUR IN, V32
   Perry S, 2020, IEEE IMAGE PROC, P3428, DOI 10.1109/ICIP40778.2020.9191308
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su HL, 2019, IEEE IMAGE PROC, P3182, DOI [10.1109/ICIP.2019.8803298, 10.1109/icip.2019.8803298]
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Tliba Marouane, 2022, QoEVMA '22: Proceedings of the 2nd Workshop on Quality of Experience in Visual Multimedia Applications, P63, DOI 10.1145/3552469.3555710
   Vaswani A., 2017, Proceedings of NeurIPS, Longbeach, P5998
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Watson A. B., 1997, P EL IM SCI TECHN C, P277
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xu L, 2017, IEEE T CIRC SYST VID, V27, P1833, DOI 10.1109/TCSVT.2016.2543099
   Yang Q, 2022, IEEE T PATTERN ANAL, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yang Q, 2022, PROC CVPR IEEE, P21147, DOI 10.1109/CVPR52688.2022.02050
   Yang Q, 2023, IEEE T PATTERN ANAL, V45, P6037, DOI 10.1109/TPAMI.2022.3213831
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhang YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1230, DOI 10.1145/3474085.3475294
   Zhang ZC, 2022, IEEE T CIRC SYST VID, V32, P7618, DOI 10.1109/TCSVT.2022.3186894
   Zhao H, 2020, IEEE INT SYM BROADB, DOI 10.1109/BMSB49480.2020.9379524
   Zhou HR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4945, DOI 10.1109/ICCV48922.2021.00492
NR 52
TC 6
Z9 6
U1 3
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4955
EP 4967
DI 10.1109/TVCG.2023.3282802
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400028
PM 37379183
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Li, YQ
   Yin, WT
   Li, JB
   Xie, XJ
AF Li, Yuqi
   Yin, Wenting
   Li, Jiabao
   Xie, Xijiong
TI Physics-Based Efficient Full Projector Compensation Using Only Natural
   Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Full projector compensation; projector display; matrix factorization
ID RADIOMETRIC COMPENSATION; DISPLAY; SYSTEM
AB Achieving practical full projector compensation requires the projection display to adapt quickly to textured projection surfaces and unexpected movements without interrupting the display procedure. A possible solution to achieve this involves using a projector and an RGB camera and correcting both color and geometry by directly capturing and analyzing the projected natural image content, without the need for additional patterns. In this study, we approach full projector compensation as a numerical optimization problem and present a physics-based framework that can handle both geometric calibration and radiometric compensation for a Projector-camera system (Procams), using only a few sampling natural images. Within the framework, we decouple and estimate the Procams' factors, such as the response function of the projector, the correspondence between the projector and camera, and the reflectance of projection surfaces. This approach provides an interpretable and flexible solution to adapt to the changes in geometry and reflectance caused by movements. Benefitting from the physics-based scheme, our method guarantees both accurate color calculation and efficient movement and reflectance estimation. Our experimental results demonstrate that our method surpasses other state-of-the-art end-to-end full projector compensation methods, with superior image quality, reduced computational time, lower memory consumption, greater geometric accuracy, and a more compact network architecture.
C1 [Li, Yuqi] Ningbo Univ, Ningbo 315211, Peoples R China.
C3 Ningbo University
RP Li, YQ (corresponding author), Ningbo Univ, Ningbo 315211, Peoples R China.
EM liyuqi1@nbu.edu.cn; 2011082084@nbu.edu.cn; 2011082305@nbu.edu.cn;
   xiexijiong@nbu.edu.cn
RI Xie, Xijiong/JDM-5573-2023; Li, Jiabao/KEJ-2204-2024
OI Yin, Wenting/0009-0000-1760-7560; Li, Jiabao/0000-0002-0679-1101
FU Ningbo Innovation Challenge Project [2022T001]; Zhejiang Engineering
   Research Center of Advanced Mass Spectrometry and Clinical Application
FX No Statement Available
CR [Anonymous], 1992, JOSA A, V9, P507
   Asayama H, 2018, IEEE T VIS COMPUT GR, V24, P1077, DOI 10.1109/TVCG.2017.2657634
   Behrmann W., 2019, P 36 INT C MACHINE L, P573
   Bermano AH, 2017, COMPUT GRAPH FORUM, V36, P311, DOI 10.1111/cgf.13128
   Bimber D., 2008, ACM SIGGRAPH CLASSES, P1
   Cotting D, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P100, DOI 10.1109/ISMAR.2004.30
   Fujii K, 2005, PROC CVPR IEEE, P814, DOI 10.1109/CVPR.2005.41
   Grossberg MD, 2004, PROC CVPR IEEE, P452
   Grundhöfer A, 2015, IEEE T IMAGE PROCESS, V24, P5086, DOI 10.1109/TIP.2015.2478388
   Grundhöfer A, 2018, COMPUT GRAPH FORUM, V37, P653, DOI 10.1111/cgf.13387
   Grundhöfer A, 2008, IEEE T VIS COMPUT GR, V14, P97, DOI 10.1109/TVCG.2007.1052
   Hashimoto N, 2021, VISUAL COMPUT, V37, P175, DOI 10.1007/s00371-019-01790-8
   Hashimoto N, 2017, INT J COMPUT GAMES T, V2017, DOI 10.1155/2017/4936285
   Huang BY, 2022, IEEE T PATTERN ANAL, V44, P2953, DOI 10.1109/TPAMI.2021.3050124
   Huang BY, 2021, IEEE T AUTOM SCI ENG, V18, P1049, DOI 10.1109/TASE.2020.2994223
   Huang BY, 2021, IEEE T VIS COMPUT GR, V27, P2725, DOI 10.1109/TVCG.2021.3067771
   Huang H., 2020, P IEEE CVF INT C COM, P7164
   Huang H., 2019, P IEEE CVF C COMP VI, P6810
   Huang TH, 2017, IEEE T IMAGE PROCESS, V26, P147, DOI 10.1109/TIP.2016.2592799
   Ibrahim G., 2020, P 26 ACM S VIRT REAL, P1
   Ibrahim MT, 2022, COMPUT GRAPH-UK, V103, P61, DOI 10.1016/j.cag.2022.01.004
   Ignatov A, 2020, IEEE COMPUT SOC CONF, P2275, DOI 10.1109/CVPRW50498.2020.00276
   Iwai D, 2015, IEEE T VIS COMPUT GR, V21, P462, DOI 10.1109/TVCG.2015.2391861
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Juang A., 2007, IEEE C COMPUT VIS PA, P1
   Kagami S, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 EMERGING TECHNOLOGIES, DOI 10.1145/3214907.3214927
   Kageyama Y, 2022, IEEE T VIS COMPUT GR, V28, P2223, DOI 10.1109/TVCG.2022.3150465
   Li Q. Yuan, 2013, P 19 ACM S VIRT REAL, P201
   Li YQ, 2018, COMPUT GRAPH FORUM, V37, P365, DOI 10.1111/cgf.13368
   Li YQ, 2018, VISUAL COMPUT, V34, P1773, DOI 10.1007/s00371-017-1469-3
   Li YQ, 2013, CHIN OPT LETT, V11, DOI 10.3788/COL201311.113301
   Li Yuqi, 2021, P IEEE CVF INT C COM, P2672
   Liang JX, 2021, OPT EXPRESS, V29, P43899, DOI 10.1364/OE.447031
   Miyashita L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275045
   Narita G, 2017, IEEE T VIS COMPUT GR, V23, P1235, DOI 10.1109/TVCG.2016.2592910
   Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977
   Qi M, 2021, Arxiv, DOI arXiv:2111.01511
   Resch C, 2014, INT SYM MIX AUGMENT, P151, DOI 10.1109/ISMAR.2014.6948421
   Sajadi B, 2012, IEEE T VIS COMPUT GR, V18, P381, DOI 10.1109/TVCG.2011.271
   Sajadi B, 2011, IEEE T VIS COMPUT GR, V17, P1209, DOI 10.1109/TVCG.2011.33
   Sajadi B, 2010, LECT NOTES COMPUT SC, V6314, P72, DOI 10.1007/978-3-642-15561-1_6
   Shahpaski M, 2017, PROC CVPR IEEE, P3596, DOI 10.1109/CVPR.2017.383
   Sugimoto M, 2021, IEEE T VIS COMPUT GR, V27, P4161, DOI 10.1109/TVCG.2021.3106511
   Takeda S, 2016, IEEE T VIS COMPUT GR, V22, P1424, DOI 10.1109/TVCG.2016.2518136
   Tehrani MA, 2021, IEEE T VIS COMPUT GR, V27, P2265, DOI 10.1109/TVCG.2019.2950942
   Terai H., 2021, arXiv
   Tone D, 2020, IEEE T VIS COMPUT GR, V26, P2030, DOI 10.1109/TVCG.2020.2973444
   Tsukamoto J, 2015, IEEE T VIS COMPUT GR, V21, P1221, DOI 10.1109/TVCG.2015.2459905
   Wetzstein G, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P391, DOI 10.1109/PG.2007.47
   Yang G., 2001, P INT C CENTR EUR CO, P328
   Yang LM, 2016, INT SYM MIX AUGMENT, P63, DOI 10.1109/ISMAR.2016.22
   Yuqi Li, 2012, 2012 International Conference on Virtual Reality and Visualization (ICVRV 2012), P7, DOI 10.1109/ICVRV.2012.15
   Zhang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7940, DOI 10.1109/ICCV48922.2021.00786
NR 53
TC 2
Z9 2
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4968
EP 4982
DI 10.1109/TVCG.2023.3281681
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400079
PM 37256799
DA 2024-11-06
ER

PT J
AU Wang, C
   Jiang, RX
   Chai, ML
   He, MM
   Chen, DD
   Liao, J
AF Wang, Can
   Jiang, Ruixiang
   Chai, Menglei
   He, Mingming
   Chen, Dongdong
   Liao, Jing
TI <i>NeRF-Art:</i> Text-Driven Neural Radiance Fields Stylization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Text-driven stylization; neural radiance fields; CLIP
ID TO-IMAGE TRANSLATION
AB As a powerful representation of 3D scenes, the neural radiance field (NeRF) enables high-quality novel view synthesis from multi-view images. Stylizing NeRF, however, remains challenging, especially in simulating a text-guided style with both the appearance and the geometry altered simultaneously. In this paper, we present NeRF-Art, a text-guided NeRF stylization approach that manipulates the style of a pre-trained NeRF model with a simple text prompt. Unlike previous approaches that either lack sufficient geometry deformations and texture details or require meshes to guide the stylization, our method can shift a 3D scene to the target style characterized by desired geometry and appearance variations without any mesh guidance. This is achieved by introducing a novel global-local contrastive learning strategy, combined with the directional constraint to simultaneously control both the trajectory and the strength of the target style. Moreover, we adopt a weight regularization method to effectively suppress cloudy artifacts and geometry noises which arise easily when the density field is transformed during geometry stylization. Through extensive experiments on various styles, we demonstrate that our method is effective and robust regarding both single-view stylization quality and cross-view consistency.
C1 [Wang, Can; Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
   [Jiang, Ruixiang] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China.
   [Chai, Menglei] Snap Inc, Santa Monica, CA 90405 USA.
   [He, Mingming] Netflix, Los Gatos, CA 95032 USA.
   [Chen, Dongdong] Microsoft Cloud AI, Redmond, WA 98052 USA.
C3 City University of Hong Kong; Hong Kong Polytechnic University; Netflix,
   Inc.
RP Liao, J (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
EM cwang355-c@my.cityu.edu.hk; rui-x.jiang@connect.polyu.hk;
   cmlatsim@gmail.com; mheah@connect.ust.hk; cddlyf@gmail.com;
   jingliao@cityu.edu.hk
RI He, Mingming/AAY-5609-2021; Chen, Dongdong/M-9516-2019
OI wang, can/0000-0002-5102-1464; LIAO, Jing/0000-0001-7014-5377; Ruixiang,
   JIANG/0000-0001-8666-6767; Chen, Dongdong/0000-0002-4642-4373
FU Research Grants Council (RGC) of Hong Kong [CityU 11216122, CityU
   9229102]
FX This work was supported in part by the General Research Fund (GRF) under
   Grant CityU 11216122, and in part by the Research Matching Grant Scheme
   (RMGS) under Grant CityU 9229102 from the Research Grants Council (RGC)
   of Hong Kong.
CR Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Cao X, 2020, IEEE WINT CONF APPL, P3326, DOI 10.1109/WACV45572.2020.9093513
   Chefer H, 2022, LECT NOTES COMPUT SC, V13673, P695, DOI 10.1007/978-3-031-19778-9_40
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chiang P.-Z., 2022, P IEEECVF WINTER C A, P1475
   Deng KL, 2022, Arxiv, DOI arXiv:2107.02791
   Deng NC, 2022, IEEE T VIS COMPUT GR, V28, P3854, DOI 10.1109/TVCG.2022.3203102
   Fan ZW, 2022, Arxiv, DOI arXiv:2204.01943
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Gal R, 2021, Arxiv, DOI arXiv:2108.00946
   Gao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5692, DOI 10.1109/ICCV48922.2021.00566
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Geiger M.NiemeyerandA., 2021, P IEEE CVF C COMP VI, P11453
   Guo J., 2021, ACM Trans. Graph, V40, P15
   Han FZ, 2023, IEEE T VIS COMPUT GR, V29, P1371, DOI 10.1109/TVCG.2021.3114308
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Höllein L, 2022, Arxiv, DOI arXiv:2112.01530
   Hong FZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530094
   Huang H.-P., 2021, P IEEE CVF INT C COM, P13869
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang YH, 2022, PROC CVPR IEEE, P18321, DOI 10.1109/CVPR52688.2022.01780
   Jain A, 2022, PROC CVPR IEEE, P857, DOI 10.1109/CVPR52688.2022.00094
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Klehm O, 2014, IEEE T VIS COMPUT GR, V20, P983, DOI 10.1109/TVCG.2014.13
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Li GH, 2023, IEEE T PATTERN ANAL, V45, P6923, DOI 10.1109/TPAMI.2021.3074057
   Li YJ, 2017, ADV NEUR IN, V30
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Liao J, 2017, Arxiv, DOI arXiv:1705.01088
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Lin S., 2022, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, P238
   Lindell DB, 2021, PROC CVPR IEEE, P14551, DOI 10.1109/CVPR46437.2021.01432
   Liu HTD, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275047
   Liu S., 2021, P IEEECVF INT C COMP, P5773
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Ma L., 2021, arXiv
   Michel O., 2021, arXiv
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Mordvintsev A., 2018, Distill, V3, DOI [DOI 10.23915/DISTILL.00012, 10.23915/distill.00012]
   Mu Fangzhou., 2021, arXiv
   Muller T., 2022, arXiv
   Niemeyer M., 2021, arXiv
   Noguchi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5742, DOI 10.1109/ICCV48922.2021.00571
   Park K, 2021, Arxiv, DOI arXiv:2106.13228
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Park T., 2020, EUR C COMP VIS, P319, DOI DOI 10.1007/978-3-030-58545-7_19
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Pumarola Albert, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P10313, DOI 10.1109/CVPR46437.2021.01018
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramon E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5600, DOI 10.1109/ICCV48922.2021.00557
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schwarz K, 2020, ADV NEUR IN, V33
   Sheng P. Li, 2019, IEEE Trans. Vis. Comput. Graph., V25, P6
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Srinivasan PP, 2021, PROC CVPR IEEE, P7491, DOI 10.1109/CVPR46437.2021.00741
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   Wang C., 2021, arXiv
   Wang KK, 2023, IEEE T VIS COMPUT GR, V29, P5097, DOI 10.1109/TVCG.2022.3202503
   Wang PA, 2021, ADV NEUR IN, V34
   Wei T., 2021, arXiv
   Xian WQ, 2021, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR46437.2021.00930
   Xinghao Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P614, DOI 10.1007/978-3-030-58539-6_37
   Yang GW, 2023, IEEE T VIS COMPUT GR, V29, P5124, DOI 10.1109/TVCG.2022.3204608
   Yariv L, 2021, ADV NEUR IN
   Ye ZP, 2023, IEEE T VIS COMPUT GR, V29, P2203, DOI 10.1109/TVCG.2021.3126659
   Yin KX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12436, DOI 10.1109/ICCV48922.2021.01223
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Zhang H, 2023, IEEE T VIS COMPUT GR, V29, P4891, DOI 10.1109/TVCG.2022.3192713
   Zhang K., 2022, arXiv, DOI DOI 10.48550/ARXIV.2206.06360
   Zhang K, 2020, Arxiv, DOI arXiv:2010.07492
   Zhang M, 2022, IEEE T VIS COMPUT GR, V28, P2926, DOI 10.1109/TVCG.2020.3041487
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480496
   Zhao YD, 2015, IEEE T VIS COMPUT GR, V21, P229, DOI 10.1109/TVCG.2014.2355221
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zisserman A., 2021, arXiv
NR 93
TC 3
Z9 3
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4983
EP 4996
DI 10.1109/TVCG.2023.3283400
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400058
PM 37279137
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Lei, N
   Li, ZZ
   Xu, ZB
   Li, Y
   Gu, XF
AF Lei, Na
   Li, Zezeng
   Xu, Zebin
   Li, Ying
   Gu, Xianfeng
TI What's the Situation With Intelligent Mesh Generation: A Survey and
   Perspectives
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep learning; mesh generation; neural network; polygonal mesh; review;
   survey
ID PARTIAL-DIFFERENTIAL-EQUATIONS; SURFACE RECONSTRUCTION; GEOMETRY;
   NETWORK
AB Intelligent Mesh Generation (IMG) represents a novel and promising field of research, utilizing machine learning techniques to generate meshes. Despite its relative infancy, IMG has significantly broadened the adaptability and practicality of mesh generation techniques, delivering numerous breakthroughs and unveiling potential future pathways. However, a noticeable void exists in the contemporary literature concerning comprehensive surveys of IMG methods. This paper endeavors to fill this gap by providing a systematic and thorough survey of the current IMG landscape. With a focus on 113 preliminary IMG methods, we undertake a meticulous analysis from various angles, encompassing core algorithm techniques and their application scope, agent learning objectives, data types, targeted challenges, as well as advantages and limitations. We have curated and categorized the literature, proposing three unique taxonomies based on key techniques, output mesh unit elements, and relevant input data types. This paper also underscores several promising future research directions and challenges in IMG.
C1 [Lei, Na] Dalian Univ Technol, Int Informat & Software Inst, Dalian 116620, Peoples R China.
   [Li, Zezeng; Xu, Zebin] Dalian Univ Technol, Sch Software, Dalian 116620, Peoples R China.
   [Li, Ying] Jilin Univ, Coll Comp Sci & Technol, Changchun 130015, Peoples R China.
   [Gu, Xianfeng] SUNY Stony Brook, Dept Comp Sci & Appl Math, Stony Brook, NY 11794 USA.
C3 Dalian University of Technology; Dalian University of Technology; Jilin
   University; State University of New York (SUNY) System; Stony Brook
   University
RP Li, Y (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130015, Peoples R China.
EM nalei@dlut.edu.cn; zezeng.lee@gmail.com; xzb0516@mail.dlut.edu.cn;
   liying@jlu.edu.cn; gu@cs.stonybrook.edu
RI liu, na/HKF-7392-2023; Li, Zezeng/KCY-8596-2024
OI Li, Ying/0000-0002-7804-149X; Lei, Na/0000-0003-3361-0756; Li,
   Zezeng/0000-0001-9064-689X; Gu, Xianfeng David/0000-0001-8226-5851
FU National Key R&D Program of China [2021YFA1003003]; National Natural
   Science Foundation of China [61936002, T2225012]
FX This research was supported by the National Key R&D Program of China
   under Grant 2021YFA1003003 and the National Natural Science Foundation
   of China under Grants 61936002 and T2225012.
CR Abouelaziz I, 2021, IEEE ACCESS, V9, P108200, DOI 10.1109/ACCESS.2021.3094663
   Abouelaziz I, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS), P617, DOI 10.1109/SITIS.2018.00099
   Abouelaziz I, 2018, IEEE IMAGE PROC, P3533, DOI 10.1109/ICIP.2018.8451763
   Abouelaziz I, 2017, IEEE IMAGE PROC, P755, DOI 10.1109/ICIP.2017.8296382
   Abouelaziz I, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P793, DOI 10.1109/SITIS.2016.130
   AHN CH, 1991, IEEE T MAGN, V27, P4201, DOI 10.1109/20.105028
   Alfonzetti S, 2003, IEEE T MAGN, V39, P1650, DOI 10.1109/TMAG.2003.810325
   Alfonzetti S, 1996, IEEE T MAGN, V32, P1349, DOI 10.1109/20.497496
   Alfonzetti S, 1998, IEEE T MAGN, V34, P3363, DOI 10.1109/20.717791
   Alfonzetti S, 2008, IEEE T MAGN, V44, P1278, DOI 10.1109/TMAG.2007.916035
   [Anonymous], 2021, Renderpeople
   [Anonymous], 2019, axyz
   Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Azinovi c D., 2022, PROC IEEECVF C COMPU, P6290
   Badki A, 2020, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR42600.2020.00292
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Berger M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451246
   Bertiche H, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667017
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Bogo F, 2017, PROC CVPR IEEE, P5573, DOI 10.1109/CVPR.2017.591
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Boscaini Davide, 2016, ADV NEURAL INFORM PR, P3197
   Boulch A, 2022, PROC CVPR IEEE, P6292, DOI 10.1109/CVPR52688.2022.00620
   Boulch A, 2021, INT CONF 3D VISION, P940, DOI 10.1109/3DV53792.2021.00102
   Bouritsas G, 2019, IEEE I CONF COMP VIS, P7212, DOI 10.1109/ICCV.2019.00731
   Boyi Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P18, DOI 10.1007/978-3-030-58565-5_2
   Bozic A, 2021, ADV NEUR IN
   Brito AD, 2008, IEEE T NEURAL NETWOR, V19, P1130, DOI 10.1109/TNN.2008.2000390
   Brock A, 2016, Arxiv, DOI [arXiv:1608.04236, 10.48550/arXiv.1608.04236]
   Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296
   Cao YP, 2018, LECT NOTES COMPUT SC, V11213, P626, DOI 10.1007/978-3-030-01240-3_38
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Charrada TB, 2022, COMPUT GRAPH FORUM, V41, P336, DOI 10.1111/cgf.14496
   Chen A., 2022, ACM Transactions on Graphics (TOG), V41, P1
   Chen XH, 2022, ENG COMPUT-GERMANY, V38, P4409, DOI 10.1007/s00366-022-01632-7
   Chen XH, 2021, COMPUT AIDED DESIGN, V141, DOI 10.1016/j.cad.2021.103104
   Chen XH, 2020, ENG APPL COMP FLUID, V14, P391, DOI 10.1080/19942060.2020.1720820
   Chen ZQ, 2021, PROC CVPR IEEE, P15735, DOI 10.1109/CVPR46437.2021.01548
   Chen ZQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480518
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Chibane J, 2020, ADV NEUR IN, V33
   Dai A, 2019, PROC CVPR IEEE, P5559, DOI 10.1109/CVPR.2019.00572
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Danerek R, 2017, COMPUT GRAPH FORUM, V36, P269, DOI 10.1111/cgf.13125
   Daroya R, 2020, IEEE COMPUT SOC CONF, P1444, DOI 10.1109/CVPRW50498.2020.00184
   Deng Z, 2023, IEEE T VIS COMPUT GR, V29, P3826, DOI 10.1109/TVCG.2022.3170853
   Dielen A, 2021, COMPUT GRAPH FORUM, V40, P181, DOI 10.1111/cgf.14366
   DOI A, 1991, IEICE TRANS COMMUN, V74, P214
   Du D, 2022, IEEE T VIS COMPUT GR, V28, P2415, DOI 10.1109/TVCG.2020.3030330
   Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Gao Jun., 2020, ADV NEURAL INFORM PR, V33, P9936, DOI DOI 10.48550/ARXIV.2011.01437
   Gao L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480503
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guo YF, 2021, LECT NOTES COMPUT SC, V13002, P477, DOI 10.1007/978-3-030-89029-2_37
   Gupta K., 2020, P NEURIPS, P1747
   Hahner S, 2022, IEEE WINT CONF APPL, P2344, DOI 10.1109/WACV51458.2022.00240
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Han XG, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073629
   Handa A, 2016, IEEE INT CONF ROBOT, P5737, DOI 10.1109/ICRA.2016.7487797
   Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Henderson P, 2020, PROC CVPR IEEE, P7495, DOI 10.1109/CVPR42600.2020.00752
   Hertz A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392471
   Hu MS, 2021, J ROCK MECH GEOTECH, V13, P912, DOI 10.1016/j.jrmge.2021.02.002
   Huang JH, 2021, PROC CVPR IEEE, P8928, DOI 10.1109/CVPR46437.2021.00882
   Jiang BY, 2022, PROC CVPR IEEE, P5595, DOI 10.1109/CVPR52688.2022.00552
   Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Keele S., 2007, Guidelines for Performing Systematic Literature Reviews in Software Engineering
   Khan D, 2022, IEEE T VIS COMPUT GR, V28, P1680, DOI 10.1109/TVCG.2020.3016645
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   LEE DT, 1980, INT J COMPUT INF SCI, V9, P219, DOI 10.1007/BF00977785
   Lei JB, 2022, IEEE T PATTERN ANAL, V44, P10068, DOI 10.1109/TPAMI.2021.3135007
   Lei N, 2019, COMPUT AIDED GEOM D, V68, P1, DOI 10.1016/j.cagd.2018.10.005
   Li CL, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3238250
   Li RZ, 2021, INT CONF 3D VISION, P555, DOI 10.1109/3DV53792.2021.00065
   Li TT, 2022, IEEE T CIRC SYST VID, V32, P4667, DOI 10.1109/TCSVT.2021.3135528
   Li X, 2020, I COMP CONF WAVELET, P101, DOI 10.1109/ICCWAMTIP51612.2020.9317479
   Li Y., 2019, P IEEE VIS COMM IM P, P1
   Li ZZ, 2022, INT CONF ACOUST SPEE, P2564, DOI 10.1109/ICASSP43922.2022.9746972
   Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308
   Lim I, 2019, LECT NOTES COMPUT SC, V11131, P349, DOI 10.1007/978-3-030-11015-4_26
   Lim W.P., 2004, P 2 INT C COMP GRAPH
   Liu HTD, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392418
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   LOHNER R, 1988, INT J NUMER METH FL, V8, P1135, DOI 10.1002/fld.1650081003
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   LOWTHER DA, 1993, IEEE T MAGN, V29, P1927, DOI 10.1109/20.250785
   Lu P., 2022, Journal of Physics: Conference Series, V2280
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Luo YM, 2021, AAAI CONF ARTIF INTE, V35, P2277
   Ma BR, 2022, PROC CVPR IEEE, P6305, DOI 10.1109/CVPR52688.2022.00621
   Ma Z., 2021, INT C MACH LEARN, P7246
   Macdonald CB, 2008, J SCI COMPUT, V35, P219, DOI 10.1007/s10915-008-9196-6
   Macdonald CB, 2011, J COMPUT PHYS, V230, P7944, DOI 10.1016/j.jcp.2011.06.021
   Macdonald CB, 2009, SIAM J SCI COMPUT, V31, P4330, DOI 10.1137/080740003
   März T, 2012, SIAM J NUMER ANAL, V50, P3303, DOI 10.1137/120865537
   Masci D., 2015, P INT C COMP VIS WOR, P37, DOI DOI 10.1109/ICCVW.2015.112
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mi ZX, 2020, PROC CVPR IEEE, P967, DOI 10.1109/CVPR42600.2020.00105
   Milano F., 2020, Adv. Neural Inf. Process. Syst, V33, P952
   Minghua Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P68, DOI 10.1007/978-3-030-58598-3_5
   Mittal P, 2022, PROC CVPR IEEE, P306, DOI 10.1109/CVPR52688.2022.00040
   Munkberg J, 2022, PROC CVPR IEEE, P8270, DOI 10.1109/CVPR52688.2022.00810
   Nash Charlie, 2020, PMLR
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pan J, 2023, NEURAL NETWORKS, V157, P288, DOI 10.1016/j.neunet.2022.10.022
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pedone M, 2021, INT C PATT RECOG, P10134, DOI 10.1109/ICPR48806.2021.9412352
   Peng S., 2020, ECCV, P523
   Peng SY, 2021, ADV NEUR IN, V34
   Pietroni N, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459941, 10.1145/3476576.3476737]
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Rakotosaona MJ, 2021, PROC CVPR IEEE, P22, DOI 10.1109/CVPR46437.2021.00009
   Rakotosaona N., 2021, ACM Trans. Graph., V40, P1
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Ray N, 2006, ACM T GRAPHIC, V25, P1460, DOI 10.1145/1183287.1183297
   Remelli E., 2020, P INT C NEUR INF PRO, V33, P22468
   Rios T, 2020, 2020 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P942, DOI 10.1109/SSCI47803.2020.9308400
   Ruuth SJ, 2008, J COMPUT PHYS, V227, P1943, DOI 10.1016/j.jcp.2007.10.009
   Schult J., 2020, P IEEECVF C COMPUTER, P8612, DOI 10.1109/CVPR42600.2020.00864
   Sharp N., 2020, EUR C COMP VIS, P762, DOI [10.1007/978-3-030-58592-145, DOI 10.1007/978-3-030-58592-145]
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Shen TC, 2021, ADV NEUR IN, V34
   Siddiqui Yawar, 2021, P IEEECVF INT C COMP, P12568
   Singh VV, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4883, DOI 10.1145/3474085.3475468
   Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91
   Slotnick J.P., 2014, Technical Report
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Smirnov M., 2020, P INT C LEARN REPR, P1
   Song Z. Cui, 2021, IEEE INT C COMPUT VI, P6514
   Sorkine O., 2003, Symposium on Geometry Processing, P42
   Sulzer R, 2021, COMPUT GRAPH FORUM, V40, P157, DOI 10.1111/cgf.14364
   Sun JM, 2021, PROC CVPR IEEE, P15593, DOI 10.1109/CVPR46437.2021.01534
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tang JP, 2022, IEEE T PATTERN ANAL, V44, P6454, DOI 10.1109/TPAMI.2021.3087358
   Tang JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6484, DOI 10.1109/ICCV48922.2021.00644
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tian L, 2009, IEEE IMAGE PROC, P3009, DOI 10.1109/ICIP.2009.5414447
   Tong F, 2020, IEEE ENG MED BIO, P1608, DOI [10.1109/EMBC44109.2020.9176655, 10.1109/embc44109.2020.9176655]
   Venkat A, 2019, IEEE INT CONF COMP V, P2178, DOI 10.1109/ICCVW.2019.00273
   Venkatesh R., 2021, P IEEE INT C COMP VI, P12653
   Wang JW, 2022, INT CONF 3D VISION, P433, DOI 10.1109/3DV57658.2022.00055
   Wang NY, 2021, IEEE T PATTERN ANAL, V43, P3600, DOI 10.1109/TPAMI.2020.2984232
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   [王年华 Wang Nianhua], 2021, [力学学报, Chinese Journal of Theoretical and Applied Mechanics], V53, P740
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang PA, 2021, ADV NEUR IN, V34
   Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113
   Wang YF, 2021, PROC CVPR IEEE, P374, DOI 10.1109/CVPR46437.2021.00044
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wang ZY, 2020, IEEE IMAGE PROC, P2666, DOI [10.1109/ICIP40778.2020.9190842, 10.1109/icip40778.2020.9190842]
   Wei XK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5785, DOI 10.1109/ICCV48922.2021.00575
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Wen PZ, 2009, CCDC 2009: 21ST CHINESE CONTROL AND DECISION CONFERENCE, VOLS 1-6, PROCEEDINGS, P5785, DOI 10.1109/CCDC.2009.5195232
   Wickramasinghe Udaranga, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12264), P299, DOI 10.1007/978-3-030-59719-1_30
   Wickramasinghe U, 2021, PROC CVPR IEEE, P11647, DOI 10.1109/CVPR46437.2021.01148
   Wiersma R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530166
   Williams F., 2022, IEEE C COMPUT VIS PA, P18500
   Williams F, 2019, PROC CVPR IEEE, P10122, DOI 10.1109/CVPR.2019.01037
   Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091
   Wu XM, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P913, DOI 10.1109/IIH-MSP.2008.110
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Xiong SY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661263
   Xu Qiangeng, 2019, Advances in Neural Information Processing Systems
   Yan GW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417832
   Yang DS, 2020, I COMP CONF WAVELET, P122, DOI 10.1109/ICCWAMTIP51612.2020.9317527
   Yang GS, 2021, PROC CVPR IEEE, P15975, DOI 10.1109/CVPR46437.2021.01572
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yangetal X., 2020, IEEE Trans. Vis. Comput. Graph., V26, P3456
   Yao S, 2005, EXPERT SYST APPL, V29, P193, DOI 10.1016/j.eswa.2005.01.019
   Yuan YJ, 2020, IEEE COMPUT SOC CONF, P1105, DOI 10.1109/CVPRW50498.2020.00145
   Zhang ZY, 2020, LECT NOTES COMPUT SC, V12139, P186, DOI 10.1007/978-3-030-50420-5_14
   Zheng Y., 2021, P IEEE CVF INT C COM, P6239
   Zheng Y. Zhu, 2021, Comput. Methods Appl. Mech. Eng., V387
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
   Zhou Yi, 2020, Advances in neural information processing systems, V33, P9251
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
NR 189
TC 2
Z9 2
U1 7
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4997
EP 5017
DI 10.1109/TVCG.2023.3281781
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400027
PM 37262120
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wang, ZX
   Wang, PF
   Wang, PS
   Dong, QJ
   Gao, JJ
   Chen, SM
   Xin, SQ
   Tu, CH
   Wang, WP
AF Wang, Zixiong
   Wang, Pengfei
   Wang, Peng-Shuai
   Dong, Qiujie
   Gao, Junjie
   Chen, Shuangmin
   Xin, Shiqing
   Tu, Changhe
   Wang, Wenping
TI Neural-IMLS: Self-Supervised Implicit Moving Least-Squares Network for
   Surface Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Implicit moving least squares; self-supervised neural network; surface
   reconstruction; implicit neural representations
AB Surface reconstruction is a challenging task when input point clouds, especially real scans, are noisy and lack normals. Observing that the Multilayer Perceptron (MLP) and the implicit moving least-square function (IMLS) provide a dual representation of the underlying surface, we introduce Neural-IMLS, a novel approach that directly learns a noise-resistant signed distance function (SDF) from unoriented raw point clouds in a self-supervised manner. In particular, IMLS regularizes MLP by providing estimated SDFs near the surface and helps enhance its ability to represent geometric details and sharp features, while MLP regularizes IMLS by providing estimated normals. We prove that at convergence, our neural network produces a faithful SDF whose zero-level set approximates the underlying surface due to the mutual learning mechanism between the MLP and the IMLS. Extensive experiments on various benchmarks, including synthetic and real scans, show that Neural-IMLS can reconstruct faithful shapes even with noise and missing parts. The source code can be found at https://github.com/bearprin/Neural-IMLS.
C1 [Wang, Zixiong; Wang, Pengfei; Dong, Qiujie; Gao, Junjie; Xin, Shiqing; Tu, Changhe] Shandong Univ, Sch Comp Sci & Technol, Jinan 251600, Peoples R China.
   [Wang, Peng-Shuai] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Chen, Shuangmin] Qingdao Univ Sci & Technol, Sch Informat & Technol, Qingdao 266101, Shandong, Peoples R China.
   [Wang, Wenping] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
C3 Shandong University; Peking University; Qingdao University of Science &
   Technology; Texas A&M University System; Texas A&M University College
   Station
RP Xin, SQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 251600, Peoples R China.
EM zixiong_wang@outlook.com; 8144756@qq.com; wangps@hotmail.com;
   qiujie.jay.dong@gmail.com; gjjsdnu@163.com; csmqq@163.com;
   xinshiqing@sdu.edu.cn; chtu@sdu.edu.cn; wenping@tamu.edu
RI Wang, Peng-Shuai/AAK-6216-2021; Wang, Zixiong/JBS-2459-2023; Gao,
   Junjie/F-7601-2012; Wang, Pengfei/Y-4426-2018
OI Wang, Peng-Shuai/0000-0001-9700-8188; Xin, Shiqing/0000-0001-8452-8723;
   Wang, Zixiong/0000-0002-6170-7339; Dong, Qiujie/0000-0001-6271-2546
FU National Key R#x0026;D Program of China [2022YFB3303200]; National
   Natural Science Foundation of China [62002190, 62272277]; Natural
   Science Foundation of Shandong Province [ZR2020MF036]
FX No Statement Available
CR Atzmon M., 2021, 9 INT C LEARN REPR
   Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Ben-Shabat Y, 2022, PROC CVPR IEEE, P19301, DOI 10.1109/CVPR52688.2022.01872
   Berger M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451246
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Boulch A, 2022, PROC CVPR IEEE, P6292, DOI 10.1109/CVPR52688.2022.00620
   Calakli F, 2011, COMPUT GRAPH FORUM, V30, P1993, DOI 10.1111/j.1467-8659.2011.02058.x
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chaton T, 2020, INT CONF 3D VISION, P190, DOI 10.1109/3DV50981.2020.00029
   Chen C, 2022, LECT NOTES COMPUT SC, V13663, P322, DOI 10.1007/978-3-031-20062-5_19
   Chen ZQ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530108
   Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700
   Cohen-Steiner D, 2004, VISUAL COMPUT, V20, P4, DOI 10.1007/s00371-003-0217-z
   Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gropp A, 2020, PR MACH LEARN RES, V119
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gschwandtner Michael, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P199, DOI 10.1007/978-3-642-24031-7_20
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415
   Hou F, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530096
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Huang JH, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555457
   Huang Z., 2022, Surface reconstruction from point clouds: A survey and a benchmark
   Huang ZY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322994
   Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kazhdan M, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14077
   Kingma D.P., 2014, P INT C LEARNING REP
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   Kolluri R, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1361192.1361195
   Laric Oliver, 2012, Three D Scans
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Li MY, 2016, COMPUT AIDED GEOM D, V48, P49, DOI 10.1016/j.cagd.2016.08.001
   Lipman Y., 2021, P 38 INT C MACHINE L, P6702
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   Ma BR, 2021, PR MACH LEARN RES, V139
   Ma BR, 2022, PROC CVPR IEEE, P6316, DOI 10.1109/CVPR52688.2022.00622
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Metzer G, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459835
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paszke A, 2019, ADV NEUR IN, V32
   Peng S., 2020, ECCV, P523
   Peng SY, 2021, ADV NEUR IN, V34
   Rakotosaona MJ, 2021, PROC CVPR IEEE, P22, DOI 10.1109/CVPR46437.2021.00009
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Schroers C, 2014, COMPUT GRAPH FORUM, V33, P195, DOI 10.1111/cgf.12445
   Sellán S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555441
   Sharma G., 2020, COMPUTER VISION ECCV, P261, DOI DOI 10.1007/978-3-030-58571-6_16
   Shen C, 2004, ACM T GRAPHIC, V23, P896, DOI 10.1145/1015706.1015816
   Sitzmann V., 2020, Adv. Neural Inf. Process. Syst., V33, P7462
   Stutz D, 2018, Arxiv, DOI arXiv:1805.07290
   Sulzer R, 2024, Arxiv, DOI [arXiv:2301.13656, DOI 10.48550/ARXIV.2301.13656]
   Takikawa Towaki, 2022, Kaolin: A Pytorch Library for Accelerating 3D Deep Learning Research
   Tancik M., 2020, Advances in Neural Information Processing Systems, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tang Jia-Heng, 2021, Advances in Neural Information Processing Systems, V34, P12648
   Tang JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6484, DOI 10.1109/ICCV48922.2021.00644
   Wang PS, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530087
   Wang YF, 2021, PROC CVPR IEEE, P374, DOI 10.1109/CVPR46437.2021.00044
   Williams F, 2021, PROC CVPR IEEE, P9944, DOI 10.1109/CVPR46437.2021.00982
   Williams F, 2019, PROC CVPR IEEE, P10122, DOI 10.1109/CVPR.2019.01037
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie YH, 2022, COMPUT GRAPH FORUM, V41, P641, DOI 10.1111/cgf.14505
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
NR 66
TC 2
Z9 2
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5018
EP 5033
DI 10.1109/TVCG.2023.3284233
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400090
PM 37289616
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Ge, T
   Luo, X
   Wang, YH
   Sedlmair, M
   Cheng, ZL
   Zhao, Y
   Liu, X
   Deussen, O
   Chen, BQ
AF Ge, Tong
   Luo, Xu
   Wang, Yunhai
   Sedlmair, Michael
   Cheng, Zhanglin
   Zhao, Ying
   Liu, Xin
   Deussen, Oliver
   Chen, Baoquan
TI Optimally Ordered Orthogonal Neighbor Joining Trees for Hierarchical
   Cluster Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Couplings; Clustering algorithms; Layout; Visualization; Biology;
   Germanium; Sorting; Neighbor joining; leaf ordering; orthogonal layout
ID VISUALIZATION; ALGORITHMS; MODEL
AB We propose to use optimally ordered orthogonal neighbor-joining (O-3 NJ) trees as a new way to visually explore cluster structures and outliers in multi-dimensional data. Neighbor-joining (NJ) trees are widely used in biology, and their visual representation is similar to that of dendrograms. The core difference to dendrograms, however, is that NJ trees correctly encode distances between data points, resulting in trees with varying edge lengths. We optimize NJ trees for their use in visual analysis in two ways. First, we propose to use a novel leaf sorting algorithm that helps users to better interpret adjacencies and proximities within such a tree. Second, we provide a new method to visually distill the cluster tree from an ordered NJ tree. Numerical evaluation and three case studies illustrate the benefits of this approach for exploring multi-dimensional data in areas such as biology or image analysis.
C1 [Ge, Tong; Luo, Xu; Wang, Yunhai] Shandong Univ, Dept Comp Sci, Jinan 250100, Shandong, Peoples R China.
   [Sedlmair, Michael] Univ Stuttgart, D-70174 Stuttgart, Germany.
   [Cheng, Zhanglin] SIAT, Shenzhen VisuCA Key Lab, Shenzhen 518055, Guangdong, Peoples R China.
   [Zhao, Ying] Cent South Univ, Changsha 410017, Hunan, Peoples R China.
   [Liu, Xin] Beijing Genom Inst BGI Shenzhen, Shenzhen 518083, Guangdong, Peoples R China.
   [Deussen, Oliver] Univ Konstanz, D-78464 Constance, Germany.
   [Chen, Baoquan] Peking Univ, Beijing 100871, Peoples R China.
C3 Shandong University; University of Stuttgart; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Central South
   University; Beijing Genomics Institute (BGI); University of Konstanz;
   Peking University
RP Wang, YH (corresponding author), Shandong Univ, Dept Comp Sci, Jinan 250100, Shandong, Peoples R China.
EM tgeconf@gmail.com; luoxu9days@gmail.com; cloudseawang@gmail.com;
   Michael.Sedlmair@visus.uni-stuttgart.de; zl.cheng@siat.ac.cn;
   zhaoying@csu.edu.cn; liuxin@genomics.cn; Oliver.Deussen@uni-konstanz.de;
   baoquan@pku.edu.cn
RI Liu, Xin/ABJ-9485-2022; Cheng, Zhanglin/AAP-1760-2021; Zhao,
   Liangyu/IAO-7294-2023; Deussen, Oliver/HKF-2004-2023
OI Liu, Xin/0000-0003-3256-2940; Cheng, Zhanglin/0000-0002-3360-2679; Luo,
   Xu/0000-0003-1501-7385
FU National Key R&D Program of China [2022ZD0160805]; NSFC [62132017,
   62141217]; Shandong Provincial Natural Science Foundation [ZQ2022JQ32];
   DFG [EXC2117-422037984, 251654672 - TRR 161]; Shenzhen Science and
   Technology Program [GJHZ20210705141402008]
FX This work was supported by the grants of the National Key R&D Program of
   China under Grant 2022ZD0160805, in part by NSFC under Grants 62132017
   and 62141217, in part by Shandong Provincial Natural Science Foundation
   under Grant ZQ2022JQ32 as well as in part by by the DFG (German Research
   Foundation) under Germany's Excellence under Grant
   Strategy-EXC2117-422037984,in part by the Project-ID 251654672 - TRR
   161, and in part by Shenzhen Science and Technology Program under Grant
   GJHZ20210705141402008.
CR Bachmaier C, 2005, LECT NOTES COMPUT SC, V3827, P1110, DOI 10.1007/11602613_110
   Bachmaier C., 2005, Drawing Phylogenetic Trees, DOI [10.1007/11602613110.2Z, DOI 10.1007/11602613110.2Z]
   Backeljau T, 1996, MOL BIOL EVOL, V13, P309, DOI 10.1093/oxfordjournals.molbev.a025590
   Bar-Joseph Z, 2003, BIOINFORMATICS, V19, P1070, DOI 10.1093/bioinformatics/btg030
   Bar-Joseph Z., 2001, Bioinformatics, V17, p$22 $29, DOI [10.1093/bioinformatics/17.supp_1.S22, DOI 10.1093/BIOINFORMATICS/17.SUPP_1.S22]
   Brehmer M., 2014, P 5 WORKSH TIM ERR N, P1, DOI DOI 10.1145/2669557.2669559
   Buneman P., 1974, Com- binatorial Theory, Ser. B., V17, P48, DOI [10.1016/0095-8956(74)90047-1, DOI 10.1016/0095-8956(74)90047-1]
   Burch M, 2011, IEEE T VIS COMPUT GR, V17, P2440, DOI 10.1109/TVCG.2011.193
   Cayton L., 2005, Tech. Rep. 12.1-17
   Chae MH, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022546
   Cuadros AM, 2007, IEEE CONF VIS ANAL, P99, DOI 10.1109/VAST.2007.4389002
   DAY WHE, 1984, J CLASSIF, V1, P7, DOI 10.1007/BF01890115
   de Oliveira MCF, 2003, IEEE T VIS COMPUT GR, V9, P378, DOI 10.1109/TVCG.2003.1207445
   Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863
   Ester M., 1996, P KDD, P226
   Etemadpour R, 2015, IEEE T VIS COMPUT GR, V21, P81, DOI 10.1109/TVCG.2014.2330617
   FARACH M, 1995, ALGORITHMICA, V13, P155, DOI 10.1007/BF01188585
   Fraley C, 1999, J CLASSIF, V16, P297, DOI 10.1007/s003579900058
   Franti P., 2017, Clustering datasets
   GRUVAEUS G, 1972, BRIT J MATH STAT PSY, V25, P200, DOI 10.1111/j.2044-8317.1972.tb00491.x
   Gyulassy A, 2006, IEEE T VIS COMPUT GR, V12, P474, DOI 10.1109/TVCG.2006.57
   Halle M., 1990, An essay on stress
   Han J, 2012, MOR KAUF D, P1
   Hebert PDN, 2003, P ROY SOC B-BIOL SCI, V270, P313, DOI 10.1098/rspb.2002.2218
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Krizhevsky A., 2009, Cifar-100 dataset
   KUIPER FK, 1975, BIOMETRICS, V31, P777, DOI 10.2307/2529565
   Landau S., 2011, Cluster Analysis, Vfifth
   Lichman M., 2013, UCI MACHINE LEARNING
   McGuffin MJ, 2010, INFORM VISUAL, V9, P115, DOI 10.1057/ivs.2009.4
   MILLIGAN GW, 1980, PSYCHOMETRIKA, V45, P325, DOI 10.1007/BF02293907
   Munzner T, 2003, ACM T GRAPHIC, V22, P453, DOI 10.1145/882262.882291
   Murtagh F, 2012, WIRES DATA MIN KNOWL, V2, P86, DOI 10.1002/widm.53
   Nakhleh L, 2005, T PHILOL SOC, V103, P171, DOI 10.1111/j.1467-968X.2005.00149.x
   Paiva JGS, 2011, IEEE T VIS COMPUT GR, V17, P2459, DOI 10.1109/TVCG.2011.212
   Paradis E, 2019, BIOINFORMATICS, V35, P526, DOI 10.1093/bioinformatics/bty633
   Procter JB, 2010, NAT METHODS, V7, pS16, DOI [10.1038/NMETH.1434, 10.1038/nmeth.1434]
   Qi JJ, 2013, NAT GENET, V45, P1510, DOI 10.1038/ng.2801
   Rambaut A, 2008, NATURE, V453, P615, DOI 10.1038/nature06945
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   SAITOU N, 1987, MOL BIOL EVOL, V4, P406, DOI 10.1093/oxfordjournals.molbev.a040454
   Sakai Ryo, 2014, F1000Res, V3, P177, DOI 10.12688/f1000research.4784.1
   Saraçli S, 2013, J INEQUAL APPL, DOI 10.1186/1029-242X-2013-203
   Sebastian P, 2010, P NATL ACAD SCI USA, V107, P14269, DOI 10.1073/pnas.1005338107
   Seo J, 2002, COMPUTER, V35, P80
   Shah SA, 2017, P NATL ACAD SCI USA, V114, P9814, DOI 10.1073/pnas.1700770114
   SHNEIDERMAN B, 1992, ACM T GRAPHIC, V11, P92, DOI 10.1145/102377.115768
   Simonsen M, 2008, LECT N BIOINFORMAT, V5251, P113, DOI 10.1007/978-3-540-87361-7_10
   Slonim N, 2000, ADV NEUR IN, V12, P617
   SOKAL ROBERT R., 1962, TAXON, V11, P33, DOI 10.2307/1217208
   Telles GP, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2162-x
   Weinkauf T, 2009, COMPUT GRAPH FORUM, V28, P1519, DOI 10.1111/j.1467-8659.2009.01528.x
   Zhao Y., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P515, DOI 10.1145/584792.584877
NR 53
TC 2
Z9 2
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5034
EP 5046
DI 10.1109/TVCG.2023.3284499
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400062
PM 37294655
DA 2024-11-06
ER

PT J
AU Hou, SY
   Tao, HY
   Bao, HJ
   Xu, WW
AF Hou, Shuaiying
   Tao, Hongyu
   Bao, Hujun
   Xu, Weiwei
TI A Two-Part Transformer Network for Controllable Motion Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Solid modeling; Training; Transformers;
   Manifolds; Correlation; Biological system modeling; Human motion
   synthesis; transformer; deep learning; heterogeneous motion; body parts
AB Although part-based motion synthesis networks have been investigated to reduce the complexity of modeling heterogeneous human motions, their computational cost remains prohibitive in interactive applications. To this end, we propose a novel two-part transformer network that aims to achieve high-quality, controllable motion synthesis results in real-time. Our network separates the skeleton into the upper and lower body parts, reducing the expensive cross-part fusion operations, and models the motions of each part separately through two streams of auto-regressive modules formed by multi-head attention layers. However, such a design might not sufficiently capture the correlations between the parts. We thus intentionally let the two parts share the features of the root joint and design a consistency loss to penalize the difference in the estimated root features and motions by these two auto-regressive modules, significantly improving the quality of synthesized motions. After training on our motion dataset, our network can synthesize a wide range of heterogeneous motions, like cartwheels and twists. Experimental and user study results demonstrate that our network is superior to state-of-the-art human motion synthesis networks in the quality of generated motions.
C1 [Hou, Shuaiying; Tao, Hongyu; Bao, Hujun; Xu, Weiwei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Xu, WW (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM 11721044@zju.edu.cn; 3170102625@zju.edu.cn; bao@cad.zju.edu.cn;
   18058700512@163.com
RI Xu, Weiwei/B-5045-2017
OI Bao, Hujun/0000-0002-2662-0334; Xu, Weiwei/0000-0003-3756-3539; Hou,
   Shuaiying/0000-0003-0689-9240
FU National Natural Science Foundation of China [61732016]; Information
   Technology Center and State Key Lab of CAD&CG at Zhejiang University
FX Weiwei Xu is partially supported by the National Natural Science
   Foundation of China under Grant 61732016. And we also thank the support
   provided by the Information Technology Center and State Key Lab of
   CAD&CG at Zhejiang University.
CR Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392462
   Adobe, 2021, Mixamo
   Brown TB, 2020, Arxiv, DOI arXiv:2005.14165
   Baevski A, 2020, Arxiv, DOI [arXiv:2006.11477, 10.48550/arXiv.2006.11477]
   Bengio S, 2015, Arxiv, DOI arXiv:1506.03099
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Chen WH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2131, DOI 10.1145/3394171.3413669
   Corona E, 2020, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR42600.2020.00702
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Degardin Bruno, 2022, P IEEE CVF WINT C AP, P1150
   Dhariwal P, 2020, Arxiv, DOI arXiv:2005.00341
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Ghosh A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1376, DOI 10.1109/ICCV48922.2021.00143
   Grassia F. S., 1998, Journal of Graphics Tools, V3, P29
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Holden D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392440
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hou SY, 2023, Arxiv, DOI arXiv:2101.12276
   Huang RZ, 2021, Arxiv, DOI arXiv:2006.06119
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jang DK, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3516429
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Lee K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459826
   Lee K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275071
   Lee S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459774
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li P., 2022, arXiv
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Li SY, 2022, PROC CVPR IEEE, P11040, DOI 10.1109/CVPR52688.2022.01077
   Li Y., 2020, PREPRINT
   Li ZM, 2018, Arxiv, DOI arXiv:1707.05363
   Ling C., 1998, Kdd, P73
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Liu LB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201315
   Liu Z, 2021, Arxiv, DOI [arXiv:2103.14030, DOI 10.48550/ARXIV.2103.14030]
   Liu ZG, 2021, AAAI CONF ARTIF INTE, V35, P2225
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, DOI 10.48550/ARXIV.1711.05101, 10.48550/arXiv.1711.05101]
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Oord A.v.d., 2016, arXiv
   Peng XB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201311
   Perez J., 2021, P 14 ACM SIGGRAPH C, P1
   Ping Yu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P18, DOI 10.1007/978-3-030-58577-8_2
   Shi MY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3407659
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Starke S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530178
   Starke S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459881
   Starke S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392450
   Starke S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356505
   Tang X., 2022, arXiv
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Vaswani A., 2017, Proceedings of NeurIPS, Longbeach, P5998
   Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901
   Wang H, 2021, IEEE T VIS COMPUT GR, V27, P216, DOI 10.1109/TVCG.2019.2936810
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang X, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/104535
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2021, IEEE T VIS COMPUT GR, V27, P14, DOI 10.1109/TVCG.2019.2938520
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Won J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459761
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yan Y., 2021, arXiv
   Zhang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201366
   Zhou HY, 2021, AAAI CONF ARTIF INTE, V35, P11106
NR 66
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5047
EP 5062
DI 10.1109/TVCG.2023.3284402
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400037
PM 37294654
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Falk, M
   Tobiasson, V
   Bock, A
   Hansen, C
   Ynnerman, A
AF Falk, Martin
   Tobiasson, Victor
   Bock, Alexander
   Hansen, Charles
   Ynnerman, Anders
TI A Visual Environment for Data Driven Protein Modeling and Validation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Proteins; Measurement; Visualization; Biological system modeling; Image
   reconstruction; Analytical models; Three-dimensional displays; Molecular
   visualization; cryo-EM; model validation; verification
ID CRYO-EM; VISUALIZATION; MICROSCOPY
AB In structural biology, validation and verification of new atomic models are crucial and necessary steps which limit the production of reliable molecular models for publications and databases. An atomic model is the result of meticulous modeling and matching and is evaluated using a variety of metrics that provide clues to improve and refine the model so it fits our understanding of molecules and physical constraints. In cryo electron microscopy (cryo-EM) the validation is also part of an iterative modeling process in which there is a need to judge the quality of the model during the creation phase. A shortcoming is that the process and results of the validation are rarely communicated using visual metaphors. This work presents a visual framework for molecular validation. The framework was developed in close collaboration with domain experts in a participatory design process. Its core is a novel visual representation based on 2D heatmaps that shows all available validation metrics in a linear fashion, presenting a global overview of the atomic model and provide domain experts with interactive analysis tools. Additional information stemming from the underlying data, such as a variety of local quality measures, is used to guide the user's attention toward regions of higher relevance. Linked with the heatmap is a three-dimensional molecular visualization providing the spatial context of the structures and chosen metrics. Additional views of statistical properties of the structure are included in the visual framework. We demonstrate the utility of the framework and its visual guidance with examples from cryo-EM.
C1 [Falk, Martin; Bock, Alexander; Hansen, Charles; Ynnerman, Anders] Linkoping Univ, S-58183 Linkoping, Sweden.
   [Falk, Martin; Bock, Alexander; Ynnerman, Anders] Swedish Esci Res Ctr SeRC, S-16429 Kista, Sweden.
   [Tobiasson, Victor] Stockholm Univ, Dept Biochem & Biophys, Sci Life Lab, S-17165 Solna, Sweden.
   [Hansen, Charles] Univ Utah, Kahlert Sch Comp, Salt Lake City, UT 84112 USA.
C3 Linkoping University; Stockholm University; Utah System of Higher
   Education; University of Utah
RP Falk, M (corresponding author), Linkoping Univ, S-58183 Linkoping, Sweden.
EM martin.falk@liu.se; victor.tobiasson@scilifelab.se;
   alexander.bock@liu.se; hansen@cs.utah.edu; anders.ynnerman@liu.se
RI Tobiasson, Victor/GRF-5270-2022
OI Bock, Alexander/0000-0002-2849-6146; Ynnerman,
   Anders/0000-0002-9466-9826; Hansen, Charles/0000-0002-8480-2152; Falk,
   Martin/0000-0003-1511-5006; Tobiasson, Victor/0000-0001-8920-017X
FU Excellence Center at Link#x00F6; ping and Lund in Information
   Technology; Swedish e-Science Research Centre (SeRC); Swedish Research
   Council (VR) [2015-05462]; NIH [R01EB023947, R01EB031872]; Knut and
   Alice Wallenberg Foundation [KAW 2019.0024]; Vinnova [2015-05462]
   Funding Source: Vinnova; Swedish Research Council [2015-05462] Funding
   Source: Swedish Research Council
FX No Statement Available
CR Bai XC, 2015, TRENDS BIOCHEM SCI, V40, P49, DOI 10.1016/j.tibs.2014.10.005
   Barad BA, 2015, NAT METHODS, V12, P943, DOI [10.1038/NMETH.3541, 10.1038/nmeth.3541]
   Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235
   Bernstein HJ, 2000, TRENDS BIOCHEM SCI, V25, P453, DOI 10.1016/S0968-0004(00)01606-6
   Boerema AP, 2018, NAT PLANTS, V4, P212, DOI 10.1038/s41477-018-0129-6
   Burnley T, 2017, ACTA CRYSTALLOGR D, V73, P469, DOI 10.1107/S2059798317007859
   Carroni M, 2016, METHODS, V95, P78, DOI 10.1016/j.ymeth.2015.11.023
   Chen VB, 2010, ACTA CRYSTALLOGR D, V66, P12, DOI 10.1107/S0907444909042073
   Chen VB, 2009, PROTEIN SCI, V18, P2403, DOI 10.1002/pro.250
   Cheng Y, 2015, CELL, V161, P438, DOI 10.1016/j.cell.2015.03.050
   Cianfrocco M. A., 2017, P PRACT EXP ADV RES, DOI [10.1145/3093338.3093390, DOI 10.1145/3093338.3093390]
   de la Rosa-Trevín JM, 2016, J STRUCT BIOL, V195, P93, DOI 10.1016/j.jsb.2016.04.010
   DeLano W.L., 2002, CCP4 Newsl, Protein Crystallogr, V40, P82, DOI DOI 10.1038/S41598-017-03842-2
   Desai N, 2017, SCIENCE, V355, P528, DOI 10.1126/science.aal2415
   Emsley P, 2010, ACTA CRYSTALLOGR D, V66, P486, DOI 10.1107/S0907444910007493
   Fernandez NF, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.151
   GehIenbor N., 2005, Information Visualization, V4, P164, DOI 10.1057/palgrave.ivs.9500094
   Gupta M, 2021, bioRxiv, DOI [10.1101/2021.05.10.443524, 10.1101/2021.05.10.443524, DOI 10.1101/2021.05.10.443524, 10.1101/2021.05.10.443524v1, DOI 10.1101/2021.05.10.443524V1]
   Herráez A, 2006, BIOCHEM MOL BIOL EDU, V34, P255, DOI 10.1002/bmb.2006.494034042644
   Humphrey W, 1996, J MOL GRAPH MODEL, V14, P33, DOI 10.1016/0263-7855(96)00018-5
   Jönsson D, 2020, IEEE T VIS COMPUT GR, V26, P3241, DOI 10.1109/TVCG.2019.2920639
   Jumper J, 2021, NATURE, V596, P583, DOI 10.1038/s41586-021-03819-2
   Kimanius D, 2016, ELIFE, V5, DOI 10.7554/eLife.18722
   Kozlíková B, 2017, COMPUT GRAPH FORUM, V36, P178, DOI 10.1111/cgf.13072
   Kucukelbir A, 2014, NAT METHODS, V11, P63, DOI [10.1038/NMETH.2727, 10.1038/nmeth.2727]
   Kühlbrandt W, 2014, SCIENCE, V343, P1443, DOI 10.1126/science.1251652
   Lawson CL, 2021, NAT METHODS, V18, P156, DOI 10.1038/s41592-020-01051-w
   Lawson CL, 2016, NUCLEIC ACIDS RES, V44, pD396, DOI 10.1093/nar/gkv1126
   Liebschner D, 2019, ACTA CRYSTALLOGR D, V75, P861, DOI 10.1107/S2059798319011471
   Pettersen EF, 2004, J COMPUT CHEM, V25, P1605, DOI 10.1002/jcc.20084
   Pettersen EF, 2021, PROTEIN SCI, V30, P70, DOI 10.1002/pro.3943
   Prisant MG, 2020, PROTEIN SCI, V29, P315, DOI 10.1002/pro.3786
   Punjani A, 2017, NAT METHODS, V14, P290, DOI [10.1038/NMETH.4169, 10.1038/nmeth.4169]
   RAMACHANDRAN GN, 1963, J MOL BIOL, V7, P95, DOI 10.1016/S0022-2836(63)80023-6
   Ramírez-Aportela E, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-020-20295-w
   Reina G., 2005, P EUROGRAPHICS IEEE, P177
   Scheres SHW, 2012, J STRUCT BIOL, V180, P519, DOI 10.1016/j.jsb.2012.09.006
   Skanberg R., 2018, P EG WORKSH MOL GRAP, P19, DOI [10.2312/molva.20181102, DOI 10.2312/MOLVA.20181102]
   Thompson RF, 2016, METHODS, V100, P3, DOI 10.1016/j.ymeth.2016.02.017
   Thorvaldsdóttir H, 2013, BRIEF BIOINFORM, V14, P178, DOI 10.1093/bib/bbs017
   Tobiasson V, 2020, ELIFE, V9, DOI 10.7554/eLife.59264
   van Heel M, 2005, J STRUCT BIOL, V151, P250, DOI 10.1016/j.jsb.2005.05.009
   Yamashita K, 2021, ACTA CRYSTALLOGR D, V77, P1282, DOI 10.1107/S2059798321009475
NR 43
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5063
EP 5073
DI 10.1109/TVCG.2023.3286582
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400069
PM 37327104
OA Green Published
DA 2024-11-06
ER

PT J
AU Borhani, Z
   Sharma, P
   Ortega, FR
AF Borhani, Zahra
   Sharma, Prashast
   Ortega, Francisco R.
TI Survey of Annotations in Extended Reality Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Annotation; augmented reality; extended reality; immersive technologies;
   virtual reality
ID VISUALIZATION; DESIGN
AB Annotation in 3D user interfaces such as Augmented Reality (AR) and Virtual Reality (VR) is a challenging and promising area; however, there are not currently surveys reviewing these contributions. In order to provide a survey of annotations for Extended Reality (XR) environments, we conducted a structured literature review of papers that used annotation in their AR/VR systems from the period between 2001 and 2021. Our literature review process consists of several filtering steps which resulted in 103 XR publications with a focus on annotation. We classified these papers based on the display technologies, input devices, annotation types, target object under annotation, collaboration type, modalities, and collaborative technologies. A survey of annotation in XR is an invaluable resource for researchers and newcomers. Finally, we provide a database of the collected information for each reviewed paper. This information includes applications, the display technologies and its annotator, input devices, modalities, annotation types, interaction techniques, collaboration types, and tasks for each paper. This database provides a rapid access to collected data and gives users the ability to search or filter the required information. This survey provides a starting point for anyone interested in researching annotation in XR environments.
C1 [Borhani, Zahra; Ortega, Francisco R.] Colorado State Univ, Ft Collins, CO 80523 USA.
   [Sharma, Prashast] Univ Florida, Gainesville, FL 32611 USA.
C3 Colorado State University; State University System of Florida;
   University of Florida
RP Borhani, Z; Ortega, FR (corresponding author), Colorado State Univ, Ft Collins, CO 80523 USA.
EM zahra.borhani@colostate.edu; prashast.sharma@ufl.edu;
   fortega@colostate.edu
RI ; Ortega, Francisco R./L-8377-2013
OI Sharma, Prashast/0009-0005-2680-7229; Ortega, Francisco
   R./0000-0002-2449-3802
FU NSF [2327569, 2238313, 2223432, 2223459, 2106590, 2016714, 2037417,
   1948254]
FX This work was supported in part by NSF under Grants 2327569,
   2238313,2223432, 2223459, 2106590, 2016714, 2037417, and 1948254.
   Recommendedfor acceptance by Kiyoshi Kiyokawa
CR Abrami Giuseppe, 2020, HT '20: Proceedings of the 31st ACM Conference on Hypertext and Social Media, P177, DOI 10.1145/3372923.3404791
   Al-Megren S, 2015, LECT NOTES COMPUT SC, V9299, P156, DOI 10.1007/978-3-319-22723-8_13
   Amma C., 2016, P CHI C HUM FACT COM, P3639
   Arora R, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5643, DOI 10.1145/3025453.3025474
   Austin CR, 2020, CARTOGR GEOGR INF SC, V47, P214, DOI 10.1080/15230406.2019.1696232
   Azuma R, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P66, DOI 10.1109/ISMAR.2003.1240689
   Bai HD, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376550
   Ball R., 2005, CHI'05 extended abstracts on Human factors in computing systems, P1196
   Baumeister J, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY WORKSHOPS, P4, DOI 10.1109/ISMARW.2015.11
   Bell B., 2002, P 15 ANN ACM S USER, P213
   Bimber O., 2005, Spatial augmented reality: merging real and virtual worlds
   Burigat S., 2006, P 8 C HUM COMP INT M, P239
   Caluya NR, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P866, DOI [10.1109/vr.2019.8798216, 10.1109/VR.2019.8798216]
   Casas S., 2020, SMART SYSTEMS DESIGN, P299, DOI [10.4018/978-1-7998-2112-0.ch015, DOI 10.4018/978-1-7998-2112-0.CH015]
   Chaconas N., 2018, P IEEE C VIRT REAL 3, P1
   Chandler T., 2015, P BIG DAT VIS AN, P8
   Chang YS, 2017, IEEE SYMP 3D USER, P182, DOI 10.1109/3DUI.2017.7893337
   Chang YS, 2017, P IEEE VIRT REAL ANN, P469, DOI 10.1109/VR.2017.7892383
   Chin G, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P11
   Choe EK, 2015, IEEE COMPUT GRAPH, V35, P28, DOI 10.1109/MCG.2015.51
   Cordeil M, 2017, IEEE T VIS COMPUT GR, V23, P441, DOI 10.1109/TVCG.2016.2599107
   D'Angelo S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2492, DOI 10.1145/2858036.2858499
   Danyluk K, 2022, IEEE T VIS COMPUT GR, V28, P1930, DOI 10.1109/TVCG.2020.3023336
   Dao B, 2013, P IEEE VIRT REAL ANN, P159, DOI 10.1109/VR.2013.6549411
   de Belen R. A. J., 2019, AIMS ELECT ELECT ENG, V3, P181
   Dominic J, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P607, DOI [10.1109/VR46266.2020.00-21, 10.1109/VR46266.2020.1581637338566]
   Dow S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1475
   Dunser A., 2008, TR200802 HUM INT TEC
   Edlin L., 2020, P INT C HUM COMP INT, P54
   Emerson L, 2021, INT SYM MIX AUGMENT, P183, DOI 10.1109/ISMAR-Adjunct54149.2021.00045
   Ens B, 2019, INT J HUM-COMPUT ST, V131, P81, DOI 10.1016/j.ijhcs.2019.05.011
   Fedorov R, 2016, LECT NOTES COMPUT SC, V9768, P281, DOI 10.1007/978-3-319-40621-3_21
   Ferdous HS, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300464
   Fiorentino M, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P86, DOI 10.1109/ISMAR.2002.1115077
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   García-Pereira I, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 1: GRAPP, P412, DOI 10.5220/0009193404120419
   García-Pereira I, 2020, MULTIMED TOOLS APPL, V79, P6483, DOI 10.1007/s11042-019-08419-x
   Gasques D., 2021, P CHI C HUM FACT COM, P14
   Gauglitz S., 2014, P 27 ANN ACM S US IN, P449
   Gauglitz S., 2014, P 20 ACM S VIRT REAL, P197
   Haller M, 2006, LECT NOTES COMPUT SC, V4282, P185
   Handa D., 2013, P INT C VIRT AUGM MI, P23
   Haouach M, 2009, 20TH ACM CONFERENCE ON HYPERTEXT AND HYPERMEDIA (HYPERTEXT 2009), P337
   Hart JD, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P212, DOI 10.1109/ISMAR-Adjunct.2018.00069
   Henderson S. J., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P191, DOI 10.1109/ISMAR.2011.6092386
   Henderson SJ, 2009, INT SYM MIX AUGMENT, P135, DOI 10.1109/ISMAR.2009.5336486
   Hertel J, 2021, INT SYM MIX AUGMENT, P431, DOI 10.1109/ISMAR52148.2021.00060
   Hincapié-Ramos JD, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1063, DOI 10.1145/2556288.2557130
   Hoang T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1419, DOI 10.1145/3025453.3025860
   Hollerer T., 2007, Proceedings of the 2007 workshop on Emerging displays technologies: images and beyond: the future of displays and interacton, P3, DOI DOI 10.1145/1278240.1278243
   Irlitti A, 2013, INT SYM MIX AUGMENT
   Ishii H., 2007, P CD ROM INT S SYMB, P262
   Kang D, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12094288
   Kasapakis V., 2018, P INT C AUGM REAL VI, P423
   Ke CZ, 2005, LECT NOTES COMPUT SC, V3784, P836
   Kim H., 2011, P IEEE 10 INT S MIX, P265
   Kim S., 2013, P IEEE INT S MIX AUG, P6
   Kim S., 2004, P ACM SIGGRAPH INT C, P336, DOI DOI 10.1145/1044588.1044662
   Kim YS, 2019, PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS '19), P255, DOI 10.1145/3343055.3359714
   Kishishita N, 2014, INT SYM MIX AUGMENT, P177, DOI 10.1109/ISMAR.2014.6948425
   Koeda M, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P288, DOI 10.1109/ISMAR.2004.15
   Kuhn V., 2020, P INT C HUM COMP INT, P299
   Kumaravel Balasaravanan Thoravi, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P182, DOI 10.1145/3379337.3415827
   Kumaravel BT, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P161, DOI 10.1145/3332165.3347872
   Langlotz T., 2013, P 25 AUSTR COMP HUM, P545, DOI DOI 10.1145/2541016.2541022
   Lee B, 2021, IEEE T VIS COMPUT GR, V27, P1171, DOI 10.1109/TVCG.2020.3030450
   Lee B, 2013, IEEE T VIS COMPUT GR, V19, P2416, DOI 10.1109/TVCG.2013.191
   Lee G. A., 2012, P 11 ACM SIGGRAPH IN, P279
   Leoet J., 2021, P AS CHI S 2021, P132
   Lien KC, 2015, 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P184, DOI 10.1109/ISMAR.2015.56
   Lin CY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P212, DOI [10.1109/VR46266.2020.1580934001692, 10.1109/VR46266.2020.00-64]
   Lin CY, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P3, DOI 10.1109/ISMAR-Adjunct.2018.00021
   Lindlbauer D, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P147, DOI 10.1145/3332165.3347945
   Liu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P6730, DOI 10.1145/3025453.3025594
   Luo WZ, 2021, INT SYM MIX AUGMENT, P334, DOI 10.1109/ISMAR-Adjunct54149.2021.00076
   MacIntyre B., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P65, DOI 10.1109/ISMAR.2011.6092371
   Madsen JB, 2016, IEEE T VIS COMPUT GR, V22, P1415, DOI 10.1109/TVCG.2016.2518318
   Mahmood T, 2019, INT SYM MIX AUGMENT, P236, DOI 10.1109/ISMAR.2019.00021
   Marner MR, 2013, INT SYM MIX AUGMENT, P39, DOI 10.1109/ISMAR.2013.6671762
   Marques B., 2022, P EUR C COMP SUPP CO, P9
   Marques B, 2022, INT J INTERACT DES M, V16, P419, DOI 10.1007/s12008-021-00798-6
   Marques B, 2022, IEEE T VIS COMPUT GR, V28, P5113, DOI 10.1109/TVCG.2021.3101545
   Marques B, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P567, DOI 10.1109/VRW52623.2021.00166
   Matos T., 2018, P 23 INT ACM C 3D WE, P4
   Matt Adcock, 2013, P 12 ACM SIGGRAPH IN, P235
   McNamara A, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P36, DOI [10.1109/ISMAR-Adjunct.2016.0033, 10.1109/ISMAR-Adjunct.2016.26]
   Medeiros ML, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P465, DOI 10.1109/ISMAR-Adjunct.2019.00125
   Mehler A, 2018, HT'18: PROCEEDINGS OF THE 29TH ACM CONFERENCE ON HYPERTEXT AND SOCIAL MEDIA, P150, DOI 10.1145/3209542.3209572
   Mohanty RR, 2018, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2018, VOL 1B
   Mohr P, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3337, DOI 10.1145/2702123.2702490
   Mohr-Ziak P., 2020, P ACM CHI C HUM FACT, P12
   Mooser J., 2007, P IEEEACM INT S MIXE, P145
   Morris M. R., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1201
   Noh S., 2015, P INT C ART REAL TEL, P61
   Nuernberger B, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P129, DOI 10.1145/2993369.2993371
   Nuernberger B, 2016, P IEEE VIRT REAL ANN, P247, DOI 10.1109/VR.2016.7504746
   Nuernberger B, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P149, DOI 10.1109/3DUI.2016.7460046
   Oda O, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P405, DOI 10.1145/2807442.2807497
   Olsson T., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P75, DOI 10.1109/ISMAR.2011.6092372
   Peña AM, 2019, LECT NOTES COMPUT SC, V11575, P338, DOI 10.1007/978-3-030-21565-1_23
   Phon DNE, 2014, INT CONF TEACH LEARN, P78, DOI 10.1109/LaTiCE.2014.23
   Pick S, 2016, IEEE T VIS COMPUT GR, V22, P1452, DOI 10.1109/TVCG.2016.2518086
   Pick S, 2015, P IEEE VIRT REAL ANN, P261, DOI 10.1109/VR.2015.7223395
   Piumsomboon T, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3186495
   Piumsomboon T, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P218, DOI 10.1109/ISMAR-Adjunct.2017.72
   Polvi J, 2018, IEEE T VIS COMPUT GR, V24, P2118, DOI 10.1109/TVCG.2017.2709746
   Radu I, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451561
   Raskar R., 2001, Proceedings AFRIGRAPH 2001. 1st International Conference on Computer Graphics, Virtual Reality and Visualisation, P101, DOI 10.1145/513867.513889
   Raskar R, 1999, AUGMENTED REALITY, P63
   Rebol M, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P653, DOI 10.1109/VRW52623.2021.00209
   Reitmayr G., 2007, P IEEEACM INT S MIXE, P67
   Ren DH, 2017, IEEE PAC VIS SYMP, P230, DOI 10.1109/PACIFICVIS.2017.8031599
   Ren D, 2016, P IEEE VIRT REAL ANN, P93, DOI 10.1109/VR.2016.7504692
   Rodrigue M, 2015, P IEEE VIRT REAL ANN, P105, DOI 10.1109/VR.2015.7223331
   Romat H., 2021, P CHI C HUM FACT COM, P4
   Romat H, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P306, DOI 10.1109/VR50410.2021.00053
   Romat H, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P749, DOI 10.1109/VRW52623.2021.00257
   Romat H, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300272
   Ryskeldiev B, 2018, COMPANION OF THE 2018 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'18), P373, DOI 10.1145/3272973.3274100
   Sankaran NK, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P664, DOI 10.1109/vr.2019.8798089
   Santos MEC, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P196, DOI 10.1109/ISMAR.2015.62
   Sasikumar P, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P393, DOI 10.1109/ISMAR-Adjunct.2019.000-3
   Schall G, 2008, INT SYM MIX AUGMENT, P95, DOI 10.1109/ISMAR.2008.4637332
   Schmalstieg D., 2016, AUGMENTED REALITY PR
   Selonen P., 2010, P 1 INT WORKSH RESTF, P54
   Sereno M, 2022, IEEE T VIS COMPUT GR, V28, P2530, DOI 10.1109/TVCG.2020.3032761
   Skinner P, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P145, DOI 10.1109/ISMAR-Adjunct.2018.00054
   Slater M, 2003, Presence Conn, V3, P1
   Speicher Maximilian, 2018, Proceedings of the ACM on Human-Computer Interaction, V2, DOI 10.1145/3229091
   Sun H., 2016, P 28 AUSTR C COMP HU, P195
   Tang A., 2003, COMP EFFECTIVENESS A, P73
   Tatzgern M, 2016, P IEEE VIRT REAL ANN, P83, DOI 10.1109/VR.2016.7504691
   Tatzgern M, 2014, 2014 IEEE VIRTUAL REALITY (VR), P27, DOI 10.1109/VR.2014.6802046
   Tatzgern M, 2013, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2013.6549347
   Tenmoku R, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P192
   Teo T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1187, DOI [10.1109/vr.2019.8798128, 10.1109/VR.2019.8798128]
   Teo T, 2018, PROCEEDINGS OF THE 30TH AUSTRALIAN COMPUTER-HUMAN INTERACTION CONFERENCE (OZCHI 2018), P406, DOI 10.1145/3292147.3292200
   Thomas Bruce H., 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P33, DOI 10.1109/ISMAR.2006.297791
   Tomlein M., 2018, PROC ACM HUMCOMPUT I, V2, P1
   Utzig S, 2019, AEROSP CONF PROC
   Volmer B, 2018, IEEE T VIS COMPUT GR, V24, P2846, DOI 10.1109/TVCG.2018.2868587
   Wang P, 2020, INT J HUM-COMPUT INT, V36, P1242, DOI 10.1080/10447318.2020.1732140
   Wang P, 2021, ROBOT CIM-INT MANUF, V72, DOI 10.1016/j.rcim.2020.102071
   Wang P, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P77, DOI 10.1109/ISMAR-Adjunct.2018.00038
   Wang Z., 2020, P SIGGRAPH AS POST, P1
   Wang Z, 2021, INT J HUM-COMPUT INT, V37, P1799, DOI 10.1080/10447318.2021.1909278
   Weibel N., 2020, P CHI C HUM FACT COM, P4
   Wither J, 2006, INT SYM MIX AUGMENT, P219
   Wither J, 2008, INT SYM MIX AUGMENT, P65, DOI 10.1109/ISMAR.2008.4637326
   Wither J, 2009, COMPUT GRAPH-UK, V33, P679, DOI 10.1016/j.cag.2009.06.001
   Wright W., 2006, Conference on Human Factors in Computing Systems. CHI2006, P801
   Xue T., 2021, P CHI C HUM FACT COM, P1
   Xue T, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P371, DOI 10.1109/AIVR50618.2020.00076
   Yamada S, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P727, DOI 10.1109/VR.2018.8446287
   Yang J, 2020, J MULTIMODAL USER IN, V14, P337, DOI 10.1007/s12193-020-00331-1
   Yonemoto Satoshi, 2014, Virtual, Augmented and Mixed Reality. Designing and Developing Virtual and Augmented Environments. 6th International Conference, VAMR 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8525, P418, DOI 10.1007/978-3-319-07458-0_39
NR 156
TC 3
Z9 3
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5074
EP 5096
DI 10.1109/TVCG.2023.3288869
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400032
PM 37352090
DA 2024-11-06
ER

PT J
AU Bearfield, CX
   Stokes, C
   Lovett, A
   Franconeri, S
AF Bearfield, Cindy Xiong
   Stokes, Chase
   Lovett, Andrew
   Franconeri, Steven
TI What Does the Chart Say? Grouping Cues Guide Viewer Comparisons and
   Conclusions in Bar Charts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Bars; Visualization; Data visualization; Color; Task analysis; Taxonomy;
   Noise level; Comparison; perception; visual grouping; bar charts; verbal
   conclusions
ID GRAPH COMPREHENSION; TOP-DOWN; SEARCH; COLOR; COMMONALITIES; PERCEPTION
AB Reading a visualization is like reading a paragraph. Each sentence is a comparison: the mean of these is higher than those; this difference is smaller than that. What determines which comparisons are made first? The viewer's goals and expertise matter, but the way that values are visually grouped together within the chart also impacts those comparisons. Research from psychology suggests that comparisons involve multiple steps. First, the viewer divides the visualization into a set of units. This might include a single bar or a grouped set of bars. Then the viewer selects and compares two of these units, perhaps noting that one pair of bars is longer than another. Viewers might take an additional third step and perform a second-order comparison, perhaps determining that the difference between one pair of bars is greater than the difference between another pair. We create a visual comparison taxonomy that allows us to develop and test a sequence of hypotheses about which comparisons people are more likely to make when reading a visualization. We find that people tend to compare two groups before comparing two individual bars and that second-order comparisons are rare. Visual cues like spatial proximity and color can influence which elements are grouped together and selected for comparison, with spatial proximity being a stronger grouping cue. Interestingly, once the viewer grouped together and compared a set of bars, regardless of whether the group is formed by spatial proximity or color similarity, they no longer consider other possible groupings in their comparisons.
C1 [Bearfield, Cindy Xiong] Georgia Tech, Atlanta, GA 30332 USA.
   [Stokes, Chase] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Lovett, Andrew] US Naval Res Lab, Washington, DC 20375 USA.
   [Franconeri, Steven] Northwestern Univ, Evanston, IL USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University of California System; University of California Berkeley;
   United States Department of Defense; United States Navy; Naval Research
   Laboratory; Northwestern University
RP Bearfield, CX (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.
EM cxiong@gatech.edu; cstokes@ischool.berkeley.edu;
   andrew.lovett@nrl.navy.mil; franconeri@northwestern.edu
OI Xiong Bearfield, Cindy/0000-0002-1451-4083; Lovett,
   Andrew/0009-0007-4463-1946; Franconeri, Steven/0000-0001-5244-9764;
   Stokes, Chase/0000-0001-7644-9021
FU NSF [IIS-2237585, IIS-2311575]
FX This work was partly supported by NSF awards under Grants IIS-2237585
   and IIS-2311575. Recommended for acceptance by R. Metoyer.
CR Ahissar M, 2004, TRENDS COGN SCI, V8, P457, DOI 10.1016/j.tics.2004.08.011
   ALLIK J, 1991, PERCEPT PSYCHOPHYS, V49, P303, DOI 10.3758/BF03205986
   Amar R, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P111, DOI 10.1109/INFVIS.2005.1532136
   Baluchi F, 2011, TRENDS NEUROSCI, V34, P210, DOI 10.1016/j.tins.2011.02.003
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Brewer C. A., 2003, Cartography Geographic Inf. Sci., V30, P5, DOI 10.1559/152304003100010929
   Brooks J. L, 2015, Traditional and New Principles of Perceptual Grouping
   Burlinson D, 2018, IEEE T VIS COMPUT GR, V24, P574, DOI 10.1109/TVCG.2017.2745086
   Burns A, 2020, 2020 IEEE WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2020), P19, DOI 10.1109/BELIV51497.2020.00010
   Bylinskii Z, 2017, MATH VIS, P235, DOI 10.1007/978-3-319-47024-5_14
   Chalbi A, 2020, IEEE T VIS COMPUT GR, V26, P386, DOI 10.1109/TVCG.2019.2934288
   Ciccione Lorenzo, 2020, Open Mind (Camb), V4, P102, DOI 10.1162/opmi_a_00037
   CLARK HH, 1972, COGNITIVE PSYCHOL, V3, P472, DOI 10.1016/0010-0285(72)90019-9
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P531, DOI 10.2307/2288400
   Cozby P. C., 2007, METHODS BEHAV RES
   Demiralp C, 2014, IEEE T VIS COMPUT GR, V20, P1933, DOI 10.1109/TVCG.2014.2346978
   DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433
   Egeth HE, 2010, ACTA PSYCHOL, V135, P130, DOI 10.1016/j.actpsy.2010.05.012
   Folk CL, 2010, PSYCHON B REV, V17, P421, DOI 10.3758/PBR.17.3.421
   Franconeri SL, 2009, COGNITION, V113, P1, DOI 10.1016/j.cognition.2009.07.002
   Franconeri SL, 2007, J EXP PSYCHOL HUMAN, V33, P1003, DOI 10.1037/0096-1523.33.5.1003
   Friel SN, 2001, J RES MATH EDUC, V32, P124, DOI 10.2307/749671
   GENTNER D, 1983, COGNITIVE SCI, V7, P155, DOI 10.1016/S0364-0213(83)80009-3
   Gleicher M, 2018, IEEE T VIS COMPUT GR, V24, P413, DOI 10.1109/TVCG.2017.2744199
   Gleicher M, 2013, IEEE T VIS COMPUT GR, V19, P2316, DOI 10.1109/TVCG.2013.183
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Halberda J, 2006, PSYCHOL SCI, V17, P572, DOI 10.1111/j.1467-9280.2006.01746.x
   Halford GS, 2005, PSYCHOL SCI, V16, P70, DOI 10.1111/j.0956-7976.2005.00782.x
   Hegdé J, 1999, NEUROREPORT, V10, P143, DOI 10.1097/00001756-199901180-00027
   Hsieh FY, 1998, STAT MED, V17, P1623, DOI 10.1002/(SICI)1097-0258(19980730)17:14<1623::AID-SIM871>3.0.CO;2-S
   Huang LQ, 2007, PSYCHOL REV, V114, P599, DOI 10.1037/0033-295X.114.3.599
   Huang LQ, 2020, J VISION, V20, DOI 10.1167/jov.20.4.10
   Huang LQ, 2002, VISION RES, V42, P1421, DOI 10.1016/S0042-6989(02)00059-7
   Hummel JE, 1997, PSYCHOL REV, V104, P427, DOI 10.1037/0033-295X.104.3.427
   Jardine N, 2020, IEEE T VIS COMPUT GR, V26, P1012, DOI 10.1109/TVCG.2019.2934786
   Jung C, 2022, IEEE T VIS COMPUT GR, V28, P1095, DOI 10.1109/TVCG.2021.3114846
   Kim L. A., 2019, P CHI C HUM FACT COM, P1
   Kim NW, 2017, ACM T COMPUT-HUM INT, V24, DOI 10.1145/3131275
   Kim Z., 2015, P 33 ANN ACM C EXT A, P1349, DOI [10.1145/2702613.2732934, DOI 10.1145/2702613.2732934]
   Lamy D, 2004, J EXP PSYCHOL HUMAN, V30, P1019, DOI 10.1037/0096-1523.30.6.1019
   Levinthal BR, 2011, PSYCHOL SCI, V22, P1132, DOI 10.1177/0956797611418346
   Love BC, 1999, COGNITIVE PSYCHOL, V38, P291, DOI 10.1006/cogp.1998.0697
   Lovett A, 2017, PSYCHOL REV, V124, P60, DOI 10.1037/rev0000039
   Lovett A, 2011, COGNITION, V121, P281, DOI 10.1016/j.cognition.2011.06.012
   Lovett A, 2009, COGNITIVE SCI, V33, P1192, DOI 10.1111/j.1551-6709.2009.01052.x
   Lovett K., 2012, P ANN M COGN SCI SOC, P701
   Mantri Prateek, 2023, IEEE Trans Vis Comput Graph, V29, P1005, DOI 10.1109/TVCG.2022.3209467
   Markman AB, 1996, MEM COGNITION, V24, P235, DOI 10.3758/BF03200884
   Michal AL, 2017, COGN RES, V2, DOI 10.1186/s41235-017-0059-2
   Michal AL, 2016, PSYCHON B REV, V23, P1802, DOI 10.3758/s13423-016-1047-0
   NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3
   Nothelfer C, 2020, IEEE T VIS COMPUT GR, V26, P311, DOI 10.1109/TVCG.2019.2934801
   Picon E., 2017, J. Vis., V17, P1284
   Pinker S., 1990, Artif. Intell. Future Testing, V73, P73, DOI DOI 10.1145/2046684.2046699
   POSNER MI, 1980, Q J EXP PSYCHOL, V32, P3, DOI 10.1080/00335558008248231
   Ripley B., 2016, PACKAGE NNET R PACKA, V7, P700, DOI DOI 10.1080/10236240601154872
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Shah P, 2002, EDUC PSYCHOL REV, V14, P47, DOI 10.1023/A:1013180410169
   Shah P, 2011, TOP COGN SCI, V3, P560, DOI 10.1111/j.1756-8765.2009.01066.x
   Shin Sungbok, 2022, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2022.3209472
   Snow M., 2013, Qualtrics Survey Software: Handbook for ResearchProfessionals
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Srinivasan N., 2021, P CHI C HUM FACT COM, P1
   Stokes C, 2022, Arxiv, DOI arXiv:2209.10789
   Szafir DA, 2018, IEEE T VIS COMPUT GR, V24, P392, DOI 10.1109/TVCG.2017.2744359
   Theeuwes J, 1996, ACTA PSYCHOL, V94, P291, DOI 10.1016/S0001-6918(96)00003-0
   Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wolfe JM, 1998, PSYCHOL SCI, V9, P33, DOI 10.1111/1467-9280.00006
   Xiong A., 2022, P CHI C HUM FACT COM, P1
   Xiong C, 2022, IEEE T VIS COMPUT GR, V28, P955, DOI 10.1109/TVCG.2021.3114823
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P3051, DOI 10.1109/TVCG.2019.2917689
   Yu D, 2019, PSYCHOL SCI, V30, P376, DOI 10.1177/0956797618822798
   Yu D, 2019, COGNITION, V182, P8, DOI 10.1016/j.cognition.2018.08.006
NR 74
TC 2
Z9 2
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5097
EP 5110
DI 10.1109/TVCG.2023.3289292
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400042
PM 37792647
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhang, PZ
   Yang, LX
   Xie, XH
   Lai, JH
AF Zhang, Pengze
   Yang, Lingxiao
   Xie, Xiaohua
   Lai, Jianhuang
TI Pose Guided Person Image Generation Via Dual-Task Correlation and
   Affinity Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Correlation; Semantics; Image synthesis; Transformers;
   Three-dimensional displays; Training; Generative adversarial networks;
   neural rendering; person image generation; pose transfer; view synthesis
AB Pose Guided Person Image Generation (PGPIG) is the task of transforming a person's image from the source pose to a target pose. Existing PGPIG methods often tend to learn an end-to-end transformation between the source image and the target image, but do not seriously consider two issues: 1) the PGPIG is an ill-posed problem, and 2) the texture mapping requires effective supervision. In order to alleviate these two challenges, we propose a novel method by incorporating Dual-task Pose Transformer Network and Texture Affinity learning mechanism (DPTN-TA). To assist the ill-posed source-to-target task learning, DPTN-TA introduces an auxiliary task, i.e., source-to-source task, by a Siamese structure and further explores the dual-task correlation. Specifically, the correlation is built by the proposed Pose Transformer Module (PTM), which can adaptively capture the fine-grained mapping between sources and targets and can promote the source texture transmission to enhance the details of the generated images. Moreover, we propose a novel texture affinity loss to better supervise the learning of texture mapping. In this way, the network is able to learn complex spatial transformations effectively. Extensive experiments show that our DPTN-TA can produce perceptually realistic person images under significant pose changes. Furthermore, our DPTN-TA is not limited to processing human bodies but can be flexibly extended to view synthesis of other objects, i.e., faces and chairs, outperforming the state-of-the-arts in terms of both LPIPS and FID. Our code is available at: https://github.com/PangzeCheung/Dual-task-Pose-Transformer-Network.
C1 [Zhang, Pengze; Xie, Xiaohua] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zhang, Pengze; Yang, Lingxiao; Xie, Xiaohua; Lai, Jianhuang] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
   [Zhang, Pengze; Yang, Lingxiao; Xie, Xiaohua; Lai, Jianhuang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University
RP Xie, XH (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.; Xie, XH (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
EM zhangpz3@mail2.sysu.edu.cn; yanglx9@mail.sysu.edu.cn;
   xiexiaoh6@mail.sysu.edu.cn; stsljh@mail.sysu.edu.cn
RI Xie, Xiaohua/AAW-8365-2020; Zhang, Pengze/JAX-8304-2023; yang,
   yong/GYQ-7408-2022
OI Xie, Xiaohua/0000-0002-0310-4679
FU National Natural Science Foundation of China [U22A2095, 62072482]
FX No Statement Available
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Albahar B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480559
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Avidan S, 1998, IEEE T VIS COMPUT GR, V4, P293, DOI 10.1109/2945.765324
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Y, 2016, IEEE T VIS COMPUT GR, V22, P2000, DOI 10.1109/TVCG.2015.2478779
   Clegg A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766986
   Deng QX, 2022, IEEE T VIS COMPUT GR, V28, P3113, DOI 10.1109/TVCG.2021.3051251
   Dosovitskiy A., 2021, PROC INT C LEARN RE, P1
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Grigorev A, 2019, PROC CVPR IEEE, P12127, DOI 10.1109/CVPR.2019.01241
   Gross I., 2008, P IEEE INT C AUT FAC, P1
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   He D., 2016, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, P820, DOI DOI 10.5555/3157096.3157188
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   Hensel M, 2017, ADV NEUR IN, V30
   Hudson DA, 2021, INT C MACH LEARN, P4487
   Jaunet T, 2022, IEEE T VIS COMPUT GR, V28, P976, DOI 10.1109/TVCG.2021.3114683
   Jiang S., 2021, arXiv
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kholgade N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601209
   Kingma D.P., 2014, P INT C LEARNING REP
   Kingma D.P., 2014, 2 INT C LEARN REPR
   Lee M, 2021, IEEE T VIS COMPUT GR, V27, P3534, DOI 10.1109/TVCG.2019.2959575
   Li F., 2019, inProc. Adv. Neural Inf. Process. Syst.
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Ling Z. Wang, IEEE Trans. Visual. Comput. Graph., DOI [10.1109/TVCG.2022.3166666.12Q, DOI 10.1109/TVCG.2022.3166666.12Q]
   Liu LJ, 2021, IEEE T VIS COMPUT GR, V27, P4009, DOI 10.1109/TVCG.2020.2996594
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lv ZY, 2021, PROC CVPR IEEE, P10801, DOI 10.1109/CVPR46437.2021.01066
   Ma LQ, 2017, ADV NEUR IN, V30
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mingyu Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P87, DOI 10.1007/978-3-030-58604-1_6
   Miyato T, 2018, ARXIV180205957, DOI DOI 10.48550/ARXIV.1802.05957
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Rematas K, 2017, IEEE T PATTERN ANAL, V39, P1576, DOI 10.1109/TPAMI.2016.2601093
   Ren YR, 2020, PROC CVPR IEEE, P7687, DOI 10.1109/CVPR42600.2020.00771
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, 10.48550/arXiv.1706.05098, DOI 10.48550/ARXIV.1706.05098]
   Sarkar Kripasindhu, 2021, arXiv, DOI [DOI 10.48550/ARXIV.2102.11263, 10.48550/arXiv.2102.11263]
   Shi DH, 2022, PROC CVPR IEEE, P11059, DOI 10.1109/CVPR52688.2022.01079
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10
   Tabejamaat F., 2021, BRIT MACH VIS C, P1
   Tian Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P942
   Touvron H, 2021, Arxiv, DOI arXiv:2012.12877
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y., 2019, P INT C LEARN REPR N
   Wang YJ, 2018, AAAI CONF ARTIF INTE, P5553
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu XG, 2019, IEEE I CONF COMP VIS, P7790, DOI 10.1109/ICCV.2019.00788
   Xu Y., 2022, P ADV NEUR INF PROC, P1
   Yin M, 2021, P IEEE CVF C COMP VI, P7220
   Zeng YH, 2023, IEEE T VIS COMPUT GR, V29, P3266, DOI 10.1109/TVCG.2022.3156949
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhang PZ, 2022, PROC CVPR IEEE, P7703, DOI 10.1109/CVPR52688.2022.00756
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2054, DOI 10.1109/ICCV48922.2021.00208
   Zhu X., 2021, P INT C LEARN REPR, P1
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 74
TC 1
Z9 1
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5111
EP 5128
DI 10.1109/TVCG.2023.3286394
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400097
PM 37318966
DA 2024-11-06
ER

PT J
AU Xie, LX
   Ouyang, Y
   Chen, LF
   Wu, ZM
   Li, Q
AF Xie, Laixin
   Ouyang, Yang
   Chen, Longfei
   Wu, Ziming
   Li, Quan
TI Towards Better Modeling With Missing Data: A Contrastive Learning-Based
   Visual Analytics Perspective
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data models; Predictive models; Training; Task analysis; Analytical
   models; Data visualization; Numerical models; Explainable AI; missing
   data; data imputation; contrastive learning
ID DIAGNOSIS
AB Missing data can pose a challenge for machine learning (ML) modeling. To address this, current approaches are categorized into feature imputation and label prediction and are primarily focused on handling missing data to enhance ML performance. These approaches rely on the observed data to estimate the missing values and therefore encounter three main shortcomings in imputation, including the need for different imputation methods for various missing data mechanisms, heavy dependence on the assumption of data distribution, and potential introduction of bias. This study proposes a Contrastive Learning (CL) framework to model observed data with missing values, where the ML model learns the similarity between an incomplete sample and its complete counterpart and the dissimilarity between other samples. Our proposed approach demonstrates the advantages of CL without requiring any imputation. To enhance interpretability, we introduce CIVis, a visual analytics system that incorporates interpretable techniques to visualize the learning process and diagnose the model status. Users can leverage their domain knowledge through interactive sampling to identify negative and positive pairs in CL. The output of CIVis is an optimized model that takes specified features and predicts downstream tasks. We provide two usage scenarios in regression and classification tasks and conduct quantitative experiments, expert interviews, and a qualitative user study to demonstrate the effectiveness of our approach. In short, this study offers a valuable contribution to addressing the challenges associated with ML modeling in the presence of missing data by providing a practical solution that achieves high predictive accuracy and model interpretability.
C1 [Xie, Laixin; Ouyang, Yang; Chen, Longfei; Li, Quan] ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
   [Wu, Ziming] Tencent Inc, Shenzhen 518054, Guangdong, Peoples R China.
   ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Shanghai, Peoples R China.
C3 ShanghaiTech University; Tencent; ShanghaiTech University
RP Li, Q (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
EM xielx@shanghaitech.edu.cn; ouyy@shanghaitech.edu.cn;
   chenlf@shanghaitech.edu.cn; jimmyzmwu@tencent.com;
   liquan@shanghaitech.edu.cn
RI Chen, LongFei/AAI-1277-2019
OI Xie, Laixin/0000-0002-7748-2971; , Yang/0009-0000-5841-7659; Chen,
   Longfei/0009-0002-4596-8093
CR Angelini M, 2022, SCIENTOMETRICS, V127, P6827, DOI 10.1007/s11192-022-04399-2
   Bors C, 2018, ACM J DATA INF QUAL, V10, DOI 10.1145/3190578
   Ceneda D, 2019, COMPUT GRAPH FORUM, V38, P861, DOI 10.1111/cgf.13730
   Chen T., 2020, INT C MACH LEARN, P1597
   Credit card, 2016, About us
   DeRose JF, 2021, IEEE T VIS COMPUT GR, V27, P1160, DOI 10.1109/TVCG.2020.3028976
   Echihabi K, 2023, VLDB J, V32, P763, DOI 10.1007/s00778-022-00771-z
   Feng SY, 2021, Arxiv, DOI arXiv:2105.03075
   Gondara L, 2018, Arxiv, DOI arXiv:1705.02737
   Gou L, 2021, IEEE T VIS COMPUT GR, V27, P261, DOI 10.1109/TVCG.2020.3030350
   Guo Z., 2020, ADV NEURAL INF PROCE, V33, P21271
   Hastie T, 2015, J MACH LEARN RES, V16, P3367
   Honaker J, 2011, J STAT SOFTW, V45, P1
   Imputation, 2007, About us
   Isenberg T, 2013, IEEE T VIS COMPUT GR, V19, P2818, DOI 10.1109/TVCG.2013.126
   Jin ZH, 2023, IEEE T VIS COMPUT GR, V29, P3024, DOI 10.1109/TVCG.2022.3148107
   Joshua Robinson, 2020, arXiv
   kaggle, 2016, House price
   Kahng M, 2019, IEEE T VIS COMPUT GR, V25, P310, DOI 10.1109/TVCG.2018.2864500
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kandel S, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P547, DOI 10.1145/2254556.2254659
   Kim KY, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-160
   La Rosa B, 2023, COMPUT GRAPH FORUM, V42, P319, DOI 10.1111/cgf.14733
   Li QB, 2021, PROC CVPR IEEE, P10708, DOI 10.1109/CVPR46437.2021.01057
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Liu SX, 2018, IEEE T VIS COMPUT GR, V24, P163, DOI 10.1109/TVCG.2017.2744378
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu X, 2023, IEEE T KNOWL DATA EN, V35, P857, DOI 10.1109/TKDE.2021.3090866
   Mackay Wendy, 2016, Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, P3558, DOI 10.1145/2851581.2856492
   McKinney W., 2011, Python for high performance and scientific computing, V14, P1
   Ming Y, 2017, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2017.8585721
   Riehmann P, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P233, DOI 10.1109/INFVIS.2005.1532152
   Rubin DB., 1987, MULTIPLE IMPUTATION, DOI 10.1002/9780470316696
   Sarma Abhraneel, 2023, IEEE Trans Vis Comput Graph, V29, P602, DOI 10.1109/TVCG.2022.3209348
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Smieja M, 2019, Arxiv, DOI arXiv:1805.07405
   Song WP, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1161, DOI 10.1145/3357384.3357925
   Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742
   Tian Yonglong, 2020, Advances in Neural Information Processing Systems, V33
   Turkay C, 2019, Arxiv, DOI arXiv:1812.08032
   van Buuren S, 2011, J STAT SOFTW, V45, P1
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P288, DOI 10.1109/TVCG.2018.2864504
   Wang T, 2020, INT C MACHINE LEARNI, V119, P9929
   Weibelzahl S, 2020, UMAP'20: PROCEEDINGS OF THE 28TH ACM CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION, P394, DOI 10.1145/3340631.3398668
   Wells T. T., 1990, RELCJ, V21, P95
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xia J, 2017, PATTERN RECOGN, V69, P52, DOI 10.1016/j.patcog.2017.04.005
   Xiang SX, 2019, IEEE CONF VIS ANAL, P57, DOI [10.1109/vast47406.2019.8986943, 10.1109/VAST47406.2019.8986943]
   Xuan H, 2020, IEEE WINT CONF APPL, P2463, DOI 10.1109/WACV45572.2020.9093432
   Yao L., 2021, P IEEECVF INT C COMP, P3591
   Yoon J, 2018, PR MACH LEARN RES, V80
   You JX, 2020, Arxiv, DOI arXiv:2010.16418
   You Y, 2020, ADV NEUR IN, V33
   Zhang C., 2022, arXiv
   Zhou C, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3985, DOI 10.1145/3447548.3467102
   Zhu R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10286, DOI 10.1109/ICCV48922.2021.01014
NR 60
TC 0
Z9 0
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5129
EP 5146
DI 10.1109/TVCG.2023.3285210
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400052
PM 37310838
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Shin, S
   Batch, A
   Butcher, PWS
   Ritsos, PD
   Elmqvist, N
AF Shin, Sungbok
   Batch, Andrea
   Butcher, Peter William Scott
   Ritsos, Panagiotis D.
   Elmqvist, Niklas
TI The Reality of the Situation: A Survey of Situated Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; data visualization; immersive analytics; situated
   visualization; situated analytics
ID MOBILE AUGMENTED REALITY; VISUALIZATION; TAXONOMY; HAND; SYSTEM
AB The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user's physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
C1 [Shin, Sungbok; Batch, Andrea; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Butcher, Peter William Scott; Ritsos, Panagiotis D.] Bangor Univ, Bangor LL57 2DG, Wales.
C3 University System of Maryland; University of Maryland College Park;
   Bangor University
RP Elmqvist, N (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM sbshin90@umd.edu; ajulca@umd.edu; p.butcher@bangor.ac.uk;
   p.ritsos@bangor.ac.uk; elm@umd.edu
RI Ritsos, Panagiotis/AAE-8990-2022
OI Elmqvist, Niklas/0000-0001-5805-5301; Ritsos,
   Panagiotis/0000-0001-9308-3885; Butcher, Peter/0000-0002-3361-627X;
   Shin, Sungbok/0000-0001-6777-8843
FU U.S. National Science Foundation [IIS-1908605]; DSP Centre - European
   Regional Development Fund; North Wales Growth Deal through Ambition
   North Wales, Welsh Government; North Wales Growth Deal through Ambition
   North Wales, U.K. Government
FX This work was supported in part by the U.S. National Science Foundation
   under Grant IIS-1908605, and in part by the DSP Centre, which has been
   partly funded by the European Regional Development Fund through Welsh
   Government and also by the North Wales Growth Deal through Ambition
   North Wales, Welsh Government and U.K. Government.
CR Abao Roland P., 2018, Procedia Computer Science, V135, P186, DOI 10.1016/j.procs.2018.08.165
   Ahn J, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808207
   Alqahtani M., 2017, P INT C COMP AUT ENG, P1, DOI [10.1145/3057039.3057062, DOI 10.1145/3057039.3057062]
   [Anonymous], 2010, Int. J. Virtual Reality, V9, P1, DOI [10.20870/IJVR.2010.9.2.2767.141, DOI 10.20870/IJVR.2010.9.2.2767.141]
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bach B, 2018, IEEE T VIS COMPUT GR, V24, P457, DOI 10.1109/TVCG.2017.2745941
   Bach B, 2016, PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES, (ISS 2016), P529, DOI 10.1145/2992154.2996365
   Badam SK, 2019, IEEE T VIS COMPUT GR, V25, P586, DOI 10.1109/TVCG.2018.2865144
   Balduini M, 2012, J WEB SEMANT, V16, P33, DOI 10.1016/j.websem.2012.06.004
   Bane R, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P231, DOI 10.1109/ISMAR.2004.36
   Barba E, 2012, P IEEE, V100, P929, DOI 10.1109/JPROC.2011.2182070
   Batch A, 2020, IEEE T VIS COMPUT GR, V26, P536, DOI 10.1109/TVCG.2019.2934803
   Behzadan AH, 2005, PROCEEDINGS OF THE 2005 WINTER SIMULATION CONFERENCE, VOLS 1-4, P1914, DOI 10.1109/WSC.2005.1574469
   Behzadan AH, 2015, ADV ENG INFORM, V29, P252, DOI 10.1016/j.aei.2015.03.005
   Besançon L, 2021, COMPUT GRAPH FORUM, V40, P293, DOI 10.1111/cgf.14189
   Bichlmeier C., 2007, Proc. IEEE ACM Int. Sym. on Mix. Aug. Reality, P1, DOI [DOI 10.1109/ISMAR.2007.4538837.18M, DOI 10.1109/ISMAR.2007.4538837]
   Bichlmeier C, 2009, INT SYM MIX AUGMENT, P173, DOI 10.1109/ISMAR.2009.5336474
   Bichlmeier C, 2009, IEEE T MED IMAGING, V28, P1498, DOI 10.1109/TMI.2009.2018622
   Billinghurst Mark, 2015, Foundations and Trends in Human-Computer Interaction, V8, P73, DOI 10.1561/1100000049
   Bonada S, 2017, COMPANION PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS 2016), P83, DOI 10.1145/3009939.3009953
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Bressa N, 2022, IEEE T VIS COMPUT GR, V28, P107, DOI 10.1109/TVCG.2021.3114835
   Brown Tom, 2020, LANGUAGE MODELS ARE
   Büschel W, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ACM ISS 2017), P62, DOI 10.1145/3132272.3134125
   Büschel W, 2019, IEEE COMPUT GRAPH, V39, P29, DOI 10.1109/MCG.2019.2897927
   Butcher PWS, 2021, IEEE T VIS COMPUT GR, V27, P3213, DOI 10.1109/TVCG.2020.2965109
   Caggianese Giuseppe, 2019, 2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS), P390, DOI 10.1109/SITIS.2019.00069
   Calinski T., 1974, Commun. Stat. Theory Methods, V3, P1, DOI 10.1080/03610927408827101
   Carmigniani J, 2011, MULTIMED TOOLS APPL, V51, P341, DOI 10.1007/s11042-010-0660-6
   Cavallo M, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P160, DOI [10.1109/ISMAR-Adjunct.2016.0068, 10.1109/ISMAR-Adjunct.2016.60]
   Chang TC, 2015, IEEE GLOB COMM CONF, DOI [10.1109/GLOCOM.2015.7417476, 10.1109/ICSENS.2015.7370446]
   Chatzopoulos D, 2017, IEEE ACCESS, V5, P6917, DOI 10.1109/ACCESS.2017.2698164
   Chen ZT, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376436
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P2645, DOI 10.1109/TVCG.2019.2892415
   Chen ZT, 2017, VIS INFORM, V1, P132, DOI 10.1016/j.visinf.2017.11.002
   Cordeil M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P200, DOI [10.1109/vr.2019.8797978, 10.1109/VR.2019.8797978]
   Cordeil M, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P71, DOI 10.1145/3126594.3126613
   Cordeil M, 2017, IEEE T VIS COMPUT GR, V23, P441, DOI 10.1109/TVCG.2016.2599107
   Datcu D., 2016, P 19 INT C SUPP GROU, P267, DOI DOI 10.1145/2957276.2957302
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   Dixon BJ, 2013, SURG ENDOSC, V27, P454, DOI 10.1007/s00464-012-2457-3
   Doil F., 2003, IPT/EGVE 2003. Seventh Immersive Projection Technology Workshop. Ninth Eurographics Workshop on Virtual Environments, P71, DOI 10.1145/769953.769962
   Doshi A, 2017, INT J ADV MANUF TECH, V89, P1279, DOI 10.1007/s00170-016-9164-5
   Dünser A, 2012, COMPUT GRAPH-UK, V36, P1084, DOI 10.1016/j.cag.2012.10.001
   Elmqvist N., 2023, COMMUN ACM
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Elmqvist N, 2013, COMPUTER, V46, P86, DOI 10.1109/MC.2013.147
   ElSayed NAM, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P203, DOI [10.1109/ISMAR-Adjunct.2016.0077, 10.1109/ISMAR-Adjunct.2016.68]
   ElSayed NAM, 2016, J VISUAL LANG COMPUT, V36, P13, DOI 10.1016/j.jvlc.2016.07.006
   Engelke U, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365715
   Ens B, 2021, IEEE T VIS COMPUT GR, V27, P1193, DOI 10.1109/TVCG.2020.3030334
   Ens Barrett., 2021, P 2021 CHI C HUMAN F, P1, DOI DOI 10.1145/3411764.3446866
   Feiner S, 1997, FIRST INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P74, DOI 10.1109/ISWC.1997.629922
   Fjukstad Bard, 2014, 5th International Conference on Information Visualization Theory and Applications (IVAPP 2014). Proceedings, P321
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Fred ALN, 2005, IEEE T PATTERN ANAL, V27, P835, DOI 10.1109/TPAMI.2005.113
   Fröhler B, 2022, COMPUT GRAPH FORUM, V41, P465, DOI 10.1111/cgf.14447
   Giiven Sinem, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P155
   Gillet A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P235, DOI 10.1109/VISUAL.2004.7
   Gomez P., 2008, P ACM S VIRT REAL SO, P267, DOI [10.1145/1450579.1450646.60R, DOI 10.1145/1450579.1450646.60R]
   Grasset R, 2012, INT SYM MIX AUGMENT, P177, DOI 10.1109/ISMAR.2012.6402555
   Güven S, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P111, DOI 10.1109/TRIDUI.2006.1618280
   Haynes PS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P243, DOI 10.1109/3DUI.2016.7460061
   Heller F, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P615, DOI 10.1145/2556288.2557021
   Henderson S, 2011, IEEE T VIS COMPUT GR, V17, P1355, DOI 10.1109/TVCG.2010.245
   Henderson S, 2010, IEEE T VIS COMPUT GR, V16, P4, DOI 10.1109/TVCG.2009.91
   Hollerer T., 1999, Digest of Papers. Third International Symposium on Wearable Computers, P79, DOI 10.1109/ISWC.1999.806664
   Höllerer T, 1999, COMPUT GRAPH-UK, V23, P779, DOI 10.1016/S0097-8493(99)00103-X
   Hube M., 2018, P ACM AVI WORKSH VIS, P12
   Hurter C, 2019, IEEE T VIS COMPUT GR, V25, P704, DOI 10.1109/TVCG.2018.2865191
   IKEA Corporation, 2017, IKEA Place
   Imottesjo H, 2018, COMPUT ENVIRON URBAN, V71, P120, DOI 10.1016/j.compenvurbsys.2018.05.003
   Jain D, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P241, DOI 10.1145/2702123.2702393
   Jiang SY, 2020, PROCEEDINGS OF IDC 2020, P349, DOI 10.1145/3392063.3394406
   Joo-Nagata J, 2017, COMPUT EDUC, V111, P1, DOI 10.1016/j.compedu.2017.04.003
   Julier S, 2002, IEEE COMPUT GRAPH, V22, P12, DOI 10.1109/MCG.2002.1028721
   Kalkofen D, 2013, INT SYM MIX AUGMENT, P1
   Ketchell W., 2019, P INT C VIRT REAL CO, P1, DOI [10.1145/3359997.3365681.79D.H, DOI 10.1145/3359997.3365681.79D.H]
   Kim H, 2018, IEEE T VIS COMPUT GR, V24, P1515, DOI 10.1109/TVCG.2018.2793680
   Kim K, 2018, IEEE T VIS COMPUT GR, V24, P2947, DOI 10.1109/TVCG.2018.2868591
   Kim V., 2021, P ACM C HUM FACT COM, DOI [10.1145/3411764.3445443.80H, DOI 10.1145/3411764.3445443.80H]
   King GR, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P52
   Kobak D, 2021, NAT BIOTECHNOL, V39, DOI 10.1038/s41587-020-00809-z
   Kobak G. C., 2021, Nat. Biotechnol., V39, P1464, DOI [10.1109/5.58325., DOI 10.1109/5.58325]
   Kotranza A, 2009, INT SYM MIX AUGMENT, P125, DOI 10.1109/ISMAR.2009.5336485
   Kraus M, 2020, IEEE T VIS COMPUT GR, V26, P525, DOI 10.1109/TVCG.2019.2934395
   Kwon OH, 2016, IEEE T VIS COMPUT GR, V22, P1802, DOI 10.1109/TVCG.2016.2520921
   Langner Ricardo, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3445593
   Lanier J., 2001, Sci. Amer., V284, P75
   Lee B, 2019, IEEE COMPUT GRAPH, V39, P16, DOI 10.1109/MCG.2019.2906513
   Lee G. A., 2012, 2012 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities (ISMAR-AMH), P57, DOI 10.1109/ISMAR-AMH.2012.6483989
   Lee S, 2015, COMPUT GEOSCI-UK, V76, P41, DOI 10.1016/j.cageo.2014.12.005
   Liestol G, 2014, P I CON VIR SYS MULT, P251, DOI 10.1109/VSMM.2014.7136657
   Lin T., 2021, P ACM C HUM FACT COM, P13, DOI [DOI 10.1145/3411764.34456492, 10.1145/3411764.34456492,9, DOI 10.1145/3411764.34456492,9, 10.1145/3411764.3445649, DOI 10.1145/3411764.3445649]
   Livingston M. A., 2002, Tech. Rep.
   Lk'Stol Gunnar, 2009, International Journal of Interactive Mobile Technologies, V3, P33, DOI 10.3991/ijim.v3s1.963
   Luboschik M, 2017, COMPANION PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS 2016), P47, DOI 10.1145/3009939.3009947
   Luchetti G, 2017, ISPRS INT J GEO-INF, V6, DOI 10.3390/ijgi6020041
   Marriott K., 2018, Immersive Analytics: Time to Reconsider the Value of 3D for Information Visualisation, P55, DOI [10.1007/9783-030-01388-2_2, DOI 10.1007/978]
   Mathews NS, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P146, DOI [10.1109/VIS49827.2021.9623287, 10.1109/VIS49827.2021.00037]
   Miles LK, 2010, PSYCHOL SCI, V21, P222, DOI 10.1177/0956797609359333
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Moritz D, 2019, IEEE T VIS COMPUT GR, V25, P438, DOI 10.1109/TVCG.2018.2865240
   Morrison A, 2011, COMPUT GRAPH-UK, V35, P789, DOI 10.1016/j.cag.2011.04.009
   Mulloni A., 2011, P 13 INT C HUMAN COM, P211, DOI DOI 10.1145/2037373.2037406
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Nassani A, 2017, SA'17: SIGGRAPH ASIA 2017 MOBILE GRAPHICS & INTERACTIVE APPLICATIONS, DOI 10.1145/3132787.3139199
   Okur A., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P211, DOI 10.1109/ISMAR.2011.6092388
   Panëels S, 2010, IEEE T HAPTICS, V3, P119, DOI [10.1109/TOH.2009.44, 10.1109/ToH.2009.44]
   Park JH, 2018, IEEE COMPUT GRAPH, V38, P67, DOI 10.1109/MCG.2018.2879066
   Patnaik B, 2019, IEEE T VIS COMPUT GR, V25, P726, DOI 10.1109/TVCG.2018.2865237
   Pirolli P., 2005, P INT C INT AN MCLEA, V5, P2
   Prouzeau A, 2020, PROCEEDINGS OF THE WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES AVI 2020, DOI 10.1145/3399715.3399743
   Rabbi I., 2013, ACTA GRAPH, V24, P29, DOI DOI 10.9790/0661-0222329
   Rekimoto J, 1998, SECOND INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P68, DOI 10.1109/ISWC.1998.729531
   Roberts JC, 2014, IEEE COMPUT GRAPH, V34, P26, DOI 10.1109/MCG.2014.82
   Robertson G. G., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P189, DOI 10.1145/108844.108883
   Rollo ME, 2017, INT J BEHAV NUTR PHY, V14, DOI 10.1186/s12966-017-0516-9
   Roodaki H, 2017, IEEE T VIS COMPUT GR, V23, P2366, DOI 10.1109/TVCG.2017.2734327
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Satkowski M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445330
   Schall G, 2009, PERS UBIQUIT COMPUT, V13, P281, DOI 10.1007/s00779-008-0204-5
   Schmalstieg T., 2016, Augmented Reality: Principles andPractice
   Schoenfelder R, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P83
   Seichter H, 2007, COMPUTER-AIDED ARCHITECTURAL DESIGN FUTURES (CAAD FUTURES) 2007, P3, DOI 10.1007/978-1-4020-6528-6_1
   Shapiro L, 2014, ROUTLEDGE HBK PHILOS, P1
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shneiderman B, 2021, ISSUES SCI TECHNOL, V37, P56
   Sicat R, 2019, IEEE T VIS COMPUT GR, V25, P715, DOI 10.1109/TVCG.2018.2865152
   Simpson J., 2017, P IEEE VIS WORKSH IM, P1
   Skulmowski A, 2018, COGN RES, V3, DOI 10.1186/s41235-018-0092-9
   Srinivasan A, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445400
   Stoakley R., 1995, P SIGCHI C HUM FACT, P265, DOI [10.1145/223904.223938, DOI 10.1145/223904.223938]
   Tanveer E., 2015, P ACM C INT US INT N, P286, DOI [DOI 10.1145/2678025.2701386, 10.1145/2678025.2701386.138M, DOI 10.1145/2678025.2701386.138M]
   Tatzgern M, 2014, 2014 IEEE VIRTUAL REALITY (VR), P27, DOI 10.1109/VR.2014.6802046
   Vanoni D, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P44, DOI 10.1109/ISM.2012.17
   Veas E, 2012, IEEE T VIS COMPUT GR, V18, P565, DOI 10.1109/TVCG.2012.44
   Wagner JA, 2020, IEEE T VIS COMPUT GR, V26, P514, DOI 10.1109/TVCG.2019.2934415
   Wang X, 2016, ADV MANUF, V4, P1, DOI 10.1007/s40436-015-0131-4
   White S, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1117
   Whitlock M, 2020, INT SYM MIX AUGMENT, P704, DOI 10.1109/ISMAR50242.2020.00101
   Whitlock M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P616, DOI [10.1109/VR46266.2020.00-20, 10.1109/VR46266.2020.1582298687237]
   Whitlock M, 2020, IEEE T VIS COMPUT GR, V26, P503, DOI 10.1109/TVCG.2019.2934282
   Wigdor D, 2011, BRAVE NUI WORLD: DESIGNING NATURAL USER INTERFACES FOR TOUCH AND GESTURE, P1
   Willett W, 2017, IEEE T VIS COMPUT GR, V23, P461, DOI 10.1109/TVCG.2016.2598608
   Williams K. Yao, P BRIT HUM COMP INT
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Xi MZ, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365721
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
   Zhao MQ, 2017, SA'17: SIGGRAPH ASIA 2017 SYMPOSIUM ON VISUALIZATION, DOI 10.1145/3139295.3139309
   Zheng MY, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P93, DOI 10.1109/ISMAR-Adjunct.2019.00039
   Zhou F, 2008, INT SYM MIX AUGMENT, P193, DOI 10.1109/ISMAR.2008.4637362
   Zollmann S, 2021, IEEE T VIS COMPUT GR, V27, P3808, DOI 10.1109/TVCG.2020.2986247
NR 154
TC 7
Z9 7
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5147
EP 5164
DI 10.1109/TVCG.2023.3285546
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400038
PM 37310839
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wen, JH
   Wang, BH
   Barbic, J
AF Wen, Jiahao
   Wang, Bohan
   Barbic, Jernej
TI Large-Strain Surface Modeling Using Plasticity
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Differential geometry; fundamental forms; large strain; plasticity;
   shape modeling; surfaces; large rotations
ID RECONSTRUCTION; ANIMATION
AB Modeling arbitrarily large deformations of surfaces smoothly embedded in three-dimensional space is challenging. We give a new method to represent surfaces undergoing large spatially varying rotations and strains, based on differential geometry, and surface first and second fundamental forms. Methods that penalize the difference between the current shape and the rest shape produce sharp spikes under large strains, and variational methods produce wiggles, whereas our method naturally supports large strains and rotations without any special treatment. For stable and smooth results, we demonstrate that the deformed surface has to locally satisfy compatibility conditions (Gauss-Codazzi equations) on the first and second fundamental forms. We then give a method to locally modify the surface first and second fundamental forms in a compatible way. We use those fundamental forms to define surface plastic deformations, and finally recover output surface vertex positions by minimizing the surface elastic energy under the plastic deformations. We demonstrate that our method makes it possible to smoothly deform triangle meshes to large spatially varying strains and rotations, while meeting user constraints.
C1 [Wen, Jiahao; Barbic, Jernej] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Wang, Bohan] Univ Southern Calif, Los Angeles, CA 90089 USA.
   [Wang, Bohan] MIT, Los Angeles, CA USA.
C3 University of Southern California; University of Southern California;
   Massachusetts Institute of Technology (MIT)
RP Barbic, J (corresponding author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM jiahaow@usc.edu; bohanwan@usc.edu; jnb@usc.edu
OI Wen, Jiahao/0000-0002-7175-653X; Wang, Bohan/0000-0003-1439-1455
FU NSF [IIS-1911224]; Bosch Research; Adobe Research; Annenberg Fellowship
FX This work was supported in part by NSF under Grant IIS-1911224, in part
   by Bosch Research, and in part by Adobe Research. The work of Jiahao Wen
   and Bohan Wang were supported by the Annenberg Fellowship. Recommended
   for acceptance by Y. He.
CR Alexa M., 2006, P ACM SIGGRAPH COURS
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bargteil AW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239467
   Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Botsch Mario, 2006, S GEOM PROC, P11
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Bouaziz S, 2012, COMPUT GRAPH FORUM, V31, P1657, DOI 10.1111/j.1467-8659.2012.03171.x
   Chen HY, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201395
   Chen W, 2018, COMPUT GRAPH FORUM, V37, P112, DOI 10.1111/cgf.13236
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Deuss M., 2015, SHAPEOP A ROBUST EXT, P505
   Esturo JM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682627
   Gilles B, 2006, LECT NOTES COMPUT SC, V4190, P289
   Gilles B, 2010, MED IMAGE ANAL, V14, P291, DOI 10.1016/j.media.2010.01.006
   Grinspun E., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P62
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Irving G., 2004, P ACM SIGGRAPH EUR S, P131, DOI DOI 10.1145/1028523.1028541
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964973
   Kircher S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356685
   Knitro A., 2019, About us
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, PROC GRAPH INTERF, P239
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   Petersen P., 2016, Classical differential geometry. lecture notes
   Schmid J, 2009, RECENT ADVANCES IN THE 3D PHYSIOLOGICAL HUMAN, P3, DOI 10.1007/978-1-84882-565-9_1
   Sifakis E., 2015, Finite Element Method Simulation of 3D De- formable Solids
   Sorkine M., 2007, P S GEOM PROC, P109, DOI DOI 10.1145/1073204.1073323
   Sorkine O., 2004, P S GEOM PROC, P175, DOI [DOI 10.1145/1057432.1057456, 10.]
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Volino P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559762
   Wang BH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3443703
   Wang Y, 2012, COMPUT GRAPH FORUM, V31, P2277, DOI 10.1111/j.1467-8659.2012.03153.x
   Wang Y., ACM T GRAPHIC, V40, P1
   Wang Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766952
   Weischedel C., 2012, A discrete geometric view on shear-deformable shell models
NR 41
TC 2
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5183
EP 5197
DI 10.1109/TVCG.2023.3289811
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400008
PM 37368795
DA 2024-11-06
ER

PT J
AU Xie, LWH
   Shu, XH
   Su, JC
   Wang, Y
   Chen, SM
   Qu, HM
AF Xie, Liwenhan
   Shu, Xinhuan
   Su, Jeon Cheol
   Wang, Yun
   Chen, Siming
   Qu, Huamin
TI Creating Emordle: Animating Word Cloud for Emotion Expression
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; Data visualization; Visualization; Videos; Tag clouds; Image
   color analysis; Entropy; Wordle; animation; affective visualization;
   authoring; casual visualization
ID VISUALIZATION; DESIGN
AB We propose emordle, a conceptual design that animates wordles (compact word clouds) to deliver their emotional context to audiences. To inform the design, we first reviewed online examples of animated texts and animated wordles, and summarized strategies for injecting emotion into the animations. We introduced a composite approach that extends an existing animation scheme for one word to multiple words in a wordle with two global factors: the randomness of text animation (entropy) and the animation speed (speed). To create an emordle, general users can choose one predefined animated scheme that matches the intended emotion class and fine-tune the emotion intensity with the two parameters. We designed proof-of-concept emordle examples for four basic emotion classes, namely happiness, sadness, anger, and fear. We conducted two controlled crowdsourcing studies to evaluate our approach. The first study confirmed that people generally agreed on the conveyed emotions from well-crafted animations, and the second one demonstrated that our identified factors helped fine-tune the extent of the emotion delivered. We also invited general users to create their own emordles based on our proposed framework. Through this user study, we confirmed the effectiveness of the approach. We concluded with implications for future research opportunities of supporting emotion expression in visualizations.
C1 [Xie, Liwenhan; Shu, Xinhuan; Su, Jeon Cheol; Qu, Huamin] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Xie, Liwenhan] Fudan Univ, Shanghai 200437, Peoples R China.
   [Wang, Yun] Microsoft Res Asia, Beijing, Peoples R China.
   [Chen, Siming] Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.
   [Chen, Siming] Shanghai Key Lab Data Sci, Shanghai 200437, Peoples R China.
C3 Hong Kong University of Science & Technology; Fudan University;
   Microsoft; Microsoft Research Asia; Fudan University
RP Chen, SM (corresponding author), Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.; Chen, SM (corresponding author), Shanghai Key Lab Data Sci, Shanghai 200437, Peoples R China.
EM liwenhan.xie@connect.ust.hk; xinhuan.shu@gmail.com;
   csjeon@connect.ust.hk; wangyun@microsoft.com; simingchen3@gmail.com;
   huamin@cse.ust.hk
RI Xie, Liwenhan/HLV-8177-2023; Chen, Siming/AAK-1874-2020
OI Xie, Liwenhan/0000-0002-2601-6313; Shu, Xinhuan/0000-0002-9736-4454
FU HKRGC General Research Fund [16210722]; Natural Science Foundation of
   China (NSFC) [62202105]; Shanghai Municipal Science and Technology
   [21ZR1403300, 21YF1402900]
FX This work was supported in part by HKRGC General Research Fund under
   Grant 16210722, in part by the Natural Science Foundation of China
   (NSFC) under Grant 62202105, and in part by Shanghai Municipal Science
   and Technology under Grants 21ZR1403300 and 21YF1402900.
CR Adobe Inc, 2022, After Effects.
   Anderson CL, 2022, IEEE T VIS COMPUT GR, V28, P2867, DOI 10.1109/TVCG.2021.3050118
   [Anonymous], 2008, P HUM FACT COMP SYST, DOI [DOI 10.1145/1358628.1358921, 10.1145/1358628.1358921]
   [Anonymous], 2022, Pixar Animation Studios
   Aoki T, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501920
   Bartram L., 2009, P 5 EUR C COMP AESTH, P129, DOI DOI 10.2312/COMPAESTH/COMPAESTH09/129-136
   Bartram L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1364, DOI 10.1145/3025453.3026041
   Bay-Wei Chang, 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P123, DOI 10.1145/288392.288585
   Brath R., 2020, Visualizing With Text., P42, DOI DOI 10.1201/9780429290565
   Chen Q, 2024, IEEE T VIS COMPUT GR, V30, P4429, DOI 10.1109/TVCG.2023.3261320
   Chevalier F., 2016, P INT WORK C ADY VIS, P280, DOI DOI 10.1145/2909132.2909255
   Chevalier F, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P683
   Chi MT, 2015, IEEE T VIS COMPUT GR, V21, P1415, DOI 10.1109/TVCG.2015.2440241
   Dang T., 2019, PROC EUROPEAN C VISU, P103, DOI DOI 10.2312/EVS.20191178
   Davis Felecia, 2015, P 2015 ACM SIGCHI C, P23, DOI [DOI 10.1145/2757226.2757231, 10.1145/2757226.2757231]
   Desai R, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300599
   Desmet P., 2003, Funology, P111, DOI [10.1007/1-4020-2967-5_12, DOI 10.1007/1-4020-2967-5_12]
   Felix C, 2018, IEEE T VIS COMPUT GR, V24, P657, DOI 10.1109/TVCG.2017.2746018
   Feng C., 2014, P ACM S APPL PERC, P23, DOI DOI 10.1145/2628257.2628264
   Feng C, 2017, LEONARDO, V50, P205, DOI 10.1162/LEON_a_01229
   Fisher D., 2010, Beautiful Visualization - Looking at Data Through the Eyes of Experts, P329
   Forlizzi J., 2003, P SIGCHI C HUM FACT, P377, DOI DOI 10.1145/642611.642677
   Hearst MA, 2020, IEEE T VIS COMPUT GR, V26, P2748, DOI 10.1109/TVCG.2019.2904683
   Hicke R. M., 2022, arXiv
   Ishizaki S., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P347, DOI 10.1145/238386.238566
   Jo J, 2015, IEEE COMPUT GRAPH, V35, P20, DOI 10.1109/MCG.2015.113
   Joonhwan Lee, 2006, Designing Interactive Systems. DIS2006, P41
   Kalra A, 2005, LECT NOTES COMPUT SC, V3585, P966, DOI 10.1007/11555261_81
   Kennedy H, 2018, SOCIOLOGY, V52, P830, DOI 10.1177/0038038516674675
   Kim NW, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300309
   Koh K, 2010, IEEE T VIS COMPUT GR, V16, P1190, DOI 10.1109/TVCG.2010.175
   Koyama Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392444
   Krcadinac U, 2016, IEEE T HUM-MACH SYST, V46, P370, DOI 10.1109/THMS.2015.2504081
   Kucher K, 2018, COMPUT GRAPH FORUM, V37, P71, DOI 10.1111/cgf.13217
   Kulahcioglu T, 2020, ACM T INTERACT INTEL, V10, DOI 10.1145/3370928
   Lan X., 2022, arXiv
   Lan XY, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517530
   Lan XY, 2022, IEEE T VIS COMPUT GR, V28, P933, DOI 10.1109/TVCG.2021.3114775
   Lan XY, 2021, IEEE T VIS COMPUT GR, V27, P2796, DOI 10.1109/TVCG.2021.3074582
   Lee B, 2010, IEEE T VIS COMPUT GR, V16, P1182, DOI 10.1109/TVCG.2010.194
   Lee D.G., 2007, Comput. Entertain., V5, P11, DOI DOI 10.1145/1279540.1279551
   Lee J.C., 2002, P 15 ANN ACM S US IN, P81, DOI DOI 10.1145/571985.571997
   Lee-Robbins Elsie, 2023, IEEE Trans Vis Comput Graph, V29, P1, DOI 10.1109/TVCG.2022.3209500
   Li GZ, 2023, IEEE T VIS COMPUT GR, V29, P5451, DOI 10.1109/TVCG.2022.3215070
   Li JB, 2023, CURR PSYCHOL, V42, P3349, DOI 10.1007/s12144-021-01693-9
   Li WC, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581470
   lipsum.com, 2022, Lorem ipsum
   Ma HW, 2023, IEEE T AFFECT COMPUT, V14, P1655, DOI 10.1109/TAFFC.2021.3104512
   Maharik R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964995
   Malik S, 2009, VISUAL COMMUNIC, V8, P469, DOI 10.1177/1470357209343375
   Méndez GG, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174212
   Minakuchi M., 2005, ACM International Conference Proceeding Series, V265, P221, DOI DOI 10.1145/1178477.1178512
   O'Donovan P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601110
   Ortony A., 1990, The Cognitive Structure of Emotions, DOI DOI 10.1017/CBO9780511571299
   Park D, 2018, IEEE T VIS COMPUT GR, V24, P3032, DOI 10.1109/TVCG.2017.2785807
   Pousman Z, 2007, IEEE T VIS COMPUT GR, V13, P1145, DOI 10.1109/TVCG.2007.70541
   Rivadeneira AW, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P995
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Scherer K. R., 2013, Components of Emotional Meaning: A Sourcebook, P281, DOI DOI 10.1093/ACPROF:OSO/9780199592746.003.0019
   Schetinger V, 2023, COMPUT GRAPH FORUM, V42, P423, DOI 10.1111/cgf.14841
   Shin M, 2023, IEEE T VIS COMPUT GR, V29, P2980, DOI 10.1109/TVCG.2022.3146329
   Shu XH, 2021, IEEE T VIS COMPUT GR, V27, P1492, DOI 10.1109/TVCG.2020.3030396
   Shu XH, 2021, J VISUAL-JAPAN, V24, P85, DOI 10.1007/s12650-020-00689-0
   Stone R. B., 2004, Design and Emotion, P212, DOI DOI 10.1201/9780203608173
   Sturdee M., 2022, P DES RES SOC JUN 25, DOI DOI 10.21606/DRS.2022.257
   Thomas F., 1995, The Illusion of Life: Disney Animation
   Tversky B, 2002, INT J HUM-COMPUT ST, V57, P247, DOI 10.1006/ijhc.1017
   Urquhart LWR, 2017, INT CONF ENG DES, P109
   VanGorp T, 2012, DESIGN FOR EMOTION, P1
   Viégas FB, 2009, IEEE T VIS COMPUT GR, V15, P1137, DOI 10.1109/TVCG.2009.171
   Wang H., 2016, CHI 04 HUM FACT COMP, DOI [DOI 10.1145/985921.986016, 10.1145/985921.986016]
   Wang Y, 2019, IEEE COMPUT GRAPH, V39, P8, DOI 10.1109/MCG.2019.2923483
   Wang YH, 2020, IEEE T VIS COMPUT GR, V26, P991, DOI 10.1109/TVCG.2019.2934783
   Wang YH, 2018, IEEE T VIS COMPUT GR, V24, P647, DOI 10.1109/TVCG.2017.2745859
   Wankhade M, 2022, ARTIF INTELL REV, V55, P5731, DOI 10.1007/s10462-022-10144-1
   Wood J., 2022, P 2 WORKSH ALT VIS, P1
   WordArts, 2022, about us
   Wu AY, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445179
   Wu YC, 2011, COMPUT GRAPH FORUM, V30, P741, DOI 10.1111/j.1467-8659.2011.01923.x
   Xu J, 2016, IEEE PAC VIS SYMP, P239, DOI 10.1109/PACIFICVIS.2016.7465278
   Zhang JE, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376172
   Zou CQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925887
NR 83
TC 3
Z9 3
U1 7
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5198
EP 5211
DI 10.1109/TVCG.2023.3286392
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400059
PM 37318965
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zheng, JW
   Hsu, JY
   Li, CC
   Lin, IC
AF Zheng, Jia-Wen
   Hsu, Jhen-Yung
   Li, Chih-Chia
   Lin, I-Chen
TI Characteristic-Preserving Latent Space for Unpaired Cross-Domain
   Translation of 3D Point Clouds
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape transfer; shape characteristics; unpaired learning; 3D point cloud
   generative model
ID NETWORK
AB This article aims at unpaired shape-to-shape transformation for 3D point clouds, for instance, turning a chair to its table counterpart. Recent work for 3D shape transfer or deformation highly relies on paired inputs or specific correspondences. However, it is usually not feasible to assign precise correspondences or prepare paired data from two domains. A few methods start to study unpaired learning, but the characteristics of a source model may not be preserved after transformation. To overcome the difficulty of unpaired learning for transformation, we propose alternately training the autoencoder and translators to construct shape-aware latent space. This latent space based on novel loss functions enables our translators to transform 3D point clouds across domains and maintain the consistency of shape characteristics. We also crafted a test dataset to objectively evaluate the performance of point-cloud translation. The experiments demonstrate that our framework can construct high-quality models and retain more shape characteristics during cross-domain translation compared to the state-of-the-art methods. Moreover, we also present shape editing applications with our proposed latent space, including shape-style mixing and shape-type shifting, which do not require retraining a model.
C1 [Zheng, Jia-Wen; Hsu, Jhen-Yung; Li, Chih-Chia; Lin, I-Chen] Natl Yang Ming Chiao Tung Univ, Coll Comp Sci, Hsinchu 30010, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Lin, IC (corresponding author), Natl Yang Ming Chiao Tung Univ, Coll Comp Sci, Hsinchu 30010, Taiwan.
EM alicegogohaha.cs07g@nctu.edu.tw; yunyung.cs08@nycu.edu.tw;
   nycu.309553007.cs09@nycu.edu.tw; ichenlin@cs.nctu.edu.tw
OI Lin, I-Chen/0000-0001-9924-4723
FU Ministry of Science and Technology, Taiwan [MOST 109-2221-E-009-122-MY3]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan under Grant MOST 109-2221-E-009-122-MY3.
CR Boulch A., 2017, 3dor@ Eurographics, V3, P1, DOI [DOI 10.2312/3DOR.201710473, 10.2312/3dor.20171047]
   Chen QM, 2022, PROC CVPR IEEE, P18593, DOI 10.1109/CVPR52688.2022.01806
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen ZQ, 2021, PROC CVPR IEEE, P15735, DOI 10.1109/CVPR46437.2021.01548
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275028
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gulrajani I., 2017, P ADV NEUR INF PROC, P5767
   Huang HB, 2018, ACM T GRAPHIC, V37, DOI [10.1145/3137609, 10.1145/3072959.3073654]
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jacobson A., 2012, ACM Trans. Graph., V31, P1
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D.P., 2014, P INT C LEARNING REP
   Lawin FJ, 2017, LECT NOTES COMPUT SC, V10424, P95, DOI 10.1007/978-3-319-64689-3_8
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li B, 2020, IEEE ACCESS, V8, P83782, DOI 10.1109/ACCESS.2020.2992554
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li CC, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585508
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li K, 2019, IEEE T VIS COMPUT GR, V25, P2255, DOI 10.1109/TVCG.2018.2832136
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Li XZ, 2022, IEEE T VIS COMPUT GR, V28, P4503, DOI 10.1109/TVCG.2021.3092570
   Li YS, 2022, IEEE T VIS COMPUT GR, V28, P3499, DOI 10.1109/TVCG.2021.3069195
   Lim I, 2019, COMPUT GRAPH FORUM, V38, P99, DOI 10.1111/cgf.13792
   Liu M.-Y., 2016, Proceedings of the 30th International Conference on Neural Information Processing Systems, P469
   Liu MY, 2017, ADV NEUR IN, V30
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu ZJ, 2019, ADV NEUR IN, V32
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527
   Odena A, 2017, PR MACH LEARN RES, V70
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rethage D, 2018, LECT NOTES COMPUT SC, V11208, P625, DOI 10.1007/978-3-030-01225-0_37
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Valsesia G., 2019, P INT C LEARN REPR M, P49
   Wang I, 2015, Sci. Syst.
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie CL, 2021, PROC CVPR IEEE, P4617, DOI 10.1109/CVPR46437.2021.00459
   Yan ZH, 2022, IEEE T VIS COMPUT GR, V28, P4304, DOI 10.1109/TVCG.2021.3086113
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang J, 2018, GRAPH MODELS, V98, P1, DOI 10.1016/j.gmod.2018.05.003
   Yang KZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459873
   Yang XB, 2020, IEEE T VIS COMPUT GR, V26, P3446, DOI 10.1109/TVCG.2020.3023634
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin KX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12436, DOI 10.1109/ICCV48922.2021.01223
   Yin KX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356494
   Yin KX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201288
   Zamorski M, 2019, Arxiv, DOI arXiv:1811.07605
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 65
TC 0
Z9 0
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5212
EP 5226
DI 10.1109/TVCG.2023.3287923
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400095
PM 37339041
DA 2024-11-06
ER

PT J
AU Zhang, Z
   Li, M
   Zhao, ZY
   Fang, Q
   Fu, XM
AF Zhang, Zheng
   Li, Mo
   Zhao, Zheng-Yu
   Fang, Qing
   Fu, Xiao-Ming
TI Practical Integer-Constrained Cone Construction for Conformal
   Parameterizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distortion; Optimization; Mesh generation; Merging; Computational
   modeling; Iterative methods; Finite element analysis;
   Integer-constrained cones; conformal parameterizations; IRLS;
   progressive rounding; relocating cones; merging cones
AB We propose a practical method to construct sparse integer-constrained cone singularities with low distortion constraints for conformal parameterizations. Our solution for this combinatorial problem is a two-stage procedure that first enhances sparsity for generating an initialization and then optimizes to reduce the number of cones and the parameterization distortion. Central to the first stage is a progressive process to determine the combinatorial variables, i.e., numbers, locations, and angles of cones. The second stage iteratively conducts adaptive cone relocations and merges close cones for optimization. We extensively test our method on a data set containing 3885 models, demonstrating practical robustness and performance. Our method achieves fewer cone singularities and lower parameterization distortion than state-of-the-art methods.
C1 [Zhang, Zheng] Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Anhui, Peoples R China.
   [Li, Mo; Fu, Xiao-Ming] Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Anhui, Peoples R China.
EM zheng1003@mail.ustc.edu.cn; lmo@mail.ustc.edu.cn;
   zyzhao18@mail.ustc.edu.cn; fq1208@mail.ustc.edu.cn; fuxm@ustc.edu.cn
RI Fu, Xiao-Ming/V-8253-2019
OI Fang, Qing/0000-0001-7934-6060; Zhao, Zheng-Yu/0000-0003-0360-5518; Li,
   Mo/0000-0003-3166-4136; Fu, Xiao-Ming/0000-0001-8479-0107
FU National Natural Science Foundation of China [62272429]; Major Porject
   of Science and Technology of Anhui Province [202203a05020050]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272429 and in part by the Major
   Porject of Science and Technology of Anhui Province under Grant
   202203a05020050.
CR Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   Amaldi E, 1998, THEOR COMPUT SCI, V209, P237, DOI 10.1016/S0304-3975(97)00115-1
   ApS M., 2022, The MOSEK Fusion API for C++ 10.0.43
   Aubin T., 2013, Some Nonlinear Problems in Riemannian Geometry
   Ben-Chen M, 2008, COMPUT GRAPH FORUM, V27, P449, DOI 10.1111/j.1467-8659.2008.01142.x
   Ben-Chen M, 2010, COMPUT GRAPH FORUM, V29, P1701, DOI 10.1111/j.1467-8659.2010.01779.x
   Bommes D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462014
   Bommes D, 2013, COMPUT GRAPH FORUM, V32, P51, DOI 10.1111/cgf.12014
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   Campen M, 2014, COMPUT GRAPH FORUM, V33, P69, DOI 10.1111/cgf.12401
   Campen M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480557
   Campen M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3360511
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   CHERRIER P, 1984, J FUNCT ANAL, V57, P154, DOI 10.1016/0022-1236(84)90094-6
   Crane K, 2010, COMPUT GRAPH FORUM, V29, P1525, DOI 10.1111/j.1467-8659.2010.01761.x
   Daubechies I, 2010, COMMUN PUR APPL MATH, V63, P1, DOI 10.1002/cpa.20303
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Dey TK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462017
   Diamanti O, 2014, COMPUT GRAPH FORUM, V33, P1, DOI 10.1111/cgf.12426
   Ebke HC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508372
   Fang Q, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480526
   Fang Q, 2020, COMPUT AIDED DESIGN, V126, DOI 10.1016/j.cad.2020.102863
   Farchi N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201375
   Fisher M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239507
   Floudas C. A., 1995, Nonlinear and Mixed-Integer Optimization: Fundamentals and Applications
   GORRY GA, 1972, MANAGE SCI, V18, P229, DOI 10.1287/mnsc.18.5.229
   He L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461965
   Jiang TF, 2014, IEEE T VIS COMPUT GR, V20, P1189, DOI 10.1109/TVCG.2013.250
   Kälberer F, 2007, COMPUT GRAPH FORUM, V26, P375, DOI 10.1111/j.1467-8659.2007.01060.x
   Kharevych L, 2006, ACM T GRAPHIC, V25, P412, DOI 10.1145/1138450.1138461
   Knöppel F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462005
   Konakovic M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925944
   Lei N, 2020, COMPUT METHOD APPL M, V366
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Li M, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530118
   Mohimani GH, 2007, LECT NOTES COMPUT SC, V4666, P389
   Myles A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185605
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Sawhney R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132705
   Segall Aviv., 2016, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. SCA'16, P85
   Soliman Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201367
   Springborn B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360676
   Vaxman A, 2016, COMPUT GRAPH FORUM, V35, P545, DOI 10.1111/cgf.12864
   Wang K, 2006, ACM T GRAPHIC, V25, P1041, DOI 10.1145/1141911.1141991
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zheng XP, 2021, COMPUT METHOD APPL M, V387, DOI 10.1016/j.cma.2021.114146
   Zhong ZC, 2014, GRAPH MODELS, V76, P468, DOI 10.1016/j.gmod.2014.03.011
NR 47
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5227
EP 5239
DI 10.1109/TVCG.2023.3287303
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400080
PM 37335785
DA 2024-11-06
ER

PT J
AU Luo, ZJ
   Du, D
   Zhu, HM
   Yu, YZ
   Fu, HB
   Han, XG
AF Luo, Zhongjin
   Du, Dong
   Zhu, Heming
   Yu, Yizhou
   Fu, Hongbo
   Han, Xiaoguang
TI <i>SketchMetaFace:</i> A Learning-Based Sketching Interface for
   High-Fidelity 3D Character Face Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Faces; Shape; Load modeling;
   Computational modeling; Image reconstruction; Face modeling; neural
   network; sketch-based 3D modeling
AB Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute significant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy workload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling diverse facial shapes and rich geometric details. In this article, we present SketchMetaFace - a sketching system targeting amateur users to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First, curvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem of mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed "Implicit and Depth Guided Mesh Modeling" (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high efficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and visual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency.
C1 [Luo, Zhongjin; Du, Dong; Zhu, Heming; Han, Xiaoguang] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Yu, Yizhou] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; University of Hong Kong;
   City University of Hong Kong
RP Han, XG (corresponding author), Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
EM 220019015@link.cuhk.edu.cn; dongdu@mail.ustc.edu.cn;
   hezhu@mpi-inf.mpg.de; yizhouy@acm.org; fuplus@gmail.com;
   hanxiaoguang@cuhk.edu.cn
RI Du, Dong/AEG-5685-2022; /F-3345-2010
OI Zhu, Heming/0000-0003-3525-9349; Han, Xiaoguang/0000-0003-0162-3296;
   /0000-0002-0470-5548; FU, Hongbo/0000-0002-0284-726X; Du,
   Dong/0000-0001-5481-389X; Luo, Zhongjin/0000-0002-3483-4236
FU NSFC [62172348]; Project Hetao Shenzhen-HK S&T Cooperation Zone
   [HZQB-KCZYZ-2021067]; National Key R&D Program of China
   [2018YFB1800800]; Shenzhen Outstanding Talents Training Fund [202002];
   Guangdong Research Projects [2017ZT07X152, 2019CX01X104]; Guangdong
   Provincial Key Laboratory of Future Networks of Intelligence
   [2022B1212010001]; Key Area R&D Program of Guangdong Province
   [2018B030338001]; Outstanding Yound Fund of Guangdong Province
   [2023B1515020055]; Shenzhen General Project [JCYJ20220530143604010];
   Hong Kong Research Grants Council [HKU17206218]; Research Grants Council
   of the Hong Kong Special Administrative Region, China [CityU 11212119];
   Centre for Applied Computing and Interactive Media (ACIM) of School of
   Creative Media, CityU
FX This work was supported in part by NSFC under Grant 62172348,in part by
   the Basic Research Project under Grant HZQB-KCZYZ-2021067 through
   project Hetao Shenzhen-HK S&T Cooperation Zone, in part by the National
   Key R&D Program of China under Grant 2018YFB1800800, in part by the
   Shenzhen Outstanding Talents Training Fund under Grant 202002, in part
   by the Guangdong Research Projects under Grants 2017ZT07X152 and
   2019CX01X104, in part by the Guangdong Provincial Key Laboratory of
   Future Networks of Intelligence under Grant 2022B1212010001, in part by
   the Shenzhen Key Laboratory of Big Data and Artificial Intelligence
   under Grant ZDSYS201707251409055, in part by the Key Area R&D Program of
   Guangdong Province under Grant 2018B030338001, in part by Outstanding
   Yound Fund of Guangdong Province under Grant 2023B1515020055, in part by
   Shenzhen General Project under Grant JCYJ20220530143604010, in part by
   Hong Kong Research Grants Council under General Research Funds under
   Grant HKU17206218, in part by Research Grants Council of the Hong Kong
   Special Administrative Region, China under Grant CityU 11212119, and in
   part by the Centre for Applied Computing and Interactive Media (ACIM) of
   School of Creative Media, CityU.
CR Tran AT, 2018, PROC CVPR IEEE, P3935, DOI 10.1109/CVPR.2018.00414
   Bae SH, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P151
   Bai ZQ, 2020, PROC CVPR IEEE, P5849, DOI 10.1109/CVPR42600.2020.00589
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Bartier PM, 1996, COMPUT GEOSCI, V22, P795, DOI 10.1016/0098-3004(96)00021-0
   Bernhardt A., 2008, P EUR WORKSH SKETCH, P57
   Borosán P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366217
   Botsch Mario, 2004, P 2004 EUR ACM SIGGR, P185, DOI DOI 10.1145/1057432.1057457
   Cai HR, 2021, Arxiv, DOI arXiv:2004.09190
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Cheng ZZ, 2022, LECT NOTES COMPUT SC, V13663, P303, DOI 10.1007/978-3-031-20062-5_18
   Chowdhury PN, 2022, INT CONF 3D VISION, P22, DOI 10.1109/3DV57658.2022.00015
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Delanoy J, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203197
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Ding C, 2016, FRONT COMPUT SCI-CHI, V10, P985, DOI 10.1007/s11704-016-5422-9
   Du D, 2022, IEEE T VIS COMPUT GR, V28, P2415, DOI 10.1109/TVCG.2020.3030330
   Du D, 2021, COMPUT GRAPH FORUM, V40, P222, DOI 10.1111/cgf.14184
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Fan LB, 2013, COMPUT GRAPH FORUM, V32, P157, DOI 10.1111/cgf.12223
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Gingold Y., 2009, ACM SIGGRAPH ASIA, P1
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guillard B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13003, DOI 10.1109/ICCV48922.2021.01278
   Han XG, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073629
   Huang HB, 2017, IEEE T VIS COMPUT GR, V23, P2003, DOI 10.1109/TVCG.2016.2597830
   Iarussi E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2710026
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Joshi P., 2008, Sustain. Bus. Manage., P49
   Karpenko OA, 2006, ACM T GRAPHIC, V25, P589, DOI 10.1145/1141911.1141928
   Kong Di, 2022, P ASIAN C COMPUTER V, P1522
   Li BQ, 2016, ACSR ADV COMPUT, V57, P47
   Li C., 2020, ACM Trans. Graph., V39, P114
   Li CL, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3238250
   Li CL, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3091108
   Liu JF, 2009, COMPUT GRAPH FORUM, V28, P2104, DOI 10.1111/j.1467-8659.2009.01418.x
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Luo L, 2022, Arxiv, DOI arXiv:2209.09043
   Luo Zhongjin, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P854, DOI 10.1145/3472749.3474791
   Mallikarjun BR, 2021, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR46437.2021.00337
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Nealen A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276429, 10.1145/1239451.1239492]
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Olsen L, 2011, IEEE COMPUT GRAPH, V31, P24, DOI 10.1109/MCG.2011.84
   Pan H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766990
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Qi AR, 2021, IEEE T IMAGE PROCESS, V30, P8595, DOI 10.1109/TIP.2021.3118975
   Qiu YD, 2021, PROC CVPR IEEE, P10231, DOI 10.1109/CVPR46437.2021.01010
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Schmidt A., 2009, ACM SIGGRAPH ASIA, P1
   Schmidt R., 2007, ACM SIGGRAPH 2007 courses, P43
   Shao C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185541
   Sharf A, 2006, COMPUT GRAPH FORUM, V25, P389, DOI 10.1111/j.1467-8659.2006.00958.x
   Sorkine O., 2004, P S GEOM PROC, P175, DOI [DOI 10.1145/1057432.1057456, 10.]
   Su WC, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203186
   Sykora D, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591011
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang Jiayun, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13808), P184, DOI 10.1007/978-3-031-25085-9_11
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wu QY, 2018, PROC CVPR IEEE, P7336, DOI 10.1109/CVPR.2018.00766
   Xiao YZ, 2022, AAAI CONF ARTIF INTE, P2839
   Xie XH, 2013, COMPUT GRAPH FORUM, V32, P233, DOI 10.1111/cgf.12200
   Xu BX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601128
   Xu P, 2023, IEEE T PATTERN ANAL, V45, P285, DOI 10.1109/TPAMI.2022.3148853
   Zhang SH, 2021, PROC CVPR IEEE, P6008, DOI 10.1109/CVPR46437.2021.00595
   Zhong Y, 2022, COMPUT GRAPH-UK, V106, P237, DOI 10.1016/j.cag.2022.06.005
   Zhong Y, 2021, IEEE T CIRC SYST VID, V31, P3518, DOI 10.1109/TCSVT.2020.3040900
   Zhong Y, 2020, INT CONF 3D VISION, P543, DOI 10.1109/3DV50981.2020.00064
NR 78
TC 1
Z9 1
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5260
EP 5275
DI 10.1109/TVCG.2023.3291703
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400075
PM 37467083
DA 2024-11-06
ER

PT J
AU Liu, C
   Guo, YH
   Yuan, XR
AF Liu, Can
   Guo, Yuhan
   Yuan, Xiaoru
TI AutoTitle: An Interactive Title Generator for Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Natural languages; Measurement; Task analysis;
   Semantics; Generators; Visualization; Deep learning; large language
   model; natural language; visualization title
AB We propose AutoTitle, an interactive visualization title generator satisfying multifarious user requirements. Factors making a good title, namely, the feature importance, coverage, preciseness, general information richness, conciseness, and non-technicality, are summarized based on the feedback from user interviews. Visualization authors need to trade off among these factors to fit specific scenarios, resulting in a wide design space of visualization titles. AutoTitle generates various titles through the process of visualization facts traversing, deep learning-based fact-to-title generation, and quantitative evaluation of the six factors. AutoTitle also provides users with an interactive interface to explore the desired titles by filtering the metrics. We conduct a user study to validate the quality of generated titles as well as the rationality and helpfulness of these metrics.
C1 [Liu, Can; Guo, Yuhan; Yuan, Xiaoru] Peking Univ, Sch Intelligence Sci & Technol, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
   [Yuan, Xiaoru] Peking Univ, Natl Engn Lab Big Data Anal & Applicat, Beijing 100871, Peoples R China.
C3 Peking University; Peking University
RP Yuan, XR (corresponding author), Peking Univ, Sch Intelligence Sci & Technol, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.; Yuan, XR (corresponding author), Peking Univ, Natl Engn Lab Big Data Anal & Applicat, Beijing 100871, Peoples R China.
EM can.liu@pku.edu.cn; yuhan.guo@pku.edu.cn; xiaoru.yuan@pku.edu.cn
RI Yuan, Xiaoru/E-1798-2013
OI Yuan, Xiaoru/0000-0002-7233-980X; Liu, Can/0000-0002-1175-0734; Guo,
   Yuhan/0009-0004-3857-7486
FU National Natural Science Foundation of China [62272012]; Lenovo AI
   Master project
FX No Statement Available
CR Brown TB, 2020, Arxiv, DOI arXiv:2005.14165
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bar-Hillel Y., 1952, RLE Technical Reports.
   Bertin J., 1983, Semiology of Graphics
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Chen ZY, 2020, Arxiv, DOI arXiv:1904.09521
   Chen ZY, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2096
   Cui WW, 2020, IEEE T VIS COMPUT GR, V26, P906, DOI 10.1109/TVCG.2019.2934785
   D'Alfonso S, 2011, INFORMATION, V2, P61, DOI 10.3390/info2010061
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Edunov S, 2018, Arxiv, DOI arXiv:1808.09381
   Flesch R, 1948, J APPL PSYCHOL, V32, P221, DOI 10.1037/h0057532
   Floridi L, 2004, MIND MACH, V14, P197, DOI 10.1023/B:MIND.0000021684.50925.c9
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gatt A, 2018, J ARTIF INTELL RES, V61, P65, DOI 10.1613/jair.5477
   Hoffman P, 1997, VISUALIZATION '97 - PROCEEDINGS, P437, DOI 10.1109/VISUAL.1997.663916
   Hu K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300358
   Hwang W, 2019, Arxiv, DOI [arXiv:1902.01069, DOI 10.48550/ARXIV.1902.01069]
   Kahou S. E., 2018, P WORKSH TRACK PROC, P1
   Kim DH, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376467
   Kong HK, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300576
   Kong HK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174012
   Lebret R., 2016, P 2016 C EMP METH NA, P1203, DOI DOI 10.18653/V1/D16-1128
   Liang P., 2009, P ANN M ASS COMP LIN, P91, DOI DOI 10.3115/1687878.1687893
   Liu C, 2020, IEEE PAC VIS SYMP, P191, DOI 10.1109/PacificVis48177.2020.1043
   Liu TY, 2018, AAAI CONF ARTIF INTE, P4881
   Lundgard A, 2022, IEEE T VIS COMPUT GR, V28, P1073, DOI 10.1109/TVCG.2021.3114770
   Montemurro MA, 2010, ADV COMPLEX SYST, V13, P135, DOI 10.1142/S0219525910002530
   OpenAI, 2022, ChatGPTAPI.
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pitler Emily, 2008, P 2008 C EMP METH NA, P186, DOI [DOI 10.3115/1613715.1613742, 10.5555/1613715.1613742, DOI 10.5555/1613715.1613742]
   Poco J, 2017, COMPUT GRAPH FORUM, V36, P353, DOI 10.1111/cgf.13193
   Bowman SR, 2016, Arxiv, DOI arXiv:1511.06349
   Radford A., 2019, Language models are unsupervised multitask learners, V1, P9
   Radford Alec., 2018, Improving language understanding by generative pre-training
   Raffel C, 2023, Arxiv, DOI arXiv:1910.10683
   Reiter E., 1997, Natural Language Engineering, V3, P57, DOI 10.1017/S1351324997001502
   Savva Manolis, 2011, P 24 ANN ACM S US IN, P393
   Scharrer L, 2017, PUBLIC UNDERST SCI, V26, P1003, DOI 10.1177/0963662516680311
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Shi DQ, 2021, IEEE T VIS COMPUT GR, V27, P453, DOI 10.1109/TVCG.2020.3030403
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Ting Z. T., 2017, P 34 INT C MACH LEAR, V70, P1587
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4446
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P895, DOI 10.1109/TVCG.2019.2934398
   Wanzer DL, 2021, EVAL PROGRAM PLANN, V84, DOI 10.1016/j.evalprogplan.2020.101896
   Wiseman S, 2017, Arxiv, DOI arXiv:1707.08052
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Yu T, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3911
NR 52
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5276
EP 5288
DI 10.1109/TVCG.2023.3290241
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400019
PM 37384476
DA 2024-11-06
ER

PT J
AU Cong, MT
   Lan, LA
   Fedkiw, R
AF Cong, Matthew
   Lan, Lana
   Fedkiw, Ronald
TI Local Geometric Indexing of High Resolution Data for Facial
   Reconstruction From Sparse Markers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Faces; Geometry; Surface reconstruction; Cameras; Point cloud
   compression; Deformation; Computer graphics; image processing and
   computer vision; interpolation
AB When considering sparse motion capture marker data, one typically struggles to balance its overfitting via a high dimensional blendshape system versus underfitting caused by smoothness constraints. With the current trend towards using more and more data, our aim is not to fit the motion capture markers with a parameterized (blendshape) model or to smoothly interpolate a surface through the marker positions, but rather to find an instance in the high resolution dataset that contains local geometry to fit each marker. Just as is true for typical machine learning applications, this approach benefits from a plethora of data, and thus we also consider augmenting the dataset via specially designed physical simulations that target the high resolution dataset such that the simulation output lies on the same so-called manifold as the data targeted.
C1 [Cong, Matthew; Lan, Lana; Fedkiw, Ronald] Ind Light & Mag, San Francisco, CA 94129 USA.
   [Fedkiw, Ronald] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
C3 Stanford University
RP Cong, MT (corresponding author), Ind Light & Mag, San Francisco, CA 94129 USA.
EM matthew.d.cong@gmail.com; lanalan@gmail.com; fedkiw@cs.stanford.edu
FU ONR [N00014-19-1-2285, N00014-21-1-2771]
FX No Statement Available
CR Beeler T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601182
   Beeler T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778777
   Beeler T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964970
   Bermano AH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2546276
   Bhat R., 2013, P 12 ACM SIGGRAPH EU, P7, DOI [10.1145/2485895.24859153J.P., DOI 10.1145/2485895.24859153J.P]
   Bickel B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276419, 10.1145/1239451.1239484]
   Bickel M., P ACM EUR SIGGRAPH S
   Ribera RBI, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073674
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Chuang C., 2002, Comput. Sci. Tech. Rep., V2, P25
   Cong K. S., 2016, P ACM SIGGRAPH EUR S, P119
   Cong M., 2015, P 14 ACM SIGGRAPH EU, P175, DOI [10.1145/2786784.278678632M, DOI 10.1145/2786784.278678632M]
   Cong M, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085040
   de Berg M., 2008, Computational Geometry: Algorithms and Applications, DOI DOI 10.1007/978-3-540-77974-2
   Dinev D, 2018, COMPUT GRAPH FORUM, V37, P93, DOI 10.1111/cgf.13515
   FUJIMOTO A, 1986, IEEE COMPUT GRAPH, V6, P16, DOI 10.1109/MCG.1986.276715
   Fyffe A., 2015, ACM Trans.Graph., V34, DOI [10.1145/26385498A., DOI 10.1145/26385498A]
   Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163
   Huynh L, 2018, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR.2018.00877
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Lan M., 2017, P ACM SIGGRAPH DIG P, DOI [10.1145/3105692.310569318C, DOI 10.1145/3105692.310569318C]
   Lefebvre M., 1974, A parametric model for human faces, P199
   Li H., 2009, P ACM SIGGRAPH AS PA, P10, DOI [10.1145/1661412.1618521, DOI 10.1145/1661412.1618521]
   Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769
   Lüders S, 2022, J COMPUT PHYS, V467, DOI 10.1016/j.jcp.2022.111476
   Ma WC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409074
   Moser L, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085086
   Orvalho V., 2012, Eurographics 2012-State of the Art Reports, P183
   Park SW, 2006, IEEE T VIS COMPUT GR, V12, P243, DOI 10.1109/TVCG.2006.27
   Pharr JR, 2010, Physically Based Rendering: From Theory toImplementation
   Piper B., 1993, Computing (Supplementum), P227, DOI 10.1007/978-3-7091-6916-2_15
   Rhee M., P 35 ANN C EUR ASS C
   Seo J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024198
   Seol Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159519
   Shewchuk JonathanRichard., 2002, 11 INT MESHING ROUND, P193
   SIBSON R, 1980, MATH PROC CAMBRIDGE, V87, P151, DOI 10.1017/S0305004100056589
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971
   Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3182644
   Williams L., 1990, Computer Graphics, V24, P235, DOI 10.1145/97880.97906
   Wu CL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925882
   Zell E, 2022, COMPUT GRAPH FORUM, V41, P121, DOI 10.1111/cgf.14463
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
   Zoss G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201382
NR 48
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5289
EP 5298
DI 10.1109/TVCG.2023.3289495
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400048
PM 37363850
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhang, YS
   Zhu, JH
   Xue, MF
   Zhang, XP
   Cao, XC
AF Zhang, Yushu
   Zhu, Jiahao
   Xue, Mingfu
   Zhang, Xinpeng
   Cao, Xiaochun
TI Adaptive 3D Mesh Steganography Based on Feature-Preserving Distortion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Steganography; Distortion; Feature
   extraction; Solid modeling; Smoothing methods; Electronic mail; 3D mesh;
   3D mesh steganography; syndrome trellis code; 3D steganalysis
ID CAPACITY; STEGANALYSIS; ALGORITHM
AB Current 3D mesh steganography algorithms relying on geometric modification are prone to detection by steganalyzers. In traditional steganography, adaptive steganography has proven to be an efficient means of enhancing steganography security. Taking inspiration from this, we propose a highly adaptive embedding algorithm, guided by the principle of minimizing a carefully crafted distortion through efficient steganography codes. Specifically, we tailor a payload-limited embedding optimization problem for 3D settings and devise a feature-preserving distortion (FPD) to measure the impact of message embedding. The distortion takes on an additive form and is defined as a weighted difference of the effective steganalytic subfeatures utilized by the current 3D steganalyzers. With practicality in mind, we refine the distortion to enhance robustness and computational efficiency. By minimizing the FPD, our algorithm can preserve mesh features to a considerable extent, including steganalytic and geometric features, while achieving a high embedding capacity. During the practical embedding phase, we employ the Q-layered syndrome trellis code (STC). However, calculating the bit modification probability (BMP) for each layer of the Q-layered STC, given the variation of Q, can be cumbersome. To address this issue, we design a universal and automatic approach for the BMP calculation. The experimental results demonstrate that our algorithm achieves state-of-the-art performance in countering 3D steganalysis.
C1 [Zhang, Yushu; Xue, Mingfu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Zhang, Yushu; Xue, Mingfu] Zhengzhou Xinda Inst Adv Technol, Zhengzhou 450001, Peoples R China.
   [Zhu, Jiahao] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen 518107, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Sun Yat Sen
   University; Fudan University; Sun Yat Sen University
RP Zhu, JH (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM yushu@nuaa.edu.cn; zhujh59@mail2.sysu.edu.cn; mingfu.xue@nuaa.edu.cn;
   zhangxinpeng@fudan.edu.cn; caoxiaochun@mail.sysu.edu.cn
OI zhang, yushu/0000-0001-8183-8435; Xue, Mingfu/0000-0003-2408-503X
FU National Key R&D Program of China [2021YFB3100400]; Open Foundation of
   Henan Key Laboratory of Cyber space Situation Awareness [HNTS2022013]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFB3100400, and in part by the Open Foundation of Henan
   Key Laboratory of Cyber space Situation Awareness under Grant
   HNTS2022013.
CR Aspert N, 2002, PROC SPIE, V4790, P211, DOI 10.1117/12.455358
   Bogomjakov A, 2008, COMPUT GRAPH FORUM, V27, P637, DOI 10.1111/j.1467-8659.2008.01161.x
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chao MW, 2009, IEEE T VIS COMPUT GR, V15, P274, DOI 10.1109/TVCG.2008.94
   Cheng YM, 2006, VISUAL COMPUT, V22, P845, DOI 10.1007/s00371-006-0069-4
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Filler T, 2010, IEEE T INF FOREN SEC, V5, P705, DOI 10.1109/TIFS.2010.2077629
   Fridrich J., STEGANOGRAPHY DIGITA
   Fridrich T., 2007, Int. Soc. Opt.Photon., P650
   Guo LJ, 2015, IEEE T INF FOREN SEC, V10, P2669, DOI 10.1109/TIFS.2015.2473815
   Guo LJ, 2014, IEEE T INF FOREN SEC, V9, P814, DOI 10.1109/TIFS.2014.2312817
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Huang NC, 2009, IEEE SIGNAL PROC LET, V16, P802, DOI 10.1109/LSP.2009.2024794
   Itier V, 2017, MULTIMED TOOLS APPL, V76, P26421, DOI 10.1007/s11042-016-4163-y
   Kaveh H, 2015, SECUR COMMUN NETW, V8, P159, DOI 10.1002/sec.968
   Kim D, 2017, LECT NOTES ELECTR EN, V424, P358, DOI 10.1007/978-981-10-4154-9_42
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Li NN, 2017, IEEE ACCESS, V5, P24457, DOI 10.1109/ACCESS.2017.2767072
   Li ZY, 2020, INFORM SCIENCES, V522, P164, DOI 10.1016/j.ins.2020.02.061
   Li ZY, 2020, IEEE T CYBERNETICS, V50, P1989, DOI 10.1109/TCYB.2018.2883082
   Li ZY, 2018, IEEE IMAGE PROC, P1683, DOI 10.1109/ICIP.2018.8451643
   Li ZY, 2017, IEEE IMAGE PROC, P510, DOI 10.1109/ICIP.2017.8296333
   Li ZY, 2017, INFORM SCIENCES, V415, P85, DOI 10.1016/j.ins.2017.06.011
   Li ZY, 2016, INT CONF ACOUST SPEE, P2144, DOI 10.1109/ICASSP.2016.7472056
   Maret T., 2004, P MULT SEC WORKSH MA, P68
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Tu H., 2010, Int.J.VirtualReality, V9, P55
   Tu SC, 2012, COMPUT GRAPH-UK, V36, P767, DOI 10.1016/j.cag.2012.06.002
   Tu SC, 2010, VISUAL COMPUT, V26, P1177, DOI 10.1007/s00371-009-0398-1
   Wagner M. G., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P201, DOI 10.1109/GMAP.2000.838252
   Wang CM, 2005, COMPUT GRAPH FORUM, V24, P591, DOI 10.1111/j.1467-8659.2005.00884.x
   Wang L., 2019, IEEEAccess, V7
   Yang I., 2014, ACM Trans. Multimedia Comput. Commun. Appl., V10
   Yang Y, 2013, IEEE T VIS COMPUT GR, V19, P45, DOI 10.1109/TVCG.2012.106
   Zhou H, 2022, IEEE T VIS COMPUT GR, V28, P5006, DOI 10.1109/TVCG.2021.3075136
   Zhou H, 2021, IEEE T VIS COMPUT GR, V27, P57, DOI 10.1109/TVCG.2019.2929041
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
NR 39
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5299
EP 5312
DI 10.1109/TVCG.2023.3289234
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400046
PM 37363849
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Popescu, V
   Sacks, E
   Cui, J
   Ashok, R
AF Popescu, Voicu
   Sacks, Elisha
   Cui, Jian
   Ashok, Rohan
TI Efficient and Robust From-Point Visibility
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE From-point visibility; aggressive visibility; exact visibility; triangle
   visibility; particle visibility; image generalization
AB This article presents two from-point visibility algorithms: one aggressive and one exact. The aggressive algorithm efficiently computes a nearly complete visible set, with the guarantee of finding all triangles of a front surface, no matter how small their image footprint. The exact algorithm starts from the aggressive visible set and finds the remaining visible triangles efficiently and robustly. The algorithms are based on the idea of generalizing the set of sampling locations defined by the pixels of an image. Starting from a conventional image with one sampling location at each pixel center, the aggressive algorithm adds sampling locations to make sure that a triangle is sampled at all the pixels it touches. Thereby, the aggressive algorithm finds all triangles that are completely visible at a pixel regardless of geometric level of detail, distance from viewpoint, or view direction. The exact algorithm builds an initial visibility subdivision from the aggressive visible set, which it then uses to find most of the hidden triangles. The triangles whose visibility status is yet to be determined are processed iteratively, with the help of additional sampling locations. Since the initial visible set is almost complete, and since each additional sampling location finds a new visible triangle, the algorithm converges in a few iterations.
C1 [Popescu, Voicu] Purdue Univ, W Lafayette, IN 47907 USA.
   [Sacks, Elisha] Purdue Univ, W Lafayette, IN 95014 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University
RP Popescu, V (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM popescu@purdue.edu; eps@cs.purdue.edu; michaelcui1986@gmail.com;
   ashokr@purdue.edu
OI Sacks, ELISHA/0009-0002-6955-2754
FU National Science Foundation [2212200, 2219842]
FX This work was supported by the National Science Foundation under Grants
   2212200 and 2219842.
CR Apostu O, 2012, COMPUT GRAPH-UK, V36, P727, DOI 10.1016/j.cag.2012.04.008
   Auzinger T, 2013, COMPUT GRAPH FORUM, V32, P409, DOI 10.1111/cgf.12061
   Bittner J, 2004, COMPUT GRAPH FORUM, V23, P615, DOI 10.1111/j.1467-8659.2004.00793.x
   Bittner J, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P207, DOI 10.1109/CGI.1998.694268
   Bittner J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531400
   Carpenter L., 1984, Computers & Graphics, V18, P103
   Catmull Edwin., 1978, ACM SIGGRAPH Computer Graphics, V12, P6
   Chandak A, 2008, IEEE T VIS COMPUT GR, V14, P1707, DOI 10.1109/TVCG.2008.111
   Charneau S, 2007, VISUAL COMPUT, V23, P773, DOI 10.1007/s00371-007-0129-4
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   Cui JA, 2010, IEEE T VIS COMPUT GR, V16, P1235, DOI 10.1109/TVCG.2010.127
   De Berg O., 2008, Computa-tional Geometry: Algorithms and Applications, V3rd
   Decoret X., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P281
   Durand F, 2002, ACM T GRAPHIC, V21, P176, DOI 10.1145/508357.508362
   Durand F, 2000, COMP GRAPH, P239, DOI 10.1145/344779.344891
   Durand F., 2010, PhD thesis
   Fousse L, 2007, ACM T MATH SOFTWARE, V33, DOI 10.1145/1236463.1236468
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Furnas G. W., 1986, SIGCHI Bull, V17, P16, DOI DOI 10.1145/22339.22342
   Goodman J. E., 2004, Computational Geometry, P927
   GOODRICH MT, 1992, CVGIP-GRAPH MODEL IM, V54, P1, DOI 10.1016/1049-9652(92)90029-W
   Gribel CJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964949
   Haumont Denis., 2005, Proceedings of the Sixteenth Eurographics Conference on Rendering Techniques, P211
   Heckbert P. S., 1984, Computers & Graphics, V18, P119
   Hladky J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356530
   Hunt W, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203191
   Johnson GS, 2005, ACM T GRAPHIC, V24, P1462, DOI 10.1145/1095878.1095889
   Jones TR, 2000, SPRING COMP SCI, P197
   Katz M. J., 1992, Computational Geometry: Theory and Applications, V2, P223, DOI 10.1016/0925-7721(92)90024-M
   Koch T, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451266
   Max N, 1995, SPRING COMP SCI, P74
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   MEHLHORN K, 1990, ALGORITHMICA, V5, P215, DOI 10.1007/BF01840386
   Milenkovic V., 2022, Int. J. Comput. Geometry Appl., V32, P39
   Mora F, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P191
   Nirenstein S., 2004, P 15 EUR C REND TECH, P207
   Overbeck R., 2007, EGSR 07, P85
   Palka S., 2014, Comput. Sci., V15
   Popescu V, 2000, COMP GRAPH, P433, DOI 10.1145/344779.344979
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   SHARIR M, 1992, ACM T GRAPHIC, V11, P1, DOI 10.1145/102377.112141
   Weiler K., 1977, ACM SIGGRAPH Comput. Graph., V11, P214, DOI [10.1145/563858.563896, DOI 10.1145/965141.563896, 10.1145/965141.563896]
   Wonka P, 2006, ACM T GRAPHIC, V25, P494, DOI 10.1145/1141911.1141914
   Yu JY, 2004, LECT NOTES COMPUT SC, V3022, P14
   Zhang H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P77, DOI 10.1145/258734.258781
   Zhou Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3452097
NR 46
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5313
EP 5327
DI 10.1109/TVCG.2023.3291138
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400006
PM 37390001
DA 2024-11-06
ER

PT J
AU Zhou, TS
   Huang, J
   Yu, T
   Shao, RZ
   Li, K
AF Zhou, Tiansong
   Huang, Jing
   Yu, Tao
   Shao, Ruizhi
   Li, Kun
TI HDhuman: High-Quality Human Novel-View Rendering From Sparse Views
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image-based rendering; neural rendering; human reconstruction;
   transformer; visibility reasoning
AB In this paper, we aim to address the challenge of novel view rendering of human performers that wear clothes with complex texture patterns using a sparse set of camera views. Although some recent works have achieved remarkable rendering quality on humans with relatively uniform textures using sparse views, the rendering quality remains limited when dealing with complex texture patterns as they are unable to recover the high-frequency geometry details that are observed in the input views. To this end, we propose HDhuman, which uses a human reconstruction network with a pixel-aligned spatial transformer and a rendering network with geometry-guided pixel-wise feature integration to achieve high-quality human reconstruction and rendering. The designed pixel-aligned spatial transformer calculates the correlations between the input views and generates human reconstruction results with high-frequency details. Based on the surface reconstruction results, the geometry-guided pixel-wise visibility reasoning provides guidance for multi-view feature integration, enabling the rendering network to render high-quality images at 2k resolution on novel views. Unlike previous neural rendering works that always need to train or fine-tune an independent network for a different scene, our method is a general framework that is able to generalize to novel subjects. Experiments show that our approach outperforms all the prior generic or specific methods on both synthetic data and real-world data. Source code and test data will be made publicly available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/HDhuman/index.html.
C1 [Zhou, Tiansong; Huang, Jing; Li, Kun] Tianjin Univ, Tianjin 300072, Peoples R China.
   [Yu, Tao; Shao, Ruizhi] Tsinghua Univ, Beijing 100190, Peoples R China.
C3 Tianjin University; Tsinghua University
RP Li, K (corresponding author), Tianjin Univ, Tianjin 300072, Peoples R China.
EM tiansong97@tju.edu.cn; hj00@tju.edu.cn; ytrock@126.com;
   shaorz20@mails.tsinghua.edu.cn; lik@tju.edu.cn
RI 邵, 睿智/JAC-0742-2023
OI Huang, Jing/0009-0003-3833-7629; Shao, Ruizhi/0000-0003-2188-1348
FU National Natural Science Foundation of China [62122058, 62171255,
   62171317]; Guoqiang Institute of Tsinghua University [2021GQG0001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62122058, 62171255, and 62171317, and
   in part by the Guoqiang Institute of Tsinghua University under Grant
   2021GQG0001.
CR Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Corona E, 2021, PROC CVPR IEEE, P11870, DOI 10.1109/CVPR46437.2021.01170
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Feng Q., 2022, Adv. Neural Inf. Process. Syst., V35, P7397
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722
   Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Kingma D.P., 2014, P INT C LEARNING REP
   Li K, 2021, IEEE T IMAGE PROCESS, V30, P5239, DOI 10.1109/TIP.2021.3080177
   Li XZ, 2023, IEEE T VIS COMPUT GR, V29, P5083, DOI 10.1109/TVCG.2022.3202240
   Liu L., 2020, P INT C NEUR INF PRO
   Liu LJ, 2021, IEEE T VIS COMPUT GR, V27, P4009, DOI 10.1109/TVCG.2020.2996594
   Liu YB, 2009, PROC CVPR IEEE, P2121, DOI [10.1109/CVPR.2009.5206712, 10.1109/CVPRW.2009.5206712]
   Liu YB, 2010, IEEE T VIS COMPUT GR, V16, P407, DOI 10.1109/TVCG.2009.88
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Riegler G, 2021, PROC CVPR IEEE, P12211, DOI 10.1109/CVPR46437.2021.01204
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Shuai Q., 2022, P ACM SIGGRAIPH C
   Suo X, 2021, PROC CVPR IEEE, P6222, DOI 10.1109/CVPR46437.2021.00616
   Thies J, 2020, Arxiv, DOI arXiv:1811.10720
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   twindom, 2023, About us
   Waechter M, 2014, LECT NOTES COMPUT SC, V8693, P836, DOI 10.1007/978-3-319-10602-1_54
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Xiang DL, 2020, INT CONF 3D VISION, P322, DOI 10.1109/3DV50981.2020.00042
   Xu TH, 2022, PROC CVPR IEEE, P15862, DOI 10.1109/CVPR52688.2022.01542
   Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973
   Yu T, 2021, PROC CVPR IEEE, P5742, DOI 10.1109/CVPR46437.2021.00569
   Zhao FQ, 2022, PROC CVPR IEEE, P7733, DOI 10.1109/CVPR52688.2022.00759
   Zheng Y., 2021, arXiv
   Zheng ZR, 2022, PROC CVPR IEEE, P15872, DOI 10.1109/CVPR52688.2022.01543
   Zhou TH, 2018, Arxiv, DOI arXiv:1805.09817
   Zins P, 2021, INT CONF 3D VISION, P494, DOI 10.1109/3DV53792.2021.00059
NR 47
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5328
EP 5338
DI 10.1109/TVCG.2023.3290543
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400045
PM 37384477
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Chittaro, L
AF Chittaro, Luca
TI Improving Knowledge Retention and Perceived Control Through Serious
   Games: A Study About Assisted Emergency Evacuation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Educational games; user interaction; user study; disability; training;
   safety
ID SELF-EFFICACY; EXTERNAL CONTROL; SAFETY LOCUS; INVOLVEMENT; PERFORMANCE;
   EXPERIENCE; PROMOTE; DESIGN
AB Digital games for education and training, also called serious games (SGs), have shown beneficial effects on learning in several studies. In addition, some studies are suggesting that SGs could improve user's perceived control, which affects the likelihood that the learned content will be applied in the real world. However, most SG studies tend to focus on immediate effects, providing no indication on knowledge and perceived control over time, especially in contrast with nongame approaches. Moreover, SG research on perceived control has focused mainly on self-efficacy, disregarding the complementary construct of locus of control (LOC). This article advances both lines of research, assessing user's knowledge and LOC over time, with a SG as well as traditional printed materials that teach the same content. Results show that the SG was more effective than printed materials for knowledge retention over time, and a better retention outcome was found also for LOC. An additional contribution of the paper is the proposal of a novel SG that targets the inclusivity goal of safe evacuation for all, extending SG research to a domain not dealt with before, i.e., assisting persons with disabilities in emergencies.
C1 [Chittaro, Luca] Univ Udine, Human Comp Interact Lab, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
C3 University of Udine
RP Chittaro, L (corresponding author), Univ Udine, Human Comp Interact Lab, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
EM luca.chittaro@uniud.it
OI CHITTARO, Luca/0000-0001-5975-4294
FU Friuli Venezia Giulia region
FX This work was supported by the Friuli Venezia Giulia region through the
   Project Servizi avanzati per il soccorso sanitario al disabile basati su
   tecnologie ICTinnovative (Advanced emergency medical services for the
   disabled based oninnovative ICT technologies). Recommended for
   acceptance by J. Stefanucci.
CR Alrehaili EA, 2022, INTERACT LEARN ENVIR, V30, P922, DOI 10.1080/10494820.2019.1703008
   [Anonymous], International day of sign languages
   Arthur E. A., 2013, Individual andTeam Skill Decay: The Science and Implications for Practice
   Bandura A, 2001, ANNU REV PSYCHOL, V52, P1, DOI 10.1146/annurev.psych.52.1.1
   Bandura A., 1997, SELF EFFICACY EXERCI
   Boyce K, 2017, FIRE SAFETY J, V91, P28, DOI 10.1016/j.firesaf.2017.05.004
   Boyle EA, 2016, COMPUT EDUC, V94, P178, DOI 10.1016/j.compedu.2015.11.003
   Buttussi F, 2018, IEEE T VIS COMPUT GR, V24, P1063, DOI 10.1109/TVCG.2017.2653117
   Buttussi F, 2013, INT J MED INFORM, V82, P798, DOI 10.1016/j.ijmedinf.2013.05.007
   Center for Development and Disability, Tips for first responders, V5
   Chee EJM, 2019, GAMES HEALTH J, V8, P187, DOI 10.1089/g4h.2018.0073
   Chittaro L, 2022, IEEE T VIS COMPUT GR, V28, P1573, DOI 10.1109/TVCG.2020.3022340
   Chittaro L, 2019, INT J HUM-COMPUT ST, V127, P112, DOI 10.1016/j.ijhcs.2018.07.006
   Chittaro L, 2016, IEEE T VIS COMPUT GR, V22, P1527, DOI 10.1109/TVCG.2015.2443787
   Chittaro L, 2015, COMPUT HUM BEHAV, V50, P508, DOI 10.1016/j.chb.2015.03.074
   Chittaro L, 2015, IEEE T VIS COMPUT GR, V21, P529, DOI 10.1109/TVCG.2015.2391853
   Clark DB, 2016, REV EDUC RES, V86, P79, DOI 10.3102/0034654315582065
   Cohen B.H., 2013, EXPLAINING PSYCHOL S
   Connolly TM, 2012, COMPUT EDUC, V59, P661, DOI 10.1016/j.compedu.2012.03.004
   Dankbaar MEW, 2017, BMC MED EDUC, V17, DOI 10.1186/s12909-016-0836-5
   Din ZU, 2019, SAFETY SCI, V115, P176, DOI 10.1016/j.ssci.2019.02.005
   Dipartimento dei Vigili del Fuoco, 2004, Il soccorso alle persone disabili
   Federal Aviation Administration, 2022, Advisory Circular AC 120- 32A
   Fu FL, 2009, COMPUT EDUC, V52, P101, DOI 10.1016/j.compedu.2008.07.004
   Grau Rosa, 2002, Int J Occup Saf Ergon, V8, P23
   Haghani M, 2019, J ADV TRANSPORT, V2019, DOI 10.1155/2019/2380348
   HCI Lab University of Udine, Help!  The serious game for IoS
   HCI Lab University of Udine, Help!  The serious game for android
   HOYT MF, 1973, J RES PERS, V7, P288, DOI 10.1016/0092-6566(73)90043-3
   Hu H, 2021, GAMES HEALTH J, V10, P139, DOI 10.1089/g4h.2020.0140
   Hu LY, 2021, FRONT PEDIATR, V9, DOI 10.3389/fped.2021.645776
   Huang JL, 2012, TRANSPORT RES F-TRAF, V15, P358, DOI 10.1016/j.trf.2011.09.002
   Huang LY, 2016, COMPUT HUM BEHAV, V55, P1085, DOI 10.1016/j.chb.2015.10.029
   Hung CM, 2014, J COMPUT EDUC, V1, P151, DOI 10.1007/s40692-014-0008-8
   Hunter DR, 2012, INT J AVIAT PSYCHOL, V22, P144, DOI 10.1080/10508414.2012.663244
   Hunter DR, 2002, AVIAT SPACE ENVIR MD, V73, P1184
   Jabbar AIA, 2015, REV EDUC RES, V85, P740, DOI 10.3102/0034654315577210
   Jones J.W., 1993, J BUS PSYCHOL, V7, P449, DOI DOI 10.1007/BF01013758
   Katz-Navon T, 2007, INT J HEALTH CARE Q, V20, P572, DOI 10.1108/09526860710822716
   Lee YH, 2015, CYBERPSYCH BEH SOC N, V18, P669, DOI 10.1089/cyber.2015.0165
   LEVENSON H, 1973, J CONSULT CLIN PSYCH, V41, P397, DOI 10.1037/h0035357
   Levenson H., 1981, Research with the locus of control construct, P15, DOI DOI 10.1016/B978-0-12-443201-7.50006-3
   Li DD, 2013, COMPUT HUM BEHAV, V29, P257, DOI 10.1016/j.chb.2012.09.002
   Meluso A, 2012, COMPUT EDUC, V59, P497, DOI 10.1016/j.compedu.2011.12.019
   MONTAG I, 1987, J APPL PSYCHOL, V72, P339, DOI 10.1037/0021-9010.72.3.339
   Peng W, 2009, HEALTH COMMUN, V24, P115, DOI 10.1080/10410230802676490
   Rahouti A, 2021, FIRE TECHNOL, V57, P3041, DOI 10.1007/s10694-021-01098-x
   Reich J.W., 2017, Perceived Control
   ROTTER JB, 1966, PSYCHOL MONOGR, V80, P1, DOI 10.1037/h0092976
   Samsung for Business, Your phone is now more powerful than your PC
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Semeraro F, 2017, RESUSCITATION, V116, P27, DOI 10.1016/j.resuscitation.2017.04.038
   Skinner EA, 1996, J PERS SOC PSYCHOL, V71, P549, DOI 10.1037/0022-3514.71.3.549
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Song X, 2017, ACM T INFORM SYST, V35, DOI 10.1145/3057280
   Sung HY, 2013, COMPUT EDUC, V63, P43, DOI 10.1016/j.compedu.2012.11.019
   Tubelo RA, 2019, INT J MED INFORM, V130, DOI 10.1016/j.ijmedinf.2019.08.004
   Wang LX, 2019, PHYSICA A, V531, DOI 10.1016/j.physa.2019.121777
   Wuebker L.J., 1986, Journal of Business and Psychology, V1, P19, DOI DOI 10.1007/BF01014164
   You XQ, 2013, ACCIDENT ANAL PREV, V57, P131, DOI 10.1016/j.aap.2013.03.036
NR 60
TC 3
Z9 3
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5339
EP 5349
DI 10.1109/TVCG.2023.3292473
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400092
PM 37405887
OA Green Submitted, hybrid
DA 2024-11-06
ER

PT J
AU Venkatakrishnan, R
   Venkatakrishnan, R
   Raveendranath, B
   Sarno, DM
   Robb, AC
   Lin, WC
   Babu, SV
AF Venkatakrishnan, Rohith
   Venkatakrishnan, Roshan
   Raveendranath, Balagopal
   Sarno, Dawn M.
   Robb, Andrew C.
   Lin, Wen-Chieh
   Babu, Sabarish V.
TI The Effects of Auditory, Visual, and Cognitive Distractions on
   Cybersickness in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Active exploration; cybersickness; distractions; electrodermal activity;
   information recall; pain reduction; spatial memory; virtual motion;
   virtual reality
ID MOTION SICKNESS; EXPOSURE; EFFICACY; DRIVERS; MUSIC; AGE
AB Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality (VR). Consequently, researchers continue to explore novel means to mitigate the undesirable effects associated with this affliction, one that may require a combination of remedies as opposed to a solitary stratagem. Inspired by research probing into the use of distractions as a means to control pain, we investigated the efficacy of this countermeasure against CS, studying how the introduction of temporally time-gated distractions affects this malady during a virtual experience featuring active exploration. Downstream of this, we studied how other aspects of the VR experience are affected by this intervention. We discuss the results of a between-subjects study manipulating the presence, sensory modality, and nature of periodic and short-lived (5-12 seconds) distractor stimuli across four experimental conditions: 1) no-distractors (ND); 2) auditory distractors (AD); 3) visual distractors (VD); 4) cognitive distractors (CD). Two of these conditions (VD and AD) formed a yoked control design wherein every matched pair of 'seers' and 'hearers' was periodically exposed to distractors that were identical in terms of content, temporality, duration, and sequence. In the CD condition, each participant had to periodically perform a 2-back working memory task, the duration and temporality of which was matched to distractors presented in each matched pair of the yoked conditions. These three conditions were compared to a baseline control group featuring no distractions. Results indicated that the reported sickness levels were lower in all three distraction groups in comparison to the control group. The intervention also increased the amount of time users were able to endure the VR simulation and avoided causing detriments to spatial memory and virtual travel efficiency. Overall, it appears that it may be possible to make users less consciously aware and bothered by the symptoms of CS, thereby reducing its perceived severity.
C1 [Venkatakrishnan, Rohith; Venkatakrishnan, Roshan; Robb, Andrew C.; Babu, Sabarish V.] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
   [Raveendranath, Balagopal; Sarno, Dawn M.] Clemson Univ, Dept Psychol, Clemson, SC 29634 USA.
   [Lin, Wen-Chieh] Natl Yang Ming Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
C3 Clemson University; Clemson University; National Yang Ming Chiao Tung
   University
RP Venkatakrishnan, R (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
EM rohithv@g.clemson.edu; rvenkat@g.clemson.edu; braveen@g.clemson.edu;
   dmsarno@g.clemson.edu; arobb@clemson.edu; wclin@cs.nctu.edu.tw;
   sbabu@clemson.edu
RI Venkatakrishnan, Roshan/JDC-3508-2023; Venkatakrishnan,
   Rohith/JCE-8736-2023
OI Babu, Sabarish/0000-0002-8348-0534; Robb, Andrew/0000-0002-0398-5576;
   Sarno, Dawn/0000-0001-5605-5957; Venkatakrishnan,
   Rohith/0000-0002-8484-3915; Venkatakrishnan, Roshan/0000-0002-6538-627X
FU US National Science Foundation [2007435]; Taiwan's National Science and
   Technology Council [109-2221-E-009-123-MY3]
FX This work was supported in part by the US National Science Foundation
   (CISE IIS HCC) under Grant 2007435 and in part by Taiwan's National
   Science and Technology Council under Grant 109-2221-E-009-123-MY3.
CR Açikel BY, 2018, SIMULAT GAMING, V49, P27, DOI 10.1177/1046878117750417
   Adhanom I, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.848001
   Ang S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P428, DOI 10.1109/VR51125.2022.00062
   Barhorst-Cates EM, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163785
   Bolas Mark, 2017, US Patent, Patent No. [9,645,395, 9645395]
   Bos JE, 2015, J VESTIBUL RES-EQUIL, V25, P23, DOI 10.3233/VES-150541
   Braithwaite J. J., 2013, PSYCHOPHYSIOLOGY, V49, P1017, DOI DOI 10.1111/J.1469-8986.2012.01384.X
   Budhiraja P, 2017, Arxiv, DOI arXiv:1710.02599
   Cao ZK, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P105, DOI 10.1109/VR.2018.8446210
   Carnegie Kieran., 2015, Mitigating visual discomfort on head mounted displays using estimated gaze dependent depth of field
   Chang E, 2020, INT J HUM-COMPUT INT, V36, P1658, DOI 10.1080/10447318.2020.1778351
   Chen YT, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P172, DOI [10.1109/VR.2019.8798338, 10.1109/vr.2019.8798338]
   Cheung B, 2005, AVIAT SPACE ENVIR MD, V76, P1099
   Chu H, 2012, J ALTERN COMPLEM MED, V18, P494, DOI 10.1089/acm.2011.0366
   Clifton J, 2020, VIRTUAL REAL-LONDON, V24, P453, DOI 10.1007/s10055-019-00407-8
   Critchley HD, 2002, NEUROSCIENTIST, V8, P132, DOI 10.1177/107385840200800209
   D'Amour S, 2017, EXP BRAIN RES, V235, P2811, DOI 10.1007/s00221-017-5009-1
   Davis S., 2014, P C INT ENT, P1
   Davis S, 2015, P 11 AUSTR C INT ENT
   Dawson ME., 2017, The electrodermal system
   de Siqueira AG, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P606, DOI 10.1109/VR50410.2021.00086
   DiZio P, 1997, ADV HUM FACT ERGON, V21, P893
   Estrada A, 2007, AVIAT SPACE ENVIR MD, V78, P408
   Farmani Y., 2018, P 44 GRAPH INT C, P168, DOI [10.20380/GI2018.21, DOI 10.20380/GI2018.23, 10.20380/GI201 8.23, 10.20380/GI2018.23]
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   FERNANDEZ E, 1989, PAIN, V38, P123, DOI 10.1016/0304-3959(89)90230-3
   Fisher DL, 2006, INJURY PREV, V12, P25, DOI 10.1136/ip.2006.012021
   Galvez-Garcia G, 2020, APPL ERGON, V82, DOI 10.1016/j.apergo.2019.102931
   Gálvez-García G, 2017, APPL ERGON, V58, P13, DOI 10.1016/j.apergo.2016.05.004
   Golding JF, 1998, BRAIN RES BULL, V47, P507, DOI 10.1016/S0361-9230(98)00091-4
   GUEDRY F E Jr, 1964, Acta Otolaryngol, V58, P377, DOI 10.3109/00016486409121398
   Habgood MPJ, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P371
   HART S G, 1988, P139
   Heinrichs WL, 2008, WORLD J SURG, V32, P161, DOI 10.1007/s00268-007-9354-2
   Hettinger J., 1992, Presence: Teleoperators Virtual Environ., V1, P306, DOI [10.1162/pres.1992.1.3.306, DOI 10.1162/PRES.1992.1.3.306]
   Hofmann DA, 1997, J MANAGE, V23, P723, DOI 10.1177/014920639702300602
   Hong Y, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281519
   Horberry T, 2006, ACCIDENT ANAL PREV, V38, P185, DOI 10.1016/j.aap.2005.09.007
   HU SQ, 1991, AVIAT SPACE ENVIR MD, V62, P308
   Huang YH, 2021, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2021), DOI 10.1145/3474451.3476236
   Ihemedu-Steinke QC, 2017, LECT NOTES COMPUT SC, V10280, P521, DOI 10.1007/978-3-319-57987-0_42
   James Craig., 2017, The effects of post-processing techniques on simulator sickness in virtual reality
   Johnson Malcolm H, 2005, Curr Pain Headache Rep, V9, P90, DOI 10.1007/s11916-005-0044-1
   JungHa Park, 2020, The International Journal of Advanced Culture Technology, V8, P89
   Kahneman D., 1973, Attention and Effort
   Kennedy RS, 2000, PRESENCE-TELEOP VIRT, V9, P463, DOI 10.1162/105474600566952
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Keshavarz B., 2014, Handbook of Virtual Environments: Design, Implementation, and Applications, V2nd, P648
   Keshavarz B, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00472
   Keshavarz B, 2015, EXP BRAIN RES, V233, P1353, DOI 10.1007/s00221-015-4209-9
   Keshavarz B, 2014, APPL ERGON, V45, P521, DOI 10.1016/j.apergo.2013.07.009
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Kim YY, 2005, PSYCHOPHYSIOLOGY, V42, P616, DOI 10.1111/j.1469-8986.2005.00349.x
   Kolasinski E. M., 1995, Simulator sickness in virtual environments, V1027
   Kooijman L., 2022 IEEE INT C SYST, P1057, DOI [10.1109/SMC53654.2022.9945475, DOI 10.1109/SMC53654.2022.9945475, DOI 10.1109/SMCS3654.2022.9945475]
   Kwok KKK, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P91, DOI 10.1109/ISMAR-Adjunct.2018.00041
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lee John D., 2008, Defining driver distraction, P31, DOI DOI 10.1201/9781420007497.CH3
   Legrain V, 2009, PAIN, V144, P230, DOI 10.1016/j.pain.2009.03.020
   Lien HC, 2003, AM J PHYSIOL-GASTR L, V284, pG481, DOI 10.1152/ajpgi.00164.2002
   Lim K, 2021, VIRTUAL REAL-LONDON, V25, P331, DOI 10.1007/s10055-020-00457-3
   Lin YX, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419984
   Lindner K., 2009, Modern Psychol. Stud., V15, P6
   Liu HM, 2020, INT SYM MIX AUGMENT, P566, DOI 10.1109/ISMAR50242.2020.00084
   Liu SH, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P817, DOI [10.1109/VR.2019.8798158, 10.1109/vr.2019.8798158]
   Loup G, 2019, INT J HUM-COMPUT INT, V35, P1270, DOI 10.1080/10447318.2018.1519164
   Malloy KM, 2010, CLIN PSYCHOL REV, V30, P1011, DOI 10.1016/j.cpr.2010.07.001
   McCarthy Cameron, 2016, 2016 IEEE EMBS International Student Conference (ISC), DOI 10.1109/EMBSISC.2016.7508621
   MCCAUL KD, 1984, PSYCHOL BULL, V95, P516, DOI 10.1037/0033-2909.95.3.516
   McCauley Michael E, 1992, Presence: Teleoperators & Virtual Environments, V1, P311, DOI [10.1162/pres.1992.1.3.311, DOI 10.1162/PRES.1992.1.3.311]
   Meusel C. R., 2014, Exploring mental effort and nausea via electrodermal activity within scenario-based tasks
   MILLER JC, 1993, AVIAT SPACE ENVIR MD, V64, P813
   Milleville-Pennel I, 2015, ACCIDENT ANAL PREV, V74, P192, DOI 10.1016/j.aap.2014.10.021
   Moghadam K, 2020, IEEE T VIS COMPUT GR, V26, P2273, DOI 10.1109/TVCG.2018.2884468
   MONEY KE, 1970, PHYSIOL REV, V50, P1, DOI 10.1152/physrev.1970.50.1.1
   Monk AF, 2011, BEHAV RES METHODS, V43, P888, DOI 10.3758/s13428-011-0074-z
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Nelson W. T., 2000, P HUM FACT ERG SOC A
   Ng AKT, 2020, DISPLAYS, V61, DOI 10.1016/j.displa.2019.08.004
   Nie GY, 2020, IEEE T VIS COMPUT GR, V26, P2535, DOI 10.1109/TVCG.2019.2893668
   Niehorster DC, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517708205
   Norouzi N, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225162
   Osborne J.W., 2000, Practical Assessment, Research, Evaluation, V7, DOI DOI 10.7275/PMGN-ZX89
   Palmisano S, 2022, VIRTUAL REAL-LONDON, V26, P1373, DOI 10.1007/s10055-022-00634-6
   Pouke M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P665, DOI 10.1109/VR.2018.8446078
   Pourmand A, 2018, CURR PAIN HEADACHE R, V22, DOI 10.1007/s11916-018-0708-2
   Qionghua W, 2019, Arxiv, DOI arXiv:1903.12617
   Ranney T. A., 2001, Citeseer, Tech. Rep. 2001-06-0177
   Reason J.T., 1975, Motion Sickness
   Rebenitsch L., 2015, XRDS: Crossroads, The ACM Magazine for Students, V22, P46
   REDD WH, 1987, J CONSULT CLIN PSYCH, V55, P391, DOI 10.1037/0022-006X.55.3.391
   Regan E. C., 1993, INT APPL MIL PSYCH S
   RICCIO G E, 1991, Ecological Psychology, V3, P195, DOI 10.1207/s15326969eco0303_2
   Richardson DC, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-68253-2
   Rizzo A, 2005, PRESENCE-TELEOP VIRT, V14, P119, DOI 10.1162/1054746053967094
   Samson MM, 2001, AGING CLIN EXP RES, V13, P16, DOI 10.1007/BF03351489
   Sang FDYP, 2003, J TRAVEL MED, V10, P108, DOI 10.2310/7060.2003.31768
   Sang FDYP, 2003, AVIAT SPACE ENVIR MD, V74, P998
   Seno T, 2011, ATTEN PERCEPT PSYCHO, V73, P1467, DOI 10.3758/s13414-011-0129-3
   Sepich NC, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.943409
   Sherman CR, 2002, J TRAVEL MED, V9, P251
   Snijders T., 2011, INTRO BASIC ADV MULT, V2nd ed
   Sousa TLV, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00892
   Sra M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300905
   Stein M., 2012, Aviation Psychology and Applied Human Factors, V2, P11, DOI [10.1027/2192-0923/a000022, DOI 10.1027/2192-0923/A000022]
   Strayer DL, 2011, PSYCHOL LEARN MOTIV, V54, P29, DOI 10.1016/B978-0-12-385527-5.00002-4
   TREISMAN M, 1977, SCIENCE, V197, P493, DOI 10.1126/science.301659
   Trutoiu LC, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P190
   Tu F. K., 2020, PhD thesis
   VASTERLING J, 1993, J BEHAV MED, V16, P65, DOI 10.1007/BF00844755
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P682, DOI [10.1109/VR46266.2020.00-13, 10.1109/VR46266.2020.1581195115265]
   Venkatakrishnan R, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1201, DOI [10.1109/VR.2019.8797728, 10.1109/vr.2019.8797728]
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P672, DOI [10.1109/VR46266.2020.00-14, 10.1109/VR46266.2020.1581256520838]
   Wald J, 2000, J BEHAV THER EXP PSY, V31, P249, DOI 10.1016/S0005-7916(01)00009-X
   Weech S, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0194137
   Weissker T, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P97, DOI 10.1109/VR.2018.8446620
   Wickens C.D., 1984, Varieties of Attention, P63
   Wickens C. D., 1980, Attention Per- form. VIII, V8, P239
   Wu F, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P103, DOI 10.1109/VR51125.2022.00028
   Zhou C, 2019, IEEE INT CONF MOB, P72, DOI 10.1109/MASSW.2019.00021
   Zyla-Labs, 2022, Woord
NR 121
TC 6
Z9 6
U1 13
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5350
EP 5369
DI 10.1109/TVCG.2023.3293405
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400094
PM 37418399
DA 2024-11-06
ER

PT J
AU Panavas, L
   Crnovrsanin, T
   Adams, JL
   Ullman, J
   Sargavad, A
   Tory, M
   Dunne, C
AF Panavas, Liudas
   Crnovrsanin, Tarik
   Adams, Jane Lydia
   Ullman, Jonathan
   Sargavad, Ali
   Tory, Melanie
   Dunne, Cody
TI Investigating the Visual Utility of Differentially Private Scatterplots
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Differential privacy; data study; scatterplots; visual utility
ID FRAMEWORK; HISTOGRAM
AB Increasingly, visualization practitioners are working with, using, and studying private and sensitive data. There can be many stakeholders interested in the resulting analyses-but widespread sharing of the data can cause harm to individuals, companies, and organizations. Practitioners are increasingly turning to differential privacy to enable public data sharing with a guaranteed amount of privacy. Differential privacy algorithms do this by aggregating data statistics with noise, and this now-private data can be released visually with differentially private scatterplots. While the private visual output is affected by the algorithm choice, privacy level, bin number, data distribution, and user task, there is little guidance on how to choose and balance the effect of these parameters. To address this gap, we had experts examine 1,200 differentially private scatterplots created with a variety of parameter choices and tested their ability to see aggregate patterns in the private output (i.e. the visual utility of the chart). We synthesized these results to provide easy-to-use guidance for visualization practitioners releasing private data through scatterplots. Our findings also provide a ground truth for visual utility, which we use to benchmark automated utility metrics from various fields. We demonstrate how multi-scale structural similarity (MS-SSIM), the metric most strongly correlated with our study's utility results, can be used to optimize parameter selection.
C1 [Panavas, Liudas; Crnovrsanin, Tarik; Adams, Jane Lydia; Ullman, Jonathan; Tory, Melanie; Dunne, Cody] Northeastern Univ, Boston, MA 02115 USA.
   [Sargavad, Ali] Univ Massachusetts, Amherst, MA 01003 USA.
C3 Northeastern University; University of Massachusetts System; University
   of Massachusetts Amherst
RP Panavas, L (corresponding author), Northeastern Univ, Boston, MA 02115 USA.
EM panavas.l@northeastern.edu; t.crnovrsanin@northeastern.edu;
   adams.jan@northeastern.edu; j.ullman@northeastern.edu;
   asarv@cs.umass.edu; m.tory@northeastern.edu; c.dunne@northeastern.edu
RI Dunne, Cody/M-4444-2019
OI Dunne, Cody/0000-0002-1609-9776; Sarvghad, Ali/0000-0003-3718-7043;
   Tory, Melanie/0000-0002-6806-9253; Panavas, Liudas/0000-0003-0428-5579;
   Crnovrsanin, Tarik/0000-0002-4397-5532; Adams, Jane/0000-0002-7826-3500
FU NIEHS Superfund Research Program [P42ES017198]
FX This work was supported by NIEHS Superfund Research Program under Grant
   P42ES017198 (PROTECT). Recommended for acceptance by Peer-Timo Bremer.
CR Abowd JM, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2867, DOI 10.1145/3219819.3226070
   ANSCOMBE FJ, 1973, AM STAT, V27, P17, DOI 10.2307/2682899
   ARNOLD C., 2020, PREPRINT
   Bhattacharjee K, 2020, COMPUT GRAPH FORUM, V39, P675, DOI 10.1111/cgf.14032
   Borkin MA, 2011, IEEE T VIS COMPUT GR, V17, P2479, DOI 10.1109/TVCG.2011.192
   Conover W. J., 1979, Tech. Rep. L.A-7677-MS, DOI [10.1161/CIRCULATIONAHA.107.700971, DOI 10.1161/CIRCULATIONAHA.107.700971]
   Dasgupta A, 2019, IEEE SYM VIS CYB SEC, DOI 10.1109/vizsec48167.2019.9161608
   Dasgupta A, 2013, COMPUT GRAPH FORUM, V32, P35, DOI 10.1111/cgf.12142
   Dhotre PS, 2017, RIVER PUBL SER COMM, P39
   Dobrota B., 2021, Master's thesis
   Dragicevic P., 2015, PhD thesis
   Dwork C., 2019, J. Privacy Confidentiality, V9, DOI DOI 10.29012/JPC.689
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2017, ANNU REV STAT APPL, V4, P61, DOI 10.1146/annurev-statistics-060116-054123
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Elmqvist N, 2015, INFORM VISUAL, V14, P250, DOI 10.1177/1473871613513228
   Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348
   FREEDMAN D, 1981, Z WAHRSCHEINLICHKEIT, V57, P453, DOI 10.1007/BF01025868
   Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372
   Friendly M, 2005, J HIST BEHAV SCI, V41, P103, DOI 10.1002/jhbs.20078
   Frigerio L, 2019, IFIP ADV INF COMM TE, V562, P151, DOI 10.1007/978-3-030-22312-0_11
   Gaboardi M, 2018, Arxiv, DOI arXiv:1609.04340
   Garrido GM, 2021, ARXIV
   Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X
   Hallgren Kevin A, 2012, Tutor Quant Methods Psychol, V8, P23
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Hay M, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P2101, DOI 10.1145/2882903.2899387
   Hay M, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P139, DOI 10.1145/2882903.2882931
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Holohan N, 2019, Arxiv, DOI arXiv:1907.02444
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Khamis H, 2008, J DIAGN MED SONOG, V24, P155, DOI 10.1177/8756479308317006
   Koo TK, 2016, J CHIROPR MED, V15, P155, DOI 10.1016/j.jcm.2016.02.012
   Kotsogiannis L, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1323, DOI 10.1145/3035918.3035945
   Kung SY, 2017, MULTIMED TOOLS APPL, V76, P3999, DOI 10.1007/s11042-015-2959-9
   Lee H. B., 2017, Master's thesis
   Li C, 2014, Arxiv, DOI arXiv:1410.0265
   Matejka J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1290, DOI 10.1145/3025453.3025912
   Matute J, 2018, IEEE T VIS COMPUT GR, V24, P542, DOI 10.1109/TVCG.2017.2744339
   Micallef L, 2017, IEEE T VIS COMPUT GR, V23, P1588, DOI 10.1109/TVCG.2017.2674978
   Nanayakkara Priyanka, 2022, arXiv
   Nida N, 2016, IIOAB J, V7, P202
   Panavas L., 2022, PREPRINT
   Pandey AV, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3659, DOI 10.1145/2858036.2858155
   Qardaji W, 2013, PROC INT CONF DATA, P757, DOI 10.1109/ICDE.2013.6544872
   Sdv-Dev, 2022, SDV-Dev/SDGym: Benchmarking synthetic data generation methods
   Sedlmair M, 2012, COMPUT GRAPH FORUM, V31, P1335, DOI 10.1111/j.1467-8659.2012.03125.x
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Soria-Comas J, 2017, IEEE T INF FOREN SEC, V12, P1418, DOI 10.1109/TIFS.2017.2663337
   South L, 2022, COMPUT GRAPH FORUM, V41, P43, DOI 10.1111/cgf.14521
   St John MF, 2021, IEEE SYM VIS CYB SEC, P26, DOI 10.1109/VizSec53666.2021.00008
   Sturges HA, 1926, J AM STAT ASSOC, V21, P65, DOI 10.1080/01621459.1926.10502161
   Thaker P, 2020, Arxiv, DOI arXiv:2006.12018
   Torfi A, 2022, INFORM SCIENCES, V586, P485, DOI 10.1016/j.ins.2021.12.018
   Wang XM, 2019, IEEE T VIS COMPUT GR, V25, P193, DOI 10.1109/TVCG.2018.2865021
   Wang XM, 2018, IEEE T VIS COMPUT GR, V24, P351, DOI 10.1109/TVCG.2017.2745139
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wilkinson L, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P157, DOI 10.1109/INFVIS.2005.1532142
   Winograd-Cort D, 2017, P ACM PROGRAM LANG, V1, DOI 10.1145/3110254
   Wood A., 2018, Vand. J. Ent. & Tech. L., V21, P209, DOI [10.2139/ssrn.3338027, DOI 10.2139/SSRN.3338027]
   Xiao YH, 2010, LECT NOTES COMPUT SC, V6358, P150, DOI 10.1007/978-3-642-15546-8_11
   Xu J, 2013, VLDB J, V22, P797, DOI 10.1007/s00778-013-0309-y
   Zhang D., 2016, Theory Pract. Differe. Privacy, V2016, P1
   Zhang D, 2021, IEEE T VIS COMPUT GR, V27, P1786, DOI 10.1109/TVCG.2020.3030369
   Zhang D, 2018, INT CONF MANAGE DATA, P115, DOI 10.1145/3183713.3196921
   Zhang SL, 2022, Arxiv, DOI arXiv:2202.09587
   Zhang Xiaojian, 2014, Proc SIAM Int Conf on Data Mining, P587, DOI 10.1137/1.9781611973440.68
   Zhou JH, 2023, IEEE T VIS COMPUT GR, V29, P809, DOI 10.1109/TVCG.2022.3209391
   Zhu TQ, 2017, IEEE T KNOWL DATA EN, V29, P1619, DOI 10.1109/TKDE.2017.2697856
   Zimmer D. M., 2007, FDN TRENDS ECONOMETR, V1, P1, DOI DOI 10.1561/0800000005
NR 71
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5370
EP 5385
DI 10.1109/TVCG.2023.3292391
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400005
PM 37405888
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Li, JC
   Lai, CF
   Wang, YF
   Luo, AL
   Yuan, XR
AF Li, Jincheng
   Lai, Chufan
   Wang, Youfen
   Luo, Ali
   Yuan, Xiaoru
TI SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating
   Classification Inspection
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Inspection; Pipelines; Stars; Surveys; Visual analytics; Telescopes;
   Task analysis; Spectral classification; visual parameter space analysis;
   decision-making; visual analytics
ID AUTOMATED CLASSIFICATION; PARAMETER SPACE; VISUALIZATION; GUIDANCE;
   ANALYTICS; SEQUENCE; STARS
AB In astronomical spectral analysis, class recognition is essential and fundamental for subsequent scientific research. The experts often perform the visual inspection after automatic classification to deal with low-quality spectra to improve accuracy. However, given the enormous spectral volume and inadequacy of the current inspection practice, such inspection is tedious and time-consuming. This article presents a visual analytics system named SpectrumVA to promote the efficiency of visual inspection while guaranteeing accuracy. We abstract inspection as a visual parameter space analysis process, using redshifts and spectral lines as parameters. Different navigation strategies are employed in the "selection-inspection-promotion" workflow. At the selection stage, we help the experts identify a spectrum of interest through spectral representations and auxiliary information. Several possible redshifts and corresponding important spectral lines are also recommended through a global-to-local strategy to provide an appropriate entry point for the inspection. The inspection stage adopts a variety of instant visual feedback to help the experts adjust the redshift and select spectral lines in an informed trial-and-error manner. Similar spectra to the inspected one rather than different ones are visualized at the promotion stage, making the inspection process more fluent. We demonstrate the effectiveness of SpectrumVA through a quantitative algorithmic assessment, a case study, interviews with domain experts, and a user study.
C1 [Li, Jincheng; Yuan, Xiaoru] Peking Univ, Sch Intelligence Sci & Technol, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
   [Lai, Chufan] Peking Univ, Sch Intelligence Sci & Technol, Beijing 100107, Peoples R China.
   [Wang, Youfen; Luo, Ali] Natl Astron Observ, CAS Key Lab Opt Astron, Beijing 101408, Peoples R China.
   [Luo, Ali; Yuan, Xiaoru] Natl Astron Observ, CAS Key Lab Opt Astron, Beijing 100871, Peoples R China.
C3 Peking University; Peking University; Chinese Academy of Sciences;
   National Astronomical Observatory, CAS; Chinese Academy of Sciences;
   National Astronomical Observatory, CAS
RP Yuan, XR (corresponding author), Peking Univ, Sch Intelligence Sci & Technol, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
EM jincheng.li@pku.edu.cn; chufan.lai@pku.edu.cn; yfwang@bao.ac.cn;
   lal@nao.ac.cn; xiaoru.yuan@pku.edu.cn
RI Li, Jin-Cheng/L-3819-2017; Yuan, Xiaoru/E-1798-2013
OI Li, Jincheng/0000-0003-2328-3624; Luo, A-Li/0000-0001-7865-2648; Yuan,
   Xiaoru/0000-0002-7233-980X
FU NSFC [62272012]
FX This work was supported by NSFC under Grant 62272012.
CR Abdurro'uf, 2022, ASTROPHYS J SUPPL S, V259, DOI 10.3847/1538-4365/ac4414
   Afzal S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P191, DOI 10.1109/VAST.2011.6102457
   Albers D, 2011, IEEE T VIS COMPUT GR, V17, P2392, DOI 10.1109/TVCG.2011.232
   Andrienko N, 2018, COMPUT GRAPH FORUM, V37, P275, DOI 10.1111/cgf.13324
   Bailer-Jones CAL, 1998, MON NOT R ASTRON SOC, V298, P361, DOI 10.1046/j.1365-8711.1998.01596.x
   Bazarghan M, 2008, Arxiv, DOI [arXiv:0804.2742, 10.48550/arXiv.0804.2742, DOI 10.48550/ARXIV.0804.2742]
   Bishop CM, 1995, Neural networks for pattern recognition
   Bittner A, 2019, ASTRON ASTROPHYS, V628, DOI 10.1051/0004-6361/201935829
   Bolton AS, 2012, ASTRON J, V144, DOI 10.1088/0004-6256/144/5/144
   Bruckner S, 2010, IEEE T VIS COMPUT GR, V16, P1468, DOI 10.1109/TVCG.2010.190
   Cavallo M, 2019, IEEE T VIS COMPUT GR, V25, P267, DOI 10.1109/TVCG.2018.2864477
   Ceneda D., 2018, EUROVA, P19, DOI DOI 10.2312/EUROVA.20181107
   Ceneda D., 2018, P 9 INT EUROVIS WORK, P23
   Ceneda D, 2019, COMPUT GRAPH FORUM, V38, P861, DOI 10.1111/cgf.13730
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chang K., d3.parcoords.js
   Collins C, 2018, VIS INFORM, V2, P166, DOI 10.1016/j.visinf.2018.09.003
   Correll M., 2011, 2011 IEEE Symposium on Biological Data Visualization, P135, DOI 10.1109/BioVis.2011.6094058
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cretignier M, 2020, ASTRON ASTROPHYS, V640, DOI 10.1051/0004-6361/202037722
   Few S., 2008, Time on the horizon
   Geng Z, 2011, IEEE T VIS COMPUT GR, V17, P2572, DOI 10.1109/TVCG.2011.166
   Gisbrecht A, 2015, NEUROCOMPUTING, V147, P71, DOI 10.1016/j.neucom.2013.11.045
   [郭炎鑫 Guo Yanxin], 2019, [天文研究与技术, Astronomical Research & Technology], V16, P335
   Hassan A, 2011, PUBL ASTRON SOC AUST, V28, P150, DOI 10.1071/AS10031
   Heer J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1303
   Heinrich Julian, 2012, Proceedings of the International Conference on Computer Graphics Theory and Applications (GRAPP 2012) and International Conference on Information Visualization Theory and Applications (IVAPP 2012), P594
   Hijikata Yoshinori., 2004, Proceedings of IUI, P198, DOI [DOI 10.1145/964442.964480, 10.1145/964442.964480]
   Ho IT, 2016, ASTROPHYS SPACE SCI, V361, DOI 10.1007/s10509-016-2865-2
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Javed W, 2010, IEEE T VIS COMPUT GR, V16, P927, DOI 10.1109/TVCG.2010.162
   Kincaid R, 2010, IEEE T VIS COMPUT GR, V16, P900, DOI 10.1109/TVCG.2010.193
   Kirkpatrick JD, 1997, ASTRON J, V113, P1421, DOI 10.1086/118357
   Kreuseler M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P49, DOI 10.1109/INFVIS.2004.2
   lamost, LAMOST data release 7 v1.2
   Lan FF, 2021, COMPUT GRAPH FORUM, V40, P635, DOI 10.1111/cgf.14332
   Lekschas F, 2020, COMPUT GRAPH FORUM, V39, P167, DOI 10.1111/cgf.13971
   Liu C, 2015, RES ASTRON ASTROPHYS, V15, P1137, DOI 10.1088/1674-4527/15/8/004
   Luciani TB, 2014, IEEE T VIS COMPUT GR, V20, P1048, DOI 10.1109/TVCG.2014.2312008
   Luo AL, 2015, RES ASTRON ASTROPHYS, V15, P1095, DOI 10.1088/1674-4527/15/8/002
   Manteiga M, 2009, ASTRON J, V137, P3245, DOI 10.1088/0004-6256/137/2/3245
   Monsell S, 2003, TRENDS COGN SCI, V7, P134, DOI 10.1016/S1364-6613(03)00028-7
   Morgan P. Keenan, 1943, An Atlas of Stellar Spectra: Withan Outline of Spectral Classification
   Palmas G, 2014, IEEE PAC VIS SYMP, P57, DOI 10.1109/PacificVis.2014.40
   Pretorius J, 2011, IEEE T VIS COMPUT GR, V17, P2402, DOI 10.1109/TVCG.2011.253
   Prusti T, 2016, ASTRON ASTROPHYS, V595, DOI 10.1051/0004-6361/201629272
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Ribicic H, 2013, IEEE T VIS COMPUT GR, V19, P1062, DOI 10.1109/TVCG.2012.175
   Saito T, 2005, INFOVIS 05: IEEE Symposium on Information Visualization, Proceedings, P173, DOI 10.1109/INFVIS.2005.1532144
   Sánchez SF, 2016, REV MEX ASTRON ASTR, V52, P171
   Sawada N, 2022, IEEE T VIS COMPUT GR, V28, P1917, DOI 10.1109/TVCG.2020.3025090
   SCHACTER DL, 1992, J COGNITIVE NEUROSCI, V4, P244, DOI 10.1162/jocn.1992.4.3.244
   Sedlmair M, 2014, IEEE T VIS COMPUT GR, V20, P2161, DOI 10.1109/TVCG.2014.2346321
   Song YH, 2012, RES ASTRON ASTROPHYS, V12, P453, DOI 10.1088/1674-4527/12/4/009
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q
   Tominski C, 2017, COMPUT GRAPH FORUM, V36, P173, DOI 10.1111/cgf.12871
   Torsney-Weir T, 2011, IEEE T VIS COMPUT GR, V17, P1892, DOI 10.1109/TVCG.2011.248
   TULVING E, 1990, SCIENCE, V247, P301, DOI 10.1126/science.2296719
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2017, MON NOT R ASTRON SOC, V465, P4311, DOI 10.1093/mnras/stw2894
   Wang YF, 2022, ASTRON ASTROPHYS, V660, DOI 10.1051/0004-6361/202142009
   Waser J, 2010, IEEE T VIS COMPUT GR, V16, P1458, DOI 10.1109/TVCG.2010.223
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Xiang M., 2016, Monthly Notices Roy. Astronomical Soc., V464, P3657
   Yang P, 2021, ARCH COMPUT METHOD E, V28, P917, DOI 10.1007/s11831-020-09401-9
   Yuan HL, 2013, ASTRON COMPUT, V3-4, P65, DOI 10.1016/j.ascom.2013.12.001
   Zhang HL, 2021, INFORM VISUAL, V20, P20, DOI 10.1177/1473871620978209
   Zhao J, 2011, IEEE T VIS COMPUT GR, V17, P2422, DOI 10.1109/TVCG.2011.195
   Zheng ZP, 2020, PUBL ASTRON SOC PAC, V132, DOI 10.1088/1538-3873/ab5ed7
   Zicheng Liao, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P51, DOI 10.1109/VAST.2010.5652467
NR 70
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5386
EP 5403
DI 10.1109/TVCG.2023.3294958
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400078
PM 37440386
DA 2024-11-06
ER

PT J
AU Wu, Q
   Bauer, D
   Doyle, MJ
   Ma, KL
AF Wu, Qi
   Bauer, David
   Doyle, Michael J.
   Ma, Kwan-Liu
TI Interactive Volume Visualization via Multi-Resolution Hash Encoding
   Based Neural Representation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Implicit neural representation; path tracing; ray marching; volume
   visualization
ID SUPERRESOLUTION; FRAMEWORK
AB Implicit neural networks have demonstrated immense potential in compressing volume data for visualization. However, despite their advantages, the high costs of training and inference have thus far limited their application to offline data processing and non-interactive rendering. In this article, we present a novel solution that leverages modern GPU tensor cores, a well-implemented CUDA machine learning framework, an optimized global-illumination-capable volume rendering algorithm, and a suitable acceleration data structure to enable real-time direct ray tracing of volumetric neural representations. Our approach produces high-fidelity neural representations with a peak signal-to-noise ratio (PSNR) exceeding 30 dB, while reducing their size by up to three orders of magnitude. Remarkably, we show that the entire training step can fit within a rendering loop, bypassing the need for pre-training. Additionally, we introduce an efficient out-of-core training strategy to support extreme-scale volume data, making it possible for our volumetric neural representation training to scale up to terascale on a workstation with an NVIDIA RTX 3090 GPU. Our method significantly outperforms state-of-the-art techniques in terms of training time, reconstruction quality, and rendering performance, making it an ideal choice for applications where fast and accurate visualization of large-scale volume data is paramount.
C1 [Wu, Qi; Bauer, David; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
   [Doyle, Michael J.] Intel Corp, Santa Clara, CA 95054 USA.
C3 University of California System; University of California Davis; Intel
   Corporation
RP Wu, Q (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM qadwu@ucdavis.edu; davbauer@ucdavis.edu; michael1.doyle@intel.com;
   klma@ucdavis.edu
RI Bauer, David/JDN-0139-2023; wu, qirui/GLU-4942-2022
OI Ma, Kwan-Liu/0000-0001-8086-0366; WU, Qi/0000-0003-0342-9366; Bauer,
   David/0000-0002-1327-3054
FU U.S. Department of Energy [DE-SC0019486]; Intel oneAPI Center of
   Excellence grant; U.S. Department of Energy (DOE) [DE-SC0019486] Funding
   Source: U.S. Department of Energy (DOE)
FX No Statement Available
CR Ballester-Ripoll R, 2020, IEEE T VIS COMPUT GR, V26, P2891, DOI 10.1109/TVCG.2019.2904063
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Brownlee C., 2021, Ray Tracing Gems II: Next Gener, P725, DOI DOI 10.1007/978-1-4842-7185-8_45
   Eisemann E., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Engel K., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P123, DOI 10.1109/LDAV.2011.6092330
   Gehring J, 2017, PR MACH LEARN RES, V70
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Guo L, 2020, IEEE PAC VIS SYMP, P71, DOI 10.1109/PacificVis48177.2020.8737
   Hadadan S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480569
   Hadwiger M, 2012, IEEE T VIS COMPUT GR, V18, P2285, DOI 10.1109/TVCG.2012.240
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P2445, DOI 10.1109/TVCG.2020.3032123
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P270, DOI 10.1109/TVCG.2021.3114815
   Han J, 2020, IEEE T VIS COMPUT GR, V26, P205, DOI 10.1109/TVCG.2019.2934255
   Harris D, 2010, DIGITAL DESIGN COMPU
   Hatcher P., 2010, P C HIGH PERF GRAPH, P57, DOI DOI 10.2312/EGGH/HPG10/057-066
   Hofmann N., 2021, Ray Tracing Gems, VII, P699
   Intel Corporation, 2022, Intel open volume kernel library
   Jain S., 2017, Proceedings of the Large Scale Data Analysis and Visualization, P1187
   Kim D, 2024, Arxiv, DOI arXiv:2208.04448
   Kingma D.P., 2014, P INT C LEARNING REP
   Klacansky P., Open Scientific Visualization Datasets
   Laine Samuli, 2013, P 5 HIGH PERF GRAPH, P137, DOI [10.1145/2492045.2492060, DOI 10.1145/2492045.2492060]
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   Lee M, 2015, J FLUID MECH, V774, P395, DOI 10.1017/jfm.2015.268
   Lu Y, 2021, COMPUT GRAPH FORUM, V40, P135, DOI 10.1111/cgf.14295
   Martel JNP, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459785
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Morrical N, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P256, DOI 10.1109/visual.2019.8933539
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Müller T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3341156
   Muller T., 2021, Tiny CUDA Neural Network Framework
   Müller T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459812
   Novák J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661292
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pharr M., 2016, Physically Based Rendering: From Theory to Implementation, V3rd
   Rieth M, 2022, COMBUST FLAME, V239, DOI 10.1016/j.combustflame.2021.111740
   Sarton J, 2020, IEEE T VIS COMPUT GR, V26, P3008, DOI 10.1109/TVCG.2019.2912752
   Sitzmann V., 2020, Advances in Neural Information Processing Systems, V33, P7462
   Szirmay-Kalos L, 2011, COMPUT GRAPH FORUM, V30, P85, DOI 10.1111/j.1467-8659.2010.01831.x
   Takikawa T, 2021, PROC CVPR IEEE, P11353, DOI 10.1109/CVPR46437.2021.01120
   Tancik M., 2020, Advances in Neural Information Processing Systems, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Theodoridis S, 2006, PATTERN RECOGNITION, 3RD EDITION, P1
   Vaswani A., 2017, Advances in neural information processing systems
   Weiss S, 2022, COMPUT GRAPH FORUM, V41, P196, DOI 10.1111/cgf.14578
   Woodcock E.R., 1965, PROC C APPL COMPUTIN, P557
   Wu Q., 2022, P EUR S PAR GRAPH VI, P37
   Wurster SW, 2023, IEEE T VIS COMPUT GR, V29, P5483, DOI 10.1109/TVCG.2022.3214420
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Zellmann S., 2022, EGPGV 2022 22 EUROGR, P61, DOI [10.2312/pgv.20221066, DOI 10.2312/PGV.20221066]
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou ZL, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095178
   Zimmermann K., 2000, 2000 IEEE Symposium on Volume Visualization (VV 2000), P7
NR 52
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5404
EP 5418
DI 10.1109/TVCG.2023.3293121
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400056
PM 37418398
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Liu, Z
   Zhao, YW
   Zhan, SJ
   Liu, YY
   Chen, RJ
   He, Y
AF Liu, Zheng
   Zhao, Yaowu
   Zhan, Sijing
   Liu, Yuanyuan
   Chen, Renjie
   He, Ying
TI PCDNF: Revisiting Learning-Based Point Cloud Denoising via Joint Normal
   Filtering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Noise reduction; Point cloud compression; Task analysis; Noise
   measurement; Feature extraction; Three-dimensional displays; Computer
   architecture; Point cloud denoising; normal filtering; 3D deep learning;
   point cloud processing
AB Point cloud denoising is a fundamental and challenging problem in geometry processing. Existing methods typically involve direct denoising of noisy input or filtering raw normals followed by point position updates. Recognizing the crucial relationship between point cloud denoising and normal filtering, we re-examine this problem from a multitask perspective and propose an end-to-end network called PCDNF for joint normal filtering-based point cloud denoising. We introduce an auxiliary normal filtering task to enhance the network's ability to remove noise while preserving geometric features more accurately. Our network incorporates two novel modules. First, we design a shape-aware selector to improve noise removal performance by constructing latent tangent space representations for specific points, taking into account learned point and normal features as well as geometric priors. Second, we develop a feature refinement module to fuse point and normal features, capitalizing on the strengths of point features in describing geometric details and normal features in representing geometric structures, such as sharp edges and corners. This combination overcomes the limitations of each feature type and better recovers geometric information. Extensive evaluations, comparisons, and ablation studies demonstrate that the proposed method outperforms state-of-the-art approaches in both point cloud denoising and normal filtering.
C1 [Liu, Zheng; Zhao, Yaowu; Liu, Yuanyuan] China Univ Geosci Wuhan, Sch Comp Sci, Wuhan 430079, Peoples R China.
   [Zhan, Sijing] China Univ Geosci Wuhan, Natl Engn Res Ctr Geog Informat Syst, Wuhan 430079, Peoples R China.
   [Liu, Yuanyuan] China Univ Geosci Wuhan, Hubei Key Lab Intelligent Geoinformat Proc, Wuhan 430079, Peoples R China.
   [Chen, Renjie] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 China University of Geosciences; China University of Geosciences; China
   University of Geosciences; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; Nanyang Technological University
RP Chen, RJ (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM liu.zheng.jojo@gmail.com; 912626756@qq.com; 1585730049@qq.com;
   liuyy@cug.edu.cn; renjiec@ustc.edu.cn; yhe@ntu.edu.sg
RI Chen, Renjie/AFU-3325-2022; He, Ying/A-3708-2011; chen,
   renjie/I-5995-2016
OI Liu, Zheng/0000-0001-6713-6680; chen, renjie/0000-0001-8395-4392
FU National Key R#x0026;D Program of China [2022YFB3904100]; NSF of China
   [62072422, 62076227]; NSF of Anhui Province [2008085MF195];
   Collaborative Innovation Center for Natural Resources Planning and
   Marine Technology of Guangzhou [2023B04J0301]; Key-Area Research and
   Development Program of Guangdong Province [2020B0101130009]; Ministry of
   Education, Singapore [MOE-T2EP20220-000, RT19/22]
FX No Statement Available
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   [Anonymous], 2022, Int. J. Comput.Vis., V130, P615
   Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911
   Ben-Shabat Yizhak, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P20, DOI 10.1007/978-3-030-58452-8_2
   Ben-Shabat Y, 2019, PROC CVPR IEEE, P10104, DOI 10.1109/CVPR.2019.01035
   Cao JJ, 2022, IEEE T IND ELECTRON, V69, P921, DOI 10.1109/TIE.2021.3053904
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Chen HL, 2023, IEEE T PATTERN ANAL, V45, P2913, DOI 10.1109/TPAMI.2022.3175183
   Chen HH, 2019, COMPUT AIDED DESIGN, V115, P122, DOI 10.1016/j.cad.2019.05.036
   Chen HH, 2020, IEEE T VIS COMPUT GR, V26, P3255, DOI 10.1109/TVCG.2019.2920817
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Digne J, 2018, IEEE T VIS COMPUT GR, V24, P2238, DOI 10.1109/TVCG.2017.2719024
   Edirimuni DD, 2023, PROC CVPR IEEE, P13530, DOI 10.1109/CVPR52729.2023.01300
   Edirimuni de Silva, IEEE Trans.Vis. Comput. Graph., DOI [10.1109/TVCG.2023.3263866.41M., DOI 10.1109/TVCG.2023.3263866.41M]
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   Hou F, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530096
   Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239473, 10.1145/1276377.1276405]
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   Liu Z, 2022, IEEE T VIS COMPUT GR, V28, P4418, DOI 10.1109/TVCG.2021.3088118
   Liu Z, 2020, COMPUT AIDED DESIGN, V127, DOI 10.1016/j.cad.2020.102857
   Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Lu XQ, 2018, IEEE T VIS COMPUT GR, V24, P2315, DOI 10.1109/TVCG.2017.2725948
   Luo ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1330, DOI 10.1145/3394171.3413727
   Luo W., 2021, P IEEE CVF INT C COM, P4583
   Mattei E, 2017, COMPUT GRAPH FORUM, V36, P123, DOI 10.1111/cgf.13068
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Pistilli Francesca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P103, DOI 10.1007/978-3-030-58565-5_7
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Qi CR, 2017, ADV NEUR IN, V30
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Sharma R, 2021, Arxiv, DOI arXiv:2102.13391
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sun C., 2022, Comput-AidedDes., V149, P28
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Wang JX, 2022, COMPUT AIDED DESIGN, V144, DOI 10.1016/j.cad.2021.103162
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei MQ, 2023, IEEE T VIS COMPUT GR, V29, P1357, DOI 10.1109/TVCG.2021.3113463
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhang J, 2022, COMPUT AIDED DESIGN, V142, DOI 10.1016/j.cad.2021.103119
   Zhang JY, 2022, IEEE T PATTERN ANAL, V44, P3450, DOI 10.1109/TPAMI.2021.3054619
   Zhou HR, 2023, IEEE T PATTERN ANAL, V45, P946, DOI 10.1109/TPAMI.2022.3145877
   Zhou J, 2022, COMPUT AIDED DESIGN, V142, DOI 10.1016/j.cad.2021.103121
   Zhou J, 2020, COMPUT AIDED DESIGN, V129, DOI 10.1016/j.cad.2020.102916
   Zhou L, 2022, GRAPH MODELS, V121, DOI 10.1016/j.gmod.2022.101140
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zhu R., 2021, P IEEE CVF INT C COM, P6118
NR 59
TC 8
Z9 8
U1 7
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5419
EP 5436
DI 10.1109/TVCG.2023.3292464
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400013
PM 37405886
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhou, WY
   Yuan, L
   Chen, SY
   Gao, L
   Hu, SM
AF Zhou, Wen-Yang
   Yuan, Lu
   Chen, Shu-Yu
   Gao, Lin
   Hu, Shi-Min
TI LC-NeRF: Local Controllable Face Generation in Neural Radiance Field
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geometry; Faces; Three-dimensional displays; Generators; Semantics;
   Codes; Controllability; 3D face generation; neural radiance fields;
   semantic manipulation
AB 3D face generation has achieved high visual quality and 3D consistency thanks to the development of neural radiance fields (NeRF). However, these methods model the whole face as a neural radiance field, which limits the controllability of the local regions. In other words, previous methods struggle to independently control local regions, such as the mouth, nose, and hair. To improve local controllability in NeRF-based face generation, we propose LC-NeRF, which is composed of a Local Region Generators Module (LRGM) and a Spatial-Aware Fusion Module (SAFM), allowing for geometry and texture control of local facial regions. The LRGM models different facial regions as independent neural radiance fields and the SAFM is responsible for merging multiple independent neural radiance fields into a complete representation. Finally, LC-NeRF enables the modification of the latent code associated with each individual generator, thereby allowing precise control over the corresponding local region. Qualitative and quantitative evaluations show that our method provides better local controllability than state-of-the-art 3D-aware face generation methods. A perception study reveals that our method outperforms existing state-of-the-art methods in terms of image quality, face consistency, and editing effects. Furthermore, our method exhibits favorable performance in downstream tasks, including real image editing and text-driven facial image editing.
C1 [Zhou, Wen-Yang; Hu, Shi-Min] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
   [Yuan, Lu] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Chen, Shu-Yu; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing 100045, Peoples R China.
C3 Tsinghua University; Stanford University; Chinese Academy of Sciences;
   Institute of Computing Technology, CAS
RP Hu, SM (corresponding author), Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
EM zhouwy19@mails.tsinghua.edu.cn; luyuan@stanford.edu;
   chenshuyu@ict.ac.cn; gaolin@ict.ac.cn; shimin@tsinghua.edu.cn
RI Wenyang, Zhou/GSD-3239-2022; Hu, Shi-Min/AAW-1952-2020
OI Yuan, Lu/0009-0004-6399-4337; Hu, Shi-Min/0000-0001-7507-6542
FU National Key R&D Program of China [2021ZD0112902]; Natural Science
   Foundation of China [62220106003]; Research Grant of Beijing Higher
   Institution Engineering Research Center; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021ZD0112902, in part by the Natural Science Foundation of
   China under Grant 62220106003, in part by the Research Grant of Beijing
   Higher Institution Engineering Research Center and Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology.
CR Chan Eric R, 2021, arXiv
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Deng Y, 2022, PROC CVPR IEEE, P10663, DOI 10.1109/CVPR52688.2022.01041
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Gropp A, 2020, Arxiv, DOI [arXiv:2002.10099, 10.48550/arXiv.2002.10099]
   Gu JT, 2021, Arxiv, DOI arXiv:2110.08985
   Hu SM, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3097-4
   Jiang K, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555377
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D.P., 2014, P INT C LEARNING REP
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   OrEl R, 2022, PROC CVPR IEEE, P13493, DOI 10.1109/CVPR52688.2022.01314
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Portenier T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201393
   Chan ER, 2021, Arxiv, DOI arXiv:2012.00926
   Roich D, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3544777
   Shi YC, 2022, PROC CVPR IEEE, P11244, DOI 10.1109/CVPR52688.2022.01097
   Shuyang Gu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P3431, DOI 10.1109/CVPR.2019.00355
   Sitzmann V., 2020, Advances in Neural Information Processing Systems, V33, P7462
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   Sun Jingxiang, 2022, arXiv
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Zhou S., 2022, ADV NEURAL INF PROCE, P30599
   Zhu PH, 2021, Arxiv, DOI arXiv:2012.09036
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 31
TC 2
Z9 2
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5437
EP 5448
DI 10.1109/TVCG.2023.3293653
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400031
PM 37459257
DA 2024-11-06
ER

PT J
AU Sevastjanova, R
   Hauptmann, H
   Deterding, S
   El-Assady, M
AF Sevastjanova, Rita
   Hauptmann, Hanna
   Deterding, Sebastian
   El-Assady, Mennatallah
TI Personalized Language Model Selection Through Gamified Elicitation of
   Contrastive Concept Preferences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptation models; Task analysis; Games; Data models; Computational
   modeling; Cognitive science; Analytical models; Language model
   personalization; gamification; visual analytics
ID GAMIFICATION
AB Language models are widely used for different Natural Language Processing tasks while suffering from a lack of personalization. Personalization can be achieved by, e.g., fine-tuning the model on training data that is created by the user (e.g., social media posts). Previous work shows that the acquisition of such data can be challenging. Instead of adapting the model's parameters, we thus suggest selecting a model that matches the user's mental model of different thematic concepts in language. In this article, we attempt to capture such individual language understanding of users. In this process, two challenges have to be considered. First, we need to counteract disengagement since the task of communicating one's language understanding typically encompasses repetitive and time-consuming actions. Second, we need to enable users to externalize their mental models in different contexts, considering that language use changes depending on the environment. In this article, we integrate methods of gamification into a visual analytics (VA) workflow to engage users in sharing their knowledge within various contexts. In particular, we contribute the design of a gameful VA playground called Concept Universe. During the four-phased game, the users build personalized concept descriptions by explaining given concept names through representative keywords. Based on their performance, the system reacts with constant visual, verbal, and auditory feedback. We evaluate the system in a user study with six participants, showing that users are engaged and provide more specific input when facing a virtual opponent. We use the generated concepts to make personalized language model suggestions.
C1 [Sevastjanova, Rita] Univ Konstanz, D-78457 Constance, Germany.
   [Hauptmann, Hanna] Univ Utrecht, NL-3584 CS Utrecht, Netherlands.
   [Deterding, Sebastian] Imperial Coll London, London SW7 2BX, England.
   [El-Assady, Mennatallah] ETH, Res Ctr Energy Networks, CH-8092 Zurich, Switzerland.
C3 University of Konstanz; Utrecht University; Imperial College London;
   Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Sevastjanova, R (corresponding author), Univ Konstanz, D-78457 Constance, Germany.
EM rita.sevastjanova@uni-konstanz.de; h.j.hauptmann@uu.nl;
   s.deterding@imperial.ac.uk; melassady@ai.ethz.ch
RI ; Hauptmann, Hanna/R-3492-2016; Deterding, Sebastian/L-4649-2017
OI El-Assady, Mennatallah/0000-0001-8526-2613; Hauptmann,
   Hanna/0000-0002-6840-5341; Deterding, Sebastian/0000-0003-0033-2104;
   SEVASTJANOVA, RITA/0000-0002-2629-9579
FU Deutsche Forschungsgemein-schaft [BU1806/10-2]; ETH AI Center
FX No Statement Available
CR [Anonymous], 2015, Int. J. Hum.- Comput. Stud., V74, P14
   [Anonymous], 2013, P INT C COMP SEM
   [Anonymous], 2011, P 44 HAW INT C SYST, DOI DOI 10.1109/HICSS.2011.339
   Bi KP, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1521, DOI 10.1145/3397271.3401192
   Bordia S, 2019, Arxiv, DOI arXiv:1904.03035
   Brandenburg FJ, 1996, LECT NOTES COMPUT SC, V1027, P76, DOI 10.1007/BFb0021792
   Cambridge U.K., 2008, P 1 INSTR C MACH LEA, P1
   Carroll J. M., 1988, Handbook of human-computer interaction, P45, DOI 10.1016/B978-0-444-70536-5.50007-5
   Choo J, 2013, IEEE T VIS COMPUT GR, V19, P1992, DOI 10.1109/TVCG.2013.212
   Cirqueira L., 2017, AN EST 23 S BRAS SIS, P209
   De Croon R, 2018, IEEE INT CONF HEALT, P53, DOI 10.1109/ICHI.2018.00014
   Deterding S., 2011, P 15 INT AC MINDTREK, P9, DOI DOI 10.1145/2181037.2181040
   Fine AB, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P7
   Glavas G, 2021, P 16 WORKSH INN US N, P110
   Han XC, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4238
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328
   Kim Y, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P866
   King M, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P2461
   Lafourcade M, 2016, LECT NOTES COMPUT SC, V9612, P258, DOI 10.1007/978-3-319-41754-7_23
   Lanser B, 2016, LREC 2016 - TENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3477
   Lauscher A, 2020, P DEEP LEARNING INSI, P43, DOI 10.18653/v1/2020.deelio-1.5
   Lee HY, 2017, IEEE-ACM T AUDIO SPE, V25, P519, DOI 10.1109/TASLP.2016.2635445
   Lee J.J., 2011, ACAD EXCHANGE Q, V15
   Mohammad SM, 2013, COMPUT INTELL-US, V29, P436, DOI 10.1111/j.1467-8640.2012.00460.x
   Moreira A, 2007, GRAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL GM/R, P61
   Ostrow KS, 2018, LECT NOTES ARTIF INT, V10947, P381, DOI 10.1007/978-3-319-93843-1_28
   Park D, 2018, IEEE T VIS COMPUT GR, V24, P361, DOI 10.1109/TVCG.2017.2744478
   Pfeiffer J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P46
   Pham M. Q., 2020, P 5 C MACH TRANSL, P617
   Phang J, 2019, Arxiv, DOI [arXiv:1811.01088, 10.48550/arXiv.1811.01088, DOI 10.48550/ARXIV.1811.01088]
   Philip J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4465
   Rogers A, 2020, T ASSOC COMPUT LING, V8, P842, DOI 10.1162/tacl_a_00349
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Saeed N, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3178155
   Sailer M, 2017, COMPUT HUM BEHAV, V69, P371, DOI 10.1016/j.chb.2016.12.033
   San Vicente I, 2016, LREC 2016 - TENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P938
   Sevastjanova Rita, 2023, IEEE Trans Vis Comput Graph, V29, P1178, DOI 10.1109/TVCG.2022.3209458
   van Ham F, 2009, IEEE T VIS COMPUT GR, V15, P953, DOI 10.1109/TVCG.2009.108
   Vaswani A., 2017, Advances in neural information processing systems
   Zhang J, 2023, J MANAGE INFORM SYST, V40, P401, DOI 10.1080/07421222.2023.2196776
NR 41
TC 1
Z9 1
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5449
EP 5465
DI 10.1109/TVCG.2023.3296905
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400073
PM 37494152
DA 2024-11-06
ER

PT J
AU Miura, S
   Fukumoto, R
   Okamura, N
   Fujie, MG
   Sugano, S
AF Miura, Satoshi
   Fukumoto, Ryota
   Okamura, Naomi
   Fujie, Masakatsu G.
   Sugano, Shigeki
TI Visual Illusion Created by a Striped Pattern Through Augmented Reality
   for the Prevention of Tumbling on Stairs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Stairs; Foot; Visualization; Augmented reality; Trajectory; Resists;
   Training; Human-computer interaction; user interfaces; virtual and
   augmented reality; virtual device interfaces
ID MINIMUM FOOT CLEARANCE; BLURRING VISION; FALLS; RISK; AGE; WALKING;
   MODEL; GAIT; CLASSIFICATION; PREDICTION
AB A fall on stairs can be a dangerous accident. An important indicator of falling risk is the foot clearance, which is the height of the foot when ascending stairs or the distance of the foot from the step when descending. We developed an augmented reality system with a holographic lens using a visual illusion to improve the foot clearance on stairs. The system draws a vertical striped pattern on the stair riser as the participant ascends the stairs to create the illusion that the steps are higher than the actual steps, and draws a horizontal striped pattern on the stair tread as the participant descends the stairs to create the illusion of narrower stairs. We experimentally evaluated the accuracy of the system and fitted a model to determine the appropriate stripe thickness. Finally, participants ascended and descended stairs before, during, and after using the augmented reality system. The foot clearance significantly improved, not only while the participants used the system but also after they used the system compared with before.
C1 [Miura, Satoshi] Tokyo Inst Technol, Dept Mech Engn, Meguro Ku, Tokyo 1528550, Japan.
   [Fukumoto, Ryota] Waseda Univ, Dept Modern Mech Engn, Tokyo 1698050, Japan.
C3 Institute of Science Tokyo; Tokyo Institute of Technology; Waseda
   University
RP Miura, S (corresponding author), Tokyo Inst Technol, Dept Mech Engn, Meguro Ku, Tokyo 1528550, Japan.
EM miura.s.aj@m.titech.ac.jp; fukuryo@toki.waseda.jp;
   n-okamura@toki.waseda.jp; mgfujie@waseda.jp; sugano@waseda.jp
OI Miura, Satoshi/0000-0001-5402-8074
FU JSPS KAKENHI [21K18075]; Murata Foundation; Hattori Foundation; Takano
   Foundation; Suzuki Foundation; Nakajima Foundation
FX This work was supported in part by JSPS KAKENHI under Grant 21K18075, in
   part by the Murata Foundation, in part by the Hattori Foundation, in
   part by the Takano Foundation, in part by the Suzuki Foundation; and in
   part by the Nakajima Foundation.
CR [Anonymous], 2023, Japanese building standards law enforcement ordinance
   Arora P, 2015, 2ND INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN) 2015, P791, DOI 10.1109/SPIN.2015.7095388
   Begg R, 2007, GAIT POSTURE, V25, P191, DOI 10.1016/j.gaitpost.2006.03.008
   Berkebile JA, 2022, IEEE T BIO-MED ENG, V69, P1909, DOI 10.1109/TBME.2021.3130540
   Bruder G, 2012, IEEE T VIS COMPUT GR, V18, P1068, DOI 10.1109/TVCG.2011.274
   Camargo J, 2021, IEEE T BIO-MED ENG, V68, P1569, DOI 10.1109/TBME.2021.3065809
   Choi I, 2021, IEEE T VIS COMPUT GR, V27, P4387, DOI 10.1109/TVCG.2020.3002245
   Dingwell JB, 2017, GAIT POSTURE, V55, P131, DOI 10.1016/j.gaitpost.2017.03.018
   Elliott D.B., 2015, Public Health Research, V3, P1, DOI DOI 10.3310/PHR03080
   Elliott DB, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0004577
   Fang Y, 2022, IEEE T BIO-MED ENG, V69, P2143, DOI 10.1109/TBME.2021.3137447
   Foster RJ, 2016, ERGONOMICS, V59, P884, DOI 10.1080/00140139.2015.1105304
   Foster RJ, 2015, INVEST OPHTH VIS SCI, V56, P2950, DOI 10.1167/iovs.14-16018
   GOMI H, 1990, PROCEEDINGS OF THE 29TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, P3289, DOI 10.1109/CDC.1990.203403
   Gonzalez-Franco M, 2020, IEEE T VIS COMPUT GR, V26, P2023, DOI 10.1109/TVCG.2020.2973075
   Hamel KA, 2005, GAIT POSTURE, V21, P135, DOI 10.1016/j.gaitpost.2004.01.006
   Hamel KA, 2004, J AM GERIATR SOC, V52, P563, DOI 10.1111/j.1532-5415.2004.52162.x
   Heasley K, 2005, INVEST OPHTH VIS SCI, V46, P3584, DOI 10.1167/iovs.05-0059
   Heasley K, 2004, INVEST OPHTH VIS SCI, V45, P2122, DOI 10.1167/iovs.03-1199
   HINDMARSH JJ, 1989, ARCH INTERN MED, V149, P2217, DOI 10.1001/archinte.149.10.2217
   Jazayeri M, 2007, NATURE, V446, P912, DOI 10.1038/nature05739
   JOFFE M, 1988, PEDIATRICS, V82, P457
   KAWATO M, 1992, BIOL CYBERN, V68, P95, DOI 10.1007/BF00201431
   Khademi G, 2021, IEEE T BIO-MED ENG, V68, P967, DOI 10.1109/TBME.2020.3016129
   L. andW. Ministry of Health, 2017, Number of mortality /composition ratio by age by type of main accident in the family by type
   Lai DTH, 2012, HUM MOVEMENT SCI, V31, P271, DOI 10.1016/j.humov.2010.07.009
   Li S, 2022, IEEE T VIS COMPUT GR, V28, P3035, DOI 10.1109/TVCG.2020.3044563
   LORD SR, 1993, AUST J PUBLIC HEALTH, V17, P240, DOI 10.1111/j.1753-6405.1993.tb00143.x
   Marshall SW, 2005, AM J PREV MED, V28, P95, DOI 10.1016/j.amepre.2004.09.015
   Miura S., 2019, J. Biomech. Sci. Eng., V14, P1, DOI [10.1299/jbse.18-00216, DOI 10.1299/JBSE.18-00216]
   Mori M., 2011, J. Soc.Biomech., V35, P201
   MULLEN KT, 1985, J PHYSIOL-LONDON, V359, P381, DOI 10.1113/jphysiol.1985.sp015591
   Norman Donald A., 1988, Design of Everyday Things
   Peck TC, 2018, IEEE T VIS COMPUT GR, V24, P1604, DOI 10.1109/TVCG.2018.2793598
   Rabe KG, 2021, IEEE T BIO-MED ENG, V68, P1379, DOI 10.1109/TBME.2020.3032077
   Resquín F, 2016, EUR J TRANSL MYOL, V26, P255, DOI 10.4081/ejtm.2016.6164
   Riess TJ, 1998, ST HEAL T, V58, P200
   Roos PE, 2013, HUM MOVEMENT SCI, V32, P984, DOI 10.1016/j.humov.2013.07.001
   Schulz BW, 2017, J BIOMECH, V55, P107, DOI 10.1016/j.jbiomech.2017.02.024
   Skarbez R, 2017, IEEE T VIS COMPUT GR, V23, P1322, DOI 10.1109/TVCG.2017.2657158
   Stacoff A, 2005, GAIT POSTURE, V21, P24, DOI 10.1016/j.gaitpost.2003.11.003
   Stolyarov R, 2021, IEEE T BIO-MED ENG, V68, P384, DOI 10.1109/TBME.2020.2994152
   Sugimoto K, 2008, IEEE DECIS CONTR P, P714, DOI 10.1109/CDC.2008.4738996
   Valois H., 1974, Vis. Res., V14, P75
   van der Zijden AM, 2017, J BIOMECH, V54, P19, DOI 10.1016/j.jbiomech.2017.01.033
   Weerdesteyn V, 2005, HUM MOVEMENT SCI, V24, P865, DOI 10.1016/j.humov.2005.10.013
   Woodward RB, 2022, IEEE T BIO-MED ENG, V69, P1202, DOI 10.1109/TBME.2021.3120616
   Yoshida A, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P83
   Yun Y, 2014, J BIOMECH, V47, P186, DOI 10.1016/j.jbiomech.2013.09.032
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zietz D, 2011, GAIT POSTURE, V34, P279, DOI 10.1016/j.gaitpost.2011.05.017
NR 51
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5466
EP 5477
DI 10.1109/TVCG.2023.3295425
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400007
PM 37450363
DA 2024-11-06
ER

PT J
AU Hu, T
   Xu, HY
   Luo, LJ
   Yu, T
   Zheng, ZR
   Zhang, H
   Liu, YB
   Zwicker, M
AF Hu, Tao
   Xu, Hongyi
   Luo, Linjie
   Yu, Tao
   Zheng, Zerong
   Zhang, He
   Liu, Yebin
   Zwicker, Matthias
TI HVTR plus plus : Image and Pose Driven Human Avatars Using Hybrid
   Volumetric-Textural Rendering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Three-dimensional displays; Avatars;
   Geometry; Image resolution; Pipelines; Telepresence; Video-based
   characters; full-body avatar; neural rendering; deep; hybrid
AB Recent neural rendering methods have made great progress in generating photorealistic human avatars. However, these methods are generally conditioned only on low-dimensional driving signals (e.g., body poses), which are insufficient to encode the complete appearance of a clothed human. Hence they fail to generate faithful details. To address this problem, we exploit driving view images (e.g., in telepresence systems) as additional inputs. We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural Rendering (HVTR++), which synthesizes 3D human avatars from arbitrary driving poses and views while staying faithful to appearance details efficiently and at high quality. First, we learn to encode the driving signals of pose and view image on a dense UV manifold of the human body surface and extract UV-aligned features, preserving the structure of a skeleton-based parametric model. To handle complicated motions (e.g., self-occlusions), we then leverage the UV-aligned features to construct a 3D volumetric representation based on a dynamic neural radiance field. While this allows us to represent 3D geometry with changing topology, volumetric rendering is computationally heavy. Hence we employ only a rough volumetric representation using a pose- and image-conditioned downsampled neural radiance field (PID-NeRF), which we can render efficiently at low resolutions. In addition, we learn 2D textural features that are fused with rendered volumetric features in image space. The key advantage of our approach is that we can then convert the fused features into a high-resolution, high-quality avatar by a fast GAN-based textural renderer. We demonstrate that hybrid rendering enables HVTR++ to handle complicated motions, render high-quality avatars under user-controlled poses/shapes, and most importantly, be efficient at inference time. Our experimental results also demonstrate state-of-the-art quantitative results.
C1 [Hu, Tao; Zwicker, Matthias] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Xu, Hongyi] ByteDanceInc, Intelligent Creat Lab, Beijing 100086, Peoples R China.
   [Luo, Linjie; Yu, Tao; Zheng, Zerong; Zhang, He; Liu, Yebin] Tsinghua Univ, Dept Automat, Beijing 100190, Peoples R China.
C3 University System of Maryland; University of Maryland College Park;
   Tsinghua University
RP Hu, T (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM taohu@umd.edu; hongyixu@bytedance.com; linjie.luo@bytedance.com;
   ytrock@mail.tsinghua.edu.cn; zzr18@mails.tsinghua.edu.cn;
   zhanghe3d@gmail.com; liuyebin@mail.tsinghua.edu.cn; zwicker@cs.umd.edu
RI Zhang, Guo-Hua/AAM-7264-2021; ZHENG, ZERONG/KVA-4699-2024; XU,
   Hongyi/IUO-8405-2023; Hu, Tao/JRY-5446-2023
OI Zheng, Zerong/0000-0003-1339-2480; Zwicker,
   Matthias/0000-0001-8630-5515; Hu, Tao/0000-0002-3741-3701; Zhang,
   He/0000-0002-7280-6746
FU NSFC [62125107, 62171255]; Guoqiang Institute of Tsinghua University
   [2021GQG0001]; NSF [1813583]
FX The work of Yebin Liu was supported in part by the NSFC under Grant
   62125107. The work of Tao Yu was supported in part by the NSFC under
   Grant 62171255 and in part by Guoqiang Institute of Tsinghua University
   under Grant 2021GQG0001. The work of Matthias Zwicker was supported by
   NSF under Grant 1813583. Work partly conducted during Tao Hu's
   internship at ByteDance.
CR Bagautdinov T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459850
   Borshukov G., 2003, P ACM SIGGRAPH, P16
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Deng B., 2020, P COMP VIS ECCV 2020, P612, DOI [10.1007/978-3-030-58571-636, DOI 10.1007/978-3-030-58571-636]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Grigorev A, 2019, PROC CVPR IEEE, P12127, DOI 10.1109/CVPR.2019.01241
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Habermann M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459749
   He T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11026, DOI 10.1109/ICCV48922.2021.01086
   Hensel M, 2017, ADV NEUR IN, V30
   Hu T, 2022, INT CONF 3D VISION, P197, DOI 10.1109/3DV57658.2022.00032
   Hu T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14508, DOI 10.1109/ICCV48922.2021.01426
   Huang JW, 2020, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR42600.2020.00163
   Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316
   Jakob Wenzel, 2010, Mitsuba Renderer
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kingma D., 2015, P INT C LEARN REPR, P273
   Kratzwald B, 2018, Arxiv, DOI arXiv:1711.11453
   Kwon Y, 2021, Arxiv, DOI [arXiv:2109.07448, DOI 10.48550/ARXIV.2109.07448]
   Lawrence J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480490
   Liu LJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480528
   Liu LJ, 2021, IEEE T VIS COMPUT GR, V27, P4009, DOI 10.1109/TVCG.2020.2996594
   Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3333002
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459863, 10.1145/3476576.3476608]
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Lombardi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201401
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma QL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10954, DOI 10.1109/ICCV48922.2021.01079
   Ma QL, 2021, PROC CVPR IEEE, P16077, DOI 10.1109/CVPR46437.2021.01582
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Martin-Brualla R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275099
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Michalkiewicz M, 2019, Arxiv, DOI arXiv:1901.06802
   Mihajlovic M, 2021, PROC CVPR IEEE, P10456, DOI 10.1109/CVPR46437.2021.01032
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Palafox P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12675, DOI 10.1109/ICCV48922.2021.01246
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Patel C, 2020, PROC CVPR IEEE, P7363, DOI 10.1109/CVPR42600.2020.00739
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Prokudin S, 2021, IEEE WINT CONF APPL, P1809, DOI 10.1109/WACV48630.2021.00185
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Raj A, 2021, PROC CVPR IEEE, P3721, DOI 10.1109/CVPR46437.2021.00372
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Remelli E., 2022, P ACM SIGGRAPH, P1
   Saito S, 2021, PROC CVPR IEEE, P2885, DOI 10.1109/CVPR46437.2021.00291
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Sarkar K., 2020, COMPUTER VISION ECCV, P596
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V, 2019, ADV NEUR IN, V32
   Sitzmann V, 2019, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2019.00254
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Tiwari G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11688, DOI 10.1109/ICCV48922.2021.01150
   Wang Shaofei, 2021, NeurIPS, V34, P2810
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Ting-Chun., 2018, ADV NEURAL INFORM PR
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng CY, 2022, Arxiv, DOI arXiv:2201.04127
   Xiang DL, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480545
   Xiu YL, 2022, PROC CVPR IEEE, P13286, DOI 10.1109/CVPR52688.2022.01294
   Xu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964927
   Xu Hongyi, 2021, Advances in Neural Information Processing Systems (NeurIPS), V34, P14955
   Yang JL, 2018, LECT NOTES COMPUT SC, V11211, P245, DOI 10.1007/978-3-030-01234-2_15
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng Y., 2021, P IEEE CVF INT C COM, P6239
   Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 79
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5478
EP 5492
DI 10.1109/TVCG.2023.3297721
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400014
PM 37478036
DA 2024-11-06
ER

PT J
AU Mavromatis, M
   Hoyet, L
   Lécuyer, A
   Dewez, D
   Argelaguet, F
AF Mavromatis, Mae
   Hoyet, Ludovic
   Lecuyer, Anatole
   Dewez, Diane
   Argelaguet, Ferran
TI To Stick or Not to Stick? Studying the Impact of Offset Recovery
   Techniques During Mid-Air Interactions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Avatars; Legged locomotion; Haptic interfaces; Springs;
   Delays; Behavioral sciences; Hand interactions; offset recovery; sticky
ID VISUAL INTERPENETRATION; AVATAR; SENSE; HAND; EMBODIMENT; APPEARANCE;
   PERCEPTION; DOMINANCE; EYE
AB During mid-air interactions, common approaches (such as the god-object method) typically rely on visually constraining the user's avatar to avoid visual interpenetrations with the virtual environment in the absence of kinesthetic feedback. This paper explores two methods which influence how the position mismatch (positional offset) between users' real and virtual hands is recovered when releasing the contact with virtual objects. The first method (sticky) constrains the user's virtual hand until the mismatch is recovered, while the second method (unsticky) employs an adaptive offset recovery method. In the first study, we explored the effect of positional offset and of motion alteration on users' behavioral adjustments and users' perception. In a second study, we evaluated variations in the sense of embodiment and the preference between the two control laws. Overall, both methods presented similar results in terms of performance and accuracy, yet, positional offsets strongly impacted motion profiles and users' performance. Both methods also resulted in comparable levels of embodiment. Finally, participants usually expressed strong preferences toward one of the two methods, but these choices were individual-specific and did not appear to be correlated solely with characteristics external to the individuals. Taken together, these results highlight the relevance of exploring the customization of motion control algorithms for avatars.
C1 [Mavromatis, Mae; Hoyet, Ludovic; Lecuyer, Anatole; Dewez, Diane; Argelaguet, Ferran] Univ Rennes, Inria, CNRS, IRISA, F-35042 Rennes, France.
C3 Inria; Universite de Rennes; Centre National de la Recherche
   Scientifique (CNRS)
RP Mavromatis, M (corresponding author), Univ Rennes, Inria, CNRS, IRISA, F-35042 Rennes, France.
EM mae.mavromatis@inria.fr; ludovic.hoyet@inria.fr;
   anatole.lecuyer@inria.fr; diane.dewez@gmail.com;
   ferran.argelaguet@inria.fr
RI Hoyet, Ludovic/IWU-9100-2023
OI Lecuyer, Anatole/0000-0002-1409-244X; Hoyet, Ludovic/0000-0002-7373-6049
FU Region Bretagne and the Inria Avatar Challenge
FX This work was supported by the Region Bretagne and the Inria Avatar
   Challenge.
CR Azmandian M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1968, DOI 10.1145/2858036.2858226
   Benda B, 2020, INT SYM MIX AUGMENT, P269, DOI 10.1109/ISMAR50242.2020.00050
   Bourdin P, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-56034-5
   Burns E, 2006, PRESENCE-TELEOP VIRT, V15, P1, DOI 10.1162/pres.2006.15.1.1
   Burns E, 2005, P IEEE VIRT REAL ANN, P3
   Burns E., 2007, Int. J. Vet. Res., V6, P11
   Burns F. P., 2006, P ACM S VIRTUAL REAL, P3, DOI DOI 10.1145/1180495.1180499
   Canales R, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343132
   Cheng LP, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P3718, DOI 10.1145/3025453.3025753
   Clarence A, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P150, DOI 10.1109/VR50410.2021.00036
   Dewez D, 2022, IEEE T VIS COMPUT GR, V28, P2047, DOI 10.1109/TVCG.2022.3150501
   Dewez L., 2021, P CHI C HUM FACT COM, P1
   Dominjon L, 2005, P IEEE VIRT REAL ANN, P19
   Esmaeili S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P453, DOI [10.1109/VR46266.2020.1581285352835, 10.1109/VR46266.2020.00-38]
   Feuchtner T, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P31, DOI 10.1145/3242587.3242594
   Feuchtner T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5145, DOI 10.1145/3025453.3025689
   Frees S, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1229855.1229857
   Fribourg R, 2020, IEEE T VIS COMPUT GR, V26, P2062, DOI 10.1109/TVCG.2020.2973077
   Gloumeau PC, 2021, IEEE T VIS COMPUT GR, V27, P2488, DOI 10.1109/TVCG.2020.2987834
   Jáuregui DAG, 2014, IEEE T VIS COMPUT GR, V20, P654, DOI 10.1109/TVCG.2014.45
   Gonzalez EJ, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364248
   Hartfill J, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489866
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kohli L., 2012, 2012 IEEE Symposium on 3D User Interfaces (3DUI), P105, DOI 10.1109/3DUI.2012.6184193
   Kohm K, 2022, ACM T APPL PERCEPT, V19, DOI 10.1145/3561055
   Koilias A, 2019, INFORMATICS-BASEL, V6, DOI 10.3390/informatics6020018
   Kokkinara E, 2014, PERCEPTION, V43, P43, DOI 10.1068/p7545
   Langbehn E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201335
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Li JL, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P120, DOI 10.1145/3267782.3267797
   Li ZP, 2022, PROC ACM INTERACT MO, V6, DOI 10.1145/3534590
   Lilija S., 2021, P IEEE VIRT REAL 3D, P1
   Lohse AL, 2019, 2019 IEEE 5TH WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), DOI 10.1109/wevr.2019.8809587
   Mayer V., 2018, P CHI C HUM FACT COM, P1
   McMahan Ryan P., 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P59, DOI 10.1007/978-3-319-39907-2_6
   MEYER DE, 1988, PSYCHOL REV, V95, P340, DOI 10.1037/0033-295X.95.3.340
   Ogawa N, 2021, IEEE T VIS COMPUT GR, V27, P3182, DOI 10.1109/TVCG.2020.2964758
   Pavlovych A, 2009, EICS'09: PROCEEDINGS OF THE ACM SIGCHI SYMPOSIUM ON ENGINEERING INTERACTIVE COMPUTING SYSTEMS, P187
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Porssut T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P529, DOI [10.1109/VR.2019.8797716, 10.1109/vr.2019.8797716]
   POSNER MI, 1976, PSYCHOL REV, V83, P157, DOI 10.1037/0033-295X.83.2.157
   Poupyrev M., 1996, P 9 ANN ACM S USER I, P79, DOI DOI 10.1145/237091.237102
   Prachyabrued M., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P59, DOI 10.1109/3DUI.2011.5759218
   Prachyabrued M, 2016, IEEE T VIS COMPUT GR, V22, P1718, DOI 10.1109/TVCG.2015.2456917
   Prachyabrued M, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P19, DOI 10.1109/3DUI.2014.6798835
   Razzaque S., 2005, Redirected Walking
   Sait MSMY, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON INTERACTIVE AND SPATIAL COMPUTING (IWISC 18), P64, DOI 10.1145/3191801.3191814
   Samad E., 2019, P CHI C HUM FACT COM, P1
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Toothman N, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P756, DOI [10.1109/vr.2019.8798108, 10.1109/VR.2019.8798108]
   Vizcay S., 2021, P INT C ART REAL TEL
   Wegner DM, 1999, AM PSYCHOL, V54, P480, DOI 10.1037/0003-066X.54.7.480
   Wen W, 2019, CONSCIOUS COGN, V73, DOI 10.1016/j.concog.2019.05.007
   Wen W, 2015, CONSCIOUS COGN, V36, P87, DOI 10.1016/j.concog.2015.06.004
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Zenner A, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P75, DOI 10.1109/VR50410.2021.00028
   Zenner A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P47, DOI [10.1109/vr.2019.8798143, 10.1109/VR.2019.8798143]
NR 57
TC 1
Z9 1
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5493
EP 5506
DI 10.1109/TVCG.2023.3295209
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400015
PM 37440385
OA Green Published
DA 2024-11-06
ER

PT J
AU Xiao, YQ
   Bai, HM
   Gao, Y
   Hu, B
   Zheng, J
   Cai, XE
   Rao, JS
   Li, XG
   Hao, AM
AF Xiao, Yanqing
   Bai, Hongming
   Gao, Yang
   Hu, Ben
   Zheng, Jia
   Cai, Xiaoe
   Rao, Jiasheng
   Li, Xiaoguang
   Hao, Aimin
TI Interactive Virtual Ankle Movement Controlled by Wrist sEMG Improves
   Motor Imagery: An Exploratory Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Stroke (medical condition); Wrist; Muscles; Legged locomotion;
   Biological system modeling; Real-time systems; VR-based stroke
   rehabilitation training; motor imagery; sEMG-based virtual feedback
ID FUNCTIONAL RECOVERY; ROBOTIC DEVICE; STROKE; REHABILITATION; EMBODIMENT;
   VALIDITY; THREATS; ANGLES; LEGS; HAND
AB Virtual reality (VR) techniques can significantly enhance motor imagery training by creating a strong illusion of action for central sensory stimulation. In this article, we establish a precedent by using surface electromyography (sEMG) of contralateral wrist movement to trigger virtual ankle movement through an improved data-driven approach with a continuous sEMG signal for fast and accurate intention recognition. Our developed VR interactive system can provide feedback training for stroke patients in the early stages, even if there is no active ankle movement. Our objectives are to evaluate: 1) the effects of VR immersion mode on body illusion, kinesthetic illusion, and motor imagery performance in stroke patients; 2) the effects of motivation and attention when utilizing wrist sEMG as a trigger signal for virtual ankle motion; 3) the acute effects on motor function in stroke patients. Through a series of well-designed experiments, we have found that, compared to the 2D condition, VR significantly increases the degree of kinesthetic illusion and body ownership of the patients, and improves their motor imagery performance and motor memory. When compared to conditions without feedback, using contralateral wrist sEMG signals as trigger signals for virtual ankle movement enhances patients' sustained attention and motivation during repetitive tasks. Furthermore, the combination of VR and feedback has an acute impact on motor function. Our exploratory study suggests that the sEMG-based immersive virtual interactive feedback provides an effective option for active rehabilitation training for severe hemiplegia patients in the early stages, with great potential for clinical application.
C1 [Xiao, Yanqing; Rao, Jiasheng] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Sch Biol Sci & Med Engn, Beijing Key Lab Biomat & Neural Regenerat, Beijing 100191, Peoples R China.
   [Bai, Hongming; Gao, Yang; Hu, Ben] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Bai, Hongming; Gao, Yang; Hu, Ben] Chinese Acad Med Sci, Res Unit Virtual Body & Virtual Surg 2019RU004, Beijing 100050, Peoples R China.
   [Zheng, Jia] Capital Med Univ, Beijing Childrens Hosp, Beijing 100045, Peoples R China.
   [Cai, Xiaoe] Beijing Haidian Hosp, Dept Rehabil Med, Beijing 100080, Peoples R China.
   [Li, Xiaoguang] Capital Med Univ, Sch Basic Med Sci, Dept Neurobiol, Beijing 100069, Peoples R China.
C3 Beihang University; Beihang University; Chinese Academy of Medical
   Sciences - Peking Union Medical College; Capital Medical University;
   Capital Medical University
RP Gao, Y (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Li, XG (corresponding author), Capital Med Univ, Sch Basic Med Sci, Dept Neurobiol, Beijing 100069, Peoples R China.
EM xioyanjingx@126.com; 18813139553@163.com; gaoyangvr@buaa.edu.cn;
   huben@buaa.edu.cn; lwc20002@126.com; cxe930316@163.com;
   raojschina@buaa.edu.cn; lxgchina@sina.com; ham@buaa.edu.cn
RI Gao, Yang/JQV-9627-2023; Zhao, Mingyu/HHS-0141-2022
OI Gao, Yang/0000-0002-9149-3554; Zheng, Jia/0009-0007-6407-7436; Rao,
   Jia-Sheng/0000-0002-5196-0912
FU National Natural Science Foundation of China [62002010, 31970970,
   82271403]; Beijing Advanced Innovation Center for Biomedical Engineering
   [ZF138G1714]; CAMS Innovation Fund for Medical Sciences (CIFMS)
   [2019-I2M-5-016]; Beijing Science and Technology Project
   [Z221100007722001]; Huawei Innovation Research Plan [TC20220616019]; Key
   Research and Development Program of Guangzhou [202206060003]; Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems [VRLAB2023A05]
FX No Statement Available
CR ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209
   [Anonymous], 2023, Amer. J. Drug AlcoholAbuse, V49, P5
   [Anonymous], 2016, Int. J. Neurorehabilitation, V3
   [Anonymous], 1993, Int. J. Aviation Psychol., V3, P904
   Arteaga MV, 2020, P IEEE RAS-EMBS INT, P416, DOI 10.1109/BioRob49111.2020.9224328
   Artemiadis PK, 2010, IEEE T ROBOT, V26, P393, DOI 10.1109/TRO.2009.2039378
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cesarei De, 2023, Psychol.Res., P1
   Chen SW, 2021, INT SYM MIX AUGMENT, P153, DOI 10.1109/ISMAR-Adjunct54149.2021.00040
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Choi JW, 2020, IEEE T NEUR SYS REH, V28, P1614, DOI 10.1109/TNSRE.2020.2998123
   Clark LA, 2019, PSYCHOL ASSESSMENT, V31, P1412, DOI 10.1037/pas0000626
   Colacino FM, 2012, MED ENG PHYS, V34, P531, DOI 10.1016/j.medengphy.2011.08.012
   Ding QC, 2017, IEEE T NEUR SYS REH, V25, P1518, DOI 10.1109/TNSRE.2016.2639527
   dos Santos LF, 2016, BIOMED ENG ONLINE, V15, DOI 10.1186/s12938-016-0289-4
   Drucker H., 1997, P 14 INT C MACH LEAR, P107
   Fauvel K, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9233137
   Fenker DB, 2008, J COGNITIVE NEUROSCI, V20, P1250, DOI 10.1162/jocn.2008.20086
   Flannelly KJ, 2018, J HEALTH CARE CHAPL, V24, P107, DOI 10.1080/08854726.2017.1421019
   Forrester LW, 2011, NEUROREHAB NEURAL RE, V25, P369, DOI 10.1177/1545968310388291
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Fusco G, 2021, PSYCHOL RES-PSYCH FO, V85, P926, DOI 10.1007/s00426-020-01366-5
   Gao Y, 2021, INT SYM MIX AUGMENT, P354, DOI 10.1109/ISMAR-Adjunct54149.2021.00080
   Carrasco DG, 2016, NEUROLOGIA, V31, P43, DOI 10.1016/j.nrl.2013.02.003
   García-Pérez MA, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00325
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Ghassemi M, 2019, IEEE T NEUR SYS REH, V27, P283, DOI 10.1109/TNSRE.2019.2894102
   Ghazizadeh A, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-33514-3
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han JD, 2015, IEEE T IND ELECTRON, V62, P4267, DOI 10.1109/TIE.2014.2387337
   HART S G, 1988, P139
   Heald E, 2017, NEUROREHAB NEURAL RE, V31, P583, DOI 10.1177/1545968317704904
   Hill AV, 1938, PROC R SOC SER B-BIO, V126, P136, DOI 10.1098/rspb.1938.0050
   Jarrassé N, 2017, IEEE T NEUR SYS REH, V25, P68, DOI 10.1109/TNSRE.2016.2563222
   Jendrassik E., 1883, Deusches Arch.Klin. Med., V33, P177
   Karim F, 2018, IEEE ACCESS, V6, P1662, DOI 10.1109/ACCESS.2017.2779939
   Kaupp C, 2018, J NEUROPHYSIOL, V119, P1095, DOI 10.1152/jn.00570.2017
   Kawase T, 2014, IEEE SYS MAN CYBERN, P1470, DOI 10.1109/SMC.2014.6974122
   Ke GL, 2017, ADV NEUR IN, V30
   Klarner T, 2016, BRAIN SCI, V6, DOI 10.3390/brainsci6040054
   Lamontagne A, 2010, NEUROREHAB NEURAL RE, V24, P457, DOI 10.1177/1545968309355985
   Li KX, 2020, BIOMED SIGNAL PROCES, V62, DOI 10.1016/j.bspc.2020.102074
   Li M, 2022, IEEE T VIS COMPUT GR, V28, P3832, DOI 10.1109/TVCG.2022.3203099
   Meng Q, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1166-z
   Mirelman A, 2009, STROKE, V40, P169, DOI 10.1161/STROKEAHA.108.516328
   Okawada M, 2020, RESTOR NEUROL NEUROS, V38, P455, DOI 10.3233/RNN-201030
   Ono Y, 2018, NEUROPSYCHOLOGIA, V114, P134, DOI 10.1016/j.neuropsychologia.2018.04.016
   Pallotti A, 2021, BIOCYBERN BIOMED ENG, V41, P605, DOI 10.1016/j.bbe.2021.03.0030168-8227/
   Panel P.-S. R. G., 1995, Patient Family Gui,
   Pau JWL, 2012, IEEE T BIO-MED ENG, V59, P2586, DOI 10.1109/TBME.2012.2206389
   Pichiorri F, 2015, ANN NEUROL, V77, P851, DOI 10.1002/ana.24390
   Platt JC, 2000, ADV NEUR IN, P61
   Pozeg P, 2017, NEUROLOGY, V89, P1894, DOI 10.1212/WNL.0000000000004585
   Rawat S, 2016, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON SYSTEM MODELING & ADVANCEMENT IN RESEARCH TRENDS (SMART-2016), P115, DOI 10.1109/SYSMART.2016.7894501
   Ren YP, 2017, IEEE T NEUR SYS REH, V25, P589, DOI 10.1109/TNSRE.2016.2584003
   Repp BH, 2004, PSYCHOL RES-PSYCH FO, V68, P252, DOI 10.1007/s00426-003-0143-8
   Roosink M, 2016, RESTOR NEUROL NEUROS, V34, P227, DOI 10.3233/RNN-150563
   Ruitenberg MFL, 2022, ANN NY ACAD SCI, V1510, P68, DOI 10.1111/nyas.14731
   Sakai K, 2020, BRAIN COGNITION, V146, DOI 10.1016/j.bandc.2020.105632
   Sakai K, 2018, NEUROCASE, V24, P245, DOI 10.1080/13554794.2019.1566477
   Sasaki A, 2020, J NEUROPHYSIOL, V124, P652, DOI 10.1152/jn.00705.2019
   Schomaker J, 2015, NEUROSCI BIOBEHAV R, V55, P268, DOI 10.1016/j.neubiorev.2015.05.002
   Schomaker J, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00918
   Schomaker J, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0050599
   Tanabe Junpei, 2022, J Phys Ther Sci, V34, P65, DOI 10.1589/jpts.34.65
   Tang ZC, 2016, IEEE T NEUR SYS REH, V24, P1342, DOI 10.1109/TNSRE.2015.2502663
   Tazoe T., 2014, J.Phys.FitnessSportsMed., V3, P181, DOI DOI 10.7600/jpfsm.3.181
   Tham J, 2018, IEEE T PROF COMMUN, V61, P178, DOI 10.1109/TPC.2018.2804238
   Tong YN, 2017, AGING DIS, V8, P364, DOI 10.14336/AD.2016.1012
   Tseng SC, 2010, J NEUROPHYSIOL, V104, P248, DOI 10.1152/jn.00906.2009
   Vourvopoulos A, 2019, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00244
   Wei WT, 2019, IEEE T BIO-MED ENG, V66, P2964, DOI 10.1109/TBME.2019.2899222
   Zhang Q, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00280
   Zhou R, 2018, J NEUROPHYSIOL, V120, P3172, DOI 10.1152/jn.00509.2017
   Zhou R, 2017, J NEUROPHYSIOL, V118, P2507, DOI 10.1152/jn.00663.2016
NR 75
TC 1
Z9 1
U1 10
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5507
EP 5524
DI 10.1109/TVCG.2023.3294342
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400011
PM 37432832
DA 2024-11-06
ER

PT J
AU Ibarrola, F
   Lawton, T
   Grace, K
AF Ibarrola, Francisco
   Lawton, Tomas
   Grace, Kazjon
TI A Collaborative, Interactive and Context-Aware Drawing Agent for
   Co-Creative Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaboration; Computational modeling; Task analysis; Artificial
   intelligence; Creativity; Refining; Real-time systems; Human-computer
   interaction; computational creativity; image generation
AB Recent advances in text-conditioned generative models have provided us with neural networks capable of creating images of astonishing quality, be they realistic, abstract, or even creative. These models have in common that (more or less explicitly) they all aim to produce a high-quality one-off output given certain conditions, and in that they are not well suited for a creative collaboration framework. Drawing on theories from cognitive science that model how professional designers and artists think, we argue how this setting differs from the former and introduce CICADA: a Collaborative, Interactive Context-Aware Drawing Agent. CICADA uses a vector-based synthesis-by-optimisation method to take a partial sketch (such as might be provided by a user) and develop it towards a goal by adding and/or sensibly modifying traces. Given that this topic has been scarcely explored, we also introduce a way to evaluate desired characteristics of a model in this context by means of proposing a diversity measure. CICADA is shown to produce sketches of quality comparable to a human user's, enhanced diversity and most importantly to be able to cope with change by continuing the sketch minding the user's contributions in a flexible manner.
C1 [Ibarrola, Francisco; Lawton, Tomas; Grace, Kazjon] Univ Sydney, Sch Architecture Design & Planning, Camperdown, NSW 2006, Australia.
C3 University of Sydney
RP Ibarrola, F (corresponding author), Univ Sydney, Sch Architecture Design & Planning, Camperdown, NSW 2006, Australia.
EM francisco.ibarrola@sydney.edu.au; tlaw8603@uni.sydney.edu.au;
   kazjon.grace@sydney.edu.au
RI Ibarrola, Francisco/ITW-1806-2023
OI Lawton, Tomas/0000-0002-5079-7885; Ibarrola,
   Francisco/0000-0003-1146-7071
FU Australian Research Council [DP200101059]; University of Sydney;
   Australian Research Council [DP200101059] Funding Source: Australian
   Research Council
FX Thiswork was supported by Australian Research Council under Grant
   DP200101059 and University of Sydney (in particular the School of
   Architecture, Designand Planning) for their financial support of this
   project. Recommended foracceptance by L. Wei.
CR Gatys LA, 2015, Arxiv, DOI [arXiv:1508.06576, 10.1167/16.12.326, DOI 10.1167/16.12.326]
   Aickin M, 1996, AM J PUBLIC HEALTH, V86, P726, DOI 10.2105/AJPH.86.5.726
   Amabile T., 1996, CREATIVITY CONTEXT U
   Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Boden M. A., 2004, CREATIVE MIND MYTHS, DOI DOI 10.4324/9780203508527
   Cha MY, 1998, ARTIFICIAL INTELLIGENCE IN DESIGN '98, P169
   Choi Y, 2020, PROC CVPR IEEE, P8185, DOI 10.1109/CVPR42600.2020.00821
   Cohen H., 1995, STANF HUM REV, V4, P141, DOI DOI 10.5555/212154.212174
   Colton Simon, 2020, Artificial Intelligence in Music, Sound, Art and Design. 9th International Conference, EvoMUSART 2020. Held as Part of EvoStar 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12103), P17, DOI 10.1007/978-3-030-43859-3_2
   Colton Simon., 2012, Computers and Creativity, P3, DOI [10.1007/978-3-642-31727-9_1, DOI 10.1007/978-3-642-31727-9_1]
   Das Ayan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P632, DOI 10.1007/978-3-030-58574-7_38
   Davis Nicholas, 2013, 9 ART INT INT DIG EN, V9, P9, DOI 10.1609/aiide.v9i6.12603
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dixon D, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P897
   Dorst K, 2015, DES THINK DES THEOR, P1, DOI 10.7551/mitpress/10096.001.0001
   Fan JE, 2019, PROCEEDINGS OF THE 2019 ON CREATIVITY AND COGNITION - C&C 19, P556, DOI 10.1145/3325480.3326578
   Frans K, 2021, Arxiv, DOI arXiv:2106.14843
   Gero JS, 2004, DESIGN STUD, V25, P373, DOI 10.1016/j.destud.2003.10.010
   GERO JS, 1990, AI MAG, V11, P26
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   Goldschmidt G., 1991, Creativ. Res. J., V4, P123, DOI [10.1080/10400419109534381, DOI 10.1080/10400419109534381]
   Grace K, 2015, INT J DES CREAT INNO, V3, P125, DOI 10.1080/21650349.2014.943295
   Ha D, 2017, Arxiv, DOI arXiv:1704.03477
   Hanin B, 2018, ADV NEUR IN, V31
   Hensel M, 2017, ADV NEUR IN, V30
   Ibarrola F., 2022, P INT C COMP CREAT, P8821
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jansson D. G., 1991, Design studies, V12, P3, DOI [10.1016/0142-694X(91)90003-F, DOI 10.1016/0142-694X(91)90003-F]
   Karimi P, 2019, IBM J RES DEV, V63, DOI 10.1147/JRD.2018.2881736
   Lawton T, 2023, PROCEEDINGS OF 2023 28TH ANNUAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2023, P264, DOI 10.1145/3581641.3584095
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Li TM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417871
   Lin YY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376258
   Lu J, 2019, ICVISP 2019: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON VISION, IMAGE AND SIGNAL PROCESSING, DOI 10.1145/3387168.3387178
   McCormack J., 2022, arXiv
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Poon J, 1997, ARTIF INTELL ENG, V11, P319, DOI 10.1016/S0954-1810(96)00047-7
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, arXiv, DOI DOI 10.48550/ARXIV.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Ritchie G, 2006, NEW GENERAT COMPUT, V24, P241, DOI 10.1007/BF03037334
   Saharia C, 2022, Arxiv, DOI [arXiv:2205.11487, 10.48550/arXiv.2205.11487]
   Schon D.A., 1992, DESIGN STUD, V13, P135, DOI DOI 10.1016/0142-694X(92)90268-F
   SCHON DA, 1992, KNOWL-BASED SYST, V5, P3, DOI 10.1016/0950-7051(92)90020-G
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Suwa M., 2000, DESIGN STUDIES, V21, P539, DOI [DOI 10.1016/S0142-694X(99)00034-4, 10.1016/S0142-694X(99)00034-4]
   Wiggins GA, 2006, KNOWL-BASED SYST, V19, P449, DOI 10.1016/j.knosys.2006.04.009
   Zhang L, 2023, Arxiv, DOI arXiv:2302.05543
NR 49
TC 2
Z9 2
U1 5
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5525
EP 5537
DI 10.1109/TVCG.2023.3293853
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400018
PM 37432831
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wu, MG
   Sun, YJ
   Jiang, SJ
AF Wu, Mingguang
   Sun, Yanjie
   Jiang, Shangjing
TI Adaptive Color Transfer From Images to Terrain Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Elevation colors; color transfer; terrain visualization
ID SEMANTIC DISCRIMINABILITY; DEPTH; MODEL
AB Terrain mapping is not only dedicated to communicating how high or steep a landscape is but can also help to indicate how we feel about a place. However, crafting effective and expressive elevation colors is challenging for both nonexperts and experts. In this article, we present a two-step image-to-terrain color transfer method that can transfer color from arbitrary images to diverse terrain models. First, we present a new image color organization method that organizes discrete, irregular image colors into a continuous, regular color grid that facilitates a series of color operations, such as local and global searching, categorical color selection and sequential color interpolation. Second, we quantify a series of cartographic concerns about elevation color crafting, such as the "lower, higher" principle, color conventions, and aerial perspectives. We also define color similarity between images and terrain visualizations with aesthetic quality. We then mathematically formulate image-to-terrain color transfer as a dual-objective optimization problem and offer a heuristic searching method to solve the problem. Finally, we compare elevation colors from our method with a standard color scheme and a representative color scale generation tool based on four test terrains. The evaluations show that the elevation colors from the proposed method are most effective and that our results are visually favorable. We also showcase that our method can transfer emotion from images to terrain visualizations.
C1 [Wu, Mingguang; Sun, Yanjie; Jiang, Shangjing] Nanjing Normal Univ, Coll Geog Sci, Nanjing 210023, Jiangsu, Peoples R China.
C3 Nanjing Normal University
RP Sun, YJ (corresponding author), Nanjing Normal Univ, Coll Geog Sci, Nanjing 210023, Jiangsu, Peoples R China.
EM wmg@njnu.edu.cn; yanjiesun@njnu.edu.cn; 211301018@njnu.edu.cn
RI Jiang, Shangjing/HGF-2021-2022
OI Jiang, Shangjing/0000-0002-6920-3901; Sun, Yanjie/0000-0002-7099-5133;
   Wu, Mingguang/0000-0001-6086-4503
FU National Natural Science Foundation of China [41971417, 41930104];
   Postgraduate Research &Practice Innovation Program of Jiangsu Province
   [KYCX22_1575, KYCX22_1559]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 41971417 and 41930104 and in part by
   the Postgraduate Research &Practice Innovation Program of Jiangsu
   Province under Grants KYCX22_1575 and KYCX22_1559
CR Anderson C. L., 2018, M.S. thesis,
   Anderson CL, 2022, IEEE T VIS COMPUT GR, V28, P2867, DOI 10.1109/TVCG.2021.3050118
   Bartram L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1364, DOI 10.1145/3025453.3026041
   Brassel K., 1974, The American Cartographer, V1, P15, DOI DOI 10.1559/152304074784107818
   Bratkova M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559759
   Brewer C. A., 2003, Cartography Geographic Inf. Sci., V30, P5, DOI 10.1559/152304003100010929
   Brewer C.A., 1993, Proceedings, Eleventh International Symposium on Computer-Assisted Cartography (Auto- Carto-11), Minneapolis, October/November, P328
   Bujack R, 2018, IEEE T VIS COMPUT GR, V24, P923, DOI 10.1109/TVCG.2017.2743978
   Casey EdwardS., 2005, Earth-Mapping: Artists Reshaping Landscape
   Clelandt M., 1937, A Practical Description of the Munsell Color System: WithSuggestions For Its Use
   Deb K, 2014, Search methodologies, P403, DOI [DOI 10.1007/978-1-4614-6940-7_15, 10.1007/978-1-4614-6940-7_15]
   EGUSA H, 1983, PERCEPTION, V12, P167, DOI 10.1068/p120167
   Eyton J.Ronald., 1990, Cartographica: The International Journal for Geographic Information and Geovisualization, V27, P20
   Frigo O, 2015, LECT NOTES COMPUT SC, V9005, P655, DOI 10.1007/978-3-319-16811-1_43
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Greenfield Gary., 2005, COMPUTATIONAL AESTHE
   Guibal CRC, 2004, PSYCHOL RES-PSYCH FO, V69, P30, DOI 10.1007/s00426-003-0167-0
   Häberling C, 2002, ISPRS J PHOTOGRAMM, V57, P134, DOI 10.1016/S0924-2716(02)00113-2
   Imhof E., 1982, CARTOGRAPHIC RELIEF
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jenny B., 2001, Cartographica, V38, P67, DOI [10.3138/F722-0825-3142-HW05, DOI 10.3138/F722-0825-3142-HW05]
   Jenny B, 2021, CARTOGR GEOGR INF SC, V48, P21, DOI 10.1080/15230406.2020.1813052
   Jenny B, 2021, IEEE T VIS COMPUT GR, V27, P1225, DOI 10.1109/TVCG.2020.3030456
   Jenny H, 2013, CARTOGR GEOGR INF SC, V40, P297, DOI 10.1080/15230406.2013.795001
   Jolivet J., 2009, P C GISCIENCE RES PO, P1
   Kennelly P. J., 2009, P INT CART C
   Kita N, 2016, COMPUT GRAPH FORUM, V35, P127, DOI 10.1111/cgf.13010
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Lin SR, 2013, COMPUT GRAPH FORUM, V32, P401, DOI 10.1111/cgf.12127
   Montello DR, 2018, HANDBOOK OF BEHAVIORAL AND COGNITIVE GEOGRAPHY, P177
   Moon P, 1944, J OPT SOC AM, V34, P46, DOI 10.1364/JOSA.34.000046
   Mukherjee K, 2022, IEEE T VIS COMPUT GR, V28, P697, DOI 10.1109/TVCG.2021.3114780
   Nardini P, 2021, IEEE T VIS COMPUT GR, V27, P3048, DOI 10.1109/TVCG.2019.2961674
   Neumann L., 2005, P COMPUTATIONAL AEST, P111
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Omkar SN, 2011, APPL SOFT COMPUT, V11, P489, DOI 10.1016/j.asoc.2009.12.008
   Ou LC, 2006, COLOR RES APPL, V31, P191, DOI 10.1002/col.20208
   Patterson T., 2001, Creating swiss-style shaded relief in photoshop
   Patterson Tom., 2011, Cartographic Perspectives, V69, P31
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Samsel F, 2021, 2021 IEEE VIS ARTS PROGRAM (VISAP 2021), P20, DOI 10.1109/VISAP52981.2021.00009
   Schloss KB, 2021, IEEE T VIS COMPUT GR, V27, P1022, DOI 10.1109/TVCG.2020.3030434
   Schloss KB, 2020, J OPT SOC AM A, V37, P813, DOI 10.1364/JOSAA.383588
   Schloss KB, 2011, ATTEN PERCEPT PSYCHO, V73, P551, DOI 10.3758/s13414-010-0027-0
   Schoenlein Melissa A, 2023, IEEE Trans Vis Comput Graph, V29, P385, DOI 10.1109/TVCG.2022.3209443
   Senanayake C. R., 2007, P BRIT MACH VIS C, P1
   Setlur V, 2016, IEEE T VIS COMPUT GR, V22, P698, DOI 10.1109/TVCG.2015.2467471
   Smart S, 2020, IEEE T VIS COMPUT GR, V26, P1215, DOI 10.1109/TVCG.2019.2934284
   Susanto Y, 2020, IEEE INTELL SYST, V35, P96, DOI 10.1109/MIS.2020.2992799
   Tsai YH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925942
   Wilms L, 2018, PSYCHOL RES-PSYCH FO, V82, P896, DOI 10.1007/s00426-017-0880-8
   Wu FZ, 2013, COMPUT GRAPH FORUM, V32, P190, DOI 10.1111/cgf.12008
   Wu MG, 2022, CARTOGR GEOGR INF SC, V49, P289, DOI 10.1080/15230406.2021.1982009
   Wu MG, 2019, CARTOGR J, V56, P161, DOI 10.1080/00087041.2018.1507182
   Yuan LP, 2022, IEEE T VIS COMPUT GR, V28, P4048, DOI 10.1109/TVCG.2021.3070876
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 58
TC 2
Z9 2
U1 12
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5538
EP 5552
DI 10.1109/TVCG.2023.3295122
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400035
PM 37440387
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhuang, JF
   Zeng, P
   Zhuang, W
   Guo, XY
   Liu, PZ
AF Zhuang, Jiafu
   Zeng, Pan
   Zhuang, Wei
   Guo, Xiaoyu
   Liu, Peizhong
TI Supervertex Sampling Network: A Geodesic Differential SLIC Approach for
   3D Mesh
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Clustering algorithms; Shape; Task analysis;
   Manifolds; Deep learning; Training; 3D mesh segmentation; 3D shape
   learning; deep clustering; graph neural network
ID SEGMENTATION
AB The analysis of 3D meshes with deep learning has become prevalent in computer graphics. As an essential structure, hierarchical representation is critical for mesh pooling in multiscale analysis. Existing clustering-based mesh hierarchy construction methods involve nonlinear discretization optimization operations, making them nondifferential and challenging to embed in other trainable networks for learning. Inspired by deep superpixel learning methods in image processing, we extend them from 2D images to 3D meshes by proposing a novel differentiable chart-based segmentation method named geodesic differential supervertex (GDSV). The key to the GDSV method is to ensure that the geodesic position updates are differentiable while satisfying the constraint that the renewed supervertices lie on the manifold surface. To this end, in addition to using the differential SLIC clustering algorithm to update the nonpositional features of the supervertices, a reparameterization trick, the Gumbel-Softmax trick, is employed to renew the geodesic positions of the supervertices. Therefore, the geodesic position update problem is converted into a linear matrix multiplication issue. The GDSV method can be an independent module for chart-based segmentation tasks. Meanwhile, it can be combined with the front-end feature learning network and the back-end task-specific network as a plug-in-plug-out module for training; and be applied to tasks such as shape classification, part segmentation, and 3D scene understanding. Experimental results show the excellent performance of our proposed algorithm on a range of datasets.
C1 [Zhuang, Jiafu] Quanzhou Normal Univ, Sch Phys & Informat Engn, Quanzhou 362000, Fujian, Peoples R China.
   [Zeng, Pan] Huaqiao Univ, Sch Med, Quanzhou 362021, Fujian, Peoples R China.
   [Zhuang, Wei] Univ Manchester, Dept Social Stat, Manchester M13 9PL, England.
   [Guo, Xiaoyu] Quanzhou Normal Univ, Coll Oceanol & Food Sci, Quanzhou 362000, Fujian, Peoples R China.
   [Liu, Peizhong] Huaqiao Univ, Coll Engn, Quanzhou 362021, Fujian, Peoples R China.
   [Liu, Peizhong] Quanzhou Med Coll, Sch Med, Quanzhou 362021, Fujian, Peoples R China.
C3 Quanzhou Normal University; Huaqiao University; University of
   Manchester; Quanzhou Normal University; Huaqiao University
RP Liu, PZ (corresponding author), Huaqiao Univ, Coll Engn, Quanzhou 362021, Fujian, Peoples R China.; Liu, PZ (corresponding author), Quanzhou Med Coll, Sch Med, Quanzhou 362021, Fujian, Peoples R China.
EM jfzhuang001@qq.com; 954181775@qq.com;
   wei.zhuang-2@postgrad.manchester.ac.uk; guoxiaoyu@qztc.edu.cn;
   pzliu@hqu.edu.cn
OI Zhuang, Wei/0000-0002-6992-4039; zhuang, jiafu/0000-0001-6072-2679; Liu,
   Peizhong/0000-0001-8785-0195; , Xiaoyu/0000-0001-7712-4149
FU Education and Scientific Research Project for Middle-Aged and Young
   Teachers of Fujian Province [JAT210302, JAT210314]; Innovation and
   Entrepreneurship Project for College Students [202110399024]; Fujian
   Provincial Science and Technology Major Project [2020HZ02014]; Fujian
   Province Science and Technology Program of China [2021H0053]; Quanzhou
   Science and Technology Major Project [2021GZ1]; Quanzhou City Science
   and Technology Program of China [2020C019R, 2021C031R]
FX No Statement Available
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Ben Izhak R, 2022, IEEE WINT CONF APPL, P2937, DOI 10.1109/WACV51458.2022.00299
   Boscaini D, 2016, ADV NEUR IN, V29
   Cangea C, 2018, Arxiv, DOI arXiv:1811.01287
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28
   Defferrard M, 2016, ADV NEUR IN, V29
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Fey M, 2019, Arxiv, DOI arXiv:1903.02428
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guo K, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2835487
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Hu RZ, 2012, COMPUT GRAPH FORUM, V31, P1703, DOI 10.1111/j.1467-8659.2012.03175.x
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   HU ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15468, DOI 10.1109/ICCV48922.2021.01520
   Huang JW, 2018, Arxiv, DOI arXiv:1802.01698
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Maddison CJ, 2017, Arxiv, DOI arXiv:1611.00712
   Jampani V, 2018, LECT NOTES COMPUT SC, V11211, P363, DOI 10.1007/978-3-030-01234-2_22
   Jiao X, 2023, COMPUT AIDED DESIGN, V160, DOI 10.1016/j.cad.2023.103512
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Kim J, 2007, INT CONF ACOUST SPEE, P429
   Lahav A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417806
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li XJ, 2022, LECT NOTES COMPUT SC, V13689, P541, DOI 10.1007/978-3-031-19818-2_31
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Liu YJ, 2016, PROC CVPR IEEE, P651, DOI 10.1109/CVPR.2016.77
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Meny J, 2021, COMPUT AIDED GEOM D, V89, DOI 10.1016/j.cagd.2021.102025
   Milano F., 2020, Adv. Neural Inf. Process. Syst, V33, P952
   Potamias RA, 2022, PROC CVPR IEEE, P18562, DOI 10.1109/CVPR52688.2022.01803
   Poulenard A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275102
   Purkait P, 2017, Arxiv, DOI arXiv:1712.03452
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiao YL, 2022, IEEE T VIS COMPUT GR, V28, P1317, DOI 10.1109/TVCG.2020.3014449
   Rakotosaona MJ, 2021, PROC CVPR IEEE, P22, DOI 10.1109/CVPR46437.2021.00009
   Rakotosaona MJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480554
   Ranjan E, 2020, AAAI CONF ARTIF INTE, V34, P5470
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   Schult J., 2020, P IEEECVF C COMPUTER, P8612, DOI 10.1109/CVPR42600.2020.00864
   Sethian JA, 2000, P NATL ACAD SCI USA, V97, P5699, DOI 10.1073/pnas.090060097
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Smirnov D., 2022, Ph.D. dissertation
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su JC, 2019, LECT NOTES COMPUT SC, V11131, P645, DOI 10.1007/978-3-030-11015-4_49
   Sun CY, 2023, COMPUT VIS MEDIA, V9, P229, DOI 10.1007/s41095-022-0281-9
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Toth C.D., 2017, Handbook of Discrete and Computational Geometry, VThird
   Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wiersma R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530166
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Yang FT, 2020, PROC CVPR IEEE, P13961, DOI 10.1109/CVPR42600.2020.01398
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Ying R, 2018, ADV NEUR IN, V31
   Zhu L, 2021, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR46437.2021.00128
NR 70
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5553
EP 5565
DI 10.1109/TVCG.2023.3294845
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400067
PM 37440384
DA 2024-11-06
ER

PT J
AU Ban, YK
   Ujitoko, Y
AF Ban, Yuki
   Ujitoko, Yusuke
TI Age and Gender Differences in the Pseudo-Haptic Effect on Computer Mouse
   Operation in a Desktop Environment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic interfaces; Mice; Visualization; Adaptive optics; Optical
   sensors; Cultural differences; Optical modulation; Haptics;
   pseudo-haptics; individual difference
ID SEX-DIFFERENCES; DEVELOPMENTAL-CHANGES; INFORMATION; VISION;
   INTEGRATION; PERCEPTION; FEEDBACK; ILLUSION
AB Pseudo-haptics is a method that can provide a haptic sensation without requiring a physical haptic device. The effect of pseudo-haptics is known to depend on the individual, but it is unclear which factors cause individual differences. As the first study establishing a calibration method for these differences in future research, we examined the differences in the pseudo-haptic effect on mouse cursor operation in a desktop environment depending on the age and gender of the user. We conducted an online experiment and collected data from more than 400 participants. The participants performed a task of lifting a virtual object with a mouse pointer. We found that the effect of pseudo-haptics was greater in younger or male participants than in older or female participants. We also found that the effect of pseudo-haptics, which varied with age and gender, can be explained by habituation to the mouse in daily life and the accuracy of detecting the pointer position using vision or proprioception. Specifically, the pseudo-haptic effect was higher for those who used the mouse more frequently and had higher accuracy in identifying the pointer position using proprioception or vision. The results of the present study not only indicate the factors that cause age and gender differences but also provide hints for calibrating these differences.
C1 [Ban, Yuki] Univ Tokyo, Grad Sch Frontier Sci, Tokyo 1138654, Japan.
   [Ujitoko, Yusuke] NTT Corp, NTT Commun Sci Labs, Tokyo 1008116, Japan.
C3 University of Tokyo; Nippon Telegraph & Telephone Corporation
RP Ban, YK (corresponding author), Univ Tokyo, Grad Sch Frontier Sci, Tokyo 1138654, Japan.
EM ban@edu.k.u-tokyo.ac.jp; yusuke.ujitoko@gmail.com
RI Ujitoko, Yusuke/AAV-2457-2021
OI Ban, Yuki/0000-0001-7349-6383; Ujitoko, Yusuke/0000-0001-6059-9324
FU JSPS KAKENHI [19K20315, 21H03478]
FX This work was supported by JSPS KAKENHI under Grants 19K20315 and
   21H03478. Recommended for acceptance by M. Marchal
CR ADAMS E, 1962, J PHILOS, V59, P177, DOI 10.2307/2023734
   Ariff G, 2002, J NEUROSCI, V22, P7721
   Ban Y, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P991, DOI 10.1109/WHC49131.2021.9517129
   Ban YK, 2018, IEEE HAPTICS SYM, P278, DOI 10.1109/HAPTICS.2018.8357188
   Ban Y, 2014, IEEE HAPTICS SYM, P557, DOI 10.1109/HAPTICS.2014.6775516
   Bertelsen AS, 2020, FOODS, V9, DOI 10.3390/foods9020146
   Bjorka H., 2007, P INT C EN INT
   Block HJ, 2020, MULTISENS RES, V34, P93, DOI 10.1163/22134808-bja10032
   Bremner AJ, 2016, CHILD DEV, V87, P962, DOI 10.1111/cdev.12511
   Declerck C, 2002, PERCEPT MOTOR SKILL, V94, P3
   DeLoach LJ, 1998, ANESTH ANALG, V86, P102, DOI 10.1097/00000539-199801000-00020
   Dewar R. E., 1967, Psychon. Sci., V9, P345
   Doherty MJ, 2010, DEVELOPMENTAL SCI, V13, P714, DOI 10.1111/j.1467-7687.2009.00931.x
   Dresslar FB., 1894, Amer.J.Psychol., V6, P313, DOI 10.2307/1411644
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   FRAISSE P, 1956, Q J EXP PSYCHOL, V8, P114, DOI 10.1080/17470215608416810
   Gepshtein S, 2003, CURR BIOL, V13, P483, DOI 10.1016/S0960-9822(03)00133-7
   Haaland D. L., 1993, Psychol.Aging, V8
   HANLEY C, 1965, CHILD DEV, V36, P437, DOI 10.2307/1126467
   Hisanaga S, 2016, SCI REP-UK, V6, DOI 10.1038/srep35265
   Honda T, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00760
   Horiguchi T, 2016, LECT NOTES COMPUT SC, V9753, P156, DOI 10.1007/978-3-319-39483-1_15
   Kashihara G., 2015, IEICE Trans., VJ98-D, P104
   Kawabe T, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.811881
   Knudson J. Woodland, 2012, Comprehensive Psychol., V1, P24
   Kokubun A, 2014, IEEE HAPTICS SYM, P415, DOI 10.1109/HAPTICS.2014.6775491
   Krishna A, 2008, J CONSUM RES, V34, P807, DOI 10.1086/523286
   KROSNICK JA, 1991, APPL COGNITIVE PSYCH, V5, P213, DOI 10.1002/acp.2350050305
   Lecuyer A., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P83, DOI 10.1109/VR.2000.840369
   Lécuyer A, 2001, P IEEE VIRT REAL ANN, P115, DOI 10.1109/VR.2001.913777
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Lecuyer J.-M., 2004, P P SIGCHI C HUMAN F, P239, DOI [10.1145/985692.985723, DOI 10.1145/985692.985723]
   LEIBOWITZ HW, 1967, AM J PSYCHOL, V80, P105, DOI 10.2307/1420548
   Maguinness C, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00795
   MASSARO DW, 1986, J EXP CHILD PSYCHOL, V41, P93, DOI 10.1016/0022-0965(86)90053-6
   Naceri A, 2011, PRESENCE-TELEOP VIRT, V20, P254, DOI 10.1162/PRES_a_00048
   Noelting G., 1961, La Structuration Progressive De La Figure De Muller-LyerEn Fonction De La Repetition Chez L'enfant Et L'adulte
   Nomoto A, 2016, PROCEEDINGS OF THE 7TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE (AUGMENTED HUMAN 2016), DOI 10.1145/2875194.2875216
   NYSSEN R, 1956, ACTA PSYCHOL, V12, P157, DOI 10.1016/0001-6918(56)90016-6
   Oppenheimer DM, 2009, J EXP SOC PSYCHOL, V45, P867, DOI 10.1016/j.jesp.2009.03.009
   Papageorgiou C, 2020, PHYSIOL MEAS, V41, DOI 10.1088/1361-6579/abb2eb
   Peck J, 2003, J CONSUM RES, V30, P430, DOI 10.1086/378619
   Piaget J., 2013, The mechanisms of perception
   Rietzler F., 2018, P CHI C HUM FACT COM, P1
   ROCK I, 1964, SCIENCE, V143, P594, DOI 10.1126/science.143.3606.594
   ROSS HE, 1987, NEUROPSYCHOLOGIA, V25, P841, DOI 10.1016/0028-3932(87)90122-9
   Samae M, 2019, BIOMED ENG INT CONF, DOI 10.1145/3290605.3300550
   Sandström CI, 1953, ACTA PSYCHOL, V9, P82, DOI 10.1016/0001-6918(53)90005-5
   SANTOSTEFANO S, 1963, Percept Mot Skills, V17, P23
   Schmidt L, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00915
   SEKIYAMA K, 1991, J ACOUST SOC AM, V90, P1797, DOI 10.1121/1.401660
   Sekiyama K, 2008, DEVELOPMENTAL SCI, V11, P306, DOI 10.1111/j.1467-7687.2008.00677.x
   Shaqiri A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-25298-8
   Sigmundsson H, 2007, SEX ROLES, V57, P181, DOI 10.1007/s11199-007-9228-y
   Skinner R. L., 1984, Clin. Orthopaedics Related Res., P51
   Skoura X, 2008, CORTEX, V44, P1271, DOI 10.1016/j.cortex.2007.07.008
   Spence C, 2011, ATTEN PERCEPT PSYCHO, V73, P971, DOI 10.3758/s13414-010-0073-7
   Taima Y, 2014, IEEE HAPTICS SYM, P175, DOI 10.1109/HAPTICS.2014.6775451
   Takamuku S, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2015.0864
   Ujitoko Y, 2021, IEEE T HAPTICS, V14, P699, DOI 10.1109/TOH.2021.3077619
   Ujitoko Y, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P181, DOI [10.1109/WHC.2019.8816100, 10.1109/whc.2019.8816100]
   Ujitoko Y, 2019, IEEE T VIS COMPUT GR, V25, P1981, DOI 10.1109/TVCG.2019.2898820
   van Mensvoort K., 2002, Proceedings Designing Interactive Systems, P345
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931
NR 65
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5566
EP 5580
DI 10.1109/TVCG.2023.3295389
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400077
PM 37450361
OA hybrid
DA 2024-11-06
ER

PT J
AU Sun, QW
   Nie, YW
   Zhang, Q
   Li, GQ
AF Sun, Qiwei
   Nie, Yongwei
   Zhang, Qing
   Li, Guiqing
TI Building Coarse to Fine Convex Hulls With Auxiliary Vertices for
   Palette-Based Image Recoloring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Point cloud compression; Image reconstruction;
   Image decomposition; Three-dimensional displays; Task analysis; Sun;
   Color palette; image recoloring; convex hull; mesh deformation
ID COLOR; MODEL
AB Constructing a convex hull for the pixel colors of an image by viewing them as 3D points can extract a set of palette colors for the image, then image recoloring can be achieved by modifying the palette colors. For better recoloring effect, the convex hull should contain more pixels (inclusive) and be more compact. Otherwise, reconstruction error would occur or the extracted palette color would be less representative, yielding wrong recoloring results or less effective edit. We observe that convex hulls constructed by prior methods can contain all the image pixels, but are far from compact. Efforts have been made to optimize the vertices of convex hull to increase the compactness but are still not perfect. In this paper, we propose a novel coarse to fine convex hull construction scheme with auxiliary vertices. We start by constructing a coarse convex hull whose vertices are directly image pixels which is thus the most compact but cannot contain all pixels. We then make a remedy by adding auxiliary vertices into the coarse convex hull to obtain a fine convex hull. More auxiliary vertices are added, more image pixels will be contained into the fine convex hull. The auxiliary vertices are image pixels too so that the compactness can still be maintained. During editing, the auxiliary vertices are not allowed to be edited for edit convenience, but deformed as-rigid-as-possible with the adjusting of other vertices. Our convex hull is both inclusive and compact. Extensive experiments validate the effectiveness of the proposed method.
C1 [Sun, Qiwei; Nie, Yongwei; Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Zhang, Qing] Sun Yat Sen Univ, Guangzhou 510275, Guangdong, Peoples R China.
C3 South China University of Technology; Sun Yat Sen University
RP Li, GQ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
EM qivigor@gmail.com; nieyongwei@scut.edu.cn; zhangq93@mail.sysu.edu.cn;
   ligq@scut.edu.cn
FU National Natural Science Foundation of China [61972160, 62072191]; NSF
   of Guangdong Province [2021A1515012301]
FX No Statement Available
CR Afifi M, 2021, PROC CVPR IEEE, P7937, DOI 10.1109/CVPR46437.2021.00785
   Aharoni-Mack Y., 2017, NPAR 17, P1
   Akimoto N, 2020, PROC CVPR IEEE, P8274, DOI 10.1109/CVPR42600.2020.00830
   Aksoy Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3002176
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   Bahng H, 2018, LECT NOTES COMPUT SC, V11216, P443, DOI 10.1007/978-3-030-01258-8_27
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Delon J, 2005, IEEE IMAGE PROC, P1317
   Delos B, 2019, Arxiv, DOI arXiv:1912.04583
   Du ZJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459675
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Garland P. S., 1997, SIGGRAPH 97, P209
   Greenfield GR, 2003, WSCG'2003, VOL 11, NO 1, CONFERENCE PROCEEDINGS, P189
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hu XH, 2019, IEEE T COMPUT IMAG, V5, P649, DOI 10.1109/TCI.2019.2908291
   Hu XH, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356534
   Huang HZ, 2014, COMPUT GRAPH FORUM, V33, P299, DOI 10.1111/cgf.12498
   Huang YF, 2018, COMPUT GRAPH FORUM, V37, P421, DOI 10.1111/cgf.13579
   Iwasa S, 2018, IEEE IMAGE PROC, P2257, DOI 10.1109/ICIP.2018.8451712
   Jeong T, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13811
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Kang JM, 2018, IEEE IMAGE PROC, P2252, DOI 10.1109/ICIP.2018.8451526
   Khodadadeh S, 2021, IEEE WINT CONF APPL, P1487, DOI 10.1109/WACV48630.2021.00153
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Kumar D., 2021, P 9 INT C LEARN REPR, P39
   Lei Y. Xing, 2020, ADV NEURAL INF PROCE, P1083
   Leung Y, 1997, IEEE T NEURAL NETWOR, V8, P601, DOI 10.1109/72.572099
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li ZQ, 2020, LECT NOTES COMPUT SC, V11961, P127, DOI 10.1007/978-3-030-37731-1_11
   Lin SR, 2017, Arxiv, DOI arXiv:1701.03754
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Pal S, 2007, IEEE T NEURAL NETWOR, V18, P600, DOI 10.1109/TNN.2007.891201
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ribeiro M, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329118
   Shen WY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925878
   Shugrina M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392461
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Tan J., 2016, ACM T GRAPHIC, V36, P1, DOI DOI 10.1145/2988229
   Tan JC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275054
   Tan JC, 2019, IEEE T VIS COMPUT GR, V25, P2791, DOI 10.1109/TVCG.2018.2858238
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang D, 2017, COMPUT GRAPH FORUM, V36, P93, DOI 10.1111/cgf.13275
   Wang YL, 2019, COMPUT GRAPH FORUM, V38, P11, DOI 10.1111/cgf.13812
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Xiao CF, 2020, COMPUT GRAPH FORUM, V39, P20, DOI 10.1111/cgf.13659
   Zhang Q, 2022, IEEE T MULTIMEDIA, V24, P1545, DOI 10.1109/TMM.2021.3067463
   Zhang Q, 2017, IEEE T IMAGE PROCESS, V26, P1952, DOI 10.1109/TIP.2017.2671779
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao NX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447647
   Zhu ZY, 2022, IEEE T MULTIMEDIA, V24, P1721, DOI 10.1109/TMM.2021.3070108
   Zhu ZY, 2021, VISUAL COMPUT, V37, P2999, DOI 10.1007/s00371-021-02240-0
NR 56
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5581
EP 5595
DI 10.1109/TVCG.2023.3296386
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400060
PM 37463085
DA 2024-11-06
ER

PT J
AU Fang, Y
   Li, MC
   Cao, YD
   Li, X
   Wolper, J
   Yang, Y
   Jiang, CFF
AF Fang, Yu
   Li, Minchen
   Cao, Yadi
   Li, Xuan
   Wolper, Joshuah
   Yang, Yin
   Jiang, Chenfanfu
TI Augmented Incremental Potential Contact for Sticky Interactions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adhesives; Friction; Computational modeling; Deformation; Solids; Force;
   Deformable models; Physically based animation; optimization time
   integration
ID FORMULATION; FRICTION; ADHESION
AB We introduce a variational formulation for simulating sticky interactions between elastoplastic solids. Our method brings a wider range of material behaviors into the reach of the Incremental Potential Contact (IPC) solver recently developed by (Li et al. 2020). Extending IPC requires several contributions. We first augment IPC with the classical Raous-Cangemi-Cocou (RCC) adhesion model. This allows us to robustly simulate the sticky interactions between arbitrary codimensional-0, 1, and 2 geometries. To enable user-friendly practical adoptions of our method, we further introduce a physically parametrized, easily controllable normal adhesion formulation based on the unsigned distance, which is fully compatible with IPC's barrier formulation. Furthermore, we propose a smoothly clamped tangential adhesion model that naturally models intricate behaviors including debonding. Lastly, we perform benchmark studies comparing our method with the classical models as well as real-world experimental results to demonstrate the efficacy of our method.
C1 [Fang, Yu; Wolper, Joshuah] Univ Penn, Philadelphia, PA 19104 USA.
   [Fang, Yu; Li, Minchen; Cao, Yadi; Li, Xuan; Jiang, Chenfanfu] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
   [Li, Minchen] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Yang, Yin] Univ Utah, Salt Lake City, UT 84112 USA.
C3 University of Pennsylvania; University of California System; University
   of California Los Angeles; Carnegie Mellon University; Utah System of
   Higher Education; University of Utah
RP Li, MC; Jiang, CFF (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
EM squarefk@gmail.com; minchernl@gmail.com; yadicao95@gmail.com;
   xuanli1@math.ucla.edu; joshwolper@gmail.com; yin.yang@utah.edu;
   chenfanfu.jiang@gmail.com
RI Li, Minchen/AAP-8184-2020; Fang, Yu/KFR-7871-2024
OI Cao, Yadi/0000-0001-8872-5759; Fang, Yu/0009-0002-7474-2421; Wolper,
   Joshuah/0000-0001-5226-8330; Li, Xuan/0000-0003-0677-8369; Jiang,
   Chenfanfu/0000-0003-3506-0583; Yang, Yin/0000-0001-7645-5931
FU NSF [2153851, 2153863, 2023780, 2301040, 2008915, 2244651, 2008564]
FX No Statement Available
CR Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Batty C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185609
   Belytschko T, 2014, Nonlinear finite elements for continua and structures
   Bergou M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778853
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Chen YQ, 2008, ACM T MATH SOFTWARE, V35, DOI 10.1145/1391989.1391995
   Chen YN, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530076
   Christensen PW, 1998, INT J NUMER METH ENG, V42, P145, DOI 10.1002/(SICI)1097-0207(19980515)42:1<145::AID-NME358>3.0.CO;2-L
   Clavet S., 2005, P 2005 ACM SIGGRAPH, P219, DOI DOI 10.1145/1073368.1073400
   DERJAGUIN BV, 1975, J COLLOID INTERF SCI, V53, P314, DOI 10.1016/0021-9797(75)90018-1
   Fang Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459757
   Fei Y., 2017, ACMTrans.Graph, V36, P1
   Fei Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356532
   Fei Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201392
   Ferguson Z, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459802
   Gascon J., 2010, Proc. Symp. Comp. Anim, P39
   Guennebaud G, 2010, Eigen v3
   Harmon D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531393
   Hu LB, 2022, COMPUT METHOD APPL M, V390, DOI 10.1016/j.cma.2021.114478
   Hu L. B., 2021, P 14 WCCMECCOMAS C P, DOI [10.23967/wccm-eccomas.2020.206ff.ffhal-03267975, DOI 10.23967/WCCM-ECCOMAS.2020.206FF.FFHAL-03267975]
   Israelachvili J., 2000, Tribology Ser., V38, P3, DOI 10.1016/S0167-8922(00)80107-8
   Jiang YP, 2022, Arxiv, DOI arXiv:2108.02080
   Jiang ZS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130895
   Johnson K.L., 1987, Contact mechanics
   Kaldor JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360664
   Kloosterman G, 2001, INT J NUMER METH ENG, V51, P865, DOI 10.1002/nme.209.abs
   Lan L, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530064
   Lan L, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530069
   Lan L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459753
   Leach Matthew., 2018, P C COMP GRAPH VIS C, P51
   Lennard-Jones JE, 1931, P PHYS SOC, V43, P461, DOI 10.1088/0959-5309/43/5/301
   Li J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201308
   Li M., 2020, Robust and accurate simulation of elastodynamics and contact
   Li MC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459767
   Li MC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392425
   Li MC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322951
   Li X., 2021, Comput. Methods Appl. Mechanics Eng, V390
   Li X, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530072
   Ly M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392396
   Nocedal J., 1999, Numerical optimization
   Ortiz M, 1999, COMPUT METHOD APPL M, V171, P419, DOI 10.1016/S0045-7825(98)00219-9
   Otaduy MA, 2009, COMPUT GRAPH FORUM, V28, P559, DOI 10.1111/j.1467-8659.2009.01396.x
   Paggi M, 2020, MECH ADV MATER STRUC, V27, P1731, DOI 10.1080/15376494.2018.1525454
   Raous M, 1999, COMPUT METHOD APPL M, V177, P383, DOI 10.1016/S0045-7825(98)00389-2
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Schüller C, 2013, COMPUT GRAPH FORUM, V32, P125, DOI 10.1111/cgf.12179
   Smith J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766947
   Sonnerlind H., 2016, How to model adhesion and decohesion in COMSOL multiphysics
   Yu N, 2004, J COLLOID INTERF SCI, V278, P428, DOI 10.1016/j.jcis.2004.06.029
   Zhao YD, 2022, COMPUT METHOD APPL M, V393, DOI 10.1016/j.cma.2022.114820
NR 50
TC 2
Z9 2
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5596
EP 5608
DI 10.1109/TVCG.2023.3295656
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400051
PM 37450362
DA 2024-11-06
ER

PT J
AU Schier, F
   Zeidler, D
   Chandran, K
   Yu, ZY
   Mcginity, M
AF Schier, Florian
   Zeidler, Daniel
   Chandran, Krishnan
   Yu, Zhongyuan
   Mcginity, Matthew
TI ViewR: Architectural-Scale Multi-User Mixed Reality With Mobile
   Head-Mounted Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; Mixed reality; Visualization; Collaboration;
   Head-mounted displays; Cameras; Resists; Visualization systems; mixed /
   augmented reality; virtual reality; collaborative systems; co-located
   systems
AB The emergence of mobile head-mounted displays with robust "inside-out" markerless tracking and video-passthrough permits the creation of novel mixed reality (MR) experiences in which architectural spaces of arbitrary size can be transformed into immersive multi-user visualisation arenas. Here we outline ViewR, an open-source framework for rapidly constructing and deploying architectural-scale multi-user MR experiences. ViewR includes tools for rapid alignment of real and virtual worlds, tracking loss detection and recovery, user trajectory visualisation and world state synchronisation between users with persistence across sessions. ViewR also provides control over the blending of the real and the virtual, specification of site-specific blending zones, and video-passthrough avatars, allowing users to see and interact with one another directly. Using ViewR, we explore the transformation of large architectural structures into immersive arenas by creating a range of experiences in various locations, with a particular focus on architectural affordances such as mezzanines, stairs, gangways and elevators. Our tests reveal that ViewR allows for experiences that would not be possible with pure virtual reality, and indicate that, with certain strategies for recovering from tracking errors, it is possible to construct large scale multi-user MR experiences using contemporary consumer virtual reality head-mounted displays.
C1 [Schier, Florian; Zeidler, Daniel; Chandran, Krishnan; Yu, Zhongyuan; Mcginity, Matthew] Tech Univ Dresden, Fac Comp Sci, D-01062 Dresden, Germany.
C3 Technische Universitat Dresden
RP Schier, F (corresponding author), Tech Univ Dresden, Fac Comp Sci, D-01062 Dresden, Germany.
EM florian.schier@tu-dresden.de; daniel.zeidler@tu-dresden.de;
   krishnan.chandran@tu-dresden.de; zhongyuan.yu@tu-dresden.de;
   matthew.mcginity@tu-dresden.de
OI McGinity, Matthew/0000-0002-8923-6284; Peringottukurussi Chandran,
   Krishnan/0000-0002-5718-7043; Schier, Florian/0000-0002-3611-8719; Yu,
   Zhongyuan/0000-0002-3671-1619; Zeidler, Daniel/0000-0003-4954-4896
FU Else Kroner Fresenius Center for Digital Health [612,024]
FX This work was supported by Else Kroner Fresenius Center for Digital
   Health under Grant # 612,024.
CR Abdlkarim D, 2024, BEHAV RES METHODS, V56, P1052, DOI 10.3758/s13428-022-02051-8
   Anhong Guo, 2019, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V3, DOI 10.1145/3351241
   Billinghurst M., 1998, Virtual Reality, V3, P25, DOI 10.1007/BF01409795
   Carnevale A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155511
   Cavallo M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P145, DOI [10.1109/vr.2019.8797733, 10.1109/VR.2019.8797733]
   de Belen R. A. J., 2019, AIMS ELECT ELECT ENG, V3, P181
   Fiala M, 2010, IEEE T PATTERN ANAL, V32, P1317, DOI 10.1109/TPAMI.2009.146
   Friston S, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489871
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   GIBSON JJ, 1978, LEONARDO, V11, P227, DOI 10.2307/1574154
   Gruenefeld U, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501821
   He WN, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P255, DOI 10.1109/VR50410.2021.00047
   He ZY, 2020, INT SYM MIX AUGMENT, P542, DOI 10.1109/ISMAR50242.2020.00082
   Holzwarth Valentin, 2021, ICVARS 2021: 2021 the 5th International Conference on Virtual and Augmented Reality Simulations, P42, DOI 10.1145/3463914.3463921
   Jinyu L., 2019, VIRTUAL REALITY INTE, V1, P386, DOI [10.1016/j.vrih.2019.07.002, DOI 10.1016/J.VRIH.2019.07.002]
   KABSCH W, 1976, ACTA CRYSTALLOGR A, V32, P922, DOI 10.1107/S0567739476001873
   Krokos E, 2019, VIRTUAL REAL-LONDON, V23, P1, DOI 10.1007/s10055-018-0346-3
   Lee B, 2021, IEEE T VIS COMPUT GR, V27, P1171, DOI 10.1109/TVCG.2020.3030450
   Luo WZ, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501946
   Barros AM, 2022, ROBOTICS, V11, DOI 10.3390/robotics11010024
   Mahmood T, 2019, INT SYM MIX AUGMENT, P236, DOI 10.1109/ISMAR.2019.00021
   Maier JRA, 2009, DESIGN STUD, V30, P393, DOI 10.1016/j.destud.2009.01.002
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   Miller J, 2020, INT J ADV MANUF TECH, V109, P1741, DOI 10.1007/s00170-020-05768-y
   Mulvany G., 2020, ser. CHI PLAY, V20, P321, DOI [10.1145/3383668.3419875, DOI 10.1145/3383668.3419875]
   Pan Y, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P759, DOI [10.1109/VRW50115.2020.00-45, 10.1109/VRW50115.2020.00230]
   Pereira N, 2021, INT SYM MIX AUGMENT, P479, DOI 10.1109/ISMAR52148.2021.00065
   Radu Iulian, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3449243
   Reimer D, 2021, COMPUTERS, V10, DOI 10.3390/computers10050058
   Rompapas D. C., 2019, Ph.D. dissertation
   Schier F, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P274, DOI 10.1109/VRW55335.2022.00064
   Servières M, 2021, J SENSORS, V2021, DOI 10.1155/2021/2054828
   Szalavari Z., 1998, Virtual Reality, V3, P37, DOI 10.1007/BF01409796
   Zafari F, 2019, IEEE COMMUN SURV TUT, V21, P2568, DOI 10.1109/COMST.2019.2911558
NR 34
TC 2
Z9 2
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5609
EP 5622
DI 10.1109/TVCG.2023.3299781
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400044
PM 37549094
OA hybrid
DA 2024-11-06
ER

PT J
AU Willett, NS
   de Goes, F
   Fleischer, K
   Meyer, M
   Burrows, C
AF Willett, Nora S.
   de Goes, Fernando
   Fleischer, Kurt
   Meyer, Mark
   Burrows, Chris
TI Stylizing Ribbons: Computing Surface Contours With Temporally Coherent
   Orientations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Cameras; Animation; Geometry; Surface
   fitting; Strips; Point cloud compression; Line work; non-photorealistic
   rendering; stylization; surface contours; silhouettes; shadow edges
AB Line work is a core element for the stylization of computer animations used by recent shows. However, existing stylization techniques are limited to edge treatments based on brush strokes or textures applied solely on top of curves. In this work, we propose new stylization effects by offering artists direct control over the inside and outside of surface contours. To this end, we introduce a method that creates ribbons, geometry strips of possibly varying width, that extrude from each side of the surface contour with temporally coherent orientations. Our contributions include the generation of spatially and temporally consistent normal orientations along visible contours and a trimming routine that converts arrangements of offset curves into ribbons free of intersections. We demonstrate the expressiveness and versatility of stylized ribbons by applying various effects on both character and shadow edges from animation sequences.
C1 [Willett, Nora S.; de Goes, Fernando; Fleischer, Kurt; Meyer, Mark; Burrows, Chris] Pixar Animat Studios Emeryville, Emeryville, CA 94608 USA.
RP Willett, NS (corresponding author), Pixar Animat Studios Emeryville, Emeryville, CA 94608 USA.
EM noraw@pixar.com; fernando@pixar.com; kurt@pixar.com; mmeyer@pixar.com;
   cburrows@pixar.com
OI Willett, Nora/0000-0002-3932-2758; de Goes, Fernando/0009-0007-7331-0674
CR Amenta N, 1998, GRAPH MODEL IM PROC, V60, P125, DOI 10.1006/gmip.1998.0465
   Asente P. J., 2010, P 7 SKETCH BAS INT M, P33
   B'enard P., 2010, International Symposium on Non-Photorealistic Animation and Rendering (NPAR), P91, DOI DOI 10.1145/1809939.1809950
   Barill G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201337
   Ben-Zvi N, 2016, COMPUT GRAPH FORUM, V35, P18, DOI 10.1111/cgf.12729
   Benard P., 2012, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, P37
   Bénard P, 2019, FOUND TRENDS COMPUT, V11, P1, DOI 10.1561/0600000075
   Bénard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929
   Bénard P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2558307
   Bessmeltsev M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3202661
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Buchholz B, 2011, P ACM SIGGRAPH EUR S, P85, DOI DOI 10.1145/2024676.2024690
   Chlumsky V, 2018, COMPUT GRAPH FORUM, V37, P273, DOI 10.1111/cgf.13265
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   Coleman P., 2020, P ACM SIGGRAPH TALKS, P1
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   DeCarlo D, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P63
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   DeCoro C, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P77
   Dimian D., 2019, P ACM SIGGRAPH COMP, P1
   Docter K., 2020, Soul
   Eisemann E, 2008, COMPUT GRAPH FORUM, V27, P1199, DOI 10.1111/j.1467-8659.2008.01258.x
   Grabli S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731056
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Hertzmann A, 2020, PERCEPTION, V49, P439, DOI 10.1177/0301006620908207
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hornung A., 2006, SGP 06, P41, DOI DOI 10.2312/SGP/SGP06/041-050
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Hunter S., 2020, Out
   Jacobson A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461916
   Jamriska O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323006
   Jiang G., 2022, EUROGRAPHICS S RENDE
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Kahrs J, 2012, Paperman
   Kalnins RD, 2003, ACM T GRAPHIC, V22, P856, DOI 10.1145/882262.882355
   Karsch K., 2011, P S NONPH AN REND, P35
   Kilgard MJ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392458
   Lengyel J., 2001, P 2001 S INTERACTIVE, P227, DOI [10.1145/364338.364407, DOI 10.1145/364338.364407]
   Lenssen JE, 2020, PROC CVPR IEEE, P11244, DOI 10.1109/CVPR42600.2020.01126
   Liu CX, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3544778
   Liu DF, 2020, PROC CVPR IEEE, P5427, DOI 10.1109/CVPR42600.2020.00547
   Liu E., 2018, ACM Trans. Graph., V37, P1
   Liu M., 2021, P IEEE CVF INT C COM
   Liu XT, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818067
   Metzer G, 2021, Arxiv, DOI arXiv:2105.01604
   Müller AC, 2014, J MACH LEARN RES, V15, P2055
   Mullen P, 2010, COMPUT GRAPH FORUM, V29, P1733, DOI 10.1111/j.1467-8659.2010.01782.x
   Nehab D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392392
   Northrup L., 2000, Proceedings of the 1st International Symposium on Non-Photorealistic Animation and Rendering, P31, DOI DOI 10.1145/340916.340920
   Pablos S., 2019, Klaus
   Persichetti P. Ramsey, 2018, Spider-Man: Into the Spider-Verse
   Postolski M, 2014, COMPUT VIS IMAGE UND, V129, P89, DOI 10.1016/j.cviu.2014.07.003
   Simo-Serra S., 2016, ACM Trans.Graph., V35, P1, DOI DOI 10.1145/2897824.2925972
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Todo H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276399
   Whited E., 2012, ACM SIGGRAPH TALKS, P1
   Zheng M., 2017, P S NONPH AN REND, P1
NR 57
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5623
EP 5634
DI 10.1109/TVCG.2023.3304641
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400096
PM 37578913
OA hybrid
DA 2024-11-06
ER

PT J
AU In, S
   Lin, T
   North, C
   Pfister, H
   Yang, YL
AF In, Sungwon
   Lin, Tica
   North, Chris
   Pfister, Hanspeter
   Yang, Yalong
TI This is the Table I Want! Interactive Data Transformation on Desktop and
   in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data science; data transformation; empirical study; immersive analytics;
   interaction; virtual/augmented/mixed reality
ID DATA VISUALIZATION; EXPLORATION; FRAMEWORK
AB Data transformation is an essential step in data science. While experts primarily use programming to transform their data, there is an increasing need to support non-programmers with user interface-based tools. With the rapid development in interaction techniques and computing environments, we report our empirical findings about the effects of interaction techniques and environments on performing data transformation tasks. Specifically, we studied the potential benefits of direct interaction and virtual reality (VR) for data transformation. We compared gesture interaction versus a standard WIMP user interface, each on the desktop and in VR. With the tested data and tasks, we found time performance was similar between desktop and VR. Meanwhile, VR demonstrates preliminary evidence to better support provenance and sense-making throughout the data transformation process. Our exploration of performing data transformation in VR also provides initial affirmation for enabling an iterative and fully immersive data science workflow.
C1 [In, Sungwon; North, Chris; Yang, Yalong] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24060 USA.
   [Yang, Yalong] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30309 USA.
   [Lin, Tica; Pfister, Hanspeter] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 30309 USA.
C3 Virginia Polytechnic Institute & State University; University System of
   Georgia; Georgia Institute of Technology; Harvard University
RP Yang, YL (corresponding author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24060 USA.
EM sungwoni@vt.edu; mlin@g.harvard.edu; north@cs.vt.edu;
   pfister@g.harvard.edu; yalong.yang@gatech.edu
RI North, Chris/T-6465-2019
OI Pfister, Hanspeter/0000-0002-3620-2582; Yang,
   Yalong/0000-0001-9414-9911; In, Sungwon/0000-0002-5316-2922; Lin,
   Tica/0000-0002-2860-0871
FU NSF I/UCRC via the NSF Center for Space, High-performance, and Resilient
   Computing (SHREC) [CNS-1822080]; NSF [III-2107328]
FX This work was supported in part by the NSF I/UCRC under Grant
   CNS-1822080 via the NSF Center for Space, High-performance, and
   Resilient Computing (SHREC) and in part by the NSF under Grant
   III-2107328.
CR Altair, 2022, US
   Amazon, 2022, AWS AUTOML SOLUTIONS
   Andrews C, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P55
   Arns L, 1999, P IEEE VIRT REAL ANN, P88, DOI 10.1109/VR.1999.756938
   Bach B, 2018, IEEE T VIS COMPUT GR, V24, P457, DOI 10.1109/TVCG.2017.2745941
   Badam SK, 2016, IEEE CONF VIS ANAL, P1, DOI 10.1109/VAST.2016.7883506
   Ball R, 2005, LECT NOTES COMPUT SC, V3585, P350, DOI 10.1007/11555261_30
   Ball R., 2005, CHI'05 extended abstracts on Human factors in computing systems, P1196
   Ball R, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P191
   Batch A, 2020, IEEE T VIS COMPUT GR, V26, P536, DOI 10.1109/TVCG.2019.2934803
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Bau O, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P37, DOI 10.1145/1449715.1449724
   Burley C. J., 2019, P C INN DAT SYST RES
   Butcher PWS, 2021, IEEE T VIS COMPUT GR, V27, P3213, DOI 10.1109/TVCG.2020.2965109
   Chen J, 2012, IEEE T VIS COMPUT GR, V18, P2130, DOI 10.1109/TVCG.2012.216
   Cockburn A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2659796
   Cordeil M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376613
   Cordeil M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P200, DOI [10.1109/vr.2019.8797978, 10.1109/VR.2019.8797978]
   Cordeil M, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P71, DOI 10.1145/3126594.3126613
   Crotty A, 2015, PROC VLDB ENDOW, V8, P2025
   Davidson K, 2023, IEEE T VIS COMPUT GR, V29, P5294, DOI 10.1109/TVCG.2022.3207357
   Desktop T., 2022, TABLEAU DESKTOP
   Drey T, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376628
   Drosos I, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20)
   Drucker S.M., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '13, P2301
   Dube TJ, 2019, IN SY AP IN WE HC, V11567, P419, DOI 10.1007/978-3-030-22643-5_33
   Elmqvist N, 2007, IEEE CONF VIS ANAL, P187, DOI 10.1109/VAST.2007.4389013
   Ens Barrett, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3446866
   Field A., 2018, DISCOVERING STAT USI
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Google, 2022, ABOUT US
   Google, 2020, JAMBOARD
   Guo P. J., 2011, P 24 ANN ACM S US IN, P65, DOI 10.1145/2047196.2047205
   Hayatpur D., 2020, P 33 ANN ACM S US IN, P818, DOI DOI 10.1145/3379337.3415878
   Horak T, 2021, IEEE T VIS COMPUT GR, V27, P1644, DOI 10.1109/TVCG.2020.3030371
   Jakobsen MR, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1451
   Javed W, 2013, COMPUT GRAPH FORUM, V32, P441, DOI 10.1111/cgf.12131
   JetBrains, 2020, DATALORE
   Jiang LL, 2013, PROC VLDB ENDOW, V6, P1342, DOI 10.14778/2536274.2536311
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Jiazhou Liu, 2022, Proceedings of the ACM on Human-Computer Interaction, DOI 10.1145/3567729
   Kandel S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3363
   Kandel S, 2011, INFORM VISUAL, V10, P271, DOI 10.1177/1473871611415994
   Kasica S, 2021, IEEE T VIS COMPUT GR, V27, P957, DOI 10.1109/TVCG.2020.3030462
   Kirschthaler Philipp, 2020, CUI '20: Proceedings of the 2nd Conference on Conversational User Interfaces, DOI 10.1145/3405755.3406119
   Kodagoda Neesha, 2013, IEEE Trans Vis Comput Graph, V19, P2217, DOI 10.1109/TVCG.2013.211
   Kraska T, 2018, PROC VLDB ENDOW, V11, P2150, DOI 10.14778/3229863.3240493
   Kwon OH, 2016, IEEE T VIS COMPUT GR, V22, P1802, DOI 10.1109/TVCG.2016.2520921
   Lages WS, 2019, PROCEEDINGS OF IUI 2019, P356, DOI 10.1145/3301275.3302278
   Lee B, 2021, IEEE T VIS COMPUT GR, V27, P1171, DOI 10.1109/TVCG.2020.3030450
   Lekschas F, 2021, IEEE T VIS COMPUT GR, V27, P358, DOI 10.1109/TVCG.2020.3028948
   Lenth RV, 2016, J STAT SOFTW, V69, P1, DOI 10.18637/jss.v069.i01
   Lisle L, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P331, DOI [10.1109/VRW50115.2020.0-203, 10.1109/VRW50115.2020.00073]
   Lisle L, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P529, DOI 10.1109/VR50410.2021.00077
   Liu JZ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P588, DOI [10.1109/VR46266.2020.00-23, 10.1109/VR46266.2020.1581122519414]
   Luo WZ, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501946
   Marrinan T, 2014, 2014 INTERNATIONAL CONFERENCE ON COLLABORATIVE COMPUTING: NETWORKING, APPLICATIONS AND WORKSHARING (COLLABORATECOM), P177, DOI 10.4108/icst.collaboratecom.2014.257337
   Marriott K., 2018, IMMERSIVE ANAL, V11190, DOI 10/kt9x
   McKinney W., 2010, P 9 PYTH SCI C SCIPY, P51
   Mendes D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P261, DOI 10.1145/2993369.2993396
   Microsoft, 2020, WHITEBOARD
   Miro, 2020, MIRO
   Nacenta Miguel A, 2013, P SIGCHI C HUM FACT, P1099, DOI DOI 10.1145/2470654.2466142
   Nandi A., 2013, P HUM FACT COMP SYST, P1203
   Nandi A, 2013, PROC VLDB ENDOW, V7, P289, DOI 10.14778/2732240.2732247
   Narechania A, 2021, IEEE T VIS COMPUT GR, V27, P369, DOI 10.1109/TVCG.2020.3030378
   Pavanatto L, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P759, DOI 10.1109/VR50410.2021.00103
   Pirolli P., 1996, P WORKSH ADV VIS INT, P67
   Purchase H. C., 2006, Graph Drawing. 14th International Symposium, GD 2006. Revised Papers (Lecture Notes in Computer Science Vol. 4372), P184
   Raman V., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P381
   Reda K, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2759, DOI 10.1145/2702123.2702406
   Rzeszotarski JM, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P897, DOI 10.1145/2556288.2557231
   Saket B, 2020, IEEE T VIS COMPUT GR, V26, P482, DOI 10.1109/TVCG.2019.2934534
   Saket B, 2018, IEEE T VIS COMPUT GR, V24, P1316, DOI 10.1109/TVCG.2017.2680452
   Sarvghad A, 2019, IEEE T VIS COMPUT GR, V25, P800, DOI 10.1109/TVCG.2018.2865075
   Satriadi Kadek Ananta, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3427329
   Satriadi KA, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517715
   Shang ZY, 2021, PROC VLDB ENDOW, V14, P2893, DOI 10.14778/3476311.3476370
   Shen LX, 2023, IEEE T VIS COMPUT GR, V29, P3121, DOI 10.1109/TVCG.2022.3148007
   Shneiderman B, 2000, IEEE INFOR VIS, P88, DOI 10.1109/IV.2000.859742
   Sicat R, 2019, IEEE T VIS COMPUT GR, V25, P715, DOI 10.1109/TVCG.2018.2865152
   Simmhan YL, 2005, SIGMOD REC, V34, P31, DOI 10.1145/1084805.1084812
   T. P. Builder, 2022, US
   Trifacta, 2022, US
   Wagner JA, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P483, DOI 10.1109/VR.2018.8447558
   Wang D., 2021, P CHI C HUM FACT COM, P1
   Wang JW, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P2509, DOI 10.1109/BigData.2015.7364047
   Wang L., 2020, P CHI C HUM FACT COM, P1
   Wickham H., 2022, Tidyr: tidy messy data
   Wickham H., 2022, Dplyr: A Grammar of Data Manipulation
   Wickham Hadley, 2016, R for data science: import, tidy, transform, visualize, and model data
   Wright W., 2006, Conference on Human Factors in Computing Systems. CHI2006, P801
   Xiong K, 2023, IEEE T VIS COMPUT GR, V29, P2950, DOI 10.1109/TVCG.2022.3144975
   Yang FM, 2021, IEEE T VIS COMPUT GR, V27, P4359, DOI 10.1109/TVCG.2020.3009003
   Yang YL, 2021, IEEE T VIS COMPUT GR, V27, P1214, DOI 10.1109/TVCG.2020.3030427
   Yang YL, 2021, IEEE T VIS COMPUT GR, V27, P4507, DOI 10.1109/TVCG.2020.3004137
   Zagermann J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1899, DOI 10.1145/3025453.3026001
   Zhou G., 2022, P CHI C HUM FACT COM, P1
NR 98
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5635
EP 5650
DI 10.1109/TVCG.2023.3299602
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400089
PM 37506003
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Prasad, V
   van Sloun, RJG
   Vilanova, A
   Pezzotti, N
AF Prasad, Vidya
   van Sloun, Ruud J. G.
   Vilanova, Anna
   Pezzotti, Nicola
TI ProactiV: Studying Deep Learning Model Behavior Under Input
   Transformations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; explainable AI; deep learning; domain shifts; model
   behavior; robustness; image-to-image translation
ID INTERACTIVE ANALYSIS
AB Deep learning (DL) models have shown performance benefits across many applications, from classification to image-to-image translation. However, low interpretability often leads to unexpected model behavior once deployed in the real world. Usually, this unexpected behavior is because the training data domain does not reflect the deployment data domain. Identifying a model's breaking points under input conditions and domain shifts, i.e., input transformations, is essential to improve models. Although visual analytics (VA) has shown promise in studying the behavior of model outputs under continually varying inputs, existing methods mainly focus on per-class or instance-level analysis. We aim to generalize beyond classification where classes do not exist and provide a global view of model behavior under co-occurring input transformations. We present a DL model-agnostic VA method (ProactiV) to help model developers proactively study output behavior under input transformations to identify and verify breaking points. ProactiV relies on a proposed input optimization method to determine the changes to a given transformed input to achieve the desired output. The data from this optimization process allows the study of global and local model behavior under input transformations at scale. Additionally, the optimization method provides insights into the input characteristics that result in desired outputs and helps recognize model biases. We highlight how ProactiV effectively supports studying model behavior with example classification and image-to-image translation tasks.
C1 [Prasad, Vidya; Vilanova, Anna; Pezzotti, Nicola] Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
   [van Sloun, Ruud J. G.] Philips Res, NL-5656 AE Eindhoven, Netherlands.
   [van Sloun, Ruud J. G.] Eindhoven Univ Technol, Dept Elect Engn, NL-5612 AZ Eindhoven, Netherlands.
   [Pezzotti, Nicola] Philips Med Syst, NL-5656 AE Eindhoven, Netherlands.
C3 Eindhoven University of Technology; Philips; Philips Research; Eindhoven
   University of Technology; Philips; Philips Healthcare
RP Prasad, V (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
EM v.prasad@tue.nl; r.j.g.v.sloun@tue.nl; a.vilanova@tue.nl;
   n.pezzotti@tue.nl
RI van Sloun, Ruud/AGX-0436-2022
OI van Sloun, Ruud JG/0000-0003-2845-0495; Prasad,
   Vidya/0000-0002-9296-3693; Vilanova, Anna/0000-0002-1034-737X
CR Alsallakh B, 2018, IEEE T VIS COMPUT GR, V24, P152, DOI 10.1109/TVCG.2017.2744683
   Barbastathis G, 2019, OPTICA, V6, P921, DOI 10.1364/OPTICA.6.000921
   Barbu A, 2019, ADV NEUR IN, V32
   Buongiorno D, 2021, NEUROCOMPUTING, V452, P549, DOI 10.1016/j.neucom.2020.06.139
   Cabrera AA, 2019, IEEE CONF VIS ANAL, P46, DOI [10.1109/VAST47406.2019.8986948, 10.1109/vast47406.2019.8986948]
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen CJ, 2021, IEEE T VIS COMPUT GR, V27, P3335, DOI 10.1109/TVCG.2020.2973258
   Cohen G, 2017, Arxiv, DOI arXiv:1702.05373
   Cohen TS, 2016, PR MACH LEARN RES, V48
   D'Amour A, 2020, Arxiv, DOI [arXiv:2011.03395, DOI 10.48550/ARXIV.2011.03395]
   Darestani MZ, 2021, PR MACH LEARN RES, V139
   Das N, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P271, DOI 10.1109/VIS47514.2020.00061
   Drenkow N, 2022, Arxiv, DOI arXiv:2112.00639
   Erhan D., 2009, Tech. Rep. Univ. Montr
   Geirhos R, 2020, NAT MACH INTELL, V2, P665, DOI 10.1038/s42256-020-00257-z
   Gou L, 2021, IEEE T VIS COMPUT GR, V27, P261, DOI 10.1109/TVCG.2020.3030350
   Guo Z., 2020, ADV NEURAL INF PROCE, V33, P21271
   He WB, 2022, IEEE T VIS COMPUT GR, V28, P1040, DOI 10.1109/TVCG.2021.3114855
   Hendrycks D., 2019, P INT C LEARN REPR
   Huang JB, 2022, Arxiv, DOI arXiv:2204.01888
   Hui L., 2021, P INT C LEARN REPR
   Huynh T, 2022, IEEE WINT CONF APPL, P986, DOI 10.1109/WACV51458.2022.00106
   Jacobsen J., 2019, P INT C LEARN REPR
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kabir H. M. Dipu, 2023, IEEE Transactions on Artificial Intelligence, P1165, DOI 10.1109/TAI.2022.3185179
   Kastryulin S, 2023, IEEE ACCESS, V11, P14154, DOI 10.1109/ACCESS.2023.3243466
   Kaul S, 2022, IEEE T VIS COMPUT GR, V28, P998, DOI 10.1109/TVCG.2021.3114779
   Kobyzev I, 2021, IEEE T PATTERN ANAL, V43, P3964, DOI 10.1109/TPAMI.2020.2992934
   Koh PW, 2021, PR MACH LEARN RES, V139
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   La Rosa B, 2023, COMPUT GRAPH FORUM, V42, P319, DOI 10.1111/cgf.14733
   Li QF, 2020, PROC CVPR IEEE, P7243, DOI 10.1109/CVPR42600.2020.00727
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Nichol A, 2022, PR MACH LEARN RES
   Olson ML, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P36, DOI 10.1109/VIS49827.2021.9623289
   Otter DW, 2021, IEEE T NEUR NET LEAR, V32, P604, DOI 10.1109/TNNLS.2020.2979670
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Park C., 2021, P EUR C VIS
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Pei KX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P1, DOI 10.1145/3132747.3132785
   Pezzotti N, 2020, IEEE T VIS COMPUT GR, V26, P1172, DOI 10.1109/TVCG.2019.2934307
   Prasad V, 2024, IEEE T VIS COMPUT GR, V30, P1502, DOI 10.1109/TVCG.2022.3219248
   Ren PZ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447582
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saunshi N, 2022, PR MACH LEARN RES, P19250
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Sietzen S, 2021, COMPUT GRAPH FORUM, V40, P253, DOI 10.1111/cgf.14418
   Sriram Anuroop, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12262), P64, DOI 10.1007/978-3-030-59713-9_7
   Suo JL, 2021, Arxiv, DOI arXiv:2109.08880
   Tang H, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851881
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tian YC, 2018, PROCEEDINGS 2018 IEEE/ACM 40TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P303, DOI 10.1145/3180155.3180220
   Turkay C, 2019, Arxiv, DOI arXiv:1812.08032
   Tyagi Anjul, 2023, IEEE Trans Vis Comput Graph, V29, P299, DOI 10.1109/TVCG.2022.3209361
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P2168, DOI 10.1109/TVCG.2019.2903943
   Wang QW, 2021, IEEE T VIS COMPUT GR, V27, P1417, DOI 10.1109/TVCG.2020.3030449
   Zbontar J, 2019, Arxiv, DOI [arXiv:1811.08839, 10.48550/arXiv.1811.08839]
   Zhang R, 2019, PR MACH LEARN RES, V97
   Zhang Xiaoyu, 2023, IEEE Trans Vis Comput Graph, V29, P842, DOI 10.1109/TVCG.2022.3209465
   Zhao ZE, 2022, IEEE T VIS COMPUT GR, V28, P780, DOI 10.1109/TVCG.2021.3114837
   Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485
NR 63
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5651
EP 5665
DI 10.1109/TVCG.2023.3301722
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400001
PM 37535493
OA Green Published
DA 2024-11-06
ER

PT J
AU Wang, XQ
   Yen, K
   Hu, YF
   Shen, HW
AF Wang, Xiaoqi
   Yen, Kevin
   Hu, Yifan
   Shen, Han-Wei
TI SmartGD: A GAN-Based Graph Drawing Framework for Diverse Aesthetic Goals
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Graph drawing; Deep learning; Generative adversarial networks;
   Stress; Generators; Training data; Deep learning for visualization;
   generative adversarial networks; graph visualization
ID LAYOUTS
AB While a multitude of studies have been conducted on graph drawing, many existing methods only focus on optimizing a single aesthetic aspect of graph layouts, which can lead to sub-optimal results. There are a few existing methods that have attempted to develop a flexible solution for optimizing different aesthetic aspects measured by different aesthetic criteria. Furthermore, thanks to the significant advance in deep learning techniques, several deep learning-based layout methods were proposed recently. These methods have demonstrated the advantages of deep learning approaches for graph drawing. However, none of these existing methods can be directly applied to optimizing non-differentiable criteria without special accommodation. In this work, we propose a novel Generative Adversarial Network (GAN) based deep learning framework for graph drawing, called SmartGD, which can optimize different quantitative aesthetic goals, regardless of their differentiability. To demonstrate the effectiveness and efficiency of SmartGD, we conducted experiments on minimizing stress, minimizing edge crossing, maximizing crossing angle, maximizing shape-based metrics, and a combination of multiple aesthetics. Compared with several popular graph drawing algorithms, the experimental results show that SmartGD achieves good performance both quantitatively and qualitatively.
C1 [Wang, Xiaoqi; Shen, Han-Wei] Ohio State Univ, Columbus, OH 43210 USA.
   [Yen, Kevin; Hu, Yifan] Yahoo Res, New York, NY 10003 USA.
C3 University System of Ohio; Ohio State University; Yahoo! Inc
RP Wang, XQ (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM wang.5502@osu.edu; kevinyen@yahooinc.com; yifanh@gmail.com;
   shen.94@osu.edu
RI Shen, Han-wei/A-4710-2012
OI Shen, Han-Wei/0000-0002-1211-2320; Yen, Kevin/0000-0002-4338-9680
FU NSF-funded AI Institute [OAC2112606]
FX No Statement Available
CR Ahmed R, 2022, IEEE T VIS COMPUT GR, V28, P2388, DOI 10.1109/TVCG.2022.3155564
   Argyriou E., 2010, P 18 INT S GRAPH DRA, P62
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bekos MA, 2021, COMPUT J, V64, P7, DOI 10.1093/comjnl/bxz133
   Blanchard AE, 2021, J CHEMINFORMATICS, V13, DOI 10.1186/s13321-021-00494-3
   Bourqui, 2021, LNCS, P375, DOI [10.1007/978303092931227, DOI 10.1007/978303092931227]
   Brandes U, 2007, LECT NOTES COMPUT SC, V4372, P42
   Davis TA, 2011, ACM T MATH SOFTWARE, V38, DOI 10.1145/2049662.2049663
   Devkota S, 2019, LECT NOTES COMPUT SC, V11904, P291, DOI 10.1007/978-3-030-35802-0_23
   Didimo W, 2011, LECT NOTES COMPUT SC, V6502, P165, DOI 10.1007/978-3-642-18469-7_15
   Dwyer T, 2009, IEEE T VIS COMPUT GR, V15, P961, DOI 10.1109/TVCG.2009.109
   Eades P, 2015, LECT NOTES COMPUT SC, V9411, P502, DOI 10.1007/978-3-319-27261-0_41
   Fey M., 2019, ICLR WORKSH REPR LEA
   FRUCHTERMAN TMJ, 1991, SOFTWARE PRACT EXPER, V21, P1129, DOI 10.1002/spe.4380211102
   Gansner ER, 2004, LECT NOTES COMPUT SC, V3383, P239
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Haleem H, 2019, IEEE COMPUT GRAPH, V39, P40, DOI 10.1109/MCG.2018.2881501
   Hu Yifan, 2005, Mathematica J., V10, P37
   Huang WD, 2013, J VISUAL LANG COMPUT, V24, P262, DOI 10.1016/j.jvlc.2011.12.002
   Jacomy M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098679
   Jiang LM, 2021, ADV NEUR IN, V34
   Jolicoeur-Martineau A, 2018, Arxiv, DOI arXiv:1807.00734
   KAMADA T, 1989, INFORM PROCESS LETT, V31, P7, DOI 10.1016/0020-0190(89)90102-6
   Kobourov S. G., 2013, Handbook of Graph Drawing and Visualization, P383
   Koren Y, 2003, LECT NOTES COMPUT SC, V2697, P496
   Kruiger JF, 2017, COMPUT GRAPH FORUM, V36, P283, DOI 10.1111/cgf.13187
   Kwon OH, 2020, IEEE T VIS COMPUT GR, V26, P665, DOI 10.1109/TVCG.2019.2934396
   Loshchilov I., 2016, ARXIV
   Mirza M, 2014, Arxiv, DOI arXiv:1411.1784
   Paszke A, 2019, ADV NEUR IN, V32
   Purchase H., 1997, Graph Drawing. 5th International Symposium, GD '97. Proceedings, P248, DOI 10.1007/3-540-63938-1_67
   Radermacher M., 2019, ACM J. Exp. Algorithmics, V24, P1
   Tiezzi M, 2024, IEEE T NEUR NET LEAR, V35, P4668, DOI 10.1109/TNNLS.2022.3184967
   Wang XQ, 2021, IEEE COMPUT GRAPH, V41, P32, DOI 10.1109/MCG.2021.3093908
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P676, DOI 10.1109/TVCG.2019.2934798
   Zheng JX, 2019, IEEE T VIS COMPUT GR, V25, P2738, DOI 10.1109/TVCG.2018.2859997
NR 38
TC 0
Z9 0
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5666
EP 5678
DI 10.1109/TVCG.2023.3306356
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400061
PM 37594870
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Peng, B
   Hu, J
   Zhou, JT
   Gao, X
   Zhang, JY
AF Peng, Bo
   Hu, Jun
   Zhou, Jingtao
   Gao, Xuan
   Zhang, Juyong
TI IntrinsicNGP: Intrinsic Coordinate Based Hash Encoding for Human NeRF
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neural rendering; intrinsic representation; human performance capture
AB Recently, many works have been proposed to use the neural radiance field for novel view synthesis of human performers. However, most of these methods require hours of training, making them difficult for practical use. To address this challenging problem, we propose IntrinsicNGP, which can be trained from scratch and achieve high-fidelity results in a few minutes with videos of a human performer. To achieve this goal, we introduce a continuous and optimizable intrinsic coordinate instead of the original explicit euclidean coordinate in the hash encoding module of InstantNGP. With this novel intrinsic coordinate, IntrinsicNGP can aggregate interframe information for dynamic objects using proxy geometry shapes. Moreover, the results trained with the given rough geometry shapes can be further refined with an optimizable offset field based on the intrinsic coordinate. Extensive experimental results on several datasets demonstrate the effectiveness and efficiency of IntrinsicNGP. We also illustrate the ability of our approach to edit the shape of reconstructed objects.
C1 [Peng, Bo; Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
EM pb15881461858@mail.ustc.edu.cn; hu997372@mail.ustc.edu.cn;
   ustc_zjt@mail.ustc.edu.cn; gx2017@mail.ustc.edu.cn; juyong@ustc.edu.cn
RI hu, jenny/AAK-8712-2020
OI Hu, Jun/0009-0007-4786-8455; Peng, Bo/0009-0005-1654-4605
FU National Natural Science Foundation of China [62122071, 62272433]
FX No Statement Available
CR Alldieck T, 2022, PROC CVPR IEEE, P1496, DOI 10.1109/CVPR52688.2022.00156
   Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Cao C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530143
   Chen A., 2023, Comput. Vis. Pattern Recognit.
   Cheng HK, 2021, PROC CVPR IEEE, P5555, DOI 10.1109/CVPR46437.2021.00551
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Dong ZJ, 2023, Arxiv, DOI arXiv:2305.02312
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Fangetal J., 2022, P SIGGRAPH AS C PAP, P1
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Gao X, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555501
   Geng C, 2023, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR52729.2023.00846
   Github, Easymocap-make human motion capture easier
   Grassal PW, 2022, PROC CVPR IEEE, P18632, DOI 10.1109/CVPR52688.2022.01810
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Hess R, 2010, BLENDER FDN ESSENTIA
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Hong Y, 2021, PROC CVPR IEEE, P535, DOI 10.1109/CVPR46437.2021.00060
   Isik M, 2023, Arxiv, DOI arXiv:2305.06356
   Jiang BY, 2022, PROC CVPR IEEE, P5595, DOI 10.1109/CVPR52688.2022.00552
   Jiang TJ, 2023, PROC CVPR IEEE, P16922, DOI 10.1109/CVPR52729.2023.01623
   Kingma D. P., 2015, P INT C LEARN REPR
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Kwon D., 2021, Comput. Vis. Pattern Recognit., V34
   Kwon Y, 2021, ADV NEUR IN, V34
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Lin SC, 2022, IEEE WINT CONF APPL, P3132, DOI 10.1109/WACV51458.2022.00319
   Liu Boning, 2023, arXiv
   Liu J.-W., 2022, arXiv
   Liu L., 2021, ACM T GRAPHIC, V40, P1
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Muller T., 2021, Tiny CUDA Neural Network Framework
   Osman Ahmed A. A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P598, DOI 10.1007/978-3-030-58539-6_36
   Parashar S, 2022, IEEE T PATTERN ANAL, V44, P6409, DOI 10.1109/TPAMI.2021.3089923
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Rosu RA, 2022, Arxiv, DOI arXiv:2211.12562
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Su SY, 2021, ADV NEUR IN, V34
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Tanciket al., 2020, Comput. Vis. Pattern Recognit.
   Tang JX, 2022, Arxiv, DOI arXiv:2205.14870
   Tang Jiaxiang, 2022, Torch-ngp: A PyTorch Implementation of Instant-ngp
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Tiwari G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11688, DOI 10.1109/ICCV48922.2021.01150
   Wang H, 2022, LECT NOTES COMPUT SC, V13691, P612, DOI 10.1007/978-3-031-19821-2_35
   Wang S, 2023, "ARAH:Animatablevolumerendering of articulated human SDFs
   Wang SF, 2022, LECT NOTES COMPUT SC, V13692, P1, DOI 10.1007/978-3-031-19824-3_1
   Weng CY, 2023, PROC CVPR IEEE, P524, DOI 10.1109/CVPR52729.2023.00058
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Wu L., 2022, P IEEECVF C COMPUTER, P16200
   Xiu Yuliang, 2022, P IEEE CVF C COMP VI, P13296
   Xu Hongyi, 2021, Advances in Neural Information Processing Systems (NeurIPS), V34, P14955
   Xu TH, 2022, PROC CVPR IEEE, P15862, DOI 10.1109/CVPR52688.2022.01542
   Zhang J., 2022, Easymocap-make human motion capture easier
   Zhao FQ, 2022, PROC CVPR IEEE, P7733, DOI 10.1109/CVPR52688.2022.00759
   Zheng YF, 2023, PROC CVPR IEEE, P21057, DOI 10.1109/CVPR52729.2023.02017
   Zheng ZR, 2022, PROC CVPR IEEE, P15872, DOI 10.1109/CVPR52688.2022.01543
   Zhi S., 2022, P INT C 3DVIS, P1
   Zielonka W, 2023, PROC CVPR IEEE, P4574, DOI 10.1109/CVPR52729.2023.00444
   Zuffi A., 2017, P IEEE C COMP VIS PA, P6365, DOI DOI 10.1109/CVPR.2017.586
NR 68
TC 0
Z9 0
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5679
EP 5692
DI 10.1109/TVCG.2023.3306078
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400049
PM 37590116
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Oreshkin, BN
   Valkanas, A
   Harvey, FG
   Menard, LS
   Bocquelet, F
   Coates, MJ
AF Oreshkin, Boris N.
   Valkanas, Antonios
   Harvey, Felix G.
   Menard, Louis-Simon
   Bocquelet, Florent
   Coates, Mark J.
TI Motion In-Betweening via Deep Δ-Interpolator
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; in-betweening; interpolation; transformer; neural networks;
   human motion
AB We show that the task of synthesizing human motion conditioned on a set of key frames can be solved more accurately and effectively if a deep learning based interpolator operates in the delta mode using the spherical linear interpolator as a baseline. We empirically demonstrate the strength of our approach on publicly available datasets achieving state-of-the-art performance. We further generalize these results by showing that the Delta-regime is viable with respect to the reference of the last known frame (also known as the zero-velocity model). This supports the more general conclusion that operating in the reference frame local to input frames is more accurate and robust than in the global (world) reference frame advocated in previous work.
C1 [Oreshkin, Boris N.] Unity Technol, San Francisco, CA 94103 USA.
   [Valkanas, Antonios; Coates, Mark J.] McGill Univ, Montreal, PQ H3A 0G4, Canada.
C3 McGill University
RP Oreshkin, BN (corresponding author), Unity Technol, San Francisco, CA 94103 USA.
EM boris.oreshkin@gmail.com; antonios.valkanas@mail.mcgill.ca;
   c212.felixh@gmail.com; louissimon.menard@unity3d.com;
   florent.bocquelet@unity3d.com; mark.coates@mcgill.ca
OI Valkanas, Antonios/0009-0001-1234-0016
FU Mitacs through the Mitacs Accelerate program
FX This work was supported by Mitacs through the Mitacs Accelerate program.
CR Abe Y., 2004, P 2004 ACM SIGGRAPHE, P173
   Beane A., 2012, 3D Animation Essentials
   Chai JX, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239459
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Duan YL, 2022, AAAI CONF ARTIF INTE, P4459
   Duan YL, 2021, Arxiv, DOI arXiv:2103.00776
   Geleijn R, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P579, DOI 10.1109/VRW52623.2021.00172
   Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Harvey FG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392480
   Harvey FG, 2018, SA'18: SIGGRAPH ASIA 2018 TECHNICAL BRIEFS, DOI 10.1145/3283254.3283277
   Holden D., 2015, P SIGGRAPH EUR S COM, P165
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Ji S. W., 2010, Tech. Rep.
   Kao HK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P147, DOI 10.1145/3394171.3413848
   Kaufmann M, 2020, INT CONF 3D VISION, P918, DOI 10.1109/3DV50981.2020.00102
   Kingma D.P., 2014, P INT C LEARNING REP
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Levine S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185524
   Li JM, 2021, INT CONF 3D VISION, P771, DOI 10.1109/3DV53792.2021.00086
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Ngo J. T., 1993, Computer Graphics Proceedings, P343, DOI 10.1145/166117.166160
   Oreshkin B. N., 2021, P INT C LEARN REPR
   Oreshkin B. N., 2022, P INT C LEARN REPR
   Paszke A, 2019, ADV NEUR IN, V32
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Press O., 2022, P INT C LEARN REPR
   Qin J, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555454
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1237, DOI 10.1145/3240508.3241388
   Tang XJ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530090
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2021, IEEE T VIS COMPUT GR, V27, P216, DOI 10.1109/TVCG.2019.2936810
   Wang H, 2015, IEEE T VIS COMPUT GR, V21, P18, DOI 10.1109/TVCG.2014.2327976
   Wang HS, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108424
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
NR 43
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5693
EP 5704
DI 10.1109/TVCG.2023.3309107
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400071
PM 37639420
DA 2024-11-06
ER

PT J
AU Fan, YT
   Zhu, LF
   Wang, H
   Song, AG
AF Fan, Yuting
   Zhu, Lifeng
   Wang, Hui
   Song, Aiguo
TI Synthesize Personalized Training for Robot-Assisted Upper Limb
   Rehabilitation With Diversity Enhancement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Optimization; Task analysis; Games; Trajectory; End effectors;
   Robot kinematics; Exercise planning; multi-objective optimization;
   serious game; upper limb rehabilitation
ID MOTOR FUNCTION; RECOVERY; CARE; STRENGTH; THERAPY
AB For upper limb rehabilitation, the robot-assisted technique in combination with serious games requires well-specified training plans. For the best quality of the rehabilitation process, customized game levels for each user are desired, while it is labor-intensive to design and adjust game levels for different individuals. We work on generating training content for a desktop end-effector rehabilitation robot and propose a method to automatically generate individualized training plans. By modeling the search of the training motions as finding optimal hand paths and trajectories, we introduce solving the design problem with a multi-objective optimization (MO) solver. We further improve the MO solver to enhance the diversity of the solutions. With the proposed approach, our system is capable of automatically generating various training plans considering the training intensity and dexterity of each joint in the upper limb. In addition, the enhanced diversity avoids repeated training plans, which helps motivate the user in the rehabilitation. We test our method with different requirements on the training plans and validate the solutions.
C1 [Fan, Yuting; Zhu, Lifeng; Song, Aiguo] Southeast Univ, Sch Instrument Sci & Engn, State Key Lab Bioelect, Jiangsu Key Lab Remote Measurement & Control, Nanjing 211189, Jiangsu, Peoples R China.
   [Wang, Hui] Chinese Acad Sci, Hefei Inst Phys Sci, Inst Intelligent Machines, Hefei 230031, Peoples R China.
C3 Southeast University - China; Chinese Academy of Sciences; Hefei
   Institutes of Physical Science, CAS
RP Zhu, LF (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, State Key Lab Bioelect, Jiangsu Key Lab Remote Measurement & Control, Nanjing 211189, Jiangsu, Peoples R China.
EM fanyuting981026@163.com; lfzhulf@gmail.com; wanghui@iim.ac.cn;
   a.g.song@seu.edu.cn
RI zhu, lifeng/IST-2069-2023
OI Song, Aiguo/0000-0002-1982-6780
FU NSFC [92148205]; Jiangsu Key Research and Development Plan; Natural
   Science Foundation of Jiangsu Province [BK20211159]
FX This work was supported in part by NSFC under Grant 92148205, in part by
   the Jiangsu Key Research and Development Plan, and in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20211159.
CR Ada L, 1996, HUM MOVEMENT SCI, V15, P671, DOI 10.1016/0167-9457(96)00015-2
   Aldwin C. M., 2017, Health,Illness, and Optimal Aging: Biological and Psychosocial Perspectives
   Almeida Y, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON BIOMEDICAL ENGINEERING SYSTEMS AND TECHNOLOGIES, VOL 5: HEALTHINF, P845, DOI 10.5220/0009369108450853
   Bandyopadhyay S, 2008, IEEE T EVOLUT COMPUT, V12, P269, DOI 10.1109/TEVC.2007.900837
   González JC, 2017, COGN SYST RES, V43, P232, DOI 10.1016/j.cogsys.2016.09.003
   Colombo R, 2012, IEEE T NEUR SYS REH, V20, P276, DOI 10.1109/TNSRE.2012.2195679
   Desrosiers J, 1999, EXP GERONTOL, V34, P393, DOI 10.1016/S0531-5565(99)00018-2
   Doumas I, 2021, J NEUROENG REHABIL, V18, DOI 10.1186/s12984-021-00889-1
   Eckert M, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020354
   Faria Ana Lucia, 2018, JMIR Rehabil Assist Technol, V5, pe10714, DOI 10.2196/10714
   Gonzalez-Mendoza A., 2019, P 16 INT C EL ENG CO, P1
   Hammedi W, 2017, J SERV MANAGE, V28, P640, DOI 10.1108/JOSM-04-2016-0116
   Hocine N, 2015, USER MODEL USER-ADAP, V25, P65, DOI 10.1007/s11257-015-9154-6
   Huang HK, 2018, IEEE T VIS COMPUT GR, V24, P2516, DOI 10.1109/TVCG.2017.2761820
   Ishibuchi H, 2008, IEEE C EVOL COMPUTAT, P2419, DOI 10.1109/CEC.2008.4631121
   Islam M., 2017, Adv Robot Autom, V6, P2
   KENDZIERSKI D, 1991, J SPORT EXERCISE PSY, V13, P50, DOI 10.1123/jsep.13.1.50
   Khan HA, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188523
   Khanafer S, 2021, EXP BRAIN RES, V239, P2285, DOI 10.1007/s00221-021-06143-3
   Knaepen K, 2014, IEEE T NEUR SYS REH, V22, P1128, DOI 10.1109/TNSRE.2014.2324153
   Krakauer JW, 2006, CURR OPIN NEUROL, V19, P84, DOI 10.1097/01.wco.0000200544.29915.cc
   Krebs HI, 2007, IEEE T NEUR SYS REH, V15, P327, DOI 10.1109/TNSRE.2007.903899
   Krebs HI, 2000, J REHABIL RES DEV, V37, P639
   Lang YN, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P297, DOI 10.1109/VR.2018.8448290
   Langhorne P, 2011, LANCET, V377, P1693, DOI 10.1016/S0140-6736(11)60325-5
   Lee S, 2018, IEEE T NEUR SYS REH, V26, P125, DOI 10.1109/TNSRE.2017.2755667
   Lee SH, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-58630-2
   Li WW, 2022, IEEE T VIS COMPUT GR, V28, P1993, DOI 10.1109/TVCG.2022.3150510
   Li WW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392393
   Lum PS, 2002, ARCH PHYS MED REHAB, V83, P952, DOI 10.1053/apmr.2001.33101
   Maciejasz P, 2014, J NEUROENG REHABIL, V11, DOI 10.1186/1743-0003-11-3
   Magermans DJ, 2005, CLIN BIOMECH, V20, P591, DOI 10.1016/j.clinbiomech.2005.02.006
   Meadmore KL, 2019, TOP STROKE REHABIL, V26, P94, DOI 10.1080/10749357.2018.1544845
   Miao Q, 2023, IEEE T COGN DEV SYST, V15, P2031, DOI 10.1109/TCDS.2021.3072096
   Morita Y, 2009, 2008 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-4, P300, DOI 10.1109/ROBIO.2009.4913020
   Morone G, 2020, EXPERT REV MED DEVIC, V17, P223, DOI 10.1080/17434440.2020.1733408
   Muratori LM, 2013, J HAND THER, V26, P94, DOI 10.1016/j.jht.2012.12.007
   Nef Tobias, 2009, 11th International Congress of the IUPESM. Medical Physics and Biomedical Engineering. World Congress 2009. Neuroengineering, Neural Systems, Rehabilitation and Prosthetics, P127, DOI 10.1007/978-3-642-03889-1_35
   OptiTrack, 2020, Optitrack for movement sciences
   Otten A, 2015, IEEE-ASME T MECH, V20, P2285, DOI 10.1109/TMECH.2014.2375272
   Pereira F, 2019, J NEUROENG REHABIL, V16, DOI 10.1186/s12984-019-0578-9
   Phinyomark A, 2012, EXPERT SYST APPL, V39, P7420, DOI 10.1016/j.eswa.2012.01.102
   Pizzolato S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186132
   Pollock A, 2014, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD010820.pub2
   Rawal A, 2018, CLIN BIOMECH, V54, P78, DOI 10.1016/j.clinbiomech.2018.03.009
   Rego P, 2010, SISTEMAS Y TECNOLOGIAS DE INFORMACION, P349
   Reis E, 2019, PHYS THER REV, V24, P84, DOI 10.1080/10833196.2019.1639012
   Rosen J, 2005, 2005 12TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS, P532
   Siddique T, 2022, MICROMACHINES-BASEL, V13, DOI 10.3390/mi13060842
   Srinivas N., 1994, Evolutionary Computation, V2, P221, DOI 10.1162/evco.1994.2.3.221
   Suman B, 2006, J OPER RES SOC, V57, P1143, DOI 10.1057/palgrave.jors.2602068
   W. H. Organization, 2006, Neurological Disorders: Public Health Challenges
   Wang HD, 2017, IEEE T CYBERNETICS, V47, P1510, DOI 10.1109/TCYB.2016.2550502
   Xie B, 2018, IEEE T VIS COMPUT GR, V24, P1661, DOI 10.1109/TVCG.2018.2793618
   Yu L, 2016, COMPUT METH PROG BIO, V128, P100, DOI 10.1016/j.cmpb.2016.02.012
   Zhang YQ, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300784
   Zhou S.-H., 2016, J. Control Decis., V3, P19
   Zitzler E, 1999, IEEE T EVOLUT COMPUT, V3, P257, DOI 10.1109/4235.797969
NR 58
TC 0
Z9 0
U1 14
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5705
EP 5718
DI 10.1109/TVCG.2023.3308940
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400065
PM 37639418
DA 2024-11-06
ER

PT J
AU Gao, QZ
   Wang, YM
   Liu, LB
   Liu, LJ
   Theobalt, C
   Chen, BQ
AF Gao, Qingzhe
   Wang, Yiming
   Liu, Libin
   Liu, Lingjie
   Theobalt, Christian
   Chen, Baoquan
TI Neural Novel Actor: Learning a Generalized Animatable Neural
   Representation for Human Actors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neural rendering; neural radiance field; human synthesis
AB We propose a new method for learning a generalized animatable neural human representation from a sparse set of multi-view imagery of multiple persons. The learned representation can be used to synthesize novel view images of an arbitrary person and further animate them with the user's pose control. While most existing methods can either generalize to new persons or synthesize animations with user control, none of them can achieve both at the same time. We attribute this accomplishment to the employment of a 3D proxy for a shared multi-person human model, and further the warping of the spaces of different poses to a shared canonical pose space, in which we learn a neural field and predict the person- and pose-dependent deformations, as well as appearance with the features extracted from input images. To cope with the complexity of the large variations in body shapes, poses, and clothing deformations, we design our neural human model with disentangled geometry and appearance. Furthermore, we utilize the image features both at the spatial point and on the surface points of the 3D proxy for predicting person- and pose-dependent properties. Experiments show that our method significantly outperforms the state-of-the-arts on both tasks.
C1 [Gao, Qingzhe] Shandong Univ, Sch Comp Sci & technol, Jinan 250100, Peoples R China.
   [Wang, Yiming] Peking Univ, Beijing 100871, Peoples R China.
   [Liu, Libin; Chen, Baoquan] Peking Univ, Natl Key Lab Gen Artificial Intelligence, Beijing 100871, Peoples R China.
   [Liu, Lingjie; Theobalt, Christian] Max Planck Inst Informat, Graph Vis & Video Grp, D-66123 Saarbrucken, Germany.
C3 Shandong University; Peking University; Peking University; Max Planck
   Society
RP Chen, BQ (corresponding author), Peking Univ, Natl Key Lab Gen Artificial Intelligence, Beijing 100871, Peoples R China.
EM gaoqingzhe97@gmail.com; wym12416@pku.edu.cn; libin.liu@pku.edu.cn;
   lliu@mpi-inf.mpg.de; theobalt@mpi-inf.mpg.de; baoquan@pku.edu.cn
OI Wang, Yiming/0009-0002-7585-6201; Liu, Libin/0000-0003-2280-6817; Gao,
   Qingzhe/0000-0002-6239-571X
FU NSFC Projects of International Cooperation and Exchanges [62161146002];
   ERC Consolidator Grant 4DReply [770784]; Lise Meitner Postdoctoral
   Fellowship
FX This work was supported by the NSFC Projects of International
   Cooperation and Exchanges under Grant 62161146002. The work of Christian
   Theobalt was supported in part by ERC Consolidator Grant 4DReply under
   Grant 770784. The work of Lingjie Liu was supported in part by Lise
   Meitner Postdoctoral Fellowship.
CR Aliev Kara-Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P696, DOI 10.1007/978-3-030-58542-6_42
   Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Chen A., 2021, P IEEE CVF INT C COM, P14124
   Chen JC, 2021, Arxiv, DOI arXiv:2106.13629
   Chen MF, 2022, LECT NOTES COMPUT SC, V13683, P222
   Chen X., 2021, arXiv
   Chibane J, 2021, PROC CVPR IEEE, P7907, DOI 10.1109/CVPR46437.2021.00782
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Gao C, 2021, Arxiv, DOI arXiv:2012.05903
   Habermann L., 2023, ACM SIGGRAPH EUROGRA
   Yao G., 2021, arXiv
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou TH, 2018, Arxiv, DOI arXiv:1805.09817
NR 17
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5719
EP 5732
DI 10.1109/TVCG.2023.3305433
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400040
PM 37585333
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Hilasaca, GM
   Marcilio-Jr, WE
   Eler, DM
   Martins, RM
   Paulovich, FV
AF Hilasaca, Gladys M.
   Marcilio-Jr, Wilson E.
   Eler, Danilo M.
   Martins, Rafael M.
   Paulovich, Fernando V.
TI A Grid-Based Method for Removing Overlaps of Dimensionality Reduction
   Scatterplot Layouts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dimensionality reduction; multidimensional projection; scatterplots;
   overlap removal
ID NEIGHBORHOOD PRESERVATION; VISUAL ANALYSIS; ADJUSTMENT
AB Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous visualization tool for analyzing multidimensional datasets. Despite their popularity, such scatterplots suffer from occlusion, especially when informative glyphs are used to represent data instances, potentially obfuscating critical information for the analysis under execution. Different strategies have been devised to address this issue, either producing overlap-free layouts that lack the powerful capabilities of contemporary DR techniques in uncovering interesting data patterns or eliminating overlaps as a post-processing strategy. Despite the good results of post-processing techniques, most of the best methods typically expand or distort the scatterplot area, thus reducing glyphs' size (sometimes) to unreadable dimensions, defeating the purpose of removing overlaps. This article presents Distance Grid (DGrid), a novel post-processing strategy to remove overlaps from DR layouts that faithfully preserves the original layout's characteristics and bounds the minimum glyph sizes. We show that DGrid surpasses the state-of-the-art in overlap removal (through an extensive comparative evaluation considering multiple different metrics) while also being one of the fastest techniques, especially for large datasets. A user study with 51 participants also shows that DGrid is consistently ranked among the top techniques for preserving the original scatterplots' visual characteristics and the aesthetics of the final results.
C1 [Hilasaca, Gladys M.] Fed Univ So Paulo UNIFESP, BR-05508220 Sao Paulo, Brazil.
   [Marcilio-Jr, Wilson E.] Sao Paulo State Univ, BR-05508070 Sao Paulo, Brazil.
   [Eler, Danilo M.] Sao Paulo State Univ, S-35252 Sao Paulo, Brazil.
   [Martins, Rafael M.; Paulovich, Fernando V.] Linnaeus Univ, NL-5612 AZ Vaxjo, Sweden.
C3 Universidade Estadual Paulista; Universidade Estadual Paulista; Linnaeus
   University
RP Paulovich, FV (corresponding author), Linnaeus Univ, NL-5612 AZ Vaxjo, Sweden.
EM gladysmarleny@gmail.com; wilson_jr@outlook.com; danilo.eler@unesp.br;
   rafael.martins@lnu.se; f.paulovich@tue.nl
RI Eler, Danilo/M-3320-2019; Martins, Rafael/H-9192-2019; Paulovich,
   Fernando/G-1329-2010
OI Eler, Danilo/0000-0002-9493-145X; Paulovich,
   Fernando/0000-0002-2316-760X
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Emerging Leaders in the Americas Program (ELAP); Government of Canada;
   CAPES-Brazil
FX The authors would like to thank all reviewers who dedicated time to this
   article. Your comments and feedback were beneficial and indeed led to
   substantial improvement in the quality and clarity of this manuscript.
   We acknowledge the support of the Natural Sciences and Engineering
   Research Council of Canada(NSERC), CAPES-Brazil, and the Emerging
   Leaders in the Americas Program (ELAP) with the support of the
   Government of Canada.
CR Anderson J. A., 1988, Neurocomputing: Foundations ofResearch
   Ramos-Guajardo AB, 2020, INT J APPROX REASON, V119, P1, DOI 10.1016/j.ijar.2019.12.012
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bertini E., 2006, Information Visualization, V5, P95, DOI 10.1057/palgrave.ivs.9500122
   Brath R, 2014, 2014 IEEE VIS INTERNATIONAL WORKSHOP ON 3DVIS (3DVIS), P25, DOI 10.1109/3DVis.2014.7160096
   Breitling R, 2004, FEBS LETT, V573, P83, DOI 10.1016/j.febslet.2004.07.055
   Brown R. A., 2015, J. Comput. Graph. Techn., V4, P50
   Cao N, 2011, IEEE T VIS COMPUT GR, V17, P2581, DOI 10.1109/TVCG.2011.188
   CARR DB, 1987, J AM STAT ASSOC, V82, P424, DOI 10.2307/2289444
   Chen F., 2020, J. Graph Algorithms Appl., V24, P683, DOI [10.7155/jgaa.00532, DOI 10.7155/JGAA.00532]
   Chen X, 2020, IEEE T VIS COMPUT GR, V26, P729, DOI 10.1109/TVCG.2019.2934541
   Cutura R, 2022, J VISUAL-JAPAN, V25, P1291, DOI 10.1007/s12650-022-00854-7
   Dua D, 2017, UCI MACHINE LEARNING
   Duarte FSLG, 2014, IEEE T VIS COMPUT GR, V20, P2063, DOI 10.1109/TVCG.2014.2346276
   Dwyer T, 2006, LECT NOTES COMPUT SC, V3843, P153
   Eler DM, 2009, VISUAL COMPUT, V25, P923, DOI 10.1007/s00371-009-0368-7
   Elmqvist N., 2005, P ACM S VIRT REAL SO, P134, DOI [10.1145/1101616.1101643, DOI 10.1145/1101616.1101643]
   Eppstein D, 2013, IEEE PAC VIS SYMP, P25, DOI 10.1109/PacificVis.2013.6596124
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Fried O, 2015, COMPUT GRAPH FORUM, V34, P155, DOI 10.1111/cgf.12549
   Fuchs J, 2014, IEEE T VIS COMPUT GR, V20, P2251, DOI 10.1109/TVCG.2014.2346426
   Gansner E., 2010, J. Graph Algorithms Appl., V14, P53
   Gomez-Nieto Erick, 2013, 2013 XXVI Conference on Graphics, Patterns and Images (SIBGRAPI 2013), P115, DOI 10.1109/SIBGRAPI.2013.25
   Gomez-Nieto E, 2014, IEEE T VIS COMPUT GR, V20, P457, DOI 10.1109/TVCG.2013.242
   Hayashi K, 1998, LECT NOTES COMPUT SC, V1547, P183
   Helliwell J. F., 2019, World Happiness Report 2019
   Jenks G.F., 1967, International Yearbook of Cartography, V7, P186, DOI DOI 10.1201/9780429464195-7
   Kammer D, 2020, IEEE T VIS COMPUT GR, V26, P1661, DOI 10.1109/TVCG.2020.2969060
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Li J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2553
   Li J, 2009, IEEE PAC VIS SYMP, P97, DOI 10.1109/PACIFICVIS.2009.4906843
   Lundberg SM, 2017, ADV NEUR IN, V30
   Marcilio WE, 2019, INFORM VISUAL, V18, P426, DOI 10.1177/1473871619845093
   Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006
   Mayorga A, 2013, IEEE T VIS COMPUT GR, V19, P1526, DOI 10.1109/TVCG.2013.65
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   McNeill G, 2017, COMPUT GRAPH FORUM, V36, P435, DOI 10.1111/cgf.13200
   Meulemans W, 2019, COMPUT GRAPH FORUM, V38, P713, DOI 10.1111/cgf.13722
   Meulemans W, 2017, IEEE T VIS COMPUT GR, V23, P381, DOI 10.1109/TVCG.2016.2598542
   Micallef L, 2017, IEEE T VIS COMPUT GR, V23, P1588, DOI 10.1109/TVCG.2017.2674978
   MISUE K, 1995, J VISUAL LANG COMPUT, V6, P183, DOI 10.1006/jvlc.1995.1010
   Nachmanson L, 2016, LECT NOTES COMPUT SC, V9801, P33, DOI 10.1007/978-3-319-50106-2_3
   Nonato LG, 2019, IEEE T VIS COMPUT GR, V25, P2650, DOI 10.1109/TVCG.2018.2846735
   Pinho R., 2009, P ACM S APPL COMP NE, P1757, DOI DOI 10.1145/1529282.1529679
   Pinho R, 2009, INFORMATION VISUALIZATION, IV 2009, PROCEEDINGS, P32, DOI 10.1109/IV.2009.12
   Quadrianto N, 2010, IEEE T PATTERN ANAL, V32, P1809, DOI 10.1109/TPAMI.2009.184
   Raisz E, 1934, GEOGR REV, V24, P292, DOI 10.2307/208794
   Rauber PE, 2018, INFORM VISUAL, V17, P282, DOI 10.1177/1473871617713337
   Sarikaya A, 2018, IEEE T VIS COMPUT GR, V24, P402, DOI 10.1109/TVCG.2017.2744184
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Strobelt H, 2012, COMPUT GRAPH FORUM, V31, P1135, DOI 10.1111/j.1467-8659.2012.03106.x
   Strong G., 2011, P GRAPH INT, P206
   Tory M, 2007, IEEE T VIS COMPUT GR, V13, P1262, DOI 10.1109/TVCG.2007.70596
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Garderen M, 2017, COMPUT GRAPH FORUM, V36, P423, DOI 10.1111/cgf.13199
   van Kreveld M, 2004, LECT NOTES COMPUT SC, V3221, P724
   Venna J, 2001, LECT NOTES COMPUT SC, V2130, P485
   Wood J, 2008, IEEE T VIS COMPUT GR, V14, P1348, DOI 10.1109/TVCG.2008.165
   Wood J, 2010, CARTOGR J, V47, P117, DOI 10.1179/000870410X12658023467367
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Yuan J, 2021, IEEE T VIS COMPUT GR, V27, P1720, DOI 10.1109/TVCG.2020.3030432
NR 61
TC 1
Z9 1
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5733
EP 5749
DI 10.1109/TVCG.2023.3309941
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400072
PM 37647195
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Lv, P
   Pei, XM
   Ren, XY
   Zhang, YZ
   Li, CC
   Xu, ML
AF Lv, Pei
   Pei, Xinming
   Ren, Xinyu
   Zhang, Yuzhen
   Li, Chaochao
   Xu, Mingliang
TI TraInterSim: Adaptive and Planning-Aware Hybrid-Driven Traffic
   Intersection Simulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Trajectory; Traffic control; Behavioral sciences; Planning;
   Optimization; Collision avoidance; Simulation; data-driven;
   heterogeneous multi-agent; traffic intersection; traffic simulation
ID CROWD SIMULATION; MODEL; DYNAMICS; FLOW; ANIMATION
AB Traffic intersections are important scenes that can be seen almost everywhere in the traffic system. Currently, most simulation methods perform well at highways and urban traffic networks. In intersection scenarios, the challenge lies in the lack of clearly defined lanes, where agents with various motion plannings converge in the central area from different directions. Traditional model-based methods are difficult to drive agents to move realistically at intersections without enough predefined lanes, while data-driven methods often require a large amount of high-quality input data. Simultaneously, tedious parameter tuning is inevitable involved to obtain the desired simulation results. In this paper, we present a novel adaptive and planning-aware hybrid-driven method (TraInterSim) to simulate traffic intersection scenarios. Our hybrid-driven method combines an optimization-based data-driven scheme with a velocity continuity model. It guides the agent's movements using real-world data and can generate those behaviors not present in the input data. Our optimization method fully considers velocity continuity, desired speed, direction guidance, and planning-aware collision avoidance. Agents can perceive others' motion plannings and relative distances to avoid possible collisions. To preserve the individual flexibility of different agents, the parameters in our method are automatically adjusted during the simulation. TraInterSim can generate realistic behaviors of heterogeneous agents in different traffic intersection scenarios in interactive rates. Through extensive experiments as well as user studies, we validate the effectiveness and rationality of the proposed simulation method.
C1 [Lv, Pei; Pei, Xinming; Ren, Xinyu; Zhang, Yuzhen; Li, Chaochao; Xu, Mingliang] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Henan, Peoples R China.
C3 Zhengzhou University
RP Xu, ML (corresponding author), Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Henan, Peoples R China.
EM ielvpei@zzu.edu.cn; kevinpeixinming@foxmail.com; renxinyu@gs.zzu.edu.cn;
   zyzzhang@gs.zzu.edu.cn; ieccli@zzu.edu.cn; iexumingliang@zzu.edu.cn
RI Pei, Xinming/JBJ-8906-2023
OI Pei, Xinming/0009-0001-3846-2986; , Pei/0000-0002-2654-0561; Li,
   Chaochao/0000-0001-9694-832X
FU Zhengzhou Major Science and Technology Project [2021KJZX0060]; National
   Natural Science Foundation of China [62036010]; Joint Fund of the
   Ministry of Education for Equipment Pre Research [8091B032257]
FX No Statement Available
CR [Anonymous], 2011, INTELLIGENT TRANSPOR
   Berseth Glen., 2014, P ACM SIGGRAPH EUR S, P113
   Bi H., 2016, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P149
   Bi HK, 2020, IEEE T VIS COMPUT GR, V26, P2335, DOI 10.1109/TVCG.2018.2889834
   Chao QW, 2023, IEEE T VIS COMPUT GR, V29, P1664, DOI 10.1109/TVCG.2021.3128286
   Chao QW, 2020, COMPUT GRAPH FORUM, V39, P287, DOI 10.1111/cgf.13803
   Chao QW, 2019, IEEE INT CONF ROBOT, P8298, DOI [10.1109/ICRA.2019.8794430, 10.1109/icra.2019.8794430]
   Chao QW, 2018, IEEE T VIS COMPUT GR, V24, P1167, DOI 10.1109/TVCG.2017.2648790
   Chao QW, 2015, COMPUT ANIMAT VIRT W, V26, P405, DOI 10.1002/cav.1654
   Chao QW, 2013, GRAPH MODELS, V75, P305, DOI 10.1016/j.gmod.2013.07.003
   Durupinar F, 2016, IEEE T VIS COMPUT GR, V22, P2145, DOI 10.1109/TVCG.2015.2501801
   Ettinger S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9690, DOI 10.1109/ICCV48922.2021.00957
   Guy S.J., 2011, P 2011 ACM SIGGRAPH, P43, DOI DOI 10.1145/2019406.2019413
   Han Y, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1974
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Huang X, 2019, IEEE INT CONF ROBOT, P9718, DOI [10.1109/icra.2019.8794282, 10.1109/ICRA.2019.8794282]
   Karamouzas I, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073705
   Karamouzas I, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.238701
   Kesting A, 2007, TRANSPORT RES REC, P86, DOI 10.3141/1999-10
   Kesting A, 2008, TRANSPORT RES REC, P148, DOI 10.3141/2088-16
   Li WZ, 2019, IEEE INT CONF ROBOT, P7625, DOI [10.1109/icra.2019.8794239, 10.1109/ICRA.2019.8794239]
   Li WZ, 2018, IET INTELL TRANSP SY, V12, P875, DOI 10.1049/iet-its.2018.0007
   Li WZ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130847
   Li WZ, 2017, IEEE INTEL TRANSP SY, V9, P100, DOI 10.1109/MITS.2017.2709804
   LIGHTHILL MJ, 1955, PROC R SOC LON SER-A, V229, P317, DOI 10.1098/rspa.1955.0089
   Lin L, 2022, IEEE INTEL TRANSP SY, V14, P197, DOI 10.1109/MITS.2021.3049404
   Lopez PA, 2018, IEEE INT C INTELL TR, P2575, DOI 10.1109/ITSC.2018.8569938
   Mao TL, 2015, COMPUT ANIMAT VIRT W, V26, P397, DOI 10.1002/cav.1642
   NAGEL K, 1992, J PHYS I, V2, P2221, DOI 10.1051/jp1:1992277
   NEWELL GF, 1961, OPER RES, V9, P209, DOI 10.1287/opre.9.2.209
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Park JH, 2013, COMPUT ANIMAT VIRT W, V24, P173, DOI 10.1002/cav.1504
   PIPES LA, 1953, J APPL PHYS, V24, P274, DOI 10.1063/1.1721265
   Ren JP, 2021, IEEE T VIS COMPUT GR, V27, P1953, DOI 10.1109/TVCG.2019.2946769
   Sarkar A, 2017, IEEE INT C INTELL TR
   Sewall J, 2010, COMPUT GRAPH FORUM, V29, P439, DOI 10.1111/j.1467-8659.2009.01613.x
   Sewall J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024169
   Sewall J, 2011, IEEE T VIS COMPUT GR, V17, P26, DOI 10.1109/TVCG.2010.27
   Shen JJ, 2012, GRAPH MODELS, V74, P265, DOI 10.1016/j.gmod.2012.04.002
   Shvetsov V, 1999, PHYS REV E, V59, P6328, DOI 10.1103/PhysRevE.59.6328
   Treiber M., 2001, Automatisierungstechnik, V49, P478, DOI 10.1524/auto.2001.49.11.478
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   van Toll W, 2021, COMPUT GRAPH FORUM, V40, P731, DOI 10.1111/cgf.142664
   Wang Y., 2014, Proceedings of the Symposium on Interactive 3D Graphics, P23
   Wilkie D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462021
   Wolinski D, 2014, COMPUT GRAPH FORUM, V33, P303, DOI 10.1111/cgf.12328
   Yang D, 2019, IEEE T INTELL TRANSP, V20, P1719, DOI 10.1109/TITS.2018.2834910
   Yang X, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1740
   Zhang HM, 2002, TRANSPORT RES B-METH, V36, P275, DOI 10.1016/S0191-2615(00)00050-3
   Zhang YZ, 2022, LECT NOTES COMPUT SC, V13668, P522, DOI 10.1007/978-3-031-20074-8_30
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
NR 53
TC 0
Z9 0
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5750
EP 5764
DI 10.1109/TVCG.2023.3307882
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400088
PM 37610911
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Croucher, C
   Powell, W
   Stevens, B
   Miller-Dicks, M
   Powell, V
   Wiltshire, TJ
   Spronck, P
AF Croucher, Charlotte
   Powell, Wendy
   Stevens, Brett
   Miller-Dicks, Matt
   Powell, Vaughan
   Wiltshire, Travis J.
   Spronck, Pieter
TI LoCoMoTe - A Framework for Classification of Natural Locomotion in VR by
   Task, Technique and Modality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human-computer interaction; machine learning; navigation; redirected
   walking; virtual reality
ID LARGE VIRTUAL ENVIRONMENTS; REDIRECTED WALKING; REALITY; ROTATION;
   TELEPRESENCE; SIMULATION; NAVIGATION; TAXONOMY; TRAVEL; GAINS
AB Virtual reality (VR) research has provided overviews of locomotion techniques, how they work, their strengths and overall user experience. Considerable research has investigated new methodologies, particularly machine learning to develop redirection algorithms. To best support the development of redirection algorithms through machine learning, we must understand how best to replicate human navigation and behaviour in VR, which can be supported by the accumulation of results produced through live-user experiments. However, it can be difficult to identify, select and compare relevant research without a pre-existing framework in an ever-growing research field. Therefore, this work aimed to facilitate the ongoing structuring and comparison of the VR-based natural walking literature by providing a standardised framework for researchers to utilise. We applied thematic analysis to study methodology descriptions from 140 VR-based papers that contained live-user experiments. From this analysis, we developed the LoCoMoTe framework with three themes: navigational decisions, technique implementation, and modalities. The LoCoMoTe framework provides a standardised approach to structuring and comparing experimental conditions. The framework should be continually updated to categorise and systematise knowledge and aid in identifying research gaps and discussions.
C1 [Croucher, Charlotte] Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, NL-5037 AB Tilburg, Netherlands.
   [Powell, Wendy] Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, NL-5037 AB Tilburg, Netherlands.
   [Stevens, Brett] Univ Portsmouth, Dept Creat Technol, Portsmouth PO1 2ER, England.
C3 Tilburg University; Tilburg University; University of Portsmouth
RP Croucher, C (corresponding author), Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, NL-5037 AB Tilburg, Netherlands.
EM c.s.croucher@tilburguniversity.edu; w.a.powell@tilburguniversity.edu;
   brett.Stevens@port.ac.uk; matt.miller-dicks@port.ac.uk;
   v.powell@tilburguniversity.edu; t.j.wiltshire@tilburguniversity.edu;
   p.spronck@tilburguniversity.edu
RI Croucher, Charlotte/JDW-1448-2023; Wiltshire, Travis/AAJ-9016-2021;
   Miller-Dicks, Matt/O-7489-2016
OI Powell, Wendy/0000-0002-7234-5628; Stevens, Brett/0000-0003-1822-489X;
   Wiltshire, Travis/0000-0001-7630-2695; Miller-Dicks,
   Matt/0000-0001-6584-2733; Croucher, Charlotte/0000-0002-1378-396X
FU Department of Cognitive Science and Artificial Intelligence at Tilburg
   University; Faculty of Creative and Cultural Industries at the
   University of Portsmouth
FX This work was supported in part by Department of Cognitive Science and
   Artificial Intelligence at Tilburg University and in part by the Faculty
   of Creative and Cultural Industries at the University of Portsmouth.
CR Abtahi P, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300752
   Agethen P, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3230648
   Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Nguyen A, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281515
   [Anonymous], 2017, P 21 ACM SIGGRAPH S, DOI DOI 10.1145/3023368.3036844
   [Anonymous], 2011, P VIRT REAL INT C LA
   Auda M., 2019, P CHI C HUM FACT COM, P1, DOI [10.1145/3290605.3300661.102E.R., DOI 10.1145/3290605.3300661.102E.R]
   Azmandian M, 2022, IEEE T VIS COMPUT GR, V28, P2288, DOI 10.1109/TVCG.2022.3150466
   Azmandian M, 2016, 2016 IEEE 2ND WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P9, DOI 10.1109/WEVR.2016.7859537
   Baars N. M., Cognition, Brain, and Consciousness:Introduction to Cognitive Neuroscience
   Bachmann ER, 2019, IEEE T VIS COMPUT GR, V25, P2022, DOI 10.1109/TVCG.2019.2898764
   Badillo S, 2020, CLIN PHARMACOL THER, V107, P871, DOI 10.1002/cpt.1796
   Berton F, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P322, DOI [10.1109/VR46266.2020.1581264804299, 10.1109/VR46266.2020.00-52]
   Berton F, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P717, DOI [10.1109/VR.2019.8798204, 10.1109/vr.2019.8798204]
   Bhandari J, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3139133
   Bianchini S, 2022, RES POLICY, V51, DOI 10.1016/j.respol.2022.104604
   Birmingham U.K., 2012, P 18 INT C EV ASS SO, P375
   Bölling L, 2019, IEEE T VIS COMPUT GR, V25, P2032, DOI 10.1109/TVCG.2019.2899228
   Boletsis Costas, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040024
   Borrego A, 2016, J NEUROENG REHABIL, V13, DOI 10.1186/s12984-016-0174-1
   Bozgeyikli E, 2019, INT J HUM-COMPUT ST, V122, P38, DOI 10.1016/j.ijhcs.2018.08.002
   Bruder G., 2009, Proceedings of the 15th Joint virtual reality Eurographics conference on Virtual Environments, P145
   Bruder G, 2015, IEEE T VIS COMPUT GR, V21, P539, DOI 10.1109/TVCG.2015.2391864
   Bruder G, 2013, DISPLAYS, V34, P132, DOI 10.1016/j.displa.2012.10.007
   Bruder G, 2012, IEEE T VIS COMPUT GR, V18, P1068, DOI 10.1109/TVCG.2011.274
   Bruder G, 2012, IEEE T VIS COMPUT GR, V18, P538, DOI 10.1109/TVCG.2012.55
   Buck LE, 2019, IEEE T VIS COMPUT GR, V25, P2123, DOI 10.1109/TVCG.2019.2899232
   Byrne D., 2022, INT J QUAL METH, V56, P1391, DOI [10.1007/s11135-021-01182-y, DOI 10.1177/1609406917733847]
   Byrne D., 2022, QUAL QUANT, V56, P1391, DOI [10.1007/s11135-021-01182-y, DOI 10.1007/S11135-021-01182-Y]
   Cardoso JCS, 2019, COMPUT GRAPH-UK, V85, P55, DOI 10.1016/j.cag.2019.09.005
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   Chang YC, 2021, IEEE ACCESS, V9, P145083, DOI 10.1109/ACCESS.2021.3118056
   Checa D, 2020, VIRTUAL REAL-LONDON, V24, P151, DOI 10.1007/s10055-019-00389-7
   Chen WY, 2019, LECT NOTES COMPUT SC, V11883, P226, DOI 10.1007/978-3-030-31908-3_14
   Chen ZY, 2021, INT SYM MIX AUGMENT, P184, DOI 10.1109/ISMAR52148.2021.00033
   Cho YH, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P448, DOI 10.1109/VR50410.2021.00068
   Cirio G., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, P155, DOI [DOI 10.1145/1643928.1643965.85L.A, DOI 10.1145/1643928.1643965, 10.1145/1643928.1643965]
   Cirio G, 2012, IEEE T VIS COMPUT GR, V18, P546, DOI 10.1109/TVCG.2012.60
   Congdon BJ, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364277
   Cools R, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357580
   Cowan K, 2019, J BUS RES, V100, P483, DOI 10.1016/j.jbusres.2018.10.063
   Oliveira VAD, 2018, LECT NOTES COMPUT SC, V10894, P500, DOI 10.1007/978-3-319-93399-3_43
   Dewez D, 2020, INT SYM MIX AUGMENT, P452, DOI 10.1109/ISMAR50242.2020.00070
   Di Luca M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445319
   Dong ZC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3345554
   Doyen S, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0029081
   Engel David., 2008, P 2008 ACM S VIRTUAL, P157, DOI 10.1145/1450579.1450612
   Farr AC, 2012, TRANSPORT REV, V32, P715, DOI 10.1080/01441647.2012.712555
   Feasel J, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P97
   Felderer M., 2020, Methods in SoftwareEngineering, P477, DOI [10.1007/978-3-030-32489-617.32E, DOI 10.1007/978-3-030-32489-617.32E]
   Fisher S. S., 1987, Proceedings of the 1986 Workshop on Interactive 3D Graphics, P77, DOI 10.1145/319120.319127
   Flick U., 2018, Qualitative Data Collection, DOI [10.4135/9781526416070, DOI 10.4135/9781526416070]
   Fontan A, 2021, NEUROIMAGE, V244, DOI 10.1016/j.neuroimage.2021.118571
   Freitag S, 2016, IEEE T VIS COMPUT GR, V22, P1462, DOI 10.1109/TVCG.2016.2518298
   Freitag S, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P119, DOI 10.1109/3DUI.2014.6798852
   Gao PZ, 2020, INT SYM MIX AUGMENT, P639, DOI 10.1109/ISMAR50242.2020.00092
   Grechkin J., 2016, P ACM S APPL PERC, P113, DOI [DOI 10.1145/2931002.2931018, 10.1145/2931002.2931018.114E]
   Grechkin TY, 2014, IEEE T VIS COMPUT GR, V20, P596, DOI 10.1109/TVCG.2014.18
   Halilaj E, 2018, J BIOMECH, V81, P1, DOI 10.1016/j.jbiomech.2018.09.009
   Hayashi D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P386, DOI [10.1109/VR.2019.8797989, 10.1109/vr.2019.8797989]
   Hirt C, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P524, DOI 10.1109/VR51125.2022.00072
   Hodgson E, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043604
   Hodgson E, 2015, BEHAV RES METHODS, V47, P296, DOI 10.3758/s13428-014-0463-1
   Hodgson E, 2014, IEEE T VIS COMPUT GR, V20, P579, DOI 10.1109/TVCG.2014.34
   Hodgson E, 2013, IEEE T VIS COMPUT GR, V19, P634, DOI 10.1109/TVCG.2013.28
   Hoshikawa Y, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P464, DOI 10.1109/VR51125.2022.00066
   Hutton Courtney, 2018, ICAT EGVE, P61
   Interrante V, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P167
   Janeh O, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343119
   Joshi Y, 2020, IEEE ACCESS, V8, P39013, DOI 10.1109/ACCESS.2020.2975032
   Jung S, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P1085, DOI 10.1145/3332165.3347868
   Kim D, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P653, DOI 10.1109/VR50410.2021.00091
   Kim M, 2017, IEEE T VIS COMPUT GR, V23, P1379, DOI 10.1109/TVCG.2017.2657139
   Ko TY, 2020, ADJUNCT PROCEEDINGS OF THE 2020 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2020), P201, DOI 10.1109/ISMAR-Adjunct51615.2020.00059
   KRIPPEND.K, 1970, EDUC PSYCHOL MEAS, V30, P61, DOI 10.1177/001316447003000105
   Krippendorff K., 2019, Content Analysis: An Introduction to Its Methodology, V4th, DOI DOI 10.4135/9781071878781
   Krippendorff K., 2011, Computing Krippendorff's alpha-reliability
   Kruse L, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P305, DOI 10.1109/VR.2018.8446216
   Kyngäs H, 2020, APPLICATION OF CONTENT ANALYSIS IN NURSING SCIENCE RESEARCH, P41, DOI 10.1007/978-3-030-30199-6_5
   Langbehn E, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343125
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Langbehn E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201335
   Langbehn E, 2017, IEEE T VIS COMPUT GR, V23, P1349, DOI 10.1109/TVCG.2017.2657220
   Lazar J., 2017, RES METHODS HUMAN CO
   Lee DY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P155, DOI [10.1109/VR46266.2020.1581309443724, 10.1109/VR46266.2020.00-70]
   Lee DY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P63, DOI [10.1109/vr.2019.8798121, 10.1109/VR.2019.8798121]
   Lee J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092832
   Lemon LL, 2020, QUAL REP, V25, P604
   Li Y, 2021, Virtual Reality Intell. Hardware, V3, P451, DOI [10.1016/j.vrih.2021.06.003, DOI 10.1016/J.VRIH.2021.06.003.72]
   Li YJ, 2022, J COMPUT SCI TECH-CH, V37, P561, DOI 10.1007/s11390-022-2266-7
   Li YJ, 2021, INT SYM MIX AUGMENT, P21, DOI 10.1109/ISMAR52148.2021.00016
   Li YJ, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P95, DOI [10.1109/VR50410.2021.00030, 10.1109/ISHC54333.2021.00026]
   Liapis C., P CHI C HUM FACT COM
   Liu X., 2021, Virtual Reality Intell. Hardware, V3, P470, DOI DOI 10.1016/J.VRIH.2021.06.004.73Y
   Marchal M, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P19, DOI 10.1109/3DUI.2010.5446238
   Marín-Morales J, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0223881
   Marsh WE, 2013, PRESENCE-VIRTUAL AUG, V22, P216, DOI 10.1162/PRES_a_00152
   Matsumoto K, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365705
   Matsumoto K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P101, DOI [10.1109/VR46266.2020.00-76, 10.1109/VR46266.2020.1581262503135]
   Matsumoto K, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P105, DOI 10.1109/3DUI.2016.7460038
   Matsumoto T., 2017, ACM SIGGRAPH 2017 PO, P1, DOI [10.1145/3102163.3102227.152, DOI 10.1145/3102163.3102227.152]
   McCullough M, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P107, DOI 10.1145/2804408.2804416
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Meyer F., 2016, P SOUND MUS COMP C, P1
   Min DH, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P164, DOI [10.1109/VR46266.2020.00-69, 10.1109/VR46266.2020.1581308731108]
   Mohler Betty., 2007, 13th Eurographics Symposium on Virtual Environments and 10th Immersive Projection Technology Workshop (IPT-EGVE 2007), P85, DOI DOI 10.2312/PE/VE2007SHORT/085-088
   Mon CS, 2019, 2019 IEEE 9TH SYMPOSIUM ON COMPUTER APPLICATIONS & INDUSTRIAL ELECTRONICS (ISCAIE), P249, DOI [10.1109/iscaie.2019.8743738, 10.1109/ISCAIE.2019.8743738]
   Mousas C, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P726, DOI [10.1109/vr.2019.8798043, 10.1109/VR.2019.8798043]
   Munn Z, 2018, BMC MED RES METHODOL, V18, DOI 10.1186/s12874-018-0611-x
   Nabiyouni Mahdi, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2015.7131717
   Nelson M, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365709
   Nescher T, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P111, DOI 10.1109/3DUI.2014.6798851
   Neth CT, 2012, IEEE T VIS COMPUT GR, V18, P1041, DOI 10.1109/TVCG.2011.275
   Nguyen A, 2020, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2020), DOI 10.1145/3385955.3407932
   Nguyen Anh, 2020, 26 ACM S VIRTUAL REA, DOI 10.1145/3385956.3418950
   Nguyen P., 2020, P 26 ACM S VIRT REAL, P1, DOI [10.1145/3385956.3422099.134M, DOI 10.1145/3385956.3422099.134M]
   Nilsson NC, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3180658
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Nilsson NC, 2016, P IEEE VIRT REAL ANN, P241, DOI 10.1109/VR.2016.7504743
   Nitzsche N, 2004, PRESENCE-TELEOP VIRT, V13, P44, DOI 10.1162/105474604774048225
   Nogalski M, 2016, P IEEE VIRT REAL ANN, P245, DOI 10.1109/VR.2016.7504745
   Nordahl R, 2006, P 9 INT WORKSH PRES, P57
   Nordahl R, 2011, PSYCHNOLOGY J, V9, P245
   O'Connor S, 2019, CLIN NURS RES, V28, P523, DOI 10.1177/1054773819845824
   Ogawa K, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P635, DOI 10.1109/VR51125.2022.00084
   Paludan A, 2016, P IEEE VIRT REAL ANN, P259, DOI 10.1109/VR.2016.7504752
   Paris RA, 2017, P IEEE VIRT REAL ANN, P261, DOI 10.1109/VR.2017.7892276
   Paul J, 2022, PSYCHOL MARKET, V39, P1099, DOI 10.1002/mar.21657
   Peck TC, 2011, P IEEE VIRT REAL ANN, P55, DOI 10.1109/VR.2011.5759437
   Peck TC, 2010, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2010.5444816
   Peck TC, 2009, IEEE T VIS COMPUT GR, V15, P383, DOI 10.1109/TVCG.2008.191
   Podkosova I, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190845
   Pyzer-Knapp EO, 2022, NPJ COMPUT MATER, V8, DOI 10.1038/s41524-022-00765-z
   Qi Sun, 2020, Real VR - Immersive Digital Reality: How to Import the Real World into Head-Mounted Immersive Displays. Lecture Notes in Computer Science (LNCS 11900), P285, DOI 10.1007/978-3-030-41816-8_12
   Ranasinghe N, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174151
   Razzaque Z. Kohn, 2001, Redirected Walking, DOI [10.2312/egs.20011036.146M, DOI 10.2312/EGS.20011036.146M]
   Reimer D, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P154, DOI [10.1109/VRW50115.2020.00032, 10.1109/VRW50115.2020.0-244]
   Rewkowski N, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P395, DOI [10.1109/vr.2019.8798286, 10.1109/VR.2019.8798286]
   Rietzler M, 2018, INT SYM MIX AUGMENT, P115, DOI 10.1109/ISMAR.2018.00041
   Roberts K, 2019, BMC MED RES METHODOL, V19, DOI 10.1186/s12874-019-0707-y
   Rocco T.S., 2009, HUM RESOUR DEV REV, V8, P120, DOI 10.1177/1534484309332617
   Rothacher Y, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-36035-6
   Ruddle RA, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2465780.2465785
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Sakono H, 2021, IEEE T VIS COMPUT GR, V27, P4278, DOI 10.1109/TVCG.2021.3106501
   Samperi R., P 26 BCS C HUM COMP
   Saunders B, 2018, QUAL QUANT, V52, P1893, DOI 10.1007/s11135-017-0574-8
   Schmelter T, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451264
   Schmitz P, 2018, IEEE T VIS COMPUT GR, V24, P1623, DOI 10.1109/TVCG.2018.2793671
   Serafin S, 2013, P IEEE VIRT REAL ANN, P161, DOI 10.1109/VR.2013.6549412
   Shibayama W, 2020, LECT NOTES COMPUT SC, V12221, P33, DOI 10.1007/978-3-030-61864-3_4
   Simeone AL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P598, DOI [10.1109/VR46266.2020.1581330966612, 10.1109/VR46266.2020.00082]
   Simeone AL, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3307, DOI 10.1145/2702123.2702389
   Sjolund LA, 2018, MEM COGNITION, V46, P89, DOI 10.3758/s13421-017-0747-7
   SLATER M, 1994, ARTIFICIAL LIFE AND VIRTUAL REALITY, P125
   Souman JL, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043607
   Srinagesh K., 2006, PRINCIPLES EXPT RES
   Steinicke F., 2008, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P149, DOI DOI 10.1145/1450579.14506112[45]F
   Steinicke F, 2010, COMPUT GRAPH-UK, V34, P26, DOI 10.1016/j.cag.2009.12.003
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Steinicke F, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P217, DOI 10.1109/CW.2008.53
   Steinicke Frank., 2009, P ACM SIGGRAPH S VID, P111
   Strauss RR, 2020, IEEE T VIS COMPUT GR, V26, P1955, DOI 10.1109/TVCG.2020.2973060
   Suma E. A., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P35, DOI 10.1109/3DUI.2011.5759214
   Suma EA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P43, DOI 10.1109/VR.2012.6180877
   Suma EA, 2012, IEEE T VIS COMPUT GR, V18, P555, DOI 10.1109/TVCG.2012.47
   Suma EA, 2011, P IEEE VIRT REAL ANN, P159, DOI 10.1109/VR.2011.5759455
   Suma EA, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P27, DOI 10.1109/3DUI.2010.5444726
   Suma EA, 2010, IEEE T VIS COMPUT GR, V16, P690, DOI 10.1109/TVCG.2009.93
   Suma S., 2007, P IEEE S 3D US INT C, DOI [10.1109/3DUI.2007.340788.154R.A., DOI 10.1109/3DUI.2007.340788.154R.A]
   Sun Q, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201294
   Sun Q, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925883
   Tawfik GM, 2019, TROP MED HEALTH, V47, DOI 10.1186/s41182-019-0165-6
   Uematsu A, 2011, NEUROSCI LETT, V505, P291, DOI 10.1016/j.neulet.2011.10.057
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Vaismoradi M, 2013, NURS HEALTH SCI, V15, P398, DOI 10.1111/nhs.12048
   van de Schoot R, 2021, NAT MACH INTELL, V3, P125, DOI 10.1038/s42256-020-00287-7
   Vasylevska K, 2017, IEEE COMPUT GRAPH, V37, P85, DOI 10.1109/MCG.2017.3621226
   Vasylevska K, 2017, IEEE SYMP 3D USER, P12, DOI 10.1109/3DUI.2017.7893312
   Vasylevska K, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P39, DOI 10.1109/3DUI.2013.6550194
   Wiener JM, 2009, SPAT COGN COMPUT, V9, P152, DOI 10.1080/13875860902906496
   Williams B., 2006, Proceedings of the 3rd Symposium on applied Perception in Graphics and visualization (ACM), P21, DOI [10.1145/1140491.1140495, DOI 10.1145/1140491.1140495]
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
   Williams B, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010329
   Williams NL, 2019, IEEE T VIS COMPUT GR, V25, P3158, DOI 10.1109/TVCG.2019.2932213
   Williams-Sanders B, 2019, LECT NOTES COMPUT SC, V11574, P277, DOI 10.1007/978-3-030-21607-8_22
   Wilson G, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173673
   Wilson PT, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P243, DOI 10.1145/3013971.3014010
   Wolf D, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376243
   Xie X., 2010, P 7 S APPL PERC GRAP, P65, DOI [10.1145/1836248.1836260, DOI 10.1145/1836248.1836260]
   Xu W, 2020, INT J QUAL METH, V19, DOI 10.1177/1609406920918810
   Yang J, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P1093, DOI 10.1145/3332165.3347875
   Zanbaka CA, 2005, IEEE T VIS COMPUT GR, V11, P694, DOI 10.1109/TVCG.2005.92
   Zank M, 2016, VISUAL COMPUT, V32, P1323, DOI 10.1007/s00371-016-1229-9
   Zank M, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P49, DOI 10.1109/3DUI.2016.7460030
   Zhang B, 2011, VISION RES, V51, P2356, DOI 10.1016/j.visres.2011.09.008
   Zhang J, 2018, IEEE T VIS COMPUT GR, V24, P1671, DOI 10.1109/TVCG.2018.2793679
   Zhang R., 2014, Proc. Proceedings of the 2nd ACM Symposium on Spatial User Interaction, P62, DOI [10.1145/2659766.2659783.177M, DOI 10.1145/2659766.2659783.177M]
   Zhang R., 2013, Proc. Proceedings of the ACM Symposium on Applied Perception (SAP), P71
   Zhang SH, 2023, IEEE T VIS COMPUT GR, V29, P2080, DOI 10.1109/TVCG.2021.3139990
   Zmuda MA, 2013, IEEE T VIS COMPUT GR, V19, P1872, DOI 10.1109/TVCG.2013.88
NR 201
TC 2
Z9 2
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5765
EP 5781
DI 10.1109/TVCG.2023.3313439
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400017
PM 37695974
OA hybrid, Green Published
DA 2024-11-06
ER

PT J
AU Ogawa, K
   Fujita, K
   Sakamoto, S
   Takashima, K
   Kitamura, Y
AF Ogawa, Kumpei
   Fujita, Kazuyuki
   Sakamoto, Shuichi
   Takashima, Kazuki
   Kitamura, Yoshifumi
TI Exploring Visual-Auditory Redirected Walking Using Auditory Cues in
   Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Legged locomotion; Location awareness; Ear; Urban areas;
   Rivers; Reliability; Redirected walking; room-scale VR; visual-auditory
   redirection
ID THRESHOLDS; ROTATION; CAPTURE; MOTION
AB We examine the effect of auditory cues occurring in reality on redirection. Specifically, we set two hypotheses: the auditory cues emanating from fixed positions in reality (Fixed sound, FS) increase the noticeability of redirection, while the auditory cues whose positions are manipulated consistently with the visual manipulation (Redirected sound, RDS) decrease the noticeability of redirection. To verify these hypotheses, we implemented an experimental environment that virtually reproduced FS and RDS conditions using binaural recording, and then we conducted a user study (N=18) to investigate the detection thresholds (DTs) for rotational manipulation and the sound localization accuracy of the auditory cues under FS and RDS, as well as the baseline condition without auditory cues (No sound, NS). The results show, against the hypotheses, FS gave a wider range of DTs than NS, while RDS gave a similar range of DTs to NS. Combining these results with those of sound localization accuracy reveals that, rather than the auditory cues affecting the participants' spatial perception in VR, the visual manipulation made their sound localization less accurate, which would be a reason for the increased range of DTs under FS. Furthermore, we conducted a follow-up user study (N=11) to measure the sound localization accuracy of FS where the auditory cues were actually placed in a real setting, and we found that the accuracy tended to be similar to that of virtually reproduced FS, suggesting the validity of the auditory cues used in this study. Given these findings, we also discuss potential applications.
C1 [Ogawa, Kumpei; Fujita, Kazuyuki; Sakamoto, Shuichi; Takashima, Kazuki; Kitamura, Yoshifumi] Tohoku Univ, Res Inst Elect Commun, Sendai, Miyagi 9808577, Japan.
C3 Tohoku University
RP Fujita, K (corresponding author), Tohoku Univ, Res Inst Elect Commun, Sendai, Miyagi 9808577, Japan.
EM kumpei.ogawa.r5@dc.tohoku.ac.jp; k-fujita@riec.tohoku.ac.jp;
   saka@ais.riec.tohoku.ac.jp; takashima@riec.tohoku.ac.jp;
   kitamura@riec.tohoku.ac.jp
OI Fujita, Kazuyuki/0000-0002-1039-0167; Ogawa, Kumpei/0000-0002-5757-7578
FU JSPS [22H00523]
FX No Statement Available
CR Congdon BJ, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364277
   Dichgans J, 1978, Handbook of Sensory Physiology, V8, P755, DOI [DOI 10.1007/978-3-642-46354-9_25, 10.1007/978-3-642-46354-925, DOI 10.1007/978-3-642-46354-925, 10.1007/978-3-642-46354-9_25]
   Dodge R, 1923, J EXP PSYCHOL, V6, P107, DOI 10.1037/h0076105
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Feigl E., 2017, P S VIRT REAL SOFTW, p45:1, DOI [10.1145/3139131.314120529R, DOI 10.1145/3139131.314120529R]
   Gao PZ, 2020, INT SYM MIX AUGMENT, P639, DOI 10.1109/ISMAR50242.2020.00092
   GARDNER MB, 1969, J ACOUST SOC AM, V45, P47, DOI 10.1121/1.1911372
   Goldstein E. B., 2010, Sensation and perception
   Gotoh T., 1977, Journal of the Acoustical Society of Japan, V33, P667
   Grechkin J., 2016, P ACM S APPL PERC, P113, DOI [DOI 10.1145/2931002.2931018, 10.1145/2931002.2931018.114E]
   Honda A, 2022, I-PERCEPTION, V13, DOI 10.1177/20416695211070616
   Iwaya Y., 2003, Acoustical Science and Technology, V24, P322, DOI 10.1250/ast.24.322
   Junker A, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P358, DOI 10.1109/VRW52623.2021.00071
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kitajima N, 1999, PERCEPT MOTOR SKILL, V89, P1139, DOI 10.2466/PMS.89.7.1139-1158
   Kruse L, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P305, DOI 10.1109/VR.2018.8446216
   LACKNER JR, 1977, AVIAT SPACE ENVIR MD, V48, P129
   Larsson P., 2004, Proceedings of 7th Annual Workshop of Presence, P252
   Lee S., 2022, P CHI C HUM FACT COM, p446:1, DOI [10.1145/3491101.351971932, DOI 10.1145/3491101.351971932]
   Li YJ, 2022, J COMPUT SCI TECH-CH, V37, P561, DOI 10.1007/s11390-022-2266-7
   Rayleigh,, 1907, PHILOS MAG, V13, P214, DOI 10.1080/14786440709463595
   MATEEFF S, 1985, PERCEPTION, V14, P721, DOI 10.1068/p140721
   Meyer M., 2016, P SOUND MUS COMP C, P293
   Nguyen A, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225155
   Nilsson NC, 2018, IEEE COMPUT GRAPH, V38, P44, DOI 10.1109/MCG.2018.111125628
   Nilsson NC, 2016, P IEEE VIRT REAL ANN, P241, DOI 10.1109/VR.2016.7504743
   Nogalski M, 2016, P IEEE VIRT REAL ANN, P245, DOI 10.1109/VR.2016.7504745
   Paludan A, 2016, P IEEE VIRT REAL ANN, P259, DOI 10.1109/VR.2016.7504752
   Razzaque S., 2001, P 22 ANN C EUR ASS C, P289, DOI DOI 10.2312/EGS.20011036
   Rewkowski N, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P395, DOI [10.1109/vr.2019.8798286, 10.1109/VR.2019.8798286]
   Riecke BE, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P147
   Serafin S, 2013, P IEEE VIRT REAL ANN, P161, DOI 10.1109/VR.2013.6549412
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Strybel TZ, 2000, J ACOUST SOC AM, V108, P3092, DOI 10.1121/1.1323720
   Thompson S.P., 1882, PHILOS MAG, V13, P406, DOI [10.1080/14786448208627205, DOI 10.1080/14786448208627205]
   THURLOW WR, 1970, AM J PSYCHOL, V83, P112, DOI 10.2307/1420861
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Weller R, 2022, VISUAL COMPUT, V38, P3475, DOI 10.1007/s00371-022-02565-4
   Williams NL, 2019, IEEE T VIS COMPUT GR, V25, P3158, DOI 10.1109/TVCG.2019.2932213
   Zhang J, 2018, IEEE T VIS COMPUT GR, V24, P1671, DOI 10.1109/TVCG.2018.2793679
NR 40
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5782
EP 5794
DI 10.1109/TVCG.2023.3309267
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400057
PM 37639419
OA hybrid
DA 2024-11-06
ER

PT J
AU Zhou, XC
   Li, BS
   Benes, B
   Fei, SL
   Pirk, S
AF Zhou, Xiaochen
   Li, Bosheng
   Benes, Bedrich
   Fei, Songlin
   Pirk, Soren
TI DeepTree: Modeling Trees With Situated Latents
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Measurement; Solid modeling; Three-dimensional displays; Shape;
   Computational modeling; Neural networks; Pipelines; Botanical tree
   models; deep learning; generative methods; shape modeling
ID VIRTUAL CLIMBING PLANTS
AB In this article, we propose DeepTree, a novel method for modeling trees based on learning developmental rules for branching structures instead of manually defining them. We call our deep neural model "situated latent" because its behavior is determined by the intrinsic state -encoded as a latent space of a deep neural model- and by the extrinsic (environmental) data that is "situated" as the location in the 3D space and on the tree structure. We use a neural network pipeline to train a situated latent space that allows us to locally predict branch growth only based on a single node in the branch graph of a tree model. We use this representation to progressively develop new branch nodes, thereby mimicking the growth process of trees. Starting from a root node, a tree is generated by iteratively querying the neural network on the newly added nodes resulting in the branching structure of the whole tree. Our method enables generating a wide variety of tree shapes without the need to define intricate parameters that control their growth and behavior. Furthermore, we show that the situated latents can also be used to encode the environmental response of tree models, e.g., when trees grow next to obstacles. We validate the effectiveness of our method by measuring the similarity of our tree models and by procedurally generated ones based on a number of established metrics for tree form.
C1 [Zhou, Xiaochen; Li, Bosheng; Benes, Bedrich] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Fei, Songlin] Purdue Univ, Dept Forestry & Nat Resources, W Lafayette, IN 47907 USA.
   [Pirk, Soren] Univ Kiel, Dept Comp Sci, D-24143 Kiel, Germany.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; University of Kiel
RP Benes, B (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM zhou1178@purdue.edu; li2343@purdue.edu; bbenes@purdue.edu;
   sfei@purdue.edu; soeren.pirk@gmail.com
RI Fei, Songlin/JTD-3325-2023; Benes, Bedrich/A-8150-2016
OI Pirk, Soren/0000-0003-1937-9797; Benes, Bedrich/0000-0002-5293-2112;
   Fei, Songlin/0000-0003-2772-0166; LI, BOSHENG/0009-0006-6490-1184; Zhou,
   Xiaochen/0000-0003-2280-7799
FU Foundation for Food and Agriculture Research, United States [602757];
   USDA NIFA [2023-68012-38992]
FX This research was partially supported in part by the Foundation for Food
   and Agriculture Research, United States Grant under Grant 602757 to
   Benes, and in part by USDA NIFA, under Grant #2023-68012-38992 to
   Benesand Fei.
CR AONO M, 1984, IEEE COMPUT GRAPH, V4, P10, DOI 10.1109/MCG.1984.276141
   Argudo O, 2020, COMPUT GRAPH FORUM, V39, P174, DOI 10.1111/cgf.13752
   Benes B, 2002, COMP ANIM CONF PROC, P33, DOI 10.1109/CA.2002.1017504
   Bradley D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461952
   Chen B., 2008, ACM Trans. Graph., V27, P38
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deussen O., 2005, DIGITAL DESIGN NATUR
   Du SL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11182074
   Estrada R, 2015, IEEE T PATTERN ANAL, V37, P1688, DOI 10.1109/TPAMI.2014.2382116
   Greene N., 1989, Computer Graphics, V23, P175, DOI 10.1145/74334.74351
   Guo JW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3394105
   Habel R, 2009, COMPUT GRAPH FORUM, V28, P523, DOI 10.1111/j.1467-8659.2009.01391.x
   Hack J. T., 1957, Studies of longitudinal stream profiles in Virginia and Mary-land, V294
   Hädrich T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459954
   Hädrich T, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13106
   Ijiri T, 2006, COMPUT GRAPH FORUM, V25, P617, DOI 10.1111/j.1467-8659.2006.00981.x
   Kohek S, 2015, COMPUTING, V97, P145, DOI 10.1007/s00607-014-0424-7
   Li BS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480525
   Li C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024161
   Lintermann B, 1999, IEEE COMPUT GRAPH, V19, P56, DOI 10.1109/38.736469
   Liu YC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480486
   Livny Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964948
   Longay A., 2012, P INT S SBIM, P107, DOI [DOI 10.2312/SBM/SBM12/107-120, 10.2312/SBM/ SBM12/107-120]
   Makowski M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323039
   Mech R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P397, DOI 10.1145/237170.237279
   Neubert B, 2011, COMPUT GRAPH FORUM, V30, P1708, DOI 10.1111/j.1467-8659.2011.01897.x
   Neubert B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239539
   Niese T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3502220
   Okabe S., 2007, P ACM SIGGRAPH COURS
   Oppenheimer P. E., 1986, Computer Graphics, V20, P55, DOI 10.1145/15886.15892
   Palubicki W, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531364
   Palubicki W, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530146
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pirk S., 2016, P ACM SIGGRAPH COURS
   Pirk S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661252
   Pirk S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366188
   Pirk S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185546
   Pirk S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130814
   Polasek T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480519
   Prusinkiewicz P, 1990, The Algorithmic Beauty of Plants
   Reeves W. T., 1985, Computer Graphics, V19, P313, DOI 10.1145/325165.325250
   Shao H., 2021, Adv. Neural Inf. Process. Syst, V34, P4829
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Tan P, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409061
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Wang Y. Zhao, 2017, ACM Trans. Graph., V36, p135:1
   Weber J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P119, DOI 10.1145/218380.218427
   Wither J, 2009, COMPUT GRAPH FORUM, V28, P541, DOI 10.1111/j.1467-8659.2009.01394.x
   Wong SK, 2016, COMPUT GRAPH FORUM, V35, P5, DOI 10.1111/cgf.12736
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289610
   Xu JR, 2021, AI OPEN, V2, P69, DOI 10.1016/j.aiopen.2021.05.002
   Xu L, 2015, COMPUT GRAPH FORUM, V34, P47, DOI 10.1111/cgf.12744
   Zhang XP, 2017, COMPUT GRAPH FORUM, V36, P402, DOI 10.1111/cgf.13088
   Zhao YL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461961
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
NR 57
TC 5
Z9 5
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5795
EP 5809
DI 10.1109/TVCG.2023.3307887
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400053
PM 37610910
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Loi, I
   Zacharaki, EI
   Moustakas, K
AF Loi, Iliana
   Zacharaki, Evangelia I.
   Moustakas, Konstantinos
TI Machine Learning Approaches for 3D Motion Synthesis and Musculoskeletal
   Dynamics Estimation: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computer graphics; animation; motion prediction; motion synthesis;
   machine learning; biomechanical simulation
AB The inference of 3D motion and dynamics of the human musculoskeletal system has traditionally been solved using physics-based methods that exploit physical parameters to provide realistic simulations. Yet, such methods suffer from computational complexity and reduced stability, hindering their use in computer graphics applications that require real-time performance. With the recent explosion of data capture (mocap, video) machine learning (ML) has started to become popular as it is able to create surrogate models harnessing the huge amount of data stemming from various sources, minimizing computational time (instead of resource usage), and most importantly, approximate real-time solutions. The main purpose of this paper is to provide a review and classification of the most recent works regarding motion prediction, motion synthesis as well as musculoskeletal dynamics estimation problems using ML techniques, in order to offer sufficient insight into the state-of-the-art and draw new research directions. While the study of motion may appear distinct to musculoskeletal dynamics, these application domains provide jointly the link for more natural computer graphics character animation, since ML-based musculoskeletal dynamics estimation enables modeling of more long-term, temporally evolving, ergonomic effects, while offering automated and fast solutions. Overall, our review offers an in-depth presentation and classification of ML applications in human motion analysis, unlike previous survey articles focusing on specific aspects of motion prediction.
C1 [Loi, Iliana; Zacharaki, Evangelia I.; Moustakas, Konstantinos] Univ Patras, Dept Elect & Comp Engn, Patras 26504, Greece.
   [Zacharaki, Evangelia I.] Univ Miami, Miller Sch Med, Miami, FL 33136 USA.
C3 University of Patras; University of Miami
RP Loi, I (corresponding author), Univ Patras, Dept Elect & Comp Engn, Patras 26504, Greece.
EM loi@ceid.upatras.gr; exz187@med.miami.edu; moustakas@ece.upatras.gr
RI Zacharaki, Evangelia/AAC-6661-2020
OI Loi, Iliana/0000-0001-9112-0638; Zacharaki,
   Evangelia/0000-0001-8228-0437; Moustakas,
   Konstantinos/0000-0001-7617-227X
FU EACEA - Erasmus+ [101082688]
FX This work was supported by the EACEA - Erasmus+ under Grant 101082688 -
   Project: EMMBIOME. Recommended for acceptance byT. Komura.
CR Sigurdsson GA, 2018, Arxiv, DOI arXiv:1804.09626
   Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392462
   Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392469
   Alemi O, 2019, Arxiv, DOI arXiv:1903.08356
   Aliakbarian S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11313, DOI 10.1109/ICCV48922.2021.01114
   Aliakbarian S, 2020, PROC CVPR IEEE, P5222, DOI 10.1109/CVPR42600.2020.00527
   [Anonymous], 2003, CMU graphics lab. Carnegie mellon university motion capture database
   [Anonymous], 2018, Adobe systems incs Mixamo
   Aristidou A, 2023, IEEE T VIS COMPUT GR, V29, P3519, DOI 10.1109/TVCG.2022.3163676
   Bergamin K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356536
   Bougares F., 2014, P C EMP METH NAT LAN, DOI DOI 10.3115/V1/D14-1179
   Briq R, 2022, Arxiv, DOI arXiv:2206.06741
   Burton WS II, 2021, J BIOMECH, V123, DOI 10.1016/j.jbiomech.2021.110439
   Cai YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11625, DOI 10.1109/ICCV48922.2021.01144
   Cao Z., 2020, P EUR C COMP VIS RCC, P387, DOI [10.1007/978-3-030-58452-8_23, DOI 10.1007/978-3-030-58452-8_23]
   Chang ZY, 2022, Arxiv, DOI arXiv:2212.08526
   Cheema N, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376701
   Chen WH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2131, DOI 10.1145/3394171.3413669
   Chollet F., 2017, Deep Learning with Python
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Dao TT, 2019, MED BIOL ENG COMPUT, V57, P1049, DOI 10.1007/s11517-018-1940-y
   Ehsani K, 2020, PROC CVPR IEEE, P221, DOI 10.1109/CVPR42600.2020.00030
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Fregly BJ, 2012, J ORTHOP RES, V30, P503, DOI 10.1002/jor.22023
   Giarmatzis G, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20236933
   Gomes TL, 2020, IEEE WINT CONF APPL, P3355, DOI 10.1109/WACV45572.2020.9093395
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Guo C, 2022, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR52688.2022.00509
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Guo W, 2022, PROC CVPR IEEE, P13043, DOI 10.1109/CVPR52688.2022.01271
   Halilaj E, 2018, J BIOMECH, V81, P1, DOI 10.1016/j.jbiomech.2018.09.009
   Harvey FG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392480
   Hassan M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11354, DOI 10.1109/ICCV48922.2021.01118
   Hassan M, 2019, IEEE I CONF COMP VIS, P2282, DOI 10.1109/ICCV.2019.00237
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Honda Y., 2020, P 31 BRIT MACH VIS C
   Hwang W, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112455
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang YF, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322966
   Jin X, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/8899880
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Lee S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322972
   Lee S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459774
   Li JM, 2021, INT CONF 3D VISION, P771, DOI 10.1109/3DV53792.2021.00086
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li PZ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530157
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Ling HY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392422
   Liu DW, 2022, MEASUREMENT, V198, DOI 10.1016/j.measurement.2022.111344
   Liu ZG, 2021, Arxiv, DOI [arXiv:2103.09755, 10.48550/arXiv.2103.09755]
   Liu ZG, 2019, PROC CVPR IEEE, P9996, DOI 10.1109/CVPR.2019.01024
   Loi I, 2021, FRONT BIOENG BIOTECH, V9, DOI 10.3389/fbioe.2021.648356
   Ma HB, 2022, PROC CVPR IEEE, P8151, DOI 10.1109/CVPR52688.2022.00799
   Ma J., 2022, arXiv, DOI DOI 10.48550/ARXIV.2212.02837
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Maeda T, 2022, PROC CVPR IEEE, P6417, DOI 10.1109/CVPR52688.2022.00632
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Mao W, 2022, PROC CVPR IEEE, P8141, DOI 10.1109/CVPR52688.2022.00798
   Mao W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13289, DOI 10.1109/ICCV48922.2021.01306
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Men QH, 2022, COMPUT GRAPH-UK, V102, P634, DOI 10.1016/j.cag.2021.09.014
   Mourot L, 2020, Arxiv, DOI arXiv:2007.01151
   Mourot L, 2022, Arxiv, DOI arXiv:2208.04598
   Muller M., 2007, Documentation mocap database hdm05
   Nowakowski K, 2022, MED BIOL ENG COMPUT, V60, P1745, DOI 10.1007/s11517-022-02567-3
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Park Jungnam, 2022, arXiv
   Pavlou M, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.646415
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459670, 10.1145/3197517.3201311]
   Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014
   Pu JF, 2022, Arxiv, DOI arXiv:2207.03682
   Punnakkal AR, 2021, PROC CVPR IEEE, P722, DOI 10.1109/CVPR46437.2021.00078
   Raab S, 2023, Arxiv, DOI arXiv:2302.05905
   Raab S, 2022, Arxiv, DOI arXiv:2206.08010
   Rane L, 2019, ANN BIOMED ENG, V47, P778, DOI 10.1007/s10439-018-02190-0
   Rasouli A, 2020, Arxiv, DOI arXiv:2007.00095
   Ren JL, 2019, IEEE INT CONF ROBOT, P5076, DOI [10.1109/ICRA.2019.8794187, 10.1109/icra.2019.8794187]
   Risvas K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P119, DOI [10.1109/VRW50115.2020.0-250, 10.1109/VRW50115.2020.00026]
   Rudenko A, 2020, INT J ROBOT RES, V39, P895, DOI 10.1177/0278364920917446
   Salzmann T, 2022, PROC CVPR IEEE, P6447, DOI 10.1109/CVPR52688.2022.00635
   Seth A, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006223
   Seth A, 2011, PROC IUTAM, V2, P212, DOI 10.1016/j.piutam.2011.04.021
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shen YJ, 2020, IEEE T VIS COMPUT GR, V26, P2620, DOI 10.1109/TVCG.2019.2893247
   Shi MY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3407659
   Shu LM, 2018, J BIOMECH, V77, P146, DOI 10.1016/j.jbiomech.2018.07.008
   Shu T., 2016, 25 INT JOINT C ART I
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Sohane A, 2022, COMPUT J, V65, P1167, DOI 10.1093/comjnl/bxaa160
   Starke S, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530178
   Starke S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459881
   Starke S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392450
   Starke S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356505
   Stetter BJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173690
   Taheri Omid, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P581, DOI 10.1007/978-3-030-58548-8_34
   Tang YY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P935
   TEVET G., 2022, arXiv
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang H, 2021, IEEE T VIS COMPUT GR, V27, P216, DOI 10.1109/TVCG.2019.2936810
   Wang JP, 2019, IEEE ACCESS, V7, P92465, DOI 10.1109/ACCESS.2019.2927606
   Wang JB, 2022, PROC CVPR IEEE, P20428, DOI 10.1109/CVPR52688.2022.01981
   Wang JB, 2021, PROC CVPR IEEE, P12201, DOI 10.1109/CVPR46437.2021.01203
   Wang X, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/104535
   Wang ZY, 2021, IEEE T VIS COMPUT GR, V27, P14, DOI 10.1109/TVCG.2019.2938520
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Xie K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11512, DOI 10.1109/ICCV48922.2021.01133
   Yang JT, 2021, P I MECH ENG I-J SYS, V235, P180, DOI 10.1177/0959651820945814
   Yasar MS, 2021, arXiv
   Yuan Y., 2020, Advances in Neural Information Processing Systems, V33, P21763, DOI 10.48550/arXiv.2006.07364
   Yuan Y, 2022, Arxiv, DOI arXiv:2212.02500
   Yun K., 2012, 2012 IEEE COMP SOC C, P28
   Zhang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201366
   Zhang J, 2022, Arxiv, DOI arXiv:2207.01435
   Zhang Y, 2021, PROC CVPR IEEE, P3371, DOI 10.1109/CVPR46437.2021.00338
   Zhang ZB, 2022, IEEE ROBOT AUTOM LET, V7, P8949, DOI 10.1109/LRA.2022.3188892
   Zhong CY, 2022, PROC CVPR IEEE, P6437, DOI 10.1109/CVPR52688.2022.00634
   Zhu YA, 2020, ARTIF INTELL MED, V103, DOI 10.1016/j.artmed.2020.101811
NR 120
TC 0
Z9 0
U1 9
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5810
EP 5829
DI 10.1109/TVCG.2023.3308753
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400083
PM 37624722
OA hybrid
DA 2024-11-06
ER

PT J
AU Grubert, J
   Witzani, L
   Otte, A
   Gesslein, T
   Kranz, M
   Kristensson, PO
AF Grubert, Jens
   Witzani, Lukas
   Otte, Alexander
   Gesslein, Travis
   Kranz, Matthias
   Kristensson, Per Ola
TI Text Entry Performance and Situation Awareness of a Joint Optical
   See-Through Head-Mounted Display and Smartphone System
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; cross-device; head-mounted display; mobile;
   multi-display; optical see-through; text entry
ID PHONE; DISTRACTION; CHALLENGES; LEGIBILITY
AB Optical see-through head-mounted displays (OST HMDs) are a popular output medium for mobile Augmented Reality (AR) applications. To date, they lack efficient text entry techniques. Smartphones are a major text entry medium in mobile contexts but attentional demands can contribute to accidents while typing on the go. Mobile multi-display ecologies, such as combined OST HMD-smartphone systems, promise performance and situation awareness benefits over single-device use. We study the joint performance of text entry on mobile phones with text output on optical see-through head-mounted displays. A series of five experiments with a total of 86 participants indicate that, as of today, the challenges in such a joint interactive system outweigh the potential benefits.
C1 [Grubert, Jens] Coburg Univ Appl Sci & Arts, Human Comp Interact Internet Things, D-96450 Coburg, Germany.
   [Witzani, Lukas] Univ Passau, D-96450 Passau, Germany.
   [Otte, Alexander] Coburg Univ Appl Sci & Arts, D-94032 Coburg, Germany.
   [Gesslein, Travis] Coburg Univ Appl Sci & Arts, Coburg CB2 1TN, Germany.
C3 Klinikum Coburg; University of Passau; Klinikum Coburg
RP Grubert, J (corresponding author), Coburg Univ Appl Sci & Arts, Human Comp Interact Internet Things, D-96450 Coburg, Germany.
EM jens.grubert@hs-coburg.de; lukas.witzani@uni-passau.de;
   alexander.otte@hs-coburg.de; travis.gesslein@hs-coburg.de;
   matthias.kranz@uni-passau.de; pok21@cam.ac.uk
RI Kranz, Matthias/E-1703-2013; Grubert, Jens/B-1012-2018
OI Kranz, Matthias/0000-0003-3835-4974; Kristensson, Per
   Ola/0000-0002-7139-871X; Grubert, Jens/0000-0002-3858-2961
CR Aksit K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130892
   Arefin MS, 2022, IEEE T VIS COMPUT GR, V28, P2014, DOI 10.1109/TVCG.2022.3150503
   Arefin MS, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P571, DOI [10.1109/VRW50115.2020.0-139, 10.1109/VRW50115.2020.00137]
   Bai HD, 2021, COMPUT GRAPH-UK, V97, P42, DOI 10.1016/j.cag.2021.04.004
   Bell B., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/502348.502363
   Bowman D. A., 2002, Proceedings of the Human Factors and Ergonomics Society 46th Annual Meeting, P2154
   Brun D, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382837
   Consolvo S, 2003, IEEE PERVAS COMPUT, V2, P24, DOI 10.1109/MPRV.2003.1203750
   Cook T, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P394, DOI 10.1109/VR.2018.8446058
   Darbar R., 2021, P GRAPH INT, P117
   Darius S, 2015, ZENTRALBLATT ARB ARB, V65, P203, DOI 10.1007/s40664-015-0026-z
   Drouot M, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P476, DOI 10.1109/VRW52623.2021.00120
   Dube TJ, 2019, IN SY AP IN WE HC, V11567, P419, DOI 10.1007/978-3-030-22643-5_33
   Dudley JJ, 2019, INT SYM MIX AUGMENT, P289, DOI 10.1109/ISMAR.2019.00027
   Dudley JJ, 2018, ACM T COMPUT-HUM INT, V25, DOI 10.1145/3232163
   Dunn D, 2017, IEEE T VIS COMPUT GR, V23, P1275, DOI 10.1109/TVCG.2017.2657058
   Eiberger A, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357588
   ENDSLEY MR, 1995, HUM FACTORS, V37, P32, DOI 10.1518/001872095779049543
   Erickson A, 2021, ACM T APPL PERCEPT, V18, DOI 10.1145/3456874
   Foerster K.T., 2014, P 13 INT C MOB UB MU, P68, DOI DOI 10.1145/2677972.2677973
   Gabbard JL, 2006, PRESENCE-TELEOP VIRT, V15, P16, DOI 10.1162/pres.2006.15.1.16
   Gabbard JL, 2019, IEEE T VIS COMPUT GR, V25, P2228, DOI 10.1109/TVCG.2018.2832633
   Gabbard JL, 2014, P IEEE, V102, P124, DOI 10.1109/JPROC.2013.2294642
   Gattullo M, 2015, IEEE COMPUT GRAPH, V35, P52, DOI 10.1109/MCG.2015.36
   Goel Mayank, 2012, P SIGCHI C HUM FACT, P2687, DOI [10.1145/2207676.2208662, 10.1145/2207676, DOI 10.1145/2207676]
   González G, 2009, NEW TRENDS ON HUMAN-COMPUTER INTERACTION: RESEARCH, DEVELOPMENT, NEW TOOLS AND METHODS, P109, DOI 10.1007/978-1-84882-352-5_11
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P151, DOI 10.1109/VR.2018.8446250
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P159, DOI 10.1109/VR.2018.8446059
   Grubert J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3933, DOI 10.1145/2702123.2702331
   Gugenheimer J, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P49, DOI 10.1145/2984511.2984576
   Gupta D., 2004, Ph.D. dissertation
   HART S G, 1988, P139
   Hincapie-Ramos J. D., 2013, P SIGCHI C HUM FACT, P3385, DOI DOI 10.1145/2470654.2466463
   Hubenschmid S, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445298
   Huckauf A., 2010, P 7 S APPL PERC GRAP, P41, DOI [10.1145/1836248.1836255, DOI 10.1145/1836248.1836255]
   Imamov S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P851, DOI [10.1109/VR46266.2020.1581435674325, 10.1109/VR46266.2020.00110]
   Itoh Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3453157
   Jiang HY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19143063
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kim S., 2004, P ACM SIGGRAPH INT C, P336, DOI DOI 10.1145/1044588.1044662
   Knierim Pascal, 2021, i-com: Journal of Interactive Media, V20, P49, DOI 10.1515/icom-2021-0003
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Kristensson PO, 2015, COMPUTER, V48, P84, DOI 10.1109/MC.2015.185
   Kristensson PO, 2009, AI MAG, V30, P85, DOI 10.1609/aimag.v30i4.2269
   Lakens D, 2018, ADV METH PRACT PSYCH, V1, P259, DOI 10.1177/2515245918770963
   Lakens D, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00863
   Langner R., 2021, P CHI C HUM FACT COM, P1
   Liu XF, 2017, IEEE T MOBILE COMPUT, V16, P394, DOI 10.1109/TMC.2016.2550447
   Lucero Andres, 2014, P 11 C ADV COMP ENT, P1, DOI DOI 10.1145/2663806.2663824
   Lyons Kent, 2004, P SIGCHI C HUM FACT, P671
   Menzner T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1080, DOI 10.1109/VR.2019.8797754
   Mohr P, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300815
   Nasar JL, 2013, ACCIDENT ANAL PREV, V57, P91, DOI 10.1016/j.aap.2013.03.021
   Normand E, 2018, INT SYM MIX AUGMENT, P123, DOI 10.1109/ISMAR.2018.00043
   Norton W. J., 2021, P FUT TECHN C, P443
   Orlosky Jason, 2013, P 2013 INT C INT US, P363, DOI [DOI 10.1145/2449396, 10.1145/2449396.2449443, DOI 10.1145/2449396.2449443]
   Oshima K, 2016, P IEEE VIRT REAL ANN, P253, DOI 10.1109/VR.2016.7504749
   Otte A, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P387, DOI 10.1109/ISMAR-Adjunct.2019.000-4
   Otte A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1729, DOI 10.1109/VR.2019.8797740
   Oulasvirta Antti, 2005, P SIGCHI C HUM FACT, P919, DOI DOI 10.1145/1054972.1055101
   Pavanatto L, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P759, DOI 10.1109/VR50410.2021.00103
   Pham DM, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364265
   Rau PLP, 2021, INFORM LEARN SCI, V122, P464, DOI 10.1108/ILS-11-2020-0236
   Reyal S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P679, DOI 10.1145/2702123.2702597
   Rheinberg F., 2002, P 1 INT POS PSYCH SU
   Rzayev R, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173619
   Schwebel DC, 2012, ACCIDENT ANAL PREV, V45, P266, DOI 10.1016/j.aap.2011.07.011
   SEARS A, 1993, BEHAV INFORM TECHNOL, V12, P17, DOI 10.1080/01449299308924362
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Taylor R., 2017, Situational awareness, P111, DOI [10.4324/9781315087924-8/SITUATIONAL-AWARENESS-RATING-TECHNIQUE-SARTDEVELOPMENT-TOOL-AIRCREW-SYSTEMS-DESIGN-TAYLOR, DOI 10.4324/9781315087924-8/SITUATIONAL-AWARENESS-RATING-TECHNIQUE-SARTDEVELOPMENT-TOOL-AIRCREW-SYSTEMS-DESIGN-TAYLOR]
   Tuceryan M, 2002, PRESENCE-TELEOP VIRT, V11, P259, DOI 10.1162/105474602317473213
   Van Dam J, 2020, J TRANSP SAF SECUR, V12, P1007, DOI 10.1080/19439962.2018.1564947
   Vertanen K., 2011, P 13 INT C HUM COMP, P295, DOI [DOI 10.1145/2037373.2037418, 10.1145/2037373.2037418]
   Vertanen K, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174200
   Vertanen K, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P659, DOI 10.1145/2702123.2702135
   Wang YH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051582
   Weir D, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2307
   Wen JQ, 2015, INT CONF PERVAS COMP, P105, DOI 10.1109/PERCOM.2015.7146516
   Wenig D, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P716, DOI 10.1145/3025453.3025852
   Wilson A, 2018, PROC SPIE, V10676, DOI 10.1117/12.2315771
   Winterbottom MD, 2007, HUM FACTORS, V49, P907, DOI 10.1518/001872007X230253
   Wolf D, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P347, DOI 10.1109/VR.2018.8448289
   Woodward J, 2023, IEEE T VIS COMPUT GR, V29, P2166, DOI 10.1109/TVCG.2022.3141585
   Xu WG, 2019, INT SYM MIX AUGMENT, P279, DOI 10.1109/ISMAR.2019.00026
   Xu WG, 2019, IEEE T VIS COMPUT GR, V25, P1991, DOI 10.1109/TVCG.2019.2898736
   Yadav AK, 2022, TRANSPORT RES F-TRAF, V91, P236, DOI 10.1016/j.trf.2022.10.008
   Zhu FY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376233
NR 89
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5830
EP 5846
DI 10.1109/TVCG.2023.3309316
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400041
PM 37639421
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Filipov, V
   Arleo, A
   Bogl, M
   Miksch, S
AF Filipov, Velitchko
   Arleo, Alessio
   Bogl, Markus
   Miksch, Silvia
TI On Network Structural and Temporal Encodings: A Space and Time Odyssey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Encoding; Animation; Layout; Visualization; Time factors;
   Topology; Human-centered computing-visualization-graph drawings;
   empirical studies in visualization
ID NODE-LINK; GRAPH; VISUALIZATION; VIOLATIONS
AB The dynamic network visualization design space consists of two major dimensions: network structural and temporal representation. As more techniques are developed and published, a clear need for evaluation and experimental comparisons between them emerges. Most studies explore the temporal dimension and diverse interaction techniques supporting the participants, focusing on a single structural representation. Empirical evidence about performance and preference for different visualization approaches is scattered over different studies, experimental settings, and tasks. This paper aims to comprehensively investigate the dynamic network visualization design space in two evaluations. First, a controlled study assessing participants' response times, accuracy, and preferences for different combinations of network structural and temporal representations on typical dynamic network exploration tasks, with and without the support of standard interaction methods. Second, the best-performing combinations from the first study are enhanced based on participants' feedback and evaluated in a heuristic-based qualitative study with visualization experts on a real-world network. Our results highlight node-link with animation and playback controls as the best-performing combination and the most preferred based on ratings. Matrices achieve similar performance to node-link in the first study but have considerably lower scores in our second evaluation. Similarly, juxtaposition exhibits evident scalability issues in more realistic analysis contexts.
C1 [Filipov, Velitchko] TU Wien, Ctr Visual Analyt Sci & Technol CVAST, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Filipov, V (corresponding author), TU Wien, Ctr Visual Analyt Sci & Technol CVAST, A-1040 Vienna, Austria.
EM velitchko.filipov@tuwien.ac.at; alessio.arleo@tuwien.ac.at;
   markus.boegl@tuwien.ac.at; silvia.miksch@tuwien.ac.at
RI Filipov, Velitchko/JSK-6634-2023; Arleo, Alessio/IRZ-8036-2023
OI Miksch, Silvia/0000-0003-4427-5703; Arleo, Alessio/0000-0003-2008-3651;
   Bogl, Markus/0000-0002-8337-4774; Filipov, Velitchko/0000-0001-9592-2179
FU ArtVis [P35767]
FX No Statement Available
CR Abdelaal Moataz, 2023, IEEE Trans Vis Comput Graph, V29, P896, DOI 10.1109/TVCG.2022.3209427
   Ahn JW, 2014, IEEE T VIS COMPUT GR, V20, P365, DOI 10.1109/TVCG.2013.238
   Aigner W., 2023, Visualization of Time -Oriented Data, V2nd
   Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   angelinipharma, US
   [Anonymous], About us
   Archambault Daniel, 2013, Graph Drawing. 20th International Symposium, GD 2012. Revised Selected Papers, P475, DOI 10.1007/978-3-642-36763-2_42
   Archambault D, 2016, INFORM SCIENCES, V330, P495, DOI 10.1016/j.ins.2015.04.017
   Archambault D, 2013, INT J HUM-COMPUT ST, V71, P1044, DOI 10.1016/j.ijhcs.2013.08.004
   Archambault D, 2011, IEEE T VIS COMPUT GR, V17, P539, DOI 10.1109/TVCG.2010.78
   Arleo A., 2022, Computer Graphics Forum
   Bach B, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P877, DOI 10.1145/2556288.2557010
   Backhaus K., 2015, Fortgeschrittene multivariate Analysemethoden: eine anwendungsorientierte Einfuhrung
   Bastian M, 2009, P INT AAAI C WEBL SO, V3, P361, DOI 10.13140/2.1.1341.1520
   Baur M, 2002, LECT NOTES COMPUT SC, V2265, P463
   Beck F, 2017, COMPUT GRAPH FORUM, V36, P133, DOI 10.1111/cgf.12791
   Behrisch M, 2016, COMPUT GRAPH FORUM, V35, P693, DOI 10.1111/cgf.12935
   Bennett C., 2007, P EUR C COMP AESTH G, P57, DOI [10.2312/compaesth/compaesth07/057-064, DOI 10.2312/COMPAESTH/COMPAESTH07/057-064]
   Bollobás B, 2003, SIAM PROC S, P132
   BONEAU CA, 1960, PSYCHOL BULL, V57, P49, DOI 10.1037/h0041412
   Bonferroni C., 1936, PUBBLICAZIONI R I SU, V8, P3, DOI DOI 10.4135/9781412961288.N455
   Bortz Jurgen, 2013, STAT SOZIALWISSENSCH
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   BOX GEP, 1964, J ROY STAT SOC B, V26, P211, DOI 10.1111/j.2517-6161.1964.tb00553.x
   Brandes U., 2003, Information Visualization, V2, P40, DOI 10.1057/palgrave.ivs.9500037
   Brandes U, 2012, LECT NOTES COMPUT SC, V7034, P99
   cc.gatech, Citevis citation datafile
   Collberg C., 2003, Proceedings of the 2003 ACM symposium on Software visualization-SoftVis '03, P77
   Crnovrsanin T., 2017, J. Graph Algorithms Appl., V21, P55
   d3js, about us
   Di Giacomo E, 2024, IEEE T VIS COMPUT GR, V30, P3503, DOI 10.1109/TVCG.2022.3233389
   DIEHL S., 2002, LECT NOTES COMPUTER, P23
   Erten C, 2004, PROC SPIE, V5295, P45, DOI 10.1117/12.539245
   Erten C, 2004, LECT NOTES COMPUT SC, V2912, P98
   Fekete J.-D., 2015, P IEEE VIS
   Filipov V, 2022, Arxiv, DOI arXiv:2208.13716
   Filipov V, 2021, IEEE PAC VIS SYMP, P131, DOI 10.1109/PacificVis52677.2021.00025
   Garnier S., 2021, viridis - Colorblind-friendly color maps for R R package version 0.6.2, V1
   Ghani S, 2012, COMPUT GRAPH FORUM, V31, P1205, DOI 10.1111/j.1467-8659.2012.03113.x
   Ghoniem M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P17, DOI 10.1109/INFVIS.2004.1
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Hadlak S., 2015, P EUR C VIS, P1
   Hagberg A., 2008, P 7 PYTH SCI C
   Hagberg A., 2020, NetworkX: Network analysis with Python
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Hedderich J., 2016, Angewandte Statistik
   Henry N, 2007, LECT NOTES COMPUT SC, V4663, P288
   Herman I, 2000, IEEE T VIS COMPUT GR, V6, P24, DOI 10.1109/2945.841119
   Javed W, 2012, IEEE PAC VIS SYMP, P1, DOI 10.1109/PacificVis.2012.6183556
   Keller R., 2006, Information Visualization, V5, P62, DOI 10.1057/palgrave.ivs.9500116
   Kerracher N., 2014, P EUR C VIS
   Linhares C. D., 2017, P S APPL COMP, P187, DOI DOI 10.1145/3019612.3019686
   Linhares CDG, 2021, J VISUAL-JAPAN, V24, P1011, DOI 10.1007/s12650-021-00759-x
   Miksch S, 2014, COMPUT GRAPH-UK, V38, P286, DOI 10.1016/j.cag.2013.11.002
   Okoe Mershack, 2018, Graph Drawing and Network Visualization. 25th International Symposium, GD 2017. Revised Selected Papers: LNCS 10692, P287, DOI 10.1007/978-3-319-73915-1_23
   Okoe M, 2019, IEEE T VIS COMPUT GR, V25, P2940, DOI 10.1109/TVCG.2018.2865940
   Posten H. O., 1984, theory and decision library., P92, DOI [DOI 10.1007/978-94-009-6528-7_23, DOI 10.1007/978-94-009-6528-723]
   Purchase H. C., 2002, Empirical Software Engineering, V7, P233, DOI 10.1023/A:1016344215610
   Purchase H. C., 2006, Graph Drawing. 14th International Symposium, GD 2006. Revised Papers (Lecture Notes in Computer Science Vol. 4372), P184
   Purchase HC, 1998, OZCHI 98 - 1998 AUSTRALASIAN COMPUTER HUMAN INTERACTION CONFERENCE, PROCEEDINGS, P80, DOI 10.1109/OZCHI.1998.732199
   Ren D, 2019, NETW SCI, V7, P242, DOI 10.1017/nws.2019.6
   ROUSSEEUW PJ, 1993, J AM STAT ASSOC, V88, P1273, DOI 10.2307/2291267
   Rufiange S, 2014, 2014 SECOND IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P137, DOI 10.1109/VISSOFT.2014.30
   Saraiya P, 2005, IEEE T VIS COMPUT GR, V11, P443, DOI 10.1109/TVCG.2005.53
   Schmider E, 2010, METHODOLOGY-EUR, V6, P147, DOI 10.1027/1614-2241/a000016
   Simonetto P, 2020, IEEE T VIS COMPUT GR, V26, P2373, DOI 10.1109/TVCG.2018.2886901
   surveyjs, SurveyJS - Survey and form JavaScript libraries
   Tutte W.T., 1963, Proc. Lond. Math. Soc, Vs3-13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   Vallat R., 2018, JOSS, V3, P1026, DOI [10.21105/joss.01026, DOI 10.21105/JOSS.01026]
   van den Elzen S, 2014, IEEE T VIS COMPUT GR, V20, P1087, DOI 10.1109/TVCG.2013.263
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Wall E, 2019, IEEE T VIS COMPUT GR, V25, P491, DOI 10.1109/TVCG.2018.2865146
   Weiss C., 2005, Basiswissen Medizinische Statistik, V4
   Yoghourdjian V, 2018, VIS INFORM, V2, P264, DOI 10.1016/j.visinf.2018.12.006
NR 75
TC 2
Z9 2
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5847
EP 5860
DI 10.1109/TVCG.2023.3310019
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400064
PM 37647194
OA hybrid
DA 2024-11-06
ER

PT J
AU Hirao, Y
   Amemiya, T
   Narumi, T
   Argelaguet, F
   Lecuyer, A
AF Hirao, Yutaro
   Amemiya, Tomohiro
   Narumi, Takuji
   Argelaguet, Ferran
   Lecuyer, Anatole
TI Leveraging Tendon Vibration to Enhance Pseudo-Haptic Perceptions in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cross-modal integration; maximum likelyhood estimation; pseudo-haptics;
   tendon vibration; virtual reality
ID ANTAGONIST MUSCLE VIBRATION; INDUCED ILLUSIONS; FORCE SENSATION; MOTOR
   IMAGERY; H-REFLEX; MOVEMENT; POSITION; SENSES; INFORMATION; ACTIVATION
AB Pseudo-haptic techniques are used to modify haptic perception by appropriately changing visual feedback to body movements. Based on the knowledge that tendon vibration can affect our somatosensory perception, this article proposes a method for leveraging tendon vibration to enhance pseudo-haptics during free arm motion. Three experiments were performed to examine the impact of tendon vibration on the range and resolution of pseudo-haptics. The first experiment investigated the effect of tendon vibration on the detection threshold of the discrepancy between visual and physical motion. The results indicated that vibrations applied to the inner tendons of the wrist and elbow increased the threshold, suggesting that tendon vibration can augment the applicable visual motion gain by approximately 13% without users detecting the visual/physical discrepancy. Furthermore, the results demonstrate that tendon vibration acts as noise on haptic motion cues. The second experiment assessed the impact of tendon vibration on the resolution of pseudo-haptics by determining the just noticeable difference in pseudo-weight perception. The results suggested that the tendon vibration does not largely compromise the resolution of pseudo-haptics. The third experiment evaluated the equivalence between the weight perception triggered by tendon vibration and that by visual motion gain, that is, the point of subjective equality. The results revealed that vibration amplifies the weight perception and its effect was equivalent to that obtained using a gain of 0.64 without vibration, implying that the tendon vibration also functions as an additional haptic cue. Our results provide design guidelines and future work for enhancing pseudo-haptics with tendon vibration.
C1 [Hirao, Yutaro; Amemiya, Tomohiro; Narumi, Takuji] Univ Tokyo, Bunkyo City, Tokyo 1138654, Japan.
   [Argelaguet, Ferran; Lecuyer, Anatole] Univ Rennes, INRIA, IRISA, CNRS, F-35042 Rennes, France.
C3 University of Tokyo; Inria; Centre National de la Recherche Scientifique
   (CNRS); Universite de Rennes
RP Hirao, Y (corresponding author), Univ Tokyo, Bunkyo City, Tokyo 1138654, Japan.
EM hirao@cyber.t.u-tokyo.ac.jp; amemiya@vr.u-tokyo.ac.jp;
   narumi@cyber.t.u-tokyo.ac.jp; ferran.argelaguet@inria.fr;
   anatole.lecuyer@inria.fr
RI ; Narumi, Takuji/K-3925-2014
OI Hirao, Yutaro/0000-0003-3546-3454; Narumi, Takuji/0000-0002-9010-1491;
   Lecuyer, Anatole/0000-0002-1409-244X; Amemiya,
   Tomohiro/0000-0002-7079-9167
FU MEXT Grant-in-Aid for Scientific Research (S) [19H05661]; JSPS
   Grant-in-Aidfor Scientific Research (A) [21H04883]; Grant-in-Aid for
   JSPS Fellows [21J12284]
FX This work was supported in part by the MEXT Grant-in-Aid for Scientific
   Research (S) under Grant 19H05661, in part by the JSPS Grant-in-Aidfor
   Scientific Research (A) under Grant 21H04883, and in part by the
   Grant-in-Aid for JSPS Fellows under Grant 21J12284.
CR [Anonymous], 1966, Muscular Afferents and Motor Control, P177
   [Anonymous], 1974, Amer. J. Phys. Med. Rehabil., V53, P143
   Argelaguet F, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2501599
   Ariff G, 2002, J NEUROSCI, V22, P7721
   Ban YK, 2018, IEEE HAPTICS SYM, P278, DOI 10.1109/HAPTICS.2018.8357188
   Bock O, 2007, J NEUROSCI METH, V160, P246, DOI 10.1016/j.jneumeth.2006.09.010
   Brun C, 2015, NEUROSCIENCE, V310, P268, DOI 10.1016/j.neuroscience.2015.09.052
   BURKE D, 1976, J PHYSIOL-LONDON, V261, P673, DOI 10.1113/jphysiol.1976.sp011580
   Burns F. P., 2006, P ACM S VIRTUAL REAL, P3, DOI DOI 10.1145/1180495.1180499
   CAFARELLI E, 1981, EXP NEUROL, V74, P331, DOI 10.1016/0014-4886(81)90173-4
   CAFARELLI E, 1986, MED SCI SPORT EXER, V18, P516
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Fallon JB, 2007, MUSCLE NERVE, V36, P21, DOI 10.1002/mus.20796
   Finney DJ., 1971, Probit analysis, V3
   Friston KJ, 2009, TRENDS COGN SCI, V13, P293, DOI 10.1016/j.tics.2009.04.005
   Fusco G, 2021, PSYCHOL RES-PSYCH FO, V85, P926, DOI 10.1007/s00426-020-01366-5
   GILHODES JC, 1986, EXP BRAIN RES, V61, P395
   Gonzales TI, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00896
   GOODWIN GM, 1972, BRAIN, V95, P705, DOI 10.1093/brain/95.4.705
   Hachisu T, 2011, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2011)
   Honda T, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00760
   Hultborn H, 1996, EXP BRAIN RES, V108, P450
   INGLIS JT, 1990, EXP BRAIN RES, V81, P573, DOI 10.1007/BF02423506
   JOHNSTON WA, 1986, ANNU REV PSYCHOL, V37, P43, DOI 10.1146/annurev.ps.37.020186.000355
   JONES LA, 1985, EXP NEUROL, V87, P35, DOI 10.1016/0014-4886(85)90131-1
   Keigo U, 2022, LECT NOTES COMPUT SC, V13235, P93, DOI 10.1007/978-3-031-06249-0_11
   Kitada R, 2002, NEUROSCIENCE, V109, P701, DOI 10.1016/S0306-4522(01)00495-X
   Kito T, 2006, BRAIN RES, V1114, P75, DOI 10.1016/j.brainres.2006.07.062
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Le Franc S, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0242416
   Lecuyer A., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P83, DOI 10.1109/VR.2000.840369
   Lecuyer J.-M., 2004, P P SIGCHI C HUMAN F, P239, DOI [10.1145/985692.985723, DOI 10.1145/985692.985723]
   Luu BL, 2011, J PHYSIOL-LONDON, V589, P3135, DOI 10.1113/jphysiol.2011.208447
   MCCLOSKE.DI, 1974, EXP NEUROL, V42, P220, DOI 10.1016/0014-4886(74)90019-3
   MCCLOSKEY DI, 1973, BRAIN RES, V61, P119, DOI 10.1016/0006-8993(73)90521-0
   Monjo F, 2018, EXP BRAIN RES, V236, P1997, DOI 10.1007/s00221-018-5280-9
   Naito E, 1999, J NEUROSCI, V19, P6134, DOI 10.1523/JNEUROSCI.19-14-06134.1999
   Narumi T, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P334, DOI 10.1109/WHC.2017.7989924
   Ogawa N, 2021, IEEE T VIS COMPUT GR, V27, P3182, DOI 10.1109/TVCG.2020.2964758
   Porro CA, 1996, J NEUROSCI, V16, P7688
   Proske U, 2019, EXP BRAIN RES, V237, P589, DOI 10.1007/s00221-018-5460-7
   Proske U, 2012, PHYSIOL REV, V92, P1651, DOI 10.1152/physrev.00048.2011
   Pusch A., 2011, P 13 INT C MULT INT, P57
   Rietzler F., 2018, P CHI C HUM FACT COM, P1
   RUNESON S, 1983, J EXP PSYCHOL GEN, V112, P585, DOI 10.1037/0096-3445.112.4.585
   Samad E., 2019, P CHI C HUM FACT COM, P1
   Schofield M. R., Tech-nol. Health Care, V23, P129
   SHADMEHR R, 1994, J NEUROSCI, V14, P3208
   SITTIG AC, 1987, EXP BRAIN RES, V67, P33, DOI 10.1007/BF00269450
   Stellmacher C, 2022, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.754511
   Taima Y, 2014, IEEE HAPTICS SYM, P175, DOI 10.1109/HAPTICS.2014.6775451
   Takamuku S, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2015.0864
   Taylor MW, 2017, MULTISENS RES, V30, P25, DOI 10.1163/22134808-00002544
   Thyrion C, 2009, J NEUROSCI, V29, P8483, DOI 10.1523/JNEUROSCI.0683-09.2009
   Tsuge M, 2012, EXP BRAIN RES, V223, P541, DOI 10.1007/s00221-012-3281-7
   Ujitoko Y, 2021, IEEE T HAPTICS, V14, P699, DOI 10.1109/TOH.2021.3077619
   Ujitoko Y, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P181, DOI [10.1109/WHC.2019.8816100, 10.1109/whc.2019.8816100]
   Ujitoko Y, 2019, IEEE T VIS COMPUT GR, V25, P1981, DOI 10.1109/TVCG.2019.2898820
   Ushiyama Keigo, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P185, DOI 10.1007/978-3-030-58147-3_21
   Ushiyama K, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451834
   Weiss Y, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581223
   Williams NL, 2019, IEEE T VIS COMPUT GR, V25, P3158, DOI 10.1109/TVCG.2019.2932213
   WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931
   Wood SA, 1996, J PHYSIOL-LONDON, V497, P279, DOI 10.1113/jphysiol.1996.sp021767
NR 65
TC 2
Z9 2
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5861
EP 5874
DI 10.1109/TVCG.2023.3310001
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400054
PM 37647196
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Le Guillou, E
   Will, M
   Guillou, P
   Lukasczyk, J
   Fortin, P
   Garth, C
   Tierny, J
AF Le Guillou, Eve
   Will, Michael
   Guillou, Pierre
   Lukasczyk, Jonas
   Fortin, Pierre
   Garth, Christoph
   Tierny, Julien
TI TTK is Getting MPI-Ready
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Pipelines; Parallel processing; Data structures; Memory management;
   Topology; Clustering algorithms; Software algorithms; Topological data
   analysis; high-performance computing; distributed-memory algorithms
ID TOPOLOGICAL SIMPLIFICATION; PARALLEL COMPUTATION; ALGORITHMS; EXTRACTION
AB This system paper documents the technical foundations for the extension of the Topology ToolKit (TTK) to distributed-memory parallelism with the Message Passing Interface (MPI). While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a versatile approach (supporting both triangulated domains and regular grids) for the support of topological analysis pipelines, i.e., a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. While developing this extension, we faced several algorithmic and software engineering challenges, which we document in this paper. Specifically, we describe an MPI extension of TTK's data structure for triangulation representation and traversal, a central component to the global performance and generality of TTK's topological implementations. We also introduce an intermediate interface between TTK and MPI, both at the global pipeline level, and at the fine-grain algorithmic level. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that parallel efficiencies range from 20% to 80% (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework induces a negligible computation time overhead. We illustrate the new distributed-memory capabilities of TTK with an example of advanced analysis pipeline, combining multiple algorithms, run on the largest publicly available dataset we have found (120 billion vertices) on a standard cluster with 64 nodes (for a total of 1536 cores). Finally, we provide a roadmap for the completion of TTK's MPI extension, along with generic recommendations for each algorithm communication category.
C1 [Le Guillou, Eve; Guillou, Pierre; Tierny, Julien] Sorbonne Univ, CNRS, F-75005 Paris, France.
   [Le Guillou, Eve; Will, Michael] Univ Lille, F-75005 Paris, France.
   [Fortin, Pierre] Univ Lille, CNRS, UMR 9189, Cent Lille,CRIStAL, F-59000 Lille, France.
C3 Sorbonne Universite; Centre National de la Recherche Scientifique
   (CNRS); Universite de Lille; Centre National de la Recherche
   Scientifique (CNRS); Universite de Lille; Centrale Lille
RP Le Guillou, E (corresponding author), Sorbonne Univ, CNRS, F-75005 Paris, France.; Le Guillou, E (corresponding author), Univ Lille, F-75005 Paris, France.
EM eve.le_guillou@sorbonne-universite.fr; mswill@rptu.de;
   Pierre.Guillou@sorbonne-universite.fr; lukasczyk@rptu.de;
   pierre.fortin@univ-lille.fr; garth@rptu.de;
   Julien.Tierny@sorbonne-universite.fr
RI Garth, Christoph/Q-5901-2018
OI Fortin, Pierre/0000-0003-3117-9122; Will, Michael/0009-0007-1344-3694;
   Garth, Christoph/0000-0003-1669-8549; Le Guillou,
   Eve/0009-0008-6123-2039; Lukasczyk, Jonas/0000-0001-6650-770X
FU European Commission [ERC-2019-COG "TORI"]
FX No Statement Available
CR Acharya A, 2015, IEEE PAC VIS SYMP, P271, DOI 10.1109/PACIFICVIS.2015.7156387
   Ahrens J., 2005, Vis. Handb., P717, DOI 10.1016/B978-012387582-2/50038-1
   Bachthaler S, 2008, IEEE T VIS COMPUT GR, V14, P1428, DOI 10.1109/TVCG.2008.119
   BANCHOFF TF, 1970, AM MATH MON, V77, P475, DOI 10.2307/2317380
   Bauer U., 2014, P 16 WORKSH ALG ENG, P31, DOI [10.1137/1.9781611973198.4, DOI 10.1137/1.9781611973198.4]
   Bauer U, 2017, J SYMB COMPUT, V78, P76, DOI 10.1016/j.jsc.2016.03.008
   Bhatia H, 2018, J COMPUT CHEM, V39, P936, DOI 10.1002/jcc.25181
   Bock A, 2018, IEEE T VIS COMPUT GR, V24, P812, DOI 10.1109/TVCG.2017.2743980
   Bremer PT, 2011, IEEE T VIS COMPUT GR, V17, P1307, DOI 10.1109/TVCG.2010.253
   Carr H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P497, DOI 10.1109/VISUAL.2004.96
   Carr HA, 2022, SYMP LARG DATA ANAL, P15, DOI 10.1109/LDAV57265.2022.9966394
   Carr HA, 2021, IEEE T VIS COMPUT GR, V27, P2437, DOI 10.1109/TVCG.2019.2948616
   Chazal F, 2013, J ACM, V60, DOI 10.1145/2535927
   Doraiswamy H, 2021, IEEE T VIS COMPUT GR, V27, P561, DOI 10.1109/TVCG.2020.3030441
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Edelsbrunner H, 2004, Jacobi Sets of Multiple Morse Functions
   Edelsbrunner H, 2009, Computational Topology An Introduction
   Edwards H. C., 2010, Tech. Rep. SAND2010-1192
   Favelier G., 2016, P IEEE SCIVIS CONT
   Forman R., 2001, Sem. Lothar. Combin., V48
   Freudenthal H, 1942, ANN MATH, V43, P580, DOI 10.2307/1968813
   Günther D, 2014, IEEE T VIS COMPUT GR, V20, P2476, DOI 10.1109/TVCG.2014.2346403
   Gueunet C., 2019, P EUR S PAR GRAPH VI, P27
   Gueunet C, 2019, IEEE T PARALL DISTR, V30, P1889, DOI 10.1109/TPDS.2019.2898436
   Gueunet C, 2016, SYMP LARG DATA ANAL, P85, DOI 10.1109/LDAV.2016.7874333
   Guillou P, 2024, IEEE T VIS COMPUT GR, V30, P1897, DOI 10.1109/TVCG.2023.3238008
   Gyulassy A, 2014, COMPUT GRAPH FORUM, V33, P51, DOI 10.1111/cgf.12361
   Gyulassy A, 2019, IEEE T VIS COMPUT GR, V25, P1183, DOI 10.1109/TVCG.2018.2864848
   Gyulassy A, 2016, IEEE T VIS COMPUT GR, V22, P916, DOI 10.1109/TVCG.2015.2467432
   Heine C, 2016, COMPUT GRAPH FORUM, V35, P643, DOI 10.1111/cgf.12933
   Huang X, 2021, PROCEEDINGS OF THE 2021 ACM INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, ICS 2021, P367, DOI 10.1145/3447818.3460358
   Ibanez DA, 2016, ACM T MATH SOFTWARE, V42, DOI 10.1145/2814935
   Kasten J, 2011, IEEE T VIS COMPUT GR, V17, P2080, DOI 10.1109/TVCG.2011.249
   Klacansky P., 2020, Open scientific visualization data sets
   Klacansky P, 2017, IEEE T VIS COMPUT GR, V23, P1782, DOI 10.1109/TVCG.2016.2570215
   KUHN HW, 1960, IBM J RES DEV, V4, P518, DOI 10.1147/rd.45.0518
   Laney D, 2006, IEEE T VIS COMPUT GR, V12, P1053, DOI 10.1109/TVCG.2006.186
   Liu GX, 2024, IEEE T VIS COMPUT GR, V30, P1271, DOI 10.1109/TVCG.2023.3327182
   Lukasczyk J, 2021, IEEE T VIS COMPUT GR, V27, P572, DOI 10.1109/TVCG.2020.3030353
   Maack RGC, 2024, IEEE T VIS COMPUT GR, V30, P1942, DOI 10.1109/TVCG.2023.3261981
   Maadasamy S, 2012, INT C HIGH PERFORM
   Maljovec D, 2016, IEEE PAC VIS SYMP, P64, DOI 10.1109/PACIFICVIS.2016.7465252
   Masood T. B., 2019, P TOP METH DAT AN VI, P327
   Message Passing Interface Forum, 2015, Mpi: A message-passing interface standard version 3.1
   Morozov D., 2014, P TOP METH DAT AN VI, VIII, P89
   Morozov D, 2016, SYMP LARG DATA ANAL, P29, DOI 10.1109/LDAV.2016.7874307
   Morozov D, 2013, ACM SIGPLAN NOTICES, V48, P93, DOI 10.1145/2517327.2442526
   Nauleau F, 2022, SYMP LARG DATA ANAL, P50, DOI 10.1109/LDAV57265.2022.9966403
   Nigmetov A., 2020, Reeber: A. library for shared- and distributed-memory parallel computation of merge trees
   Nigmetov A, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356188
   Olejniczak M, 2023, PHYS CHEM CHEM PHYS, V25, P5942, DOI 10.1039/d2cp05893f
   Olejniczak M, 2020, INT J QUANTUM CHEM, V120, DOI 10.1002/qua.26133
   OpenMP Architecture Review Board, 2020, OpenMP application program interface version 5.1
   Pascucci V, 2004, ALGORITHMICA, V38, P249, DOI 10.1007/s00453-003-1052-3
   Robins V, 2011, IEEE T PATTERN ANAL, V33, P1646, DOI 10.1109/TPAMI.2011.95
   Shivashankar N, 2016, IEEE T VIS COMPUT GR, V22, P1745, DOI 10.1109/TVCG.2015.2452919
   Shivashankar N, 2012, COMPUT GRAPH FORUM, V31, P965, DOI 10.1111/j.1467-8659.2012.03089.x
   Smirnov D., 2020, P TOP METH DAT AN VI, VV, P19
   Soler M, 2019, SYMP LARG DATA ANAL, P62, DOI [10.1109/ldav48142.2019.8944365, 10.1109/LDAV48142.2019.8944365]
   Sousbie T, 2011, MON NOT R ASTRON SOC, V414, P350, DOI 10.1111/j.1365-2966.2011.18394.x
   Tierny J, 2018, IEEE T VIS COMPUT GR, V24, P832, DOI 10.1109/TVCG.2017.2743938
   Tierny J, 2017, IEEE T VIS COMPUT GR, V23, P960, DOI 10.1109/TVCG.2016.2599017
   Tierny J, 2012, IEEE T VIS COMPUT GR, V18, P2005, DOI 10.1109/TVCG.2012.228
   TTK Contributors, 2022, TTK online example database
   TTK Contributors, 2020, TTK Data
   Werner K, 2021, IEEE T VIS COMPUT GR, V27, P3585, DOI 10.1109/TVCG.2021.3076875
   Zhang W., 2019, JOSS, V4, P1370, DOI DOI 10.21105/JOSS.01370
NR 67
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5875
EP 5892
DI 10.1109/TVCG.2024.3390219
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400076
PM 38630564
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Nguyen, W
   Gramann, K
   Gehrke, L
AF Nguyen, Willy
   Gramann, Klaus
   Gehrke, Lukas
TI Modeling the Intent to Interact With VR Using Physiological Features
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Electromyography; Electroencephalography; Muscles; Electrodes; Task
   analysis; Physiology; Brain modeling; Brain-computer interfaces;
   electroencephalography; virtual reality
AB Objective: Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction. Methods: In this paper, we explored the use of physiological features to model the user's intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features. Results & Conclusion: We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users' intent to interact with the virtual objects (EEG: 62% +/- 10%, EMG: 72% +/- 9%, EEG-EMG: 69% +/- 10%) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users' mental states. Our work is a further step towards a useful classification of users' intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces.
C1 [Nguyen, Willy] Univ Paris Saclay, F-91190 Gif Sur Yvette, France.
   [Gramann, Klaus; Gehrke, Lukas] TU Berlin, D-10623 Berlin, Germany.
C3 Universite Paris Saclay; Universite Paris Cite; Technical University of
   Berlin
RP Nguyen, W (corresponding author), Univ Paris Saclay, F-91190 Gif Sur Yvette, France.
EM willy.nguyen@hotmail.fr; klaus.gramann@tu-berlin.de;
   lukas.gehrke@tu-berlin.de
RI Gehrke, Lukas/KAM-0750-2024; Gramann, Klaus/AAD-7313-2019
OI NGUYEN, WILLY/0000-0002-2904-8011; Gramann, Klaus/0000-0003-2673-1832;
   Gehrke, Lukas/0000-0003-3661-1973
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [GR2627/13-1]
FX The work of Lukas Gehrke was supported by the Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) under Grant
   GR2627/13-1.
CR Bawa A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155799
   Bigdely-Shamlo N, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00016
   Chandra S, 2021, IEEE T BIO-MED ENG, V68, P1389, DOI 10.1109/TBME.2020.3032354
   Chatrian G. E., 1985, American Journal of EEG Technology, V25, P83
   David-John Brendan., 2021, P ACM S EYE TRACK RE, P1
   DEECKE L, 1969, EXP BRAIN RES, V7, P158
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Fischer G, 2001, USER MODEL USER-ADAP, V11, P65, DOI 10.1023/A:1011145532042
   Gehrke L., 2022, Affordances in Everyday Life: A Multidisciplinary Collection of Essays, P173
   Gehrke Lukas, 2022, Figshare, DOI 10.6084/m9.figshare.19345016.v1
   Gehrke L, 2022, J NEURAL ENG, V19, DOI 10.1088/1741-2552/ac69bc
   Gehrke L, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300657
   Hermens HJ, 2000, J ELECTROMYOGR KINES, V10, P361, DOI 10.1016/S1050-6411(00)00027-4
   Horvitz E, 1999, P SIGCHI C HUM FACT, P159, DOI [10.1145/302979.303030, DOI 10.1145/302979.303030]
   HUDGINS B, 1993, IEEE T BIO-MED ENG, V40, P82, DOI 10.1109/10.204774
   Jonker T. R., 2020, P CHI AI4HCI WORKSH
   Kasahara S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300873
   Khairuddin IM, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.379
   Kim AKH, 2021, STAT-US, V10, DOI 10.1002/sta4.384
   Kinugasa R, 2023, IEEE ACCESS, V11, P6394, DOI 10.1109/ACCESS.2023.3237557
   Kirchner EA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0085060
   Klug M., 2022, bioRxiv
   Kohrs C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0146250
   Kosaki T, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P793, DOI 10.1109/ICMA.2017.8015917
   Krol LR, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/ab5bb5
   Lew Eileen, 2012, Front Neuroeng, V5, P13, DOI 10.3389/fneng.2012.00013
   LIBET B, 1983, ELECTROEN CLIN NEURO, V56, P367, DOI 10.1016/0013-4694(83)90262-6
   Muller-Putz G. R., 2007, P 1 COST NEUR WORKGR
   Niijima A, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545666
   Novak D, 2013, IEEE T BIO-MED ENG, V60, P2645, DOI 10.1109/TBME.2013.2262455
   Pangratz E, 2023, 2023 PROCEEDINGS OF THE 15TH CONFERENCE ON CREATIVITY AND COGNITION, C&C 2023, P129, DOI 10.1145/3591196.3593340
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Saha S, 2021, FRONT SYST NEUROSCI, V15, DOI 10.3389/fnsys.2021.578875
   Schultze-Kraft M, 2021, ENEURO, V8, DOI 10.1523/ENEURO.0425-20.2020
   Schultze-Kraft M, 2020, P ROY SOC B-BIOL SCI, V287, DOI 10.1098/rspb.2019.2928
   Schurger A, 2021, TRENDS COGN SCI, V25, P558, DOI 10.1016/j.tics.2021.04.001
   Shibasaki H, 2006, CLIN NEUROPHYSIOL, V117, P2341, DOI 10.1016/j.clinph.2006.04.025
   Suhail TA, 2022, BIOMED SIGNAL PROCES, V77, DOI 10.1016/j.bspc.2022.103742
   Trigili E, 2019, J NEUROENG REHABIL, V16, DOI 10.1186/s12984-019-0512-1
   Tsukamoto M, 2007, J ROBOT MECHATRON, V19, P381, DOI 10.20965/jrm.2007.p0381
   Winter D.A., 2009, Biomechanics and Motor Control of Human Movement, V2
   Yu DF, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545632
   Zander TO, 2016, P NATL ACAD SCI USA, V113, P14898, DOI 10.1073/pnas.1605155114
   Zhang YX, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555461
NR 44
TC 1
Z9 1
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5893
EP 5900
DI 10.1109/TVCG.2023.3308787
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400055
PM 37624723
DA 2024-11-06
ER

PT J
AU Finke, L
   Weitz, E
AF Finke, Lennart
   Weitz, Edmund
TI A Phenomenological Approach to Interactive Knot Diagrams
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational topology; knot diagrams; user interfaces
ID CLASSIFICATION
AB Knot diagrams are among the most common visual tools in topology. Computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. Still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. We introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. This allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. An implementation of this method is provided.
C1 [Finke, Lennart] Georg August Univ Gottingen, D-37073 Gottingen, Germany.
   [Weitz, Edmund] Hsch Angew Wissensch Hamburg, D-20999 Hamburg, Germany.
C3 University of Gottingen; Hochschule Angewandte Wissenschaft Hamburg
RP Finke, L (corresponding author), Georg August Univ Gottingen, D-37073 Gottingen, Germany.
EM l.finke@stud.uni-goettingen.de; edmund.weitz@haw-hamburg.de
OI Weitz, Edmund/0000-0003-2368-6132; Finke, Lennart/0009-0003-6908-314X
CR Alam MJ, 2014, LECT NOTES COMPUT SC, V8392, P144
   Alexander JW, 1928, T AM MATH SOC, V30, P275, DOI 10.2307/1989123
   Bar-Natan D., 2005, KNOT ATLAS
   Brasher R, 2013, BIOCHEM SOC T, V41, P606, DOI 10.1042/BST20120278
   Catmull E., 1974, Com- puter Aided Geometric Design, P317, DOI DOI 10.1016/B978-0-12-079050-0.50020-5
   Costagliola G., 2016, J. Vis. Lang. Sentient Syst., V2, P16
   Culler M., 2017, SnapPy, a com- puter program for studying the geometry and topology of 3-manifolds
   DOWKER CH, 1983, TOPOL APPL, V16, P19, DOI 10.1016/0166-8641(83)90004-4
   Eades P, 2023, Arxiv, DOI arXiv:2309.02852
   Horner W. G., 1819, Philos. Trans. Roy. Soc. London, V109, P308
   Horowitz J., 2013, Knot identification tool
   Kindermann P., 2017, P INT S GRAPH DRAW N, P113
   Lackenby M, 2015, ANN MATH, V182, P491, DOI 10.4007/annals.2015.182.2.3
   Lehniand J., 2011, Paper.js
   Liu H, 2021, IEEE T VIS COMPUT GR, V27, P593, DOI 10.1109/TVCG.2020.3028893
   Livingston C., 2023, Knotinfo: Table of knot invariants
   Lopez R., 2003, The Alexander polynomial, coloring, and determinants of knots
   Miller K., 2022, Knotfolio
   PERKO KA, 1974, P AM MATH SOC, V45, P262, DOI 10.2307/2040074
   Reidemeister K., 1927, ABH MATH SEM U HAMB, V5, P24
   Rolfsen D., 2003, Knots and Links, V346
   Scharein RG, 2002, MATH VISUAL, P277
   Seed C., 2016, Knotkit
   Simonetto P, 2016, IEEE T VIS COMPUT GR, V22, P678, DOI 10.1109/TVCG.2015.2467992
   Stacey A., 2011, spath3
   Swenton F., 2021, Knot-like objects
   TAMASSIA R, 1987, SIAM J COMPUT, V16, P421, DOI 10.1137/0216030
   Vandermonde Alexandre-Theophile, 1771, Memoires de l'Academie Royale des Sciences, P566
   Yu C, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3439429
   Zhang H, 2012, IEEE T VIS COMPUT GR, V18, P2051, DOI 10.1109/TVCG.2012.242
   Zibrowius C., 2023, kht++, a program for computing Khovanov invariants for links and tangles
NR 31
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5901
EP 5907
DI 10.1109/TVCG.2024.3405369
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400099
PM 38787659
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhang, Y
   Liu, JS
   Lai, CF
   Zhou, Y
   Chen, SM
AF Zhang, Yang
   Liu, Jisheng
   Lai, Chufan
   Zhou, Yuan
   Chen, Siming
TI Interpreting High-Dimensional Projections With Capacity
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distortion; Measurement; Distortion measurement; Data visualization;
   Statistical analysis; Clustering algorithms; Visual analytics;
   Dimensional reduction; interactive visual exploration; projection
   algorithm evaluation
ID VISUAL ANALYSIS; VISUALIZATION; TOPOLOGY
AB Dimensionality reduction (DR) algorithms are diverse and widely used for analyzing high-dimensional data. Various metrics and tools have been proposed to evaluate and interpret the DR results. However, most metrics and methods fail to be well generalized to measure any DR results from the perspective of original distribution fidelity or lack interactive exploration of DR results. There is still a need for more intuitive and quantitative analysis to interactively explore high-dimensional data and improve interpretability. We propose a metric and a generalized algorithm-agnostic approach based on the concept of capacity to evaluate and analyze the DR results. Based on our approach, we develop a visual analytic system HiLow for exploring high-dimensional data and projections. We also propose a mixed-initiative recommendation algorithm that assists users in interactively DR results manipulation. Users can compare the differences in data distribution after the interaction through HiLow. Furthermore, we propose a novel visualization design focusing on quantitative analysis of differences between high and low-dimensional data distributions. Finally, through user study and case studies, we validate the effectiveness of our approach and system in enhancing the interpretability of projections and analyzing the distribution of high and low-dimensional data.
C1 [Zhang, Yang; Zhou, Yuan; Chen, Siming] Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.
   [Liu, Jisheng] Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.
   [Lai, Chufan] Peking Univ, Beijing 100871, Peoples R China.
C3 Fudan University; Fudan University; Peking University
RP Chen, SM (corresponding author), Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.
EM yang_zhang21@m.fudan.edu.cn; 19307130217@fudan.edu.cn;
   chufan.lai.1990@gmail.com; yuanzhou@fudan.edu.cn;
   simingchen@fudan.edu.cn
RI Chen, Siming/AAK-1874-2020
FU National Natural Science Foundation of China (NSFC) [62202105]; Shanghai
   Municipal Science and Technology Major Project [2018SHZDZX01,
   2021SHZDZX0103]; Sailing Program [21YF1402900]; ZJLab;  [21ZR1403300]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 62202105, in part by Shanghai
   Municipal Science and Technology Major Project under Grants
   2018SHZDZX01, and 2021SHZDZX0103, in part by General Program under Grant
   21ZR1403300,in part by Sailing Program under Grant 21YF1402900, and in
   part by ZJLab.
CR Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   Arora S., 2018, C LEARN THEOR, V75, P1455
   Aupetit M, 2007, NEUROCOMPUTING, V70, P1304, DOI 10.1016/j.neucom.2006.11.018
   Broyden C. G., 1970, Journal of the Institute of Mathematics and Its Applications, V6, P222
   Cavallo M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174209
   Chapra S, 2015, Numerical methods for engineers, V7th
   Dowling M, 2019, IEEE T VIS COMPUT GR, V25, P172, DOI 10.1109/TVCG.2018.2865047
   Elhamdadi H, 2022, IEEE T VIS COMPUT GR, V28, P769, DOI 10.1109/TVCG.2021.3114784
   Espadoto M, 2021, IEEE T VIS COMPUT GR, V27, P2153, DOI 10.1109/TVCG.2019.2944182
   Falconer K., 2014, FRACTAL GEOMETRY MAT
   Faust R, 2019, IEEE T VIS COMPUT GR, V25, P481, DOI 10.1109/TVCG.2018.2865194
   FLETCHER R, 1970, COMPUT J, V13, P317, DOI 10.1093/comjnl/13.3.317
   Ghosh A, 2022, IEEE T KNOWL DATA EN, V34, P2227, DOI 10.1109/TKDE.2020.3005878
   Ghrist R, 2008, B AM MATH SOC, V45, P61, DOI 10.1090/s0273-0979-07-01191-3
   Gisbrecht A, 2015, WIRES DATA MIN KNOWL, V5, P51, DOI 10.1002/widm.1147
   GOLDFARB D, 1970, MATH COMPUT, V24, P23, DOI 10.2307/2004873
   Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009
   Heulot N., 2012, P IEEE INFOVIS
   Hoffman P., 1999, P WORKSH NEW PAR INF, P9, DOI [10.1145/331770.331775, DOI 10.1145/331770.331775]
   Jeon H, 2022, IEEE T VIS COMPUT GR, V28, P551, DOI 10.1109/TVCG.2021.3114833
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Krause J, 2016, SYMP LARG DATA ANAL, P11, DOI 10.1109/LDAV.2016.7874305
   Lespinats S, 2007, IEEE T NEURAL NETWOR, V18, P1265, DOI 10.1109/TNN.2007.891682
   Lespinats S, 2011, COMPUT GRAPH FORUM, V30, P113, DOI 10.1111/j.1467-8659.2010.01835.x
   Lewis J., 2012, Proceedings of the 34th Annual Conference of the Cognitive Science Society, P671
   Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nocedal J, 2006, SPRINGER SER OPER RE, P30
   Nonato LG, 2019, IEEE T VIS COMPUT GR, V25, P2650, DOI 10.1109/TVCG.2018.2846735
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P564, DOI 10.1109/TVCG.2007.70443
   Poggio T, 2017, INT J AUTOM COMPUT, V14, P503, DOI 10.1007/s11633-017-1054-2
   Press W. H., 1992, Numerical recipes in c, V2nd ed.
   Ruotsalo T, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1759, DOI 10.1145/2505515.2505644
   Schreck T, 2010, INFORM VISUAL, V9, P181, DOI 10.1057/ivs.2010.2
   Sedlmair M., 2012, Tech. Rep. TR-2012-03
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Seifert C., 2010, P EUROVAST 2010 INT, P13, DOI [10.2312/PE/EuroVAST/EuroVAST10/013-018, DOI 10.2312/PE/EUROVAST/EUROVAST10/013-018]
   Self J. Z., 2016, P WORKSH HUM LOOP DA, P1
   SHANNO DF, 1970, MATH COMPUT, V24, P647, DOI 10.2307/2004840
   Smilkov D, 2016, Arxiv, DOI arXiv:1611.05469
   Stahnke J, 2016, IEEE T VIS COMPUT GR, V22, P629, DOI 10.1109/TVCG.2015.2467717
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Torgerson WS, 1952, PSYCHOMETRIKA, V17, P401
   Van Der Maaten L., 2009, J MACH LEARN RES, V10, P66
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang RR, 2021, Arxiv, DOI arXiv:1909.13322
   Xia JZ, 2022, IEEE T VIS COMPUT GR, V28, P529, DOI 10.1109/TVCG.2021.3114694
   Xia JZ, 2018, IEEE T VIS COMPUT GR, V24, P236, DOI 10.1109/TVCG.2017.2744098
   Xiao YH, 2008, COMPUT MATH APPL, V56, P1001, DOI 10.1016/j.camwa.2008.01.028
   Zarghili A., 2011, P INT C INT SYST DAT, P78
   Zhou L, 2018, IEEE T VIS COMPUT GR, V24, P1997, DOI 10.1109/TVCG.2017.2698041
NR 52
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6038
EP 6055
DI 10.1109/TVCG.2023.3324851
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000013
PM 37938967
DA 2024-11-06
ER

PT J
AU Zhong, HL
   Zhang, JB
   Liao, J
AF Zhong, Hongliang
   Zhang, Jingbo
   Liao, Jing
TI VQ-NeRF: Neural Reflectance Decomposition and Editing With Vector
   Quantization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Reflectivity; Lighting; Three-dimensional displays; Rendering (computer
   graphics); Geometry; Computational modeling; Image color analysis;
   Neural implicit fields; physically based rendering; vector quantization
AB We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene, which reduces redundancy in the material segmentation process. To improve usability, we also develop an interactive interface to further assist material editing. We evaluate our model on both computer-generated and real-world scenes, demonstrating its superior performance. To the best of our knowledge, our model is the first to enable discrete material editing in 3D scenes.
C1 [Zhong, Hongliang; Zhang, Jingbo; Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Liao, J (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM hlzhong2-c@my.cityu.edu.hk; jbzhang6-c@my.cityu.edu.hk;
   jingliao@cityu.edu.hk
RI Zhong, Hongliang/LDE-6340-2024; Zhang, Jingbo/GRX-3761-2022; Zhong,
   Hongliang/KTI-6761-2024
OI LIAO, Jing/0000-0001-7014-5377; Zhang, Jingbo/0000-0003-0009-2315;
   Zhong, Hongliang/0009-0002-0840-8812
FU GRF Research Grants Council (RGC) of the Hong Kong Special
   Administrative Region, China [CityU 11208123]
FX This work was supported by a GRF grant from the Research Grants Council
   (RGC) of the Hong Kong Special Administrative Region,China under Grant
   CityU 11208123.
CR Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Bi S, 2020, Arxiv, DOI [arXiv:2008.03824, 10.48550/arXiv.2008.03824]
   Boss M, 2021, ADV NEUR IN, V34
   Boss M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12664, DOI 10.1109/ICCV48922.2021.01245
   Boss M, 2020, PROC CVPR IEEE, P3981, DOI 10.1109/CVPR42600.2020.00404
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Dana KJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P460, DOI 10.1109/ICCV.2001.937661
   Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765
   Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378
   Guo J, 2020, IEEE T VIS COMPUT GR, V26, P1476, DOI 10.1109/TVCG.2018.2872709
   Hasselgren J, 2022, Arxiv, DOI arXiv:2206.03380
   Joy A., 2022, arXiv
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kang KZ, 2023, IEEE T VIS COMPUT GR, V29, P1450, DOI 10.1109/TVCG.2021.3117370
   Kim YM, 2021, IEEE T VIS COMPUT GR, V27, P2992, DOI 10.1109/TVCG.2019.2960776
   Kuang ZF, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530177
   Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641
   Li Z, 2023, PROC CVPR IEEE, P12499, DOI 10.1109/CVPR52729.2023.01203
   Li ZQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275055
   Meka A, 2018, PROC CVPR IEEE, P6315, DOI 10.1109/CVPR.2018.00661
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Munkberg J, 2022, PROC CVPR IEEE, P8270, DOI 10.1109/CVPR52688.2022.00810
   Sato Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P379, DOI 10.1145/258734.258885
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Srinivasan PP, 2021, PROC CVPR IEEE, P7491, DOI 10.1109/CVPR46437.2021.00741
   van den Oord A, 2017, ADV NEUR IN, V30
   Wang KK, 2023, IEEE T VIS COMPUT GR, V29, P5097, DOI 10.1109/TVCG.2022.3202503
   Wang P, 2021, Arxiv, DOI arXiv:2106.10689
   Wang ZA, 2023, PROC CVPR IEEE, P8370, DOI 10.1109/CVPR52729.2023.00809
   Wu HZ, 2016, IEEE T VIS COMPUT GR, V22, P2012, DOI 10.1109/TVCG.2015.2498617
   Yang WQ, 2022, LECT NOTES COMPUT SC, V13661, P266, DOI 10.1007/978-3-031-19769-7_16
   Yao Y, 2022, LECT NOTES COMPUT SC, V13691, P700, DOI 10.1007/978-3-031-19821-2_40
   Yu J., 2021, arXiv, DOI 10.48550/arxiv.2110.04627
   Zhang K, 2021, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR46437.2021.00541
   Zhang XM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480496
   Zhang YQ, 2022, PROC CVPR IEEE, P18622, DOI 10.1109/CVPR52688.2022.01809
NR 36
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6247
EP 6260
DI 10.1109/TVCG.2023.3330518
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000018
PM 37956018
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Bobák, P
   Cmolík, L
   Cadík, M
AF Bobak, Petr
   Cmolik, Ladislav
   Cadik, Martin
TI Reinforced Labels: Multi-Agent Deep Reinforcement Learning for
   Point-Feature Label Placement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Labeling; Reinforcement learning; Deep learning; Solid modeling; Layout;
   Data visualization; Visual analytics; Point-feature label placement;
   machine learning; multi-agent reinforcement learning
ID VISUALIZATIONS
AB Over the recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this article, we are introducing Reinforcement Learning (RL) to label placement, a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning to learn the label placement strategy, the first machine-learning-driven labeling method, in contrast to the existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and the compared methods designed by human experts in terms of completeness (i.e., the number of placed labels). The trade-off is increased computation time, making the proposed method slower than the compared methods. Nevertheless, our method is ideal for scenarios where the labeling can be computed in advance, and completeness is essential, such as cartographic maps, technical drawings, and medical atlases. Additionally, we conducted a user study to assess the perceived performance. The outcomes revealed that the participants considered the proposed method to be significantly better than the other examined methods. This indicates that the improved completeness is not just reflected in the quantitative metrics but also in the subjective evaluation by the participants.
C1 [Bobak, Petr; Cadik, Martin] Brno Univ Technol, Fac Informat Technol, Brno 60190, Czech Republic.
   [Cmolik, Ladislav] Czech Tech Univ, Fac Elect Engn, Prague, 16636, Czech Republic.
C3 Brno University of Technology; Czech Technical University Prague
RP Bobák, P (corresponding author), Brno Univ Technol, Fac Informat Technol, Brno 60190, Czech Republic.
EM ibobak@fit.vutbr.cz; cmolikl@fel.cvut.cz; cadik@fit.vutbr.cz
RI Čmolík, Ladislav/GXW-1250-2022; Bobak, Petr/AAP-9830-2020; Cadik,
   Martin/O-4824-2014
OI Cmolik, Ladislav/0000-0002-8546-6568; Bobak, Petr/0000-0003-1914-4592;
   Cadik, Martin/0000-0001-7058-9912
FU Deep-Learning Approach to Topographical Image Analysis [LTAIZ19004];
   Ministry of Education, Youth and Sports of the Czech Republic
   [SMSM2019LTAIZ]; Grant Agency of CTU in Prague [SGS22/173/OHK3/3T/13];
   Research of Modern Computer Graphics Methods 2022-2024
FX This work was supported in part by project LTAIZ19004 Deep-Learning
   Approach to Topographical Image Analysis, in part by the Ministry of
   Education, Youth and Sports of the Czech Republic within the activity
   INTER-EXCELENCE (LT), subactivity INTER-ACTION (LTA), under Grant
   SMSM2019LTAIZ, and in part by the Grant Agency of CTU in Prague under
   Grant SGS22/173/OHK3/3T/13, in part by the Research of Modern Computer
   Graphics Methods 2022-2024. Computational resources were mainly supplied
   by the project "e-Infrastruktura CZ" (e-INFRA CZ ID:90140) supported in
   part by the Ministry of Education, Youth and Sports of the Czech
   Republic.
CR Alvim ACF, 2009, EUR J OPER RES, V192, P396, DOI 10.1016/j.ejor.2007.10.002
   Arulkumaran K, 2017, IEEE SIGNAL PROC MAG, V34, P26, DOI 10.1109/MSP.2017.2743240
   Bekos MA, 2019, COMPUT GRAPH FORUM, V38, P833, DOI 10.1111/cgf.13729
   Bobák P, 2020, COMPUT GRAPH-UK, V91, P265, DOI 10.1016/j.cag.2020.08.005
   Brockman G, 2016, Arxiv, DOI arXiv:1606.01540
   Bylinskii Z, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/3126594.3126653
   Canese L, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11114948
   Chazelle B., 1999, Advances in Discrete and Computational Geometry, P407
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P917, DOI 10.1109/TVCG.2019.2934810
   Christensen J, 1995, ACM T GRAPHIC, V14, P203, DOI 10.1145/212332.212334
   Da Col S, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P4696, DOI 10.1145/3459637.3481968
   Deng Y, 2017, IEEE T NEUR NET LEAR, V28, P653, DOI 10.1109/TNNLS.2016.2522401
   Doddi S, 1997, PROCEEDINGS OF THE EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P148
   Nguyen DT, 2018, ADV NEUR IN, V31
   HIRSCH SA, 1982, AM CARTOGRAPHER, V9, P5, DOI 10.1559/152304082783948367
   Hu RZ, 2021, IEEE T VIS COMPUT GR, V27, P3034, DOI 10.1109/TVCG.2021.3052167
   Imhof E., 1975, AM CARTOGRAPHER, V2, P128, DOI DOI 10.1559/152304075784313304
   Jaunet T, 2020, COMPUT GRAPH FORUM, V39, P49, DOI 10.1111/cgf.13962
   Kittivorawong C, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P101, DOI 10.1109/VIS47514.2020.00027
   Levine S, 2017, SPR PROC ADV ROBOT, V1, P173, DOI 10.1007/978-3-319-50115-4_16
   Levine S, 2016, J MACH LEARN RES, V17
   Li K., 2017, P 5 INT C LEARN REPR, P1
   Li L, 2016, ISPRS INT J GEO-INF, V5, DOI 10.3390/ijgi5090159
   Liang E, 2018, PR MACH LEARN RES, V80
   Lowe R, 2017, ADV NEUR IN, V30
   Lu FY, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8050237
   Luboschik M, 2008, IEEE T VIS COMPUT GR, V14, P1237, DOI 10.1109/TVCG.2008.152
   Marks J., 1991, Tech. Rep. TR-05-91
   Mirhoseini A, 2021, NATURE, V594, P207, DOI 10.1038/s41586-021-03544-w
   Mishra A, 2022, IEEE PAC VIS SYMP, P111, DOI 10.1109/PacificVis53943.2022.00020
   Mote K., 2007, Information Visualization, V6, P249, DOI DOI 10.1057/PALGRAVE.IVS.9500163
   Pan X., 2017, P BRIT MACH VIS C
   Pavlovec V, 2022, IEEE T VIS COMPUT GR, V28, P604, DOI 10.1109/TVCG.2021.3114854
   Perez-Ortiz M, 2017, Arxiv, DOI [arXiv:1712.03686, DOI 10.48550/ARXIV.1712.03686]
   Rabello RL, 2014, EUR J OPER RES, V234, P802, DOI 10.1016/j.ejor.2013.10.021
   Schubert S, 2019, IEEE INT VEH SYM, P653, DOI [10.1109/IVS.2019.8813862, 10.1109/ivs.2019.8813862]
   Schulman J., 2016, P INT C LEARN REPR I, P1
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tang T, 2021, IEEE T VIS COMPUT GR, V27, P294, DOI 10.1109/TVCG.2020.3030467
   Tsukida K., 2011, UWEETech. Rep. 206
   van Kreveld M, 1999, COMP GEOM-THEOR APPL, V13, P21, DOI 10.1016/S0925-7721(99)00005-X
   Wagner F, 2001, ALGORITHMICA, V30, P334, DOI 10.1007/s00453-001-0009-7
   Wang JP, 2022, IEEE T VIS COMPUT GR, V28, P4141, DOI 10.1109/TVCG.2021.3076749
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P288, DOI 10.1109/TVCG.2018.2864504
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wu AY, 2021, IEEE T VIS COMPUT GR, V27, P464, DOI 10.1109/TVCG.2020.3030423
   Yamamoto M, 2002, GEOINFORMATICA, V6, P77, DOI 10.1023/A:1013720231747
   Yamamoto M., 2005, P 7 S BRAS GEOINF, P122
   Yoeli P., 1972, Cartogr J, V9, P99, DOI [DOI 10.1179/000870472787352505, 10.1179/caj.1972.9.2.99, DOI 10.1179/CAJ.1972.9.2.99]
   Zhou MY, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2389, DOI 10.1145/3447548.3467279
   Zoraster S., 1997, Cartography and Geographic Information Systems, V24, P228, DOI https://doi.org/10.1559/152304097782439259
   Zoraster S., 1986, Cartographica: The International Journal for Geographic Information and Geovisualization, V23, P16
NR 53
TC 0
Z9 0
U1 6
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5908
EP 5922
DI 10.1109/TVCG.2023.3313729
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000014
PM 37695975
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhang, SH
   Li, HT
   Qu, HM
   Wang, Y
AF Zhang, Songheng
   Li, Haotian
   Qu, Huamin
   Wang, Yong
TI <i>AdaVis</i>: Adaptive and Explainable Visualization Recommendation for
   Tabular Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization recommendation; logical reasoning; data visualization;
   knowledge graph; Visualization recommendation; logical reasoning; data
   visualization; knowledge graph
AB Automated visualization recommendation facilitates the rapid creation of effective visualizations, which is especially beneficial for users with limited time and limited knowledge of data visualization. There is an increasing trend in leveraging machine learning (ML) techniques to achieve an end-to-end visualization recommendation. However, existing ML-based approaches implicitly assume that there is only one appropriate visualization for a specific dataset, which is often not true for real applications. Also, they often work like a black box, and are difficult for users to understand the reasons for recommending specific visualizations. To fill the research gap, we propose AdaVis, an adaptive and explainable approach to recommend one or multiple appropriate visualizations for a tabular dataset. It leverages a box embedding-based knowledge graph to well model the possible one-to-many mapping relations among different entities (i.e., data features, dataset columns, datasets, and visualization choices). The embeddings of the entities and relations can be learned from dataset-visualization pairs. Also, AdaVis incorporates the attention mechanism into the inference framework. Attention can indicate the relative importance of data features for a dataset and provide fine-grained explainability. Our extensive evaluations through quantitative metric evaluations, case studies, and user interviews demonstrate the effectiveness of AdaVis.
C1 [Zhang, Songheng; Wang, Yong] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
   [Li, Haotian; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Singapore Management University; Hong Kong University of Science &
   Technology
RP Wang, Y (corresponding author), Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
EM shzhang.2021@phdcs.smu.edu.sg; haotian.li@connect.ust.hk;
   huamin@cse.ust.hk; yongwang@smu.edu.sg
RI Wang, Yong/HKF-3903-2023
FU Ministry of Education,Singapore, under its Academic Research Fund Tier 2
   [T2EP20222-0049]; HK RGC GRF [16210722]
FX This work was supported in part by the Ministry of Education,Singapore,
   under its Academic Research Fund Tier 2 (Proposal ID: T2EP20222-0049)
   and in part by HK RGC GRF under Grant 16210722. Recommended for
   acceptance by D. Gotz.
CR Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Battle L, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174168
   Bordes A., 2013, Advances in neural information processing systems, V26, P1
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Burges C, 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   Chen W, 2021, NEURAL NETWORKS, V136, P126, DOI 10.1016/j.neunet.2021.01.001
   Chen Z, 2020, IEEE ACCESS, V8, P192435, DOI 10.1109/ACCESS.2020.3030076
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P917, DOI 10.1109/TVCG.2019.2934810
   Cui Z, 2019, INFORM VISUAL, V18, P251, DOI 10.1177/1473871618806555
   Dibia V, 2019, IEEE COMPUT GRAPH, V39, P33, DOI 10.1109/MCG.2019.2924636
   Ding R, 2019, INT CONF MANAGE DATA, P317, DOI 10.1145/3299869.3314037
   Guest G., 2011, APPL THEMATIC ANAL
   Haas P. J., 2017, arXiv
   Hamilton William L., 2018, ADV NEURAL INFORM PR, P2030
   Harris C, 2021, Arxiv, DOI arXiv:2103.11297
   Heer J., 2005, P SIGCHI C HUM FACT, P421
   Hu K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300358
   Hu ZW, 2022, Arxiv, DOI arXiv:2205.00782
   Ji GL, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P687
   Ji SX, 2022, IEEE T NEUR NET LEAR, V33, P494, DOI 10.1109/TNNLS.2021.3070843
   Sarker MK, 2017, Arxiv, DOI arXiv:1710.04324
   Key A., 2012, P ACM SIGMOD INT C M, P681, DOI DOI 10.1145/2213836.2213931
   Li HT, 2022, IEEE T VIS COMPUT GR, V28, P195, DOI 10.1109/TVCG.2021.3114863
   Li XH, 2022, IEEE T KNOWL DATA EN, V34, P29, DOI 10.1109/TKDE.2020.2983930
   Li Yixuan, 2023, IEEE Trans Vis Comput Graph, V29, P95, DOI 10.1109/TVCG.2022.3209461
   Lin YK, 2015, Arxiv, DOI arXiv:1506.00379
   Lin YN, 2024, IEEE T VIS COMPUT GR, V30, P4108, DOI 10.1109/TVCG.2023.3251344
   Liu H, 2002, DATA MIN KNOWL DISC, V6, P393, DOI 10.1023/A:1016304305535
   Luo YY, 2018, PROC INT CONF DATA, P101, DOI 10.1109/ICDE.2018.00019
   Ma WZ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P1210, DOI 10.1145/3308558.3313607
   MACKINLAY J, 1986, ACM T GRAPHIC, V5, P110, DOI 10.1145/22949.22950
   Mackinlay JD, 2007, IEEE T VIS COMPUT GR, V13, P1137, DOI 10.1109/TVCG.2007.70594
   Moritz D, 2019, IEEE T VIS COMPUT GR, V25, P438, DOI 10.1109/TVCG.2018.2865240
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Murdoch WJ, 2019, P NATL ACAD SCI USA, V116, P22071, DOI 10.1073/pnas.1900654116
   Nickel M., 2011, ICML
   Raschka S, 2014, Arxiv, DOI arXiv:1410.5330
   Ren H., 2020, Advances in Neural Information Processing Systems, V33, P19716
   Ren HY, 2020, Arxiv, DOI arXiv:2002.05969
   Saket B, 2019, IEEE T VIS COMPUT GR, V25, P2505, DOI 10.1109/TVCG.2018.2829750
   Satyanarayan A, 2016, IEEE T VIS COMPUT GR, V22, P659, DOI 10.1109/TVCG.2015.2467091
   Seneviratne O, 2019, P WORKSH SEM WEB SOL, V2477, P55
   Seo J., 2005, Information Visualization, V4, P96, DOI 10.1057/palgrave.ivs.9500091
   Vartak M, 2014, PROC VLDB ENDOW, V7, P1581, DOI 10.14778/2733004.2733035
   Vilnis L, 2018, Arxiv, DOI arXiv:1805.06627
   Wang QW, 2022, IEEE T VIS COMPUT GR, V28, P5134, DOI 10.1109/TVCG.2021.3106142
   Wang Q, 2017, IEEE T KNOWL DATA EN, V29, P2724, DOI 10.1109/TKDE.2017.2754499
   Wang X, 2019, AAAI CONF ARTIF INTE, P5329
   Wang Y, 2020, IEEE T VIS COMPUT GR, V26, P676, DOI 10.1109/TVCG.2019.2934798
   Wang Z, 2014, AAAI CONF ARTIF INTE, P1112
   Ward M.O., 2015, INTERACTIVE DATA VIS
   Wickham H, 2010, J COMPUT GRAPH STAT, V19, P3, DOI 10.1198/jcgs.2009.07098
   Wills G, 2010, INFORM VISUAL, V9, P47, DOI 10.1057/ivs.2008.27
   Wongsuphasawat K, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2648
   Wongsuphasawat K, 2016, IEEE T VIS COMPUT GR, V22, P649, DOI 10.1109/TVCG.2015.2467191
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Yang B., 2014, arXiv, DOI DOI 10.48550/ARXIV.1412.6575
   Yuan J, 2022, IUI'22: 27TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P878, DOI 10.1145/3490099.3511146
   Zeng ZH, 2022, IEEE T VIS COMPUT GR, V28, P346, DOI 10.1109/TVCG.2021.3114814
   Zhang Zhanqiu, 2021, ADV NEUR IN, V34
   Zhou M., 2020, P 27 ACM SIGKDD C KN
   Zhu SJ, 2020, VIS INFORM, V4, P24, DOI 10.1016/j.visinf.2020.07.002
NR 62
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5923
EP 5938
DI 10.1109/TVCG.2023.3316469
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000034
PM 37721882
OA Green Submitted, Green Accepted
DA 2024-11-06
ER

PT J
AU Ramamurthi, Y
   Chattopadhyay, A
AF Ramamurthi, Yashwanth
   Chattopadhyay, Amit
TI A Topological Distance Between Multi-Fields Based on Multi-Dimensional
   Persistence Diagrams
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topology; Shape; Complexity theory; Extraterrestrial measurements;
   Distortion measurement; Distortion; Discrete Fourier transforms; Data
   analysis; multi-dimensional persistence diagram; multi-dimensional reeb
   graph; multi-field; reeb space; shape matching; topological distance;
   visualization
AB The problem of computing topological distance between two scalar fields based on Reeb graphs or contour trees has been studied and applied successfully to various problems in topological shape matching, data analysis, and visualization. However, generalizing such results for computing distance measures between two multi-fields based on their Reeb spaces is still in its infancy. Towards this, in the current article we propose a technique to compute an effective distance measure between two multi-fields by computing a novel multi-dimensional persistence diagram (MDPD) corresponding to each of the (quantized) Reeb spaces. First, we construct a multi-dimensional Reeb graph (MDRG), which is a hierarchical decomposition of the Reeb space into a collection of Reeb graphs. The MDPD corresponding to each MDRG is then computed based on the persistence diagrams of the component Reeb graphs of the MDRG. Our distance measure extends the Wasserstein distance between two persistence diagrams of Reeb graphs to MDPDs of MDRGs. We prove that the proposed measure is a pseudo-metric and satisfies a stability property. Effectiveness of the proposed distance measure has been demonstrated in (i) shape retrieval contest data - SHREC 2010 and (ii) Pt-CO bond detection data from computational chemistry. Experimental results show that the proposed distance measure based on the Reeb spaces has more discriminating power in clustering the shapes and detecting the formation of a stable Pt-CO bond as compared to the similar measures between Reeb graphs.
C1 [Ramamurthi, Yashwanth; Chattopadhyay, Amit] Int Inst Informat Technol IIIT, Bangalore 560100, Karnataka, India.
C3 International Institute of Information Technology Bangalore (IIIT
   Bangalore)
RP Chattopadhyay, A (corresponding author), Int Inst Informat Technol IIIT, Bangalore 560100, Karnataka, India.
EM yashwanth@iiitb.org; a.chattopadhyay@iiitb.ac.in
OI Chattopadhyay, Amit/0000-0003-4691-3019; Ramamurthi,
   Yashwanth/0000-0003-4933-0898
FU Science and Engineering Research Board (SERB), India
   [SERB/CRG/2018/000702]; International Institute of Information
   Technology (IIITB), Bangalore
FX This work was supported in part by Science and Engineering Research
   Board (SERB), India under Grant SERB/CRG/2018/000702, and inpart by the
   International Institute of Information Technology (IIITB), Bangalore for
   funding this project and for generous travel support.
CR Adams H, 2017, J MACH LEARN RES, V18
   Agarwal PK, 2006, DISCRETE COMPUT GEOM, V36, P553, DOI 10.1007/s00454-006-1265-8
   Agarwal T., 2021, Topological Methods in Data Analysis and Visualization, VVI, P197
   [Anonymous], 1979, INFORM RETRIEVAL
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bauer U., 2014, P 30 ANN S COMP GEOM, P464
   Biasotti S, 2011, PATTERN RECOGN LETT, V32, P1735, DOI 10.1016/j.patrec.2011.07.014
   Bjerkevik Havard Bakke, 2021, PREPRINT
   Botnan MB, 2023, Arxiv, DOI [arXiv:2203.14289, 10.48550/ARXIV.2203.14289, DOI 10.48550/ARXIV.2203.14289, 10.48550/arXiv.2203.14289]
   Carlsson G, 2010, J COMPUT GEOM, V1, P72
   Carlsson G, 2009, DISCRETE COMPUT GEOM, V42, P71, DOI 10.1007/s00454-009-9176-0
   Carr H, 2014, IEEE T VIS COMPUT GR, V20, P1100, DOI 10.1109/TVCG.2013.269
   Carriere M., 2017, P INT S COMP GEOM BR
   Carriere M., 2020, Advances in Neural Information Processing Systems, V33, P22432
   Cerri A, 2013, MATH METHOD APPL SCI, V36, P1543, DOI 10.1002/mma.2704
   Chattopadhyay A., 2014, P EUR C VIS, P1
   Chattopadhyay A, 2016, COMP GEOM-THEOR APPL, V58, P1, DOI 10.1016/j.comgeo.2016.05.006
   Chazal F, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P237, DOI 10.1145/1542362.1542407
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   Corbet Rene, 2019, Comput Graph X, V2, DOI 10.1016/j.cagx.2019.100005
   de Silva V, 2016, DISCRETE COMPUT GEOM, V55, P854, DOI 10.1007/s00454-016-9763-9
   Dey T. K., 2018, P 34 INT S COMP GEOM, p32:1
   Dey T. K., 2022, Computational Topology for Data Analysis
   Dey T.K., 2014, P 30 ANN S COMP GEOM, P345, DOI DOI 10.1145/2582112.2582165
   Dey T. K., 2015, P 31 INT S COMP GEOM, P491
   Dey Tamal, 2016, P 27 S DISCR ALG, P997, DOI DOI 10.1137/1.9781611974331.CH71
   Dey Tamal K., 2022, Journal of Applied and Computational Topology, DOI DOI 10.1007/S41468-022-00087-5
   Di Fabio B, 2016, DISCRETE COMPUT GEOM, V55, P423, DOI 10.1007/s00454-016-9758-6
   Dimakis N, 2009, J PHYS CHEM C, V113, P18730, DOI 10.1021/jp9036809
   Duke D, 2012, IEEE T VIS COMPUT GR, V18, P2033, DOI 10.1109/TVCG.2012.287
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   EDELSBRUNNER H., 2010, Computational Topology: An Introduction (Applied Mathematics)
   Edelsbrunner H, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P242, DOI 10.1145/1377676.1377720
   Favelier G, 2019, IEEE T VIS COMPUT GR, V25, P1152, DOI 10.1109/TVCG.2018.2864432
   Hardy G., 1988, Inequalities, V2nd
   Hensel F, 2021, FRONT ARTIF INTELL, V4, DOI 10.3389/frai.2021.681108
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Huettenberger L, 2013, COMPUT GRAPH FORUM, V32, P341, DOI 10.1111/cgf.12121
   Junyi Tu, 2019, Advances in Visual Computing. 14th International Symposium on Visual Computing, ISVC 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11844), P99, DOI 10.1007/978-3-030-33720-9_8
   Kendrick I, 2010, J AM CHEM SOC, V132, P17611, DOI 10.1021/ja1081487
   Kerber M., 2020, P 36 INT S COMP GEOM
   Kerber M., 2019, P 35 INT S COMP GEOM
   Kerber Michael, 2017, Journal of Experimental Algorithmics (JEA), V22, P1, DOI 10.1145/3064175
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Leifman G., 2003, P 4 ISR KOR BI NAT C, P159
   Lesnick M, 2015, FOUND COMPUT MATH, V15, P613, DOI 10.1007/s10208-015-9255-y
   Li CY, 2014, PROC CVPR IEEE, P2003, DOI 10.1109/CVPR.2014.257
   Lian Z., 2010, EUR GRAPH WORKSH 3D
   Loiseaux D, 2023, Arxiv, DOI arXiv:2206.02026
   Patra A., 2018, Surface properties, adsorption, and phase transitions with a dispersion-corrected density functional
   Pont M, 2022, IEEE T VIS COMPUT GR, V28, P291, DOI 10.1109/TVCG.2021.3114839
   Ramamurthi Y, 2022, IEEE T VIS COMPUT GR, V28, P4360, DOI 10.1109/TVCG.2021.3087273
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Saeki O., 2014, Visualizing Multivariate Data Using Singularity Theory, P51
   Saeki O, 2004, LECT NOTES MATH, V1854, P1
   Scaramuccia S, 2020, COMP GEOM-THEOR APPL, V89, DOI 10.1016/j.comgeo.2020.101623
   Schreiber H., Sophia
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/SMI.2004.1314504
   Singh G Memoli F Carlsson G., 2007, EUROGRAPHICS S POINT
   Somorjai G. A., 2010, INTRO SURFACE CHEM C
   Sridharamurthy R, 2023, IEEE T VIS COMPUT GR, V29, P1518, DOI 10.1109/TVCG.2021.3122176
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   The GUDHI Project, 2020, GUDHI User and Reference Manual
   Tierny J, 2017, IEEE T VIS COMPUT GR, V23, P960, DOI 10.1109/TVCG.2016.2599017
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P1, DOI 10.1007/978-3-540-71050-9
   Wetzels F, 2022, 2022 IEEE WORKSHOP ON TOPOLOGICAL DATA ANALYSIS AND VISUALIZATION (TOPOINVIS 2022), P29, DOI 10.1109/TopoInVis57755.2022.00010
   Yan L, 2021, COMPUT GRAPH FORUM, V40, P599, DOI 10.1111/cgf.14331
   Zhang X., 2004, Texas Inst. Comput. Appl. Math., Tech. Rep.
   Zomorodian A, 2005, DISCRETE COMPUT GEOM, V33, P249, DOI 10.1007/s00454-004-1146-y
NR 70
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5939
EP 5952
DI 10.1109/TVCG.2023.3314763
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000042
PM 37703168
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Phutke, SS
   Murala, S
AF Phutke, Shruti S.
   Murala, Subrahmanyam
TI Image Inpainting via Correlated Multi-Resolution Feature Projection
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computer architecture; Image reconstruction; Decoding; Image resolution;
   Generators; Semantics; Convolution; Multi-resolution; non-local
   attention; feature projection; image inpainting; object removal
AB With the advancement in image editing applications, image inpainting is gaining more attention due to its ability to recover corrupted images efficiently. Also, the existing methods for image inpainting either use two-stage coarse-to-fine architectures or single-stage architectures with a deeper network. On the other hand, shallow network architectures lack the quality of results and the methods with remarkable inpainting quality have high complexity in terms of number of parameters or average run time. Despite the improvement in the inpainting quality, these methods still lack the correlated local and global information. In this work, we propose a single-stage multi-resolution generator architecture for image inpainting with moderate complexity and superior outcomes. Here, a multi-kernel non-local (MKNL) attention block is proposed to merge the feature maps from all the resolutions. Further, a feature projection block is proposed to project features of MKNL to respective decoder for effective reconstruction of image. Also, a valid feature fusion block is proposed to merge encoder skip connection features at valid region and respective decoder features at hole region. This ensures that there will not be any redundant feature merging while reconstruction of image. Effectiveness of the proposed architecture is verified on CelebA-HQ Liu, et al. 2015, Karras et al. 2017, and Places2 Zhou et al. 2018 datasets corrupted with publicly available NVIDIA mask dataset Liu et al. 2018. The detailed ablation study, extensive result analysis, and application of object removal prove the robustness of the proposed method over existing state-of-the-art methods for image inpainting.
C1 [Phutke, Shruti S.] Indian Inst Technol Ropar, Dept Elect Engn, Comp Vis & Pattern Recognit Lab, Rupnagar 140001, Punjab, India.
   [Murala, Subrahmanyam] Trinity Coll Dublin, Sch Comp Sci & Stat, CVPR Lab, Dublin D02 PN40, Ireland.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Ropar; Trinity College Dublin
RP Murala, S (corresponding author), Trinity Coll Dublin, Sch Comp Sci & Stat, CVPR Lab, Dublin D02 PN40, Ireland.
EM 2018eez0019@iitrpr.ac.in; subbumurala@iitrpr.ac.in
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Buyssens P, 2017, IEEE T IMAGE PROCESS, V26, P525, DOI 10.1109/TIP.2016.2619263
   Chen C, 2021, IEEE WINT CONF APPL, P3625, DOI 10.1109/WACV48630.2021.00367
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Ding D, 2019, IEEE T IMAGE PROCESS, V28, P1705, DOI 10.1109/TIP.2018.2880681
   Ghorai M, 2019, IEEE T IMAGE PROCESS, V28, P5495, DOI 10.1109/TIP.2019.2920528
   Gilbert A, 2020, IEEE T VIS COMPUT GR, V26, P2417, DOI 10.1109/TVCG.2018.2889297
   Goodfellow L. J., 2014, Generative Adversarial Networks
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   Hu J., 2018, P 2018 IEEE CVF C CO, P7132
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jam J, 2021, IEEE WINT CONF APPL, P2713, DOI 10.1109/WACV48630.2021.00276
   Jin DR, 2019, IEEE T CIRC SYST VID, V29, P1310, DOI 10.1109/TCSVT.2018.2839351
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Karras T, 2018, Arxiv, DOI arXiv:1710.10196
   Kawai N, 2016, IEEE T VIS COMPUT GR, V22, P1236, DOI 10.1109/TVCG.2015.2462368
   Kingma D.P., 2014, P INT C LEARNING REP
   Lahiri A, 2020, PROC CVPR IEEE, P13693, DOI 10.1109/CVPR42600.2020.01371
   Li JY, 2020, PROC CVPR IEEE, P7757, DOI 10.1109/CVPR42600.2020.00778
   Li JY, 2019, IEEE I CONF COMP VIS, P5961, DOI 10.1109/ICCV.2019.00606
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li SY, 2020, IEEE IMAGE PROC, P1008, DOI [10.1109/icip40778.2020.9191263, 10.1109/ICIP40778.2020.9191263]
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Li ZD, 2015, IEEE T IMAGE PROCESS, V24, P1138, DOI 10.1109/TIP.2014.2383322
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Luo GB, 2020, IEEE T PATTERN ANAL, V42, P1289, DOI 10.1109/TPAMI.2019.2899837
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Mori S, 2020, IEEE T VIS COMPUT GR, V26, P2994, DOI 10.1109/TVCG.2020.3003768
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Phutke SS, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109040
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Shin YG, 2021, IEEE T NEUR NET LEAR, V32, P252, DOI 10.1109/TNNLS.2020.2978501
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wadhwa G, 2021, IEEE WINT CONF APPL, P3911, DOI 10.1109/WACV48630.2021.00396
   Wang Y, 2018, Arxiv, DOI [arXiv:1810.08771, 10.48550/arXiv.1810.08771]
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yi ZL, 2020, PROC CVPR IEEE, P7505, DOI 10.1109/CVPR42600.2020.00753
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang LF, 2021, IEEE T CYBERNETICS, V51, P673, DOI [10.1109/TCYB.2019.2910151, 10.1109/TCYB.2019.2935066]
   Zhang Y., 2019, ICLR
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 53
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5953
EP 5964
DI 10.1109/TVCG.2023.3315061
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000016
PM 37703169
DA 2024-11-06
ER

PT J
AU Tian, H
   Zhu, CY
   Shi, YF
   Xu, K
AF Tian, Hui
   Zhu, Chenyang
   Shi, Yifei
   Xu, Kai
TI SuperUDF: Self-Supervised UDF Estimation for Surface Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface reconstruction; Point cloud compression; Estimation;
   Three-dimensional displays; Shape; Geometry; Feature extraction; UDF;
   point cloud reconstruction; implicit surface
AB Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code will be released after accteptance.
C1 [Tian, Hui; Zhu, Chenyang; Shi, Yifei; Xu, Kai] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Zhu, CY; Xu, K (corresponding author), Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
EM tianhui13@nudt.edu.cn; zhuchenyang07@nudt.edu.cn; yifei.j.shi@gmail.com;
   kevin.kai.xu@gmail.com
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [62002375, 62002376,
   62132021, 62325211, 62372457, 62002379]; National Science Foundation of
   Hunan Province of China [2021JJ40696, 2021RC3071, 2022RC1104,
   2023JJ20051]; NUDT Research [ZK22-52]; Science and Technology Innovation
   Program of Hunan Province [2023RC3011]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102200, in part by the
   National Natural Science Foundation of China under Grants 62002375,
   62002376, 62132021, 62325211, 62372457, and 62002379, in part by the
   National Science Foundation of Hunan Province of China under Grants
   2021JJ40696, 2021RC3071, 2022RC1104, and 2023JJ20051, in part by NUDT
   Research under Grant ZK22-52, and in part by the Science and Technology
   Innovation Program of Hunan Province under Grant 2023RC3011.
CR Atzmon M, 2020, Arxiv, DOI arXiv:2006.05400
   Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Boulch A, 2022, PROC CVPR IEEE, P6292, DOI 10.1109/CVPR52688.2022.00620
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chibane J, 2020, ADV NEUR IN, V33
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Erler P, 2024, Arxiv, DOI arXiv:2007.10453
   Guillard B, 2022, Arxiv, DOI arXiv:2111.14549
   Ioffe Sergey, 2015, P MACHINE LEARNING R, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kazhdan M., 2013, Jpn. Inst. Energy, V4, P56
   Kingma D., 2014, Comput. Sci., V15, P145
   Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239473, 10.1145/1276377.1276405]
   Liu SL, 2021, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR46437.2021.00183
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Loshchilov I., 2017, P INT C LEARN REPR T
   Ma BR, 2021, PR MACH LEARN RES, V139
   Ma BR, 2022, PROC CVPR IEEE, P6305, DOI 10.1109/CVPR52688.2022.00621
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mi ZX, 2020, PROC CVPR IEEE, P967, DOI 10.1109/CVPR42600.2020.00105
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Peng S., 2020, ECCV, P523
   Richa JP, 2022, Arxiv, DOI arXiv:2203.09167
   Tancik M., 2020, P 34 INT C NEURAL IN
   Venkatesh R, 2020, Arxiv, DOI arXiv:2011.02570
   Venkatesh R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12633, DOI 10.1109/ICCV48922.2021.01242
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Ye JL, 2022, PROC CVPR IEEE, P12819, DOI 10.1109/CVPR52688.2022.01249
   Zhao F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12654, DOI 10.1109/ICCV48922.2021.01244
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou JS, 2022, ADV NEUR IN
NR 32
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5965
EP 5975
DI 10.1109/TVCG.2023.3318085
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000001
PM 37738188
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Chandio, Y
   Bashir, N
   Interrante, V
   Anwar, FM
AF Chandio, Yasra
   Bashir, Noman
   Interrante, Victoria
   Anwar, Fatima M.
TI Investigating the Correlation Between Presence and Reaction Time in
   Mixed Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual environments; Virtual reality; Time measurement; Correlation;
   Particle measurements; Mixed reality; Atmospheric measurements; presence
ID VIRTUAL ENVIRONMENTS; SPATIAL PRESENCE; PERFORMANCE; EXPERIENCE; SENSE;
   BEHAVIOR; ILLUSION
AB Measuring presence is critical to improving user involvement and performance in Mixed Reality (MR). Presence, a crucial aspect of MR, is traditionally gauged using subjective questionnaires, leading to a lack of time-varying responses and susceptibility to user bias. Inspired by the existing literature on the relationship between presence and human performance, the proposed methodology systematically measures a user's reaction time to a visual stimulus as they interact within a manipulated MR environment. We explore the user reaction time as a quantity that can be easily measured using the systemic tools available in modern MR devices. We conducted an exploratory study (N = 40) with two experiments designed to alter the users' sense of presence by manipulating place illusion and plausibility illusion. We found a significant correlation between presence scores and reaction times with a correlation coefficient -0.65, suggesting that users with a higher sense of presence responded more swiftly to stimuli. We develop a model that estimates a user's presence level using the reaction time values with high accuracy of up to 80%. While our study suggests that reaction time can be used as a measure of presence, further investigation is needed to improve the accuracy of the model.
C1 [Chandio, Yasra; Bashir, Noman; Interrante, Victoria; Anwar, Fatima M.] Univ Massachusetts Amherst, Amherst, MA 01003 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst
RP Chandio, Y (corresponding author), Univ Massachusetts Amherst, Amherst, MA 01003 USA.
EM ychandio@umass.edu; nbashir@umass.edu; interran@umn.edu;
   fanwar@umass.edu
OI Interrante, Victoria/0000-0002-3313-6663; Chandio,
   Yasra/0000-0002-3436-6452; Bashir, Noman/0000-0001-9304-910X
FU NSF [2237485]
FX This work was supported by NSF under Grant 2237485.
CR [Anonymous], 2023, Mixed reality capture
   [Anonymous], 2023, Hololens 2
   [Anonymous], 2023, Coffee Stack Game
   [Anonymous], 2022, Air tap gesture
   [Anonymous], Windows Device Portal
   [Anonymous], 2023, Intractable objects
   [Anonymous], 2023, Fruit ninja game
   Baños RM, 2000, CYBERPSYCHOL BEHAV, V3, P327, DOI 10.1089/10949310050078760
   BARFIELD W, 1993, ADV HUM FACT ERGON, V19, P699
   Barfield W, 2016, PRESENCE-TELEOP VIRT, V25, P148, DOI 10.1162/PRES_a_00252
   Basdogan C., 2000, ACM Transactions on Computer-Human Interaction, V7, P443, DOI 10.1145/365058.365082
   Baumgartner T, 2006, CYBERPSYCHOL BEHAV, V9, P30, DOI 10.1089/cpb.2006.9.30
   Brubach L, 2022, IEEE T VIS COMPUT GR, V28, P2267, DOI 10.1109/TVCG.2022.3150496
   Bystrom KE, 1999, PRESENCE-TELEOP VIRT, V8, P241, DOI 10.1162/105474699566107
   Cho D, 2003, P IEEE VIRT REAL ANN, P273
   Cooper N, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191846
   Darken R P, 1999, Cyberpsychol Behav, V2, P337, DOI 10.1089/cpb.1999.2.337
   De Boeck P, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00102
   de Grosbois J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00800
   Deary IJ, 2011, BEHAV RES METHODS, V43, P258, DOI 10.3758/s13428-010-0024-1
   Erickson A., 2020, P ACM S SPAT US INT
   Felton WM, 2022, INT J HUM-COMPUT INT, V38, P1, DOI 10.1080/10447318.2021.1921368
   Freeman J, 2000, PROC SPIE, V3959, P530, DOI 10.1117/12.387207
   Freeman J, 2000, PRESENCE-TELEOP VIRT, V9, P149, DOI 10.1162/105474600566691
   Freeman J, 1999, PRESENCE-TELEOP VIRT, V8, P1, DOI 10.1162/105474699566017
   Gandy M., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P127, DOI 10.1109/ISMAR.2010.5643560
   Giesel M, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78378-z
   GRAHAM FK, 1992, ATTENTION AND INFORMATION PROCESSING IN INFANTS AND ADULTS, P3
   Grassini S, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00349
   Heeter C, 2003, PRESENCE-TELEOP VIRT, V12, P335, DOI 10.1162/105474603322391587
   Hoffman HG, 2003, CYBERPSYCHOL BEHAV, V6, P127, DOI 10.1089/109493103321640310
   Huang MP, 1999, STUD HEALTH TECHNOL, V62, P148
   Ijsselsteijn W, 1998, DISPLAYS, V18, P207, DOI 10.1016/S0141-9382(98)00022-5
   IJsselsteijn W., 2001, P PRES C
   Insko B.E., 2003, EMERGING COMMUNICATI, V5, P109
   Jang DP, 2002, CYBERPSYCHOL BEHAV, V5, P11, DOI 10.1089/109493102753685845
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Khenak N, 2020, IEEE T VIS COMPUT GR, V26, P3467, DOI 10.1109/TVCG.2020.3023574
   Kim T., 1997, Journal of Computer-Mediated Communication, V3, P2, DOI [10.1111/j.1083-6101.1997.tb00073.x, DOI 10.1111/J.1083-6101.1997.TB00073.X]
   Laarni J., 2003, P ANN INT WORKSH PRE
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   Larsson P, 2001, CYBERPSYCHOL BEHAV, V4, P239, DOI 10.1089/109493101300117929
   Lessiter J, 2001, PRESENCE-TELEOP VIRT, V10, P282, DOI 10.1162/105474601300343612
   Lombard M., 1997, Journal of Computer-Mediated Communication, V3, pJCMC321, DOI 10.1111/j.1083-6101.1997.tb00072.x
   Maneuvrier A, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.571713
   Marucci M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84196-8
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Microsoft, 2021, Hololens 2 gestures for authoring and navigating in dynamics 365 guides
   Microsoft, 2022, Improve visual quality and comfort Internet
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   Nash EB, 2000, INT J HUM-COMPUT INT, V12, P1, DOI 10.1207/S15327590IJHC1201_1
   Neo JRJ, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.603750
   Nesbitt K, 2017, DISPLAYS, V48, P1, DOI 10.1016/j.displa.2017.01.002
   Nichols S, 2000, INT J HUM-COMPUT ST, V52, P471, DOI 10.1006/ijhc.1999.0343
   Nunez D., 2004, P 3 INT C COMPUTER G, V1, P83, DOI [10.1145/1029949.1029964, DOI 10.1145/1029949.1029964]
   Prothero J.D., 1995, Proceedings of the Conference on Experimental Analysis and Measurement of Situation Awareness, P359
   Putze S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376144
   Regenbrecht H, 2021, Arxiv, DOI arXiv:2103.02831
   Retaux X., 2003, PSYCHNOLOGY J, V1, P283
   Safikhani S, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489884
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Schubert Thomas W., 2003, Zeitschrift fur Medienpsychologie, V15, P69
   Schwind V, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300590
   Schwind V, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P507, DOI 10.1145/3116595.3116596
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Scikit learn, 2023, Random Forest Classifier.
   Scikit-Learn, 2023, Soft voting/majority rule classifier for unfitted estimators
   Scikit-Learn, 2023, Gradient boosting for classification
   Sheridan T. B., 1992, Presence, Teleoperators Virtual Environ., V1, P120, DOI 10.1162/pres.1992.1.1.120
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M., 1996, VRST'96. Proceedings of the ACM Symposium on Virtual Reality and Technology, P163
   Slater M, 2000, PRESENCE-TELEOP VIRT, V9, P413, DOI 10.1162/105474600566925
   Slater M., 1999, P EUR WORKSH VIRT EN, P8
   Slater M., 1995, ACM Transactions on Computer Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Spagnolli A, 2003, INT J HUM-COMPUT ST, V59, P797, DOI 10.1016/S1071-5819(03)00120-4
   Speicher M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300767
   Steed A., 2003, ONLINE PROC PRESENCE
   STEUER J, 1992, J COMMUN, V42, P73, DOI 10.1111/j.1460-2466.1992.tb00812.x
   Stevens B, 2001, LECT NOTES COMPUT SC, V2058, P194
   Stevens B, 2002, PRESENCE-TELEOP VIRT, V11, P79, DOI 10.1162/105474602317343677
   Szczurowski K, 2017, P I CON VIR SYS MULT, P101
   Turner S., 2003, P 3 UK ENV PSYCH C, P1
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Wagner I, 2009, PRESENCE-VIRTUAL AUG, V18, P249, DOI 10.1162/pres.18.4.249
   Wiederhold B. K., 2005, Virtual environments in clinical psy- chology and neuroscience: Methods and techniques in advanced patient- therapist interaction, P52
   Wiesing M, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0231152
   Wilson JR, 1997, ADV HUM FACT ERGON, V21, P889
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
NR 91
TC 3
Z9 3
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5976
EP 5992
DI 10.1109/TVCG.2023.3319563
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000027
PM 37751337
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wu, K
   Zhao, ZY
   Zhang, Z
   Liu, LG
   Fu, XM
AF Wu, Kang
   Zhao, Zheng-Yu
   Zhang, Zheng
   Liu, Ligang
   Fu, Xiao-Ming
TI Piecewise Developable Modeling via Implicit Neural Deformation and
   Feature-Guided Cutting
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Deformation; Approximation error; Computational modeling;
   Three-dimensional displays; Merging; Topology; Piecewise developable
   modeling; implicit neural shape representation; manufacturability
   constraints; approximation error
ID BEZIER; APPROXIMATION; ALGORITHM; SURFACES
AB We propose a novel and automatic method to model shapes using a small set of discrete developable patches. Central to our approach is using implicit neural shape representation that makes our algorithm independent of tessellation and allows us to obtain the Gaussian curvature of each point analytically. With this powerful representation, we first deform the input shape to be an almost developable shape with clear and sparse salient feature curves. Then, we convert the deformed implicit field to a triangle mesh, which is further cut to disk topology along parts of the sparse feature curves. Finally, we achieve the resulting piecewise developable mesh by alternatingly optimizing discrete developability, enforcing manufacturability constraints, and merging patches. The feasibility and practicability of our method are demonstrated over various shapes. Compared to the state-of-the-art methods, our method achieves a better tradeoff between the number of developable patches and the approximation error.
C1 [Wu, Kang; Zhao, Zheng-Yu; Liu, Ligang; Fu, Xiao-Ming] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
   [Zhang, Zheng] Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM kang910042009@gmail.com; zyzhao18@mail.ustc.edu.cn;
   zheng1003@mail.ustc.edu.cn; lgliu@ustc.edu.cn; fuxm@ustc.edu.cn
RI Fu, Xiao-Ming/V-8253-2019
OI Zhao, Zheng-Yu/0000-0003-0360-5518; Fu, Xiao-Ming/0000-0001-8479-0107
FU National Natural Science Foundation of China [62272429]; Major Project
   of Science and Technology of Anhui Province [202203a05020050]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272429 and the in part by the Major
   Project of Science and Technology of Anhui Province under Grant
   202203a05020050.
CR Aumann G, 2004, COMPUT AIDED GEOM D, V21, P661, DOI 10.1016/j.cagd.2004.04.007
   Aumann G, 2003, COMPUT AIDED GEOM D, V20, P601, DOI 10.1016/j.cagd.2003.07.001
   Binninger A, 2021, COMPUT GRAPH FORUM, V40, P289, DOI 10.1111/cgf.14374
   Bo P, 2007, COMPUT GRAPH FORUM, V26, P365, DOI 10.1111/j.1467-8659.2007.01059.x
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen DS, 2013, COMPUT GRAPH FORUM, V32, P305, DOI 10.1111/cgf.12050
   Chen HY, 1999, GRAPH MODEL IM PROC, V61, P110, DOI 10.1006/gmip.1999.0487
   Chen M, 2010, VISUAL COMPUT, V26, P853, DOI 10.1007/s00371-010-0467-5
   Chu CH, 2002, COMPUT AIDED DESIGN, V34, P511, DOI 10.1016/S0010-4485(01)00122-1
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Decaudin P, 2006, COMPUT GRAPH FORUM, V25, P625, DOI 10.1111/j.1467-8659.2006.00982.x
   Deng Y, 2021, PROC CVPR IEEE, P10281, DOI 10.1109/CVPR46437.2021.01015
   Dijkstra E. W., 1959, Numer. Math, V1, P269, DOI [DOI 10.1007/BF01386390, 10.1007/BF01386390]
   Gavriil K, 2019, COMPUT AIDED DESIGN, V111, P29, DOI 10.1016/j.cad.2019.01.006
   Grinspun E., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P62
   Gropp A., 2020, INT C MACH LEARN, V119, P3789
   Haeberli P., 2002, U.S. Patent, Patent No. 6493603
   Ion A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417835
   Jadon E, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555414
   Jiang CG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392430
   Jiang ZS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130895
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Jung A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2749458
   Kernighan B. W., 1970, Bell System Technical Journal, V49, P291
   Keuper M, 2015, IEEE I CONF COMP VIS, P1751, DOI 10.1109/ICCV.2015.204
   Kingma D.P., 2014, P INT C LEARNING REP
   Kruskal J. B., 1956, Proceedings of the American Mathematical Society, V7, P48, DOI [10.1090/S0002-9939-1956-0078686-7, DOI 10.1090/S0002-9939-1956-0078686-7, http://dx.doi.org/10.1090/S0002-9939-1956-0078686-7]
   Lang J., 1992, Computer-Aided Geometric Design, V9, P291, DOI 10.1016/0167-8396(92)90036-O
   Limper M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201328
   Liu H, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323000
   Liu LG, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201331
   Liu YJ, 2009, IEEE T AUTOM SCI ENG, V6, P700, DOI 10.1109/TASE.2008.2009926
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Maekawa T, 1998, J MECH DESIGN, V120, P453, DOI 10.1115/1.2829173
   Massarwi F, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P148, DOI 10.1109/PG.2007.16
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mitani J, 2004, ACM T GRAPHIC, V23, P259, DOI 10.1145/1015706.1015711
   Narain R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462010
   Niemeyer M, 2019, IEEE I CONF COMP VIS, P5378, DOI 10.1109/ICCV.2019.00548
   Oechsle M, 2019, IEEE I CONF COMP VIS, P4530, DOI 10.1109/ICCV.2019.00463
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Poranne R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130845
   POTTMANN H, 1995, COMPUT AIDED GEOM D, V12, P513, DOI 10.1016/0167-8396(94)00031-M
   Rabinovich M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3180494
   Rabinovich M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275088
   Rose K., 2007, SGP 07, P163
   Sellán S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392419
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   Smith J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766947
   Sorkine O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2002.1183795
   Stein O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201303
   Su JP, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392435
   Tang CC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2832906
   Tian Zhang, 1996, SIGMOD Record, V25, P103, DOI 10.1145/235968.233324
   Wang CCL, 2004, VISUAL COMPUT, V20, P521, DOI 10.1007/s00371-004-0256-0
   Wang CCL, 2004, ENG COMPUT-GERMANY, V20, P54, DOI 10.1007/s00366-004-0272-8
   Wang CCL, 2008, COMPUT AIDED DESIGN, V40, P109, DOI 10.1016/j.cad.2007.06.001
   Yang Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392434
   Zhao ZY, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592140
   Zhao ZY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530117
   Zheng ZR, 2021, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR46437.2021.00148
NR 61
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5993
EP 6004
DI 10.1109/TVCG.2023.3319487
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000029
PM 37768793
DA 2024-11-06
ER

PT J
AU Crisan, A
   Shang, M
   Brochu, E
AF Crisan, Anamaria
   Shang, Maddie
   Brochu, Eric
TI Eliciting Model Steering Interactions From Users via Data and Visual
   Design Probes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Probes; Machine learning; Encoding; Semantics;
   Prototypes; Data visualization; Design probes; interactive machine
   learning; model steering; semantic interactions
ID DIRECT-MANIPULATION; VISUALIZATION; GUIDANCE
AB Visual and interactive machine learning systems (IML) are becoming ubiquitous as they empower individuals with varied machine learning expertise to analyze data. However, it remains complex to align interactions with visual marks to a user's intent for steering machine learning models. We explore using data and visual design probes to elicit users' desired interactions to steer ML models via visual encodings within IML interfaces. We conducted an elicitation study with 20 data analysts with varying expertise in ML. We summarize our findings as pairs of target-interaction, which we compare to prior systems to assess the utility of the probes. We additionally surfaced insights about factors influencing how and why participants chose to interact with visual encodings, including refraining from interacting. Finally, we reflect on the value of gathering such formative empirical evidence via data and visual design probes ahead of developing IML prototypes.
C1 [Crisan, Anamaria] Tableau Res, Seattle, WA 98103 USA.
   [Shang, Maddie; Brochu, Eric] Tableau, Vancouver, BC V6B 1A6, Canada.
RP Crisan, A (corresponding author), Tableau Res, Seattle, WA 98103 USA.
EM anamaria.crisan@gmail.com; mshang@tableau.com; ebrochu@tableau.com
CR Bae J, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3340960
   Bansal Gagan, 2019, P AAAI C HUM COMP CR, V7, P2, DOI [10.1609/HCOMP.V7I1.5285, 10.1609/hcomp, DOI 10.1609/HCOMP]
   Bernard J, 2018, VISUAL COMPUT, V34, P1189, DOI 10.1007/s00371-018-1500-3
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Boukhelifa N, 2020, IEEE COMPUT GRAPH, V40, P88, DOI 10.1109/MCG.2020.3017064
   Bradel L, 2014, IEEE CONF VIS ANAL, P163, DOI 10.1109/VAST.2014.7042492
   Brehmer M., 2014, PROC WORKSHOP TIME E, P147
   Broadley C., 2013, Ph.D. dissertation
   Brown ET, 2012, IEEE CONF VIS ANAL, P83, DOI 10.1109/VAST.2012.6400486
   Browne JT, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312877
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chang JC, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2334, DOI 10.1145/3025453.3026044
   Choo J, 2013, IEEE T VIS COMPUT GR, V19, P1992, DOI 10.1109/TVCG.2013.212
   Collins C, 2018, VIS INFORM, V2, P166, DOI 10.1016/j.visinf.2018.09.003
   Combemale B, 2021, IEEE SOFTWARE, V38, P71, DOI 10.1109/MS.2020.2995125
   Creswell J. W., 2018, QUAL INQ
   Crisan A., 2021, CHI, P1, DOI DOI 10.1145/3411764.3445775
   Crisan Anamaria, 2021, P CHI C HUM FACT COM
   Dakuo Wang, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359313
   Davani Aida Mostafazadeh, 2021, arXiv
   Dellermann D, 2021, Arxiv, DOI arXiv:2105.03354
   Demiralp C, 2014, IEEE T VIS COMPUT GR, V20, P1933, DOI 10.1109/TVCG.2014.2346978
   Denton E, 2021, Arxiv, DOI [arXiv:2112.04554, 10.48550/ARXIV.2112.04554, DOI 10.48550/ARXIV.2112.04554]
   Dove G, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P278, DOI 10.1145/3025453.3025739
   Drucker SM, 2011, LECT NOTES COMPUT SC, V6948, P187, DOI 10.1007/978-3-642-23765-2_13
   Dudley JJ, 2018, ACM T INTERACT INTEL, V8, DOI 10.1145/3185517
   El-Assady M, 2018, IEEE T VIS COMPUT GR, V24, P382, DOI 10.1109/TVCG.2017.2745080
   Endert A, 2017, COMPUT GRAPH FORUM, V36, P458, DOI 10.1111/cgf.13092
   Endert A, 2013, IEEE COMPUT GRAPH, V33, P6, DOI 10.1109/MCG.2013.53
   Endert A, 2012, IEEE T VIS COMPUT GR, V18, P2879, DOI 10.1109/TVCG.2012.260
   Gehrmann S, 2020, IEEE T VIS COMPUT GR, V26, P884, DOI 10.1109/TVCG.2019.2934595
   Gil Y, 2019, PROCEEDINGS OF IUI 2019, P614, DOI 10.1145/3301275.3302324
   Gordon ML, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502004
   Graham Connor, 2008, P 10 ANN C PART DES, P194
   Hartmann B, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P145
   Hogan T, 2016, IEEE T VIS COMPUT GR, V22, P2579, DOI 10.1109/TVCG.2015.2511718
   Hohman F, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376177
   Hohman F, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300809
   Holstein K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300830
   Hutchins J. D., 1985, Hum.-Comput. Interact., V1, P311, DOI [DOI 10.1207/S15327051HCI0104, DOI 10.1207/S15327051HCI0104_2]
   Hutchinson Hilary, 2003, C HUM FACT COMP SYST
   Jiang L, 2019, J VISUAL-JAPAN, V22, P401, DOI 10.1007/s12650-018-0531-1
   Karmaker SK, 2021, Arxiv, DOI arXiv:2010.10777
   Kim B., 2015, Interactive and interpretable machine learning models for human machine collaboration
   Kim YS, 2021, IEEE T VIS COMPUT GR, V27, P989, DOI 10.1109/TVCG.2020.3028984
   Kim YS, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300912
   Kindlmann G, 2014, IEEE T VIS COMPUT GR, V20, P2181, DOI 10.1109/TVCG.2014.2346325
   Koch B., 2021, P NEUROIPS
   Lam H, 2012, IEEE T VIS COMPUT GR, V18, P1520, DOI 10.1109/TVCG.2011.279
   Lee D. J.-L., 2019, IEEE Data Eng. Bull., V42, P59
   Lee H, 2012, COMPUT GRAPH FORUM, V31, P1155, DOI 10.1111/j.1467-8659.2012.03108.x
   MAULSBY D, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P277
   McNutt A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376420
   Mishra S, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445096
   Padilla Lace, 2020, UNCERTAINTY VISUALIZ
   Pu XY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376466
   Qian Yang, 2020, CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3313831.3376301
   Ramos G, 2020, HUM-COMPUT INTER-US, V35, P413, DOI 10.1080/07370024.2020.1734931
   Russakovsky O, 2015, PROC CVPR IEEE, P2121, DOI 10.1109/CVPR.2015.7298824
   Sacha D, 2017, NEUROCOMPUTING, V268, P164, DOI 10.1016/j.neucom.2017.01.105
   Sacha D, 2017, IEEE T VIS COMPUT GR, V23, P241, DOI 10.1109/TVCG.2016.2598495
   Saket B, 2020, IEEE T VIS COMPUT GR, V26, P482, DOI 10.1109/TVCG.2019.2934534
   Saket B, 2017, IEEE T VIS COMPUT GR, V23, P331, DOI 10.1109/TVCG.2016.2598839
   Sambasivan Nithya, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3445518
   Sanders EBN, 2014, CODESIGN, V10, P5, DOI 10.1080/15710882.2014.888183
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1007/BF00116037
   SHNEIDERMAN B, 1983, COMPUTER, V16, P57, DOI 10.1109/MC.1983.1654471
   Shneiderman B, 2020, INT J HUM-COMPUT INT, V36, P495, DOI 10.1080/10447318.2020.1741118
   Sperrle F, 2021, COMPUT GRAPH FORUM, V40, P543, DOI 10.1111/cgf.14329
   Sperrle F., 2018, P IEEE VIS WORK MACH, P1
   Sperrle F, 2021, COMPUT GRAPH-UK, V100, P93, DOI 10.1016/j.cag.2021.06.016
   Stiennon N., 2022, P NEUROIPS
   Strobelt H, 2022, IEEE T VIS COMPUT GR, V28, P1106, DOI 10.1109/TVCG.2021.3114845
   Stumpf S, 2009, INT J HUM-COMPUT ST, V67, P639, DOI 10.1016/j.ijhcs.2009.03.004
   Subramonyam H., 2021, How can human-centered design shape data-centric AI?
   Subramonyam H, 2021, PROCEEDINGS OF THE 2021 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2021), P1529, DOI 10.1145/3461778.3462012
   Subramonyam H, 2021, IUI '21 - 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P48, DOI 10.1145/3397481.3450640
   Valdez AC, 2018, IEEE T VIS COMPUT GR, V24, P584, DOI 10.1109/TVCG.2017.2744138
   van Wijk JJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P79
   Wall E, 2018, IEEE T VIS COMPUT GR, V24, P288, DOI 10.1109/TVCG.2017.2745078
   Xin DR, 2021, Arxiv, DOI arXiv:2101.04834
   Yang Q, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P573, DOI 10.1145/3196709.3196729
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Zanzotto FM, 2019, J ARTIF INTELL RES, V64, P243, DOI 10.1613/jair.1.11345
NR 84
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6005
EP 6019
DI 10.1109/TVCG.2023.3322898
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000048
PM 37844008
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Tang, JS
   Zhang, B
   Yang, BX
   Zhang, T
   Chen, D
   Ma, LZ
   Wen, F
AF Tang, Junshu
   Zhang, Bo
   Yang, Binxin
   Zhang, Ting
   Chen, Dong
   Ma, Lizhuang
   Wen, Fang
TI 3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D morphable models; 3D-aware GAN; controllable 3 D portrait generation;
   neural radiance field; 3D morphable models; 3D-aware GAN; controllable 3
   D portrait generation; neural radiance field
ID VIDEO; IMAGE
AB In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas when animating expressions. We solve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic field. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.
C1 [Tang, Junshu; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Zhang, Bo; Zhang, Ting; Chen, Dong; Wen, Fang] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Yang, Binxin] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
   [Yang, Binxin] Zhangjiang Lab, Shanghai 201210, Peoples R China.
C3 Shanghai Jiao Tong University; Microsoft; Microsoft Research Asia;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Zhangjiang Laboratory
RP Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Zhang, B (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM tangjs@sjtu.edu.cn; zhanbo@microsoft.com; tennyson@mail.ustc.edu.cn;
   tinzhan@microsoft.com; doch@microsoft.com; ma-lz@cs.sjtu.edu.cn;
   fangwen@microsoft.com
RI Zhang, Tingting/N-9698-2015; Wen, Fang/HSH-6146-2023
OI Zhang, Ting/0000-0002-3952-2522; Chen, Dong/0000-0003-0588-9331
FU National Natural Science Foundation of China [61972157, 72192821];
   Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0102]; Shanghai Science and Technology Commission
   [21511101200]; Shanghai Sailing Program [22YF1420300, 23YF1410500];
   Young Elite Scientists Sponsorship Program by CAST [2022QNRC001]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61972157 and Grant 72192821, in part by the Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0102, in part by the Shanghai Science and Technology
   Commission under Grant 21511101200, in part by the Shanghai Sailing
   Program under Grant 22YF1420300 and 23YF1410500, and in part by the
   Young Elite Scientists Sponsorship Program by CAST under Grant
   2022QNRC001.
CR Aggarwal A., 2021, International Journal of Information Management Data Insights, V1, DOI [10.1016/j.jjimei.2020.100004, DOI 10.1016/J.JJIMEI.2020.100004]
   Athar S, 2022, PROC CVPR IEEE, P20332, DOI 10.1109/CVPR52688.2022.01972
   Bergman AW, 2022, Arxiv, DOI arXiv:2206.14314
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580
   Cao C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925873
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Casas D, 2014, COMPUT GRAPH FORUM, V33, P371, DOI 10.1111/cgf.12296
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Dai B., 2019, arXiv
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Deng Y, 2022, PROC CVPR IEEE, P10663, DOI 10.1109/CVPR52688.2022.01041
   Deng Y, 2020, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR42600.2020.00520
   DeVries T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14284, DOI 10.1109/ICCV48922.2021.01404
   Di X, 2017, Arxiv, DOI arXiv:1801.00077
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Gadelha M, 2017, INT CONF 3D VISION, P402, DOI 10.1109/3DV.2017.00053
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Geng ZL, 2019, PROC CVPR IEEE, P9813, DOI 10.1109/CVPR.2019.01005
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Gu JT, 2021, Arxiv, DOI arXiv:2110.08985
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   Härkönen E, 2020, ADV NEUR IN, V33
   Hensel M, 2017, ADV NEUR IN, V30
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hyeongwoo Kim, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3197517.3201283
   Ichim AE, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766974
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2020, ADV NEUR IN, V33
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kato H, 2020, Arxiv, DOI arXiv:2006.12057
   Kingma D.P., 2014, P INT C LEARNING REP
   Laine S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417861
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu MY, 2021, P IEEE, V109, P839, DOI 10.1109/JPROC.2021.3049196
   Livingstone SR, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196391
   Locatello F, 2019, PR MACH LEARN RES, V97
   Lombardi S, 2019, Arxiv, DOI arXiv:1906.07751
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Lu YY, 2018, LECT NOTES COMPUT SC, V11216, P293, DOI 10.1007/978-3-030-01258-8_18
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Martel J, 2021, Arxiv, DOI arXiv:2105.02788
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Mo SW, 2020, Arxiv, DOI arXiv:2002.10964
   Nguyen-Phuoc TH., 2020, ARXIV200208988, V33, P6767
   Nickabadi A, 2022, Arxiv, DOI arXiv:2205.10587
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   OrEl R, 2022, PROC CVPR IEEE, P13493, DOI 10.1109/CVPR52688.2022.01314
   Ouyang H., 2022, arXiv
   Pan XA, 2021, Arxiv, DOI arXiv:2011.00844
   Parent R., 2012, Computer Animation: Algorithms and Techniques
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Piao JT, 2021, PROC CVPR IEEE, P15614, DOI 10.1109/CVPR46437.2021.01536
   Piao JT, 2019, IEEE I CONF COMP VIS, P9397, DOI 10.1109/ICCV.2019.00949
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Radford A, 2021, PR MACH LEARN RES, V139
   Rajetal A., 2021, P IEEE CVF C COMP VI, P11733
   Ren YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13739, DOI 10.1109/ICCV48922.2021.01350
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Serengil SI, 2021, 2021 7TH INTERNATIONAL CONFERENCE ON ENGINEERING AND EMERGING TECHNOLOGIES (ICEET 2021), P863, DOI 10.1109/ICEET53442.2021.9659697
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shi YC, 2021, PROC CVPR IEEE, P6254, DOI 10.1109/CVPR46437.2021.00619
   Shoshan A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14063, DOI 10.1109/ICCV48922.2021.01382
   Siarohin A, 2019, ADV NEUR IN, V32
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V., 2020, Advances in Neural Information Processing Systems, V33, P7462
   Sitzmann V, 2019, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2019.00254
   Su F., 2021, Preprints
   Sun JX, 2023, PROC CVPR IEEE, P20991, DOI 10.1109/CVPR52729.2023.02011
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   Sun Jingxiang, 2022, arXiv
   Sun KQ, 2022, Arxiv, DOI arXiv:2206.08361
   Szab¢ A, 2019, Arxiv, DOI arXiv:1910.00287
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang JS, 2021, IEEE MULTIMEDIA, V28, P42, DOI 10.1109/MMUL.2021.3061544
   Tang JS, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102852
   Teng Zhang, 2020, 2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET), P67, DOI 10.1109/CCET50901.2020.9213159
   Tewari A, 2022, COMPUT GRAPH FORUM, V41, P703, DOI 10.1111/cgf.14507
   Tewari A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417803
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Nguyen-Phuoc T, 2019, IEEE I CONF COMP VIS, P7587, DOI 10.1109/ICCV.2019.00768
   Tianye Li, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3130800.3130813
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Vora S, 2021, Arxiv, DOI arXiv:2111.13260
   Wang A., 2018, P EUR C COMP VIS WOR, P692
   Wang C, 2022, Arxiv, DOI arXiv:2104.11228
   Wang P, 2021, Arxiv, DOI arXiv:2106.10689
   Wang TF, 2022, PROC CVPR IEEE, P11369, DOI 10.1109/CVPR52688.2022.01109
   Wang ZY, 2021, PROC CVPR IEEE, P5700, DOI 10.1109/CVPR46437.2021.00565
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu y., 2022, arXiv
   Xia WH, 2023, IEEE T PATTERN ANAL, V45, P3121, DOI 10.1109/TPAMI.2022.3181070
   Xiang JF, 2023, Arxiv, DOI arXiv:2206.07255
   Xie CL, 2021, PROC CVPR IEEE, P4617, DOI 10.1109/CVPR46437.2021.00459
   Xie YH, 2022, COMPUT GRAPH FORUM, V41, P641, DOI 10.1111/cgf.14505
   Xu Sicheng, 2020, P IEEE CVF C COMP VI, P7710
   Yariv L, 2021, ADV NEUR IN
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Z., 2016, IEEE Signal Process.Lett., V23
   Zhao XM, 2022, Arxiv, DOI arXiv:2207.10642
   Zhu J.-Y., 2018, P NEURIPS, P118
NR 124
TC 2
Z9 2
U1 6
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6020
EP 6037
DI 10.1109/TVCG.2023.3323578
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000008
PM 37847635
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Alomari, HW
   Vendome, C
   Rizkallah, L
AF Alomari, Hakam W.
   Vendome, Christopher
   Rizkallah, Lane
TI A Comprehensive Evaluation Framework of Software Visualizations
   Effectiveness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Taxonomy; Task analysis; Software systems; Maintenance
   engineering; Scalability; Data visualization; Software visualizations;
   evaluation framework; effectiveness; expressiveness
ID ARCHITECTURE VISUALIZATION; DESIGN
AB Visualizations are useful in dealing with complex software systems, especially in maintenance and evolution tasks. Software visualization tools can help reduce the cognitive burden on practitioners when trying to understand these systems. However, a major challenge in designing new visualization techniques and tools is evaluating their effectiveness for specific tasks and users. If a visualization tool is not effective for practitioners, they are unlikely to adopt it. Existing evaluation frameworks for visualizations mainly focus on expressiveness, which refers to the ability of the visualization to show all necessary information. However, evaluating the effectiveness of visualizations is an open research problem, especially in terms of quantifying it. To address this problem, we propose a multi-dimensional evaluation framework that focuses on evaluating visualizations in terms of their qualitative, quantitative, and cognitive aspects. The framework includes seven main dimensions and twenty-eight features, with the effectiveness dimension being further subdivided into four sub-dimensions. We validate our framework by using it to evaluate a number of software visualization tools. This validation demonstrates that the framework can be applied to design and evaluate new software visualization techniques and tools.
C1 [Alomari, Hakam W.; Vendome, Christopher; Rizkallah, Lane] Miami Univ, Dept Comp Sci & Software Engn, Oxford, OH 45056 USA.
C3 University System of Ohio; Miami University
RP Alomari, HW (corresponding author), Miami Univ, Dept Comp Sci & Software Engn, Oxford, OH 45056 USA.
EM alomarhw@miamioh.edu; vendomcg@miamioh.edu; rizkalle@miamioh.edu
OI Alomari, Hakam/0000-0002-8554-3236; Rizkallah, Lane/0009-0005-6710-3594
CR Alomari HW, 2016, 2016 IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION, P101, DOI 10.1109/VISSOFT.2016.22
   Alomari HW, 2014, J SOFTW-EVOL PROC, V26, P931, DOI 10.1002/smr.1651
   Baecker R, 1997, COMMUN ACM, V40, P44, DOI 10.1145/248448.248458
   BASILI VR, 1984, IEEE T SOFTWARE ENG, V10, P728, DOI 10.1109/TSE.1984.5010301
   BASILI VR, 1988, IEEE T SOFTWARE ENG, V14, P758, DOI 10.1109/32.6156
   Bassil R. K., 2001, P WORKSH SOFTW VIS, P33
   Bedu L, 2019, 2019 SEVENTH IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P33, DOI 10.1109/VISSOFT.2019.00013
   Berander P., 2006, INT S EMPIRICAL SOFT, P316
   BESHERS C, 1993, IEEE COMPUT GRAPH, V13, P41, DOI 10.1109/38.219450
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Caldiera VRBG, 1994, Encyclopedia Softw, P528
   Campos J, 2012, IEEE INT CONF AUTOM, P378, DOI 10.1145/2351676.2351752
   Card J. D., 1999, Readings inInformation Visualization: Using Vision to Think
   Diehl S, 2014, LECT NOTES COMPUT SC, V8380, P13, DOI 10.1007/978-3-319-06793-3_2
   Fritz T., 2010, P ACM IEEE 32 INT C, V1, P175, DOI DOI 10.1145/1806799.1806828
   Gallagher K, 2005, 3RD IEEE INTERNATIONAL WORKSHOP ON VISUALIZING SOFTWARE FOR UNDERSTANDING AND ANALYSIS, PROCEEEDINGS, P76
   Gallagher K, 2008, IEEE T SOFTWARE ENG, V34, P260, DOI 10.1109/TSE.2007.70757
   Hundhausen CD, 2002, J VISUAL LANG COMPUT, V13, P259, DOI 10.1006/S1045-926X(02)00028-9
   Isenberg T, 2013, IEEE T VIS COMPUT GR, V19, P2818, DOI 10.1109/TVCG.2013.126
   Knight C., 1998, Visualisation for program comprehension: Information and issues
   Knight C., 2002, Handbook of Software Engineering and Knowledge Engineering: Emerging Technologies, V2, P131
   LaToza Thomas D., 2010, EVALUATION USABILITY, DOI [DOI 10.1145/1937117.1937125, 10.1145/1937117.1937125]
   Lopez-Herrejon RE, 2018, J SOFTW-EVOL PROC, V30, DOI 10.1002/smr.1912
   MACKINLAY J, 1986, ACM T GRAPHIC, V5, P110, DOI 10.1145/22949.22950
   Maletic JI, 2002, VISSOFT 2002: FIRST INTERNATIONAL WORKSHOP ON VISUALIZING SOFTWARE FOR UNDERSTANDING AND ANALYSIS, PROCEEDINGS, P32, DOI 10.1109/VISSOF.2002.1019792
   Mattila A.-L., 2016, P 20 INT AC MINDTR C, P262, DOI [DOI 10.1145/2994310.2994327, 10.1145/2994310.2994327]
   MAYER RE, 1991, J EDUC PSYCHOL, V83, P484, DOI 10.1037/0022-0663.83.4.484
   Merino E., 2019, arXiv
   Merino L, 2018, J SYST SOFTWARE, V144, P165, DOI 10.1016/j.jss.2018.06.027
   Merino L, 2018, J SOFTW-EVOL PROC, V30, DOI 10.1002/smr.1923
   Merino L, 2016, 2016 IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION, P61, DOI 10.1109/VISSOFT.2016.10
   Meyer M., 2006, Proceedings of the 2006 ACM symposium on Software visualization, P135
   Myers B. A., 1990, Journal of Visual Languages and Computing, V1, P97, DOI 10.1016/S1045-926X(05)80036-9
   Newman CD, 2016, 2016 IEEE/ACM 38TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING COMPANION (ICSE-C), P621, DOI 10.1145/2889160.2889173
   Nielsen J., 1990, SIGCHI Bulletin, P249
   Novais RL, 2013, INFORM SOFTWARE TECH, V55, P1860, DOI 10.1016/j.infsof.2013.05.008
   Price B, 1998, SOFTWARE VISUALIZATION, P3
   Price B. A., 1993, Journal of Visual Languages and Computing, V4, P211, DOI 10.1006/jvlc.1993.1015
   Price B.A., 1992, P 25 HAWAII INT C SY, P597
   Reniers D, 2014, SCI COMPUT PROGRAM, V79, P224, DOI 10.1016/j.scico.2012.05.002
   ROMAN GC, 1993, COMPUTER, V26, P11, DOI 10.1109/2.247643
   Schots M, 2014, 2014 SECOND IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P70, DOI 10.1109/VISSOFT.2014.20
   Sensalire M, 2009, IEEE INT WORK VIS SO, P19, DOI 10.1109/VISSOF.2009.5336431
   Sensalire M, 2008, SOFTVIS 2008: PROCEEDINGS OF THE 4TH ACM SYMPOSIUM ON SOFTWARE VISUALIZATION, P87
   Seriai A, 2014, 2014 SECOND IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P60, DOI 10.1109/VISSOFT.2014.19
   Shahin M, 2014, J SYST SOFTWARE, V94, P161, DOI 10.1016/j.jss.2014.03.071
   Shneiderman B., 2016, Designing the User Interface: Strategies for Effective Human -Computer Interaction.
   Shneiderman C., Designing the User Interface: Strategies for EffectiveHuman-Computer Interaction
   Sillito J., 2006, P 14 ACM SIGSOFT INT, P23, DOI DOI 10.1145/1181775.1181779
   SINGH G, 1990, CG INTERNATIONAL 90, P331
   Solingen Van, 1999, The Goal/Question/Metric Method:A Practical Guide for Quality Improvement of Software Development
   Stasko J. T., 1992, Proceedings. 1992 IEEE Workshop on Visual Languages (Cat. No.92TH0471-3), P3, DOI 10.1109/WVL.1992.275790
   Stasko J. T., 1998, Software Visualization: Programming as a Multimedia Experience, P19
   Storey MAD, 1999, J SYST SOFTWARE, V44, P171, DOI 10.1016/S0164-1212(98)10055-9
   Storey Margaret-Anne D., 2005, P 2005 ACM S SOFTWAR, P193, DOI DOI 10.1145/1056018.1056045
   Voinea Lucian., 2006, EUROVIS, P187
   WEISER M, 1984, IEEE T SOFTWARE ENG, V10, P352, DOI 10.1109/TSE.1984.5010248
   Wohlin C., 2012, Experimentation in software engineering
NR 58
TC 0
Z9 0
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6056
EP 6074
DI 10.1109/TVCG.2023.3321211
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000021
PM 37782605
DA 2024-11-06
ER

PT J
AU Battle, L
   Ottley, A
AF Battle, Leilani
   Ottley, Alvitta
TI What Do We Mean When We Say "Insight"? A Formal Synthesis of Existing
   Theory
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data models; Data mining; Data and knowledge visualization;
   Visualization techniques and methodologies
ID VISUAL ANALYTICS; VISUALIZATION; MODEL; DESIGN; TASKS; GRAMMAR; SYSTEM;
   QUERY
AB Researchers have derived many theoretical models for specifying users' insights as they interact with a visualization system. These representations are essential for understanding the insight discovery process, such as when inferring user interaction patterns that lead to insight or assessing the rigor of reported insights. However, theoretical models can be difficult to apply to existing tools and user studies, often due to discrepancies in how insight and its constituent parts are defined. This article calls attention to the consistent structures that recur across the visualization literature and describes how they connect multiple theoretical representations of insight. We synthesize a unified formalism for insights using these structures, enabling a wider audience of researchers and developers to adopt the corresponding models. Through a series of theoretical case studies, we use our formalism to compare and contrast existing theories, revealing interesting research challenges in reasoning about a user's domain knowledge and leveraging synergistic approaches in data mining and data management research.
C1 [Battle, Leilani] Univ Washington, Seattle, WA 98195 USA.
   [Ottley, Alvitta] Washington Univ St Louis, St Louis, MO 63130 USA.
C3 University of Washington; University of Washington Seattle; Washington
   University (WUSTL)
RP Battle, L (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM leibatt@cs.washington.edu; alvitta@wustl.edu
OI Ottley, Alvitta/0000-0002-9485-276X; Battle, Leilani/0000-0003-3870-636X
FU National Science Foundation (NSF) [2141506, 2142977, 2118201]
FX This work was supported by the National Science Foundation (NSF) under
   Grants 2141506, 2142977, and 2118201.
CR Amar R, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P111, DOI 10.1109/INFVIS.2005.1532136
   Amar RA, 2005, IEEE T VIS COMPUT GR, V11, P432, DOI 10.1109/TVCG.2005.63
   Andrienko N, 2018, COMPUT GRAPH FORUM, V37, P275, DOI 10.1111/cgf.13324
   Andrienko N., 2006, Exploratory Analysis of Spatial and Temporal Data: A Systematic Approach
   Battle L, 2023, Arxiv, DOI arXiv:2307.06551
   Battle L, 2022, IEEE VIS CONF, P1, DOI 10.1109/VIS54862.2022.00009
   Battle L, 2021, IEEE T VIS COMPUT GR, V27, P1128, DOI 10.1109/TVCG.2020.3028891
   Battle L, 2019, COMPUT GRAPH FORUM, V38, P145, DOI 10.1111/cgf.13678
   Battle L, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1363, DOI 10.1145/2882903.2882919
   Bertin J., 1983, Semiology of Graphics
   Bizer C, 2009, INT J SEMANT WEB INF, V5, P1, DOI 10.4018/jswis.2009081901
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Chandramouli B, 2014, PROC VLDB ENDOW, V8, P401, DOI 10.14778/2735496.2735503
   Chang R, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.22
   Chen Y, 2009, IEEE PAC VIS SYMP, P49, DOI 10.1109/PACIFICVIS.2009.4906837
   Choe EK, 2015, IEEE COMPUT GRAPH, V35, P28, DOI 10.1109/MCG.2015.51
   CODD EF, 1970, COMMUN ACM, V13, P377, DOI 10.1145/357980.358007
   Cutler Z, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P116, DOI 10.1109/VIS47514.2020.00030
   Demiralp C, 2017, PROC VLDB ENDOW, V10, P1937, DOI 10.14778/3137765.3137813
   Dou WW, 2009, IEEE COMPUT GRAPH, V29, P52, DOI 10.1109/MCG.2009.49
   Feng M, 2019, IEEE T VIS COMPUT GR, V25, P501, DOI 10.1109/TVCG.2018.2865117
   Gathani S, 2022, COMPUT GRAPH FORUM, V41, P489, DOI 10.1111/cgf.14557
   Gomez SR, 2014, IEEE CONF VIS ANAL, P63, DOI 10.1109/VAST.2014.7042482
   Gotz D, 2006, IEEE CONF VIS ANAL, P51
   Gotz D, 2009, INFORM VISUAL, V8, P42, DOI 10.1057/ivs.2008.31
   Green TM, 2008, IEEE CONF VIS ANAL, P91, DOI 10.1109/VAST.2008.4677361
   Guo H, 2016, IEEE T VIS COMPUT GR, V22, P51, DOI 10.1109/TVCG.2015.2467613
   Harrison L, 2014, IEEE T VIS COMPUT GR, V20, P1943, DOI 10.1109/TVCG.2014.2346979
   He C, 2021, IEEE T VIS COMPUT GR, V27, P3410, DOI 10.1109/TVCG.2020.2977634
   Heer J., 2021, Arquero| Arquero
   Hogan A, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447772
   Jain S, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P281, DOI 10.1145/2882903.2882957
   Ji SX, 2022, IEEE T NEUR NET LEAR, V33, P494, DOI 10.1109/TNNLS.2021.3070843
   Kale A, 2022, IEEE T VIS COMPUT GR, V28, P1150, DOI 10.1109/TVCG.2021.3114824
   Kandogan E, 2018, HILDA'18: PROCEEDINGS OF THE WORKSHOP ON HUMAN-IN-THE-LOOP DATA ANALYTICS, DOI 10.1145/3209900.3209912
   Kang J. S. Yi. Y.-A., 2008, P C TIM ERR NOV EV M
   Kang YA, 2012, IEEE T VIS COMPUT GR, V18, P2869, DOI 10.1109/TVCG.2012.224
   Karer B, 2021, IEEE T VIS COMPUT GR, V27, P1011, DOI 10.1109/TVCG.2020.3030376
   Lam H, 2018, IEEE T VIS COMPUT GR, V24, P435, DOI 10.1109/TVCG.2017.2744319
   Li HT, 2022, IEEE T VIS COMPUT GR, V28, P195, DOI 10.1109/TVCG.2021.3114863
   Liu ZC, 2014, IEEE T VIS COMPUT GR, V20, P2122, DOI 10.1109/TVCG.2014.2346452
   Mathisen A, 2019, COMPUT GRAPH FORUM, V38, P649, DOI 10.1111/cgf.13717
   McCabe T. J., 1976, IEEE Transactions on Software Engineering, VSE-2, P308, DOI 10.1109/TSE.1976.233837
   McNutt A, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445356
   Meijer E, 2011, COMMUN ACM, V54, P45, DOI [10.1145/2001269.2001285, 10.1145/2016036.2024658]
   Monadjemi S, 2021, IEEE T VIS COMPUT GR, V27, P412, DOI 10.1109/TVCG.2020.3030430
   NONAKA I, 1991, HARVARD BUS REV, V69, P96
   North C, 2006, IEEE COMPUT GRAPH, V26, P6, DOI 10.1109/MCG.2006.70
   North C, 2011, INFORM VISUAL, V10, P162, DOI 10.1177/1473871611415989
   Ottley A, 2019, COMPUT GRAPH FORUM, V38, P41, DOI 10.1111/cgf.13670
   Pike WA, 2009, INFORM VISUAL, V8, P263, DOI 10.1057/ivs.2009.22
   Pirolli P., 2005, P INT C INT AN MCLEA, V5, P2
   Plaisant C, 2008, IEEE T VIS COMPUT GR, V14, P120, DOI 10.1109/TVCG.2007.70412
   Pohl M, 2012, KUNSTL INTELL, V26, P151, DOI 10.1007/s13218-012-0167-6
   Pousman Z, 2007, IEEE T VIS COMPUT GR, V13, P1145, DOI 10.1109/TVCG.2007.70541
   Rind A, 2016, INFORM VISUAL, V15, P288, DOI 10.1177/1473871615621602
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Saraiya P, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P1, DOI 10.1109/INFVIS.2004.5
   Saraiya P, 2005, IEEE T VIS COMPUT GR, V11, P443, DOI 10.1109/TVCG.2005.53
   Saraiya P, 2006, IEEE T VIS COMPUT GR, V12, P1511, DOI 10.1109/TVCG.2006.85
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Shrinivasan Yedendra B., 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P123, DOI 10.1109/VAST.2009.5333023
   Shrinivasan YB, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1237
   Smuc M, 2009, IEEE COMPUT GRAPH, V29, P29, DOI 10.1109/MCG.2009.53
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Stolte C, 2002, IEEE T VIS COMPUT GR, V8, P52, DOI 10.1109/2945.981851
   Suh A, 2023, Arxiv, DOI arXiv:2204.14267
   Toniolo A, 2023, INTELL SYST APPL, V17, DOI 10.1016/j.iswa.2022.200151
   Vega Development Team, 2023, Transforms
   Wilkinson L., 2012, Handbook of Computational Statistics, P375, DOI [10.1007/978-3-642-21551-3_13, DOI 10.1007/978-3-642-21551-3_13]
   Willett W, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3131
   Xu K, 2020, COMPUT GRAPH FORUM, V39, P757, DOI 10.1111/cgf.14035
   Yun W, 2021, INT J INTELL SYST, V36, P1686, DOI 10.1002/int.22357
   Zehrung R, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445195
   Zeng ZH, 2022, IEEE T VIS COMPUT GR, V28, P346, DOI 10.1109/TVCG.2021.3114814
   Zgraggen E, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174053
   Zgraggen E, 2017, IEEE T VIS COMPUT GR, V23, P1977, DOI 10.1109/TVCG.2016.2607714
NR 78
TC 2
Z9 2
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6075
EP 6088
DI 10.1109/TVCG.2023.3326698
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000031
PM 37874712
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Wei, GS
   Pan, H
   Zhuang, SJ
   Zhou, YF
   Li, CJ
AF Wei, Guangshun
   Pan, Hao
   Zhuang, Shaojie
   Zhou, Yuanfeng
   Li, Changjian
TI iPUNet: Iterative Cross Field Guided Point Cloud Upsampling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Arbitrary ratios; cross field; flow-guided upsampling; iterative
   refinement; point cloud; sharp feature alignment
AB Point clouds acquired by 3D scanning devices are often sparse, noisy, and non-uniform, causing a loss of geometric features. To facilitate the usability of point clouds in downstream applications, given such input, we present a learning-based point upsampling method, i.e., iPUNet, which generates dense and uniform points at arbitrary ratios and better captures sharp features. To generate feature-aware points, we introduce cross fields that are aligned to sharp geometric features by self-supervision to guide point generation. Given cross field defined frames, we enable arbitrary ratio upsampling by learning at each input point a local parameterized surface. The learned surface consumes the neighboring points and 2D tangent plane coordinates as input, and maps onto a continuous surface in 3D where arbitrary ratios of output points can be sampled. To solve the non-uniformity of input points, on top of the cross field guided upsampling, we further introduce an iterative strategy that refines the point distribution by moving sparse points onto the desired continuous 3D surface in each iteration. Within only a few iterations, the sparse points are evenly distributed and their corresponding dense samples are more uniform and better capture geometric features. Through extensive evaluations on diverse scans of objects and scenes, we demonstrate that iPUNet is robust to handle noisy and non-uniformly distributed inputs, and outperforms state-of-the-art point cloud upsampling methods.
C1 [Wei, Guangshun; Zhuang, Shaojie; Zhou, Yuanfeng] Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
   [Pan, Hao] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Li, Changjian] Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Scotland.
C3 Shandong University; Microsoft; Microsoft Research Asia; University of
   Edinburgh
RP Zhou, YF (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
EM guangshunwei@gmail.com; haopan@microsoft.com; sjzhuang@mail.sdu.edu.cn;
   yfzhou@sdu.edu.cn; changjian.li@ed.ac.uk
RI Zhuang, Shaojie/JEF-3178-2023; Zhou, Yuanfeng/AAT-4670-2020
OI Wei, Guangshun/0000-0002-6045-4392; PAN, Hao/0000-0003-3628-9777;
   Zhuang, Shaojie/0000-0003-3973-3925; Li, Changjian/0000-0003-0448-4957
FU School of Informatics; National Natural Science Foundation of China
   [U1909210, 62172257]
FX The work of Changjian Li was supported in part by the start-up grant
   from the School of Informatics. This work was supported in part by the
   National Natural Science Foundation of China under Grants U1909210 and
   62172257. Recommended for acceptance by Z. Deng.
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   Chen HH, 2020, IEEE T VIS COMPUT GR, V26, P3255, DOI 10.1109/TVCG.2019.2920817
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dell'Eva A, 2022, INT CONF 3D VISION, P465, DOI 10.1109/3DV57658.2022.00058
   Diamanti O, 2014, COMPUT GRAPH FORUM, V33, P1, DOI 10.1111/cgf.12426
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Feng J. Li, 2021, arXiv
   Fu KX, 2021, PROC CVPR IEEE, P8889, DOI [10.1109/CVPR46437.2021.00878, 10.1109/TPAMI.2022.3204713]
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Huang JW, 2019, Arxiv, DOI arXiv:1903.12305
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Jakob W, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818078
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   Lai YK, 2010, IEEE T VIS COMPUT GR, V16, P95, DOI 10.1109/TVCG.2009.59
   Li RH, 2021, PROC CVPR IEEE, P344, DOI 10.1109/CVPR46437.2021.00041
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li YZ, 2018, ADV NEUR IN, V31
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239473, 10.1145/1276377.1276405]
   Mao AH, 2023, IEEE T VIS COMPUT GR, V29, P4964, DOI 10.1109/TVCG.2022.3196334
   Metzer G, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3470645
   Panozzo D, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601179
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qian J., ser. Lecture
   Qian Y, 2021, IEEE T IMAGE PROCESS, V30, P8354, DOI 10.1109/TIP.2021.3115385
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Willis KDD, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459818
   Wu SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818073
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yang YQ, 2020, PROC CVPR IEEE, P13575, DOI 10.1109/CVPR42600.2020.01359
   Ye SQ, 2022, IEEE T VIS COMPUT GR, V28, P3206, DOI 10.1109/TVCG.2021.3058311
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu X. Li, ser.Lecture
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhao WB, 2022, PROC CVPR IEEE, P1998, DOI 10.1109/CVPR52688.2022.00204
NR 44
TC 0
Z9 0
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6089
EP 6103
DI 10.1109/TVCG.2023.3324924
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000022
PM 37856273
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Yu, JX
   Nai, WZ
   Sun, XY
AF Yu, Jiaxin
   Nai, Weizhi
   Sun, Xiaoying
TI Accurate Raycasting Selection With Rotation Gesture Using a 6-DOF
   Tracking Device
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; 6-DOF; Three-dimensional displays; Jitter; Target
   tracking; User interfaces; Aerospace electronics; pen; raycasting; Vive;
   target selection
ID DESIGN
AB A six degrees of freedom (6-DOF) controller is a commonly used input device in three-dimensional user interface (3DUI) applications. However, for the fundamental task of target selection in 3D space, the selection accuracy decreases owing to the Heisenberg effect during the manipulation of the 6-DOF controller. Based on the pointing action of a 6-DOF device, we establish the mathematical model for raycasting and analyze the possibility of using a rotation gesture to select a target. This study proposes a target selection method using the axial rotation of the user's wrist, which reduces the negative impact of a discrete input for triggering the selection on accuracy. The detection model can identify the start time of the user's rotation action. The custom designed control display gain (CD gain) function can maintain the ray's stability during the rotation gesture. This method was verified using a 6-DOF pen and Vive controller in three experiments. The results show that the accuracy of the proposed rotation gesture-based raycasting target selection method is superior to that of the traditional button-pressing method, and it can be integrated into existing tracking systems for further application.
C1 [Yu, Jiaxin; Nai, Weizhi; Sun, Xiaoying] Jilin Univ, Dept Commun Engn, Changchun 130025, Peoples R China.
C3 Jilin University
RP Sun, XY (corresponding author), Jilin Univ, Dept Commun Engn, Changchun 130025, Peoples R China.
EM jxyu18@mails.jlu.edu; naiwz@jlu.edu.cn; sunxy@jlu.edu.cn
OI Yu, Jiaxin/0000-0002-0297-4605; Nai, Weizhi/0000-0002-2447-1814
FU National Key Research and Development Program of China [2016YFB1001304]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2016YFB1001304.
CR Argelaguet F, 2013, COMPUT GRAPH-UK, V37, P121, DOI 10.1016/j.cag.2012.12.003
   Argelaguet F, 2009, IEEE COMPUT GRAPH, V29, P34, DOI 10.1109/MCG.2009.117
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Bateman S, 2013, INT J HUM-COMPUT ST, V71, P511, DOI 10.1016/j.ijhcs.2012.12.006
   Batmaz AU, 2022, IEEE T VIS COMPUT GR, V28, P3939, DOI 10.1109/TVCG.2022.3203105
   Batmaz AU, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P23, DOI [10.1109/VRW50115.2020.00012, 10.1109/VRW50115.2020.0-264]
   Batmaz AU, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1687, DOI [10.1109/VR.2019.8798038, 10.1109/vr.2019.8798038]
   Batmaz Anil Ufuk, 2020, P FUT TECHN C, P792
   Bi XJ, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P191
   Bowman D., 2004, 3D USER INTERFACES T
   Bowman D. A., 2002, Virtual Reality, V6, P122, DOI 10.1007/s100550200013
   Cashion J, 2012, IEEE T VIS COMPUT GR, V18, P634, DOI 10.1109/TVCG.2012.40
   Chen J, 2021, IEEE SENS J, V21, P1756, DOI 10.1109/JSEN.2020.3016292
   Crossan A., 2008, P C HUM COMP INT MOB, P435
   Frees S, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1229855.1229857
   Gallo L, 2012, INT J HUM-COMPUT ST, V70, P287, DOI 10.1016/j.ijhcs.2011.12.001
   Kim M, 2019, INT J HUM-COMPUT INT, V35, P1147, DOI 10.1080/10447318.2018.1514163
   Kopper R., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P67, DOI 10.1109/3DUI.2011.5759219
   Lin SK, 2017, IEEE T MOBILE COMPUT, V16, P872, DOI 10.1109/TMC.2016.2567378
   Moore AG, 2018, INT J HUM-COMPUT ST, V120, P36, DOI 10.1016/j.ijhcs.2018.07.003
   Nai WZ, 2019, IEEE T NEUR SYS REH, V27, P1893, DOI 10.1109/TNSRE.2019.2931778
   Nancel M, 2015, ACM T COMPUT-HUM INT, V22, DOI 10.1145/2766448
   Ni T, 2011, INT J HUM-COMPUT ST, V69, P551, DOI 10.1016/j.ijhcs.2011.05.001
   Pavlovych A, 2009, EICS'09: PROCEEDINGS OF THE ACM SIGCHI SYMPOSIUM ON ENGINEERING INTERACTIVE COMPUTING SYSTEMS, P187
   Pham DM, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364264
   Rahman M, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1943
   Ro H, 2017, IEEE SYS MAN CYBERN, P2873, DOI 10.1109/SMC.2017.8123063
   Shoemaker G, 2012, ACM T COMPUT-HUM INT, V19, DOI 10.1145/2395131.2395135
   Soukoreff RW, 2004, INT J HUM-COMPUT ST, V61, P751, DOI 10.1016/j.ijhcs.2004.09.001
   Teather RJ, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P43, DOI 10.1109/3DUI.2009.4811204
   Tian F, 2013, INT J HUM-COMPUT ST, V71, P551, DOI 10.1016/j.ijhcs.2012.12.004
   Tian F, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1371
   Tian F, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P303
   Tu HW, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300848
   Walmsley WS, 2014, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2542544
   Yan XZ, 2020, IEEE T INSTRUM MEAS, V69, P995, DOI 10.1109/TIM.2019.2911758
   Young TS, 2017, IEEE SYMP 3D USER, P26, DOI 10.1109/3DUI.2017.7893314
NR 37
TC 0
Z9 0
U1 3
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6104
EP 6117
DI 10.1109/TVCG.2023.3324373
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000003
PM 37831579
DA 2024-11-06
ER

PT J
AU Ma, CF
   Yang, Y
   Guo, J
   Wei, MQ
   Wang, CJ
   Guo, YW
   Wang, WP
AF Ma, Changfeng
   Yang, Yang
   Guo, Jie
   Wei, Mingqiang
   Wang, Chongjun
   Guo, Yanwen
   Wang, Wenping
TI Collaborative Completion and Segmentation for Partial Point Clouds With
   Outliers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaborative completion and segmentation; deep learning; outliers;
   point cloud completion; shape analysis; Collaborative completion and
   segmentation; deep learning; outliers; point cloud completion; shape
   analysis
AB Outliers will inevitably creep into the captured point cloud during 3D scanning, degrading cutting-edge models on various geometric tasks heavily. This paper looks at an intriguing question that whether point cloud completion and segmentation can promote each other to defeat outliers. To answer it, we propose a collaborative completion and segmentation network, termed CS-Net, for partial point clouds with outliers. Unlike most of existing methods, CS-Net does not need any clean (or say outlier-free) point cloud as input or any outlier removal operation. CS-Net is a new learning paradigm that makes completion and segmentation networks work collaboratively. With a cascaded architecture, our method refines the prediction progressively. Specifically, after the segmentation network, a cleaner point cloud is fed into the completion network. We design a novel completion network which harnesses the labels obtained by segmentation together with farthest point sampling to purify the point cloud and leverages KNN-grouping for better generation. Benefited from segmentation, the completion module can utilize the filtered point cloud which is cleaner for completion. Meanwhile, the segmentation module is able to distinguish outliers from target objects more accurately with the help of the clean and complete shape inferred by completion. Besides the designed collaborative mechanism of CS-Net, we establish a benchmark dataset of partial point clouds with outliers. Extensive experiments show clear improvements of our CS-Net over its competitors, in terms of outlier robustness and completion accuracy.
C1 [Ma, Changfeng; Yang, Yang; Guo, Jie; Wang, Chongjun; Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Peoples R China.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210016, Peoples R China.
   [Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Nanjing University; Nanjing University of Aeronautics & Astronautics;
   University of Hong Kong
RP Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Peoples R China.
EM changfengma@smail.nju.edu.cn; yyang_nju@outlook.com; guojie@nju.edu.cn;
   mingqiang.wei@gmail.com; chjwang@nju.edu.cn; ywguo@nju.edu.cn;
   wenping@tamu.edu
OI Ma, Changfeng/0000-0001-8732-7038
FU National Natural Science Foundation of China [62032011, 61972194];
   Guangdong Basic and Applied Basic Research Foundation [2022A1515010170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62032011 and 61972194, and in part by
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2022A1515010170.
CR Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15447, DOI 10.1109/ICCV48922.2021.01518
   Chen X., 2019, arXiv
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen ZK, 2023, PROC CVPR IEEE, P13581, DOI 10.1109/CVPR52729.2023.01305
   Cheng R, 2021, PROC CVPR IEEE, P12542, DOI 10.1109/CVPR46437.2021.01236
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dovrat O, 2019, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR.2019.00287
   Nguyen DT, 2016, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2016.612
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gong BC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12468, DOI 10.1109/ICCV48922.2021.01226
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hassani K, 2019, IEEE I CONF COMP VIS, P8159, DOI 10.1109/ICCV.2019.00825
   Huang TX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12488, DOI 10.1109/ICCV48922.2021.01228
   Huang ZT, 2020, PROC CVPR IEEE, P7659, DOI 10.1109/CVPR42600.2020.00768
   Jiang MY, 2018, Arxiv, DOI [arXiv:1807.00652, 10.48550/arXiv.1807.00652, DOI 10.48550/ARXIV.1807.00652]
   Joseph-Rivlin M, 2019, IEEE INT CONF COMP V, P4085, DOI 10.1109/ICCVW.2019.00503
   Katz S, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276407, 10.1145/1239451.1239475]
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Landrieu L, 2019, PROC CVPR IEEE, P7432, DOI 10.1109/CVPR.2019.00762
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li KJ, 2018, LECT NOTES COMPUT SC, V11216, P508, DOI 10.1007/978-3-030-01258-8_31
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li YZ, 2018, ADV NEUR IN, V31
   Li ZZ, 2023, IEEE T MULTIMEDIA, V25, P3432, DOI 10.1109/TMM.2022.3160604
   Liang ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2763, DOI 10.1109/ICCV48922.2021.00278
   Lin CH, 2017, Arxiv, DOI arXiv:1706.07036
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu MH, 2019, Arxiv, DOI arXiv:1912.00280
   Mehra R, 2010, COMPUT GRAPH-UK, V34, P219, DOI 10.1016/j.cag.2010.03.002
   Qi C.R., 2017, ArXiv, P5099
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903
   Ren DY, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3387-7
   Sarmad M, 2019, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR.2019.00605
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Spurek P, 2021, Arxiv, DOI arXiv:2102.05973
   Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209
   Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Unal O, 2021, IEEE WINT CONF APPL, P2949, DOI 10.1109/WACV48630.2021.00299
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wang YD, 2020, Arxiv, DOI arXiv:2008.07358
   Wang YD, 2022, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR52688.2022.00162
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Wu T, 2021, ADV NEUR IN, V34
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Xie CL, 2021, PROC CVPR IEEE, P4617, DOI 10.1109/CVPR46437.2021.00459
   Xie H., 2020, P EUR C COMP VIS, P365, DOI DOI 10.1007/978-3-030-58545-721
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2021, PROC CVPR IEEE, P15358, DOI 10.1109/CVPR46437.2021.01511
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang JM, 2023, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR52729.2023.00515
   Zhang JM, 2021, IEEE ROBOT AUTOM LET, V6, P596, DOI 10.1109/LRA.2020.3048658
   Zhang Q., 2020, P EUR C COMP VIS, P512, DOI 10.1007/978-3-030-58595-2_31
   Zhang XC, 2021, PROC CVPR IEEE, P15885, DOI 10.1109/CVPR46437.2021.01563
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhou QY, 2018, Arxiv, DOI [arXiv:1801.09847, 10.48550/arXiv.1801.09847, DOI 10.48550/ARXIV.1801.09847]
   Zhu Z, 2023, IEEE I CONF COMP VIS, P14462, DOI 10.1109/ICCV51070.2023.01334
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 74
TC 1
Z9 1
U1 14
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6118
EP 6129
DI 10.1109/TVCG.2023.3328354
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000030
PM 37903041
DA 2024-11-06
ER

PT J
AU Huang, YW
   Zhou, YF
   Chen, R
   Pan, CH
   Shu, XH
   Weng, D
   Wu, YC
AF Huang, Yanwei
   Zhou, Yunfan
   Chen, Ran
   Pan, Changhao
   Shu, Xinhuan
   Weng, Di
   Wu, Yingcai
TI Interactive Table Synthesis With Natural Language
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Pipelines; Natural languages; Data visualization; Data
   models; Codes; Training; Data transformation; human -AI collaboration;
   large language model; natural language interface
ID FRAMEWORK; EXAMPLE
AB Tables are a ubiquitous data format for insight communication. However, transforming data into consumable tabular views remains a challenging and time-consuming task. To lower the barrier of such a task, research efforts have been devoted to developing interactive approaches for data transformation, but many approaches still presume that their users have considerable knowledge of various data transformation concepts and functions. In this study, we leverage natural language (NL) as the primary interaction modality to improve the accessibility of average users to performing complex data transformation and facilitate intuitive table generation and editing. Designing an NL-driven data transformation approach introduces two challenges: 1) NL-driven synthesis of interpretable pipelines and 2) incremental refinement of synthesized tables. To address these challenges, we present NL2Rigel, an interactive tool that assists users in synthesizing and improving tables from semi-structured text with NL instructions. Based on a large language model and prompting techniques, NL2Rigel can interpret the given NL instructions into a table synthesis pipeline corresponding to Rigel specifications, a declarative language for tabular data transformation. An intuitive interface is designed to visualize the synthesis pipeline and the generated tables, helping users understand the transformation process and refine the results efficiently with targeted NL instructions. The comprehensiveness of NL2Rigel is demonstrated with an example gallery, and we further confirmed NL2Rigel's usability with a comparative user study by showing that the task completion time with NL2Rigel is significantly shorter than that with the original version of Rigel with comparable completion rates.
C1 [Huang, Yanwei; Zhou, Yunfan; Chen, Ran; Pan, Changhao; Weng, Di; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, software Engn, Hangzhou 310027, Peoples R China.
   [Zhou, Yunfan] Zhejiang Univ, Comp Sci & technol, Hangzhou, Peoples R China.
   [Chen, Ran; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
   [Pan, Changhao] Zhejiang Univ, artificial intelligence, Hangzhou, Peoples R China.
   [Weng, Di] Zhejiang Univ, Sch Software Technol, Hangzhou, Peoples R China.
   [Shu, Xinhuan] Univ Edinburgh, Edinburgh EH8 9YL, Scotland.
C3 Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang
   University; Zhejiang University; University of Edinburgh
RP Weng, D (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, software Engn, Hangzhou 310027, Peoples R China.
EM huangyw@zju.edu.cn; yunfzhou@zju.edu.cn; chenran928@zju.edu.cn;
   panch@zju.edu.cn; dweng@zju.edu.cn; ycwu@zju.edu.cn
RI Weng, Di/ABG-7408-2020
OI Huang, Yanwei/0009-0001-9453-7815; Chen, Ran/0000-0002-2770-4070; Pan,
   Changhao/0009-0004-6023-1764; Shu, Xinhuan/0000-0002-9736-4454; Weng,
   Di/0000-0003-2712-7274; Zhou, Yunfan/0009-0009-7814-4390
FU National Key R&D Program of China [2022YFE0137800]; Key "Pioneer" R&D
   Projects of Zhejiang Province [2023C01120]; NSFC [U22A2032];
   Collaborative Innovation Center of Artificial Intelligence by MOE;
   Zhejiang Provincial Government (ZJU)
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2022YFE0137800, in part by the Key "Pioneer" R&D Projects of
   Zhejiang Province under Grant 2023C01120, in part by the NSFC under
   Grant U22A2032, and in part by the Collaborative Innovation Center of
   Artificial Intelligence by MOE and Zhejiang Provincial Government (ZJU).
CR Abedjan Z, 2015, P C INN DAT SYST RES, P883
   Abedjan Z, 2016, PROC INT CONF DATA, P1134, DOI 10.1109/ICDE.2016.7498319
   AHMED R, 1991, COMPUTER, V24, P19, DOI 10.1109/2.116885
   Androutsopoulos Ion., 1995, Natural Language Engineering, V1, P29, DOI [10.1017/S135132490000005X, DOI 10.1017/S135132490000005X, 10.1017/S0269888900005476]
   Barowy DW, 2015, ACM SIGPLAN NOTICES, V50, P218, DOI [10.1145/2737924.2737952, 10.1145/2813885.2737952]
   Baudisch P., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P31, DOI 10.1145/502348.502354
   Berant J, 2019, PROC INT CONF DATA, P1570, DOI 10.1109/ICDE.2019.00144
   Brohan A., 2023, C ROBOT LEARNING, P287
   Brown T.B., 2020, Advances in neural information processing systems, V33, P1877
   Chang KSP, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2497, DOI 10.1145/2858036.2858430
   Chasins SE, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P963, DOI 10.1145/3242587.3242661
   Chen Ran, 2023, IEEE Trans Vis Comput Graph, V29, P128, DOI 10.1109/TVCG.2022.3209385
   Chen Zhutian, 2023, IEEE Trans Vis Comput Graph, V29, P918, DOI 10.1109/TVCG.2022.3209497
   Chowdhery A, 2022, Arxiv, DOI arXiv:2204.02311
   Chung JJY, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501819
   Coronel C., 2016, DATABASE SYSTEMS DES
   Dang H, 2022, Arxiv, DOI [arXiv:2209.01390, 10.48550/ARXIV.2209.01390, DOI 10.48550/ARXIV.2209.01390]
   Drosos I, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20)
   Fast E, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174047
   Feng Y, 2024, INT J ENVIRON HEAL R, V34, P2333, DOI [10.1109/TVCG.2023.3240003, 10.1080/09603123.2023.2246383]
   Gao T, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P489, DOI 10.1145/2807442.2807478
   Gardner M., 2018, P ANN M ASS COMP LIN, P17
   Gathani S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376485
   github, The dagre library
   github, Arquero, a javascript library for query processing and transformation ofdata tables
   Gulwani S, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P803, DOI 10.1145/2588555.2612177
   Gulwani S, 2012, COMMUN ACM, V55, P97, DOI 10.1145/2240236.2240260
   Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423
   Guo P. J., 2011, P 24 ANN ACM S US IN, P65, DOI 10.1145/2047196.2047205
   Hammer J., 1997, SIGMOD Record, V26, P532, DOI 10.1145/253262.253395
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   He YY, 2018, PROC VLDB ENDOW, V11, P1165, DOI 10.14778/3231751.3231766
   Huang Jieying, 2023, IEEE Trans Vis Comput Graph, V29, P1200, DOI 10.1109/TVCG.2022.3209453
   Jaimovitch-López G, 2023, MACH LEARN, V112, P2053, DOI 10.1007/s10994-022-06259-9
   Ji ZW, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3571730
   Jiang E, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501870
   Jin ZJ, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P683, DOI 10.1145/3035918.3064034
   Kandel S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3363
   Kandel S, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P547, DOI 10.1145/2254556.2254659
   Kandel S, 2011, INFORM VISUAL, V10, P271, DOI 10.1177/1473871611415994
   Kasica S, 2021, IEEE T VIS COMPUT GR, V27, P957, DOI 10.1109/TVCG.2020.3030462
   Kim H, 2020, PROC VLDB ENDOW, V13, P1737, DOI 10.14778/3401960.3401970
   Kim TS, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501931
   Ko A. J., 2004, P SIGCHI C HUM FACT, P151, DOI DOI 10.1145/985692.985712
   Lakshmanan LVS, 2001, ACM T DATABASE SYST, V26, P476, DOI 10.1145/503099.503102
   Le V, 2014, ACM SIGPLAN NOTICES, V49, P542, DOI [10.1145/2666356.2594333, 10.1145/2594291.2594333]
   Li GY, 2022, IEEE INFOCOM SER, P1, DOI [10.1109/INFOCOM48880.2022.9796694, 10.1109/TVCG.2022.3209354]
   Liang PRY, 2013, Arxiv, DOI arXiv:1309.4408
   Liu MXY, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580817
   Luo YY, 2022, IEEE T VIS COMPUT GR, V28, P217, DOI 10.1109/TVCG.2021.3114848
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mayer M, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P291, DOI 10.1145/2807442.2807459
   Miceli-Barone AV, 2023, Arxiv, DOI arXiv:2305.15507
   Narayan A, 2022, PROC VLDB ENDOW, V16, P738, DOI 10.14778/3574245.3574258
   Narechania A, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581509
   Narechania A, 2021, IUI '21 - 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P597, DOI 10.1145/3397481.3450667
   Narechania A, 2021, IEEE T VIS COMPUT GR, V27, P369, DOI 10.1109/TVCG.2020.3030378
   Nazabal A, 2020, Arxiv, DOI arXiv:2004.12929
   OpenAl Inc, Models - OpenAI API
   OpenRefine Inc, 2010, ABOUT us
   pandas.pydata, Pandas-Python Data Analysis Library
   Ragavan SS, 2022, IUI'22: 27TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P345, DOI 10.1145/3490099.3511161
   Raman V., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P381
   Reynolds L, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451760
   Satyanarayan A, 2017, IEEE T VIS COMPUT GR, V23, P341, DOI 10.1109/TVCG.2016.2599030
   Setlur V, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P365, DOI 10.1145/2984511.2984588
   Shneiderman B., 2016, Designing the User Interface: Strategies for Effective Human -Computer Interaction.
   Singh R, 2016, PROC VLDB ENDOW, V9, P816, DOI 10.14778/2977797.2977807
   Srinivasan Arjun, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P864, DOI 10.1145/3472749.3474792
   Srinivasan A, 2019, PROCEEDINGS OF IUI 2019, P661, DOI 10.1145/3301275.3302292
   Srinivasan A, 2018, IEEE T VIS COMPUT GR, V24, P511, DOI 10.1109/TVCG.2017.2745219
   Sukhobok D, 2017, 2017 3RD INTERNATIONAL CONFERENCE ON BIG DATA INNOVATIONS AND APPLICATIONS (INNOVATE-DATA), P25, DOI 10.1109/Innovate-Data.2017.10
   Trifacta Inc., about us
   Wachtel A, 2016, PROCEDIA COMPUT SCI, V84, P49, DOI 10.1016/j.procs.2016.04.065
   Wang Bryan, 2022, arXiv
   Wang CL, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445249
   Wang Y, 2023, IEEE T VIS COMPUT GR, V29, P1222, DOI 10.1109/TVCG.2022.3209357
   Wei JS, 2022, Arxiv, DOI [arXiv:2201.11903, 10.48550/arXiv.2201.11903]
   Wickham H., 2022, Tidyr: tidy messy data
   Wickham H., 2022, Dplyr: A Grammar of Data Manipulation
   Wu TS, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517582
   Yu BW, 2020, IEEE T VIS COMPUT GR, V26, P1, DOI 10.1109/TVCG.2019.2934668
   Zhou MY, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2389, DOI 10.1145/3447548.3467279
   Zhou MY, 2020, AAAI CONF ARTIF INTE, V34, P320
NR 84
TC 0
Z9 0
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6130
EP 6145
DI 10.1109/TVCG.2023.3329120
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000028
PM 37910408
DA 2024-11-06
ER

PT J
AU Huang, WC
   Wong, SK
   Volonte, M
   Babu, SV
AF Huang, Wei-Chia
   Wong, Sai-Keung
   Volonte, Matias
   Babu, Sabarish V.
TI Impact of Socio-Demographic Attributes and Mutual Gaze of Virtual Humans
   on Users' Visual Attention and Collision Avoidance in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collision avoidance; human-computer interaction; user studies; virtual
   humans and crowds; virtual reality
ID INTERPERSONAL DISTANCE; PERSONAL-SPACE; BEHAVIOR; LOOKING; ENVIRONMENTS;
   APPEARANCE; CHARACTERS; GENDER; MOTION; BODY
AB This study investigated the extent that the non-verbal behaviors of virtual humans (VHs) and their socio-demographic attributes altered users' collision avoidance behaviors in Virtual Reality (VR). Users interacted with VHs representing different levels of ethnicities and gender, exhibiting different conditions of physical movement, and gaze behaviors. The VHs were depicted in three major ethnic conditions namely Asian, Caucasian, and Black. The physical movement states of the VHs were either static in the path of the user or walking toward the user in the opposite direction. The non-verbal gaze behavior of the VHs was either direct gaze or averted gaze. We used an HTC Vive tracking system to track users' performing real walking while we collected objective measures (i,e., continuous gaze, fixation gaze, clearance distance, and travel length), and subjective variables (i.e., game experiences and social presence). The results showed that the ethnicity of the VHs significantly impacted the gaze behavior of the users, and the gender of the VHs affected the user avoidance movement and their reciprocal gaze behavior. Our results revealed that users' physical movement, gaze behaviors, and collision avoidance were moderated by the VHs' perceived ethnicity, gender, and gaze behaviors. Understanding the impact of the socio-demographics attributes of VHs and their gaze behavior on users' collision avoidance is critical for applications in which users are navigating through virtual traffic, crowd, and other inter-personal simulations.
C1 [Huang, Wei-Chia; Wong, Sai-Keung] Natl Yang Ming Chiao Tung Univ, Hsinchu 30010, Taiwan.
   [Volonte, Matias; Babu, Sabarish V.] Clemson Univ, Clemson, SC 29534 USA.
C3 National Yang Ming Chiao Tung University; Clemson University
RP Wong, SK (corresponding author), Natl Yang Ming Chiao Tung Univ, Hsinchu 30010, Taiwan.
EM michae805@gmail.com; cswingo@cs.nycu.edu.tw; mvolont@clemson.edu;
   sbabu@clemson.edu
OI Babu, Sabarish/0000-0002-8348-0534; volonte, matias/0000-0002-9423-3408
FU MOST (Taiwan) [109-2221-E-009-121-MY3, 110-2811-E-009-508,
   111-2811-E-A49-508]; Higher Education Sprout Project of the National
   Yang Ming Chiao Tung University; Ministry of Education (MOE),Taiwan; US
   National Science Foundation (CISE IIS HCC) [2007435]
FX This work was supported in part by MOST under Grants
   109-2221-E-009-121-MY3, 110-2811-E-009-508, and 111-2811-E-A49-508
   (Taiwan), in part by the Higher Education Sprout Project of the National
   Yang Ming Chiao Tung University and Ministry of Education (MOE),Taiwan,
   and in part by the US National Science Foundation (CISE IIS HCC) under
   Grant 2007435.
CR Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Amaoka T, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P364, DOI 10.1109/CW.2009.19
   Babu S, 2006, LECT NOTES ARTIF INT, V4133, P169
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   BAILEY KG, 1972, PSYCHOL REP, V30, P263, DOI 10.2466/pr0.1972.30.1.263
   Bee N, 2009, LECT NOTES ARTIF INT, V5773, P229, DOI 10.1007/978-3-642-04380-2_26
   Berton F, 2022, IEEE T VIS COMPUT GR, V28, P2589, DOI 10.1109/TVCG.2020.3041341
   Berton F, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P717, DOI [10.1109/VR.2019.8798204, 10.1109/vr.2019.8798204]
   Bhalla A., 2021, P 7 ACM CYB PHYS SYS, P41, DOI DOI 10.1145/3457339.3457983
   Bickmore T, 2005, TEXT SPEECH LANG TEC, V30, P23
   Binaee K., 2016, P ACM S APPL PERC, P15
   Biocca F., 2001, 4 ANN INT WORKSH PRE, P1
   Bönsch A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P199
   Bönsch A, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P145, DOI 10.1109/3DUI.2016.7460045
   Bourgaize SM, 2021, J MOTOR BEHAV, V53, P166, DOI 10.1080/00222895.2020.1742083
   BRADY AT, 1978, BRIT J SOC CLIN PSYC, V17, P127, DOI 10.1111/j.2044-8260.1978.tb00254.x
   Brave S, 2005, INT J HUM-COMPUT ST, V62, P161, DOI 10.1016/j.ijhcs.2004.11.002
   Bruneau J, 2015, IEEE T VIS COMPUT GR, V21, P520, DOI 10.1109/TVCG.2015.2391862
   Burgoon J.E., 1976, Human Communication Research, V2, P131, DOI DOI 10.1111/J.1468-2958.1976.TB00706.X
   Chihak BJ, 2010, J EXP PSYCHOL HUMAN, V36, P1535, DOI 10.1037/a0020560
   Cinelli ME, 2008, GAIT POSTURE, V28, P596, DOI 10.1016/j.gaitpost.2008.04.006
   Cohen J., 1998, Statistical power analysis for the behavioral sciences
   Collova JR, 2017, J EXP PSYCHOL HUMAN, V43, P1857, DOI 10.1037/xhp0000460
   Dickinson P, 2019, VIRTUAL REAL-LONDON, V23, P19, DOI 10.1007/s10055-018-0365-0
   Duchowski Andrew T., 2022, Procedia Computer Science, P1641, DOI 10.1016/j.procs.2022.09.221
   Elliot AJ, 2008, J PERS SOC PSYCHOL, V95, P1150, DOI 10.1037/0022-3514.95.5.1150
   Fajen BR, 2003, J EXP PSYCHOL HUMAN, V29, P343, DOI 10.1037/0096-1523.29.2.343
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Felnhofer A., 2012, P INT SOC PRES RES A, P103
   Garza R, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916631614
   Gérin-Lajoie M, 2005, MOTOR CONTROL, V9, P242, DOI 10.1123/mcj.9.3.242
   GIBSON JJ, 1963, AM J PSYCHOL, V76, P386, DOI 10.2307/1419779
   Hall C, 2011, J SEX RES, V48, P461, DOI 10.1080/00224499.2010.521899
   HALL ET, 1968, CURR ANTHROPOL, V9, P83, DOI 10.1086/200975
   HALL ET, 1963, AM ANTHROPOL, V65, P1003, DOI 10.1525/aa.1963.65.5.02a00020
   Hall ET, 1966, The hidden dimension, V609
   Hamer K, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-25905-9
   Hamilton AFD, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0080
   Harms C., 2004, P PRES 2004, P7
   Hasler B.S., 2012, J INTERCULTURAL COMM, V41, P238, DOI [10.1080/17475759.2012.728764, DOI 10.1080/17475759.2012.728764]
   Hecht H, 2019, ACTA PSYCHOL, V193, P113, DOI 10.1016/j.actpsy.2018.12.009
   Hewig J, 2008, J NONVERBAL BEHAV, V32, P67, DOI 10.1007/s10919-007-0043-5
   Hietanen JK, 2008, NEUROPSYCHOLOGIA, V46, P2423, DOI 10.1016/j.neuropsychologia.2008.02.029
   Holmqvist K, 2023, BEHAV RES METHODS, V55, P364, DOI 10.3758/s13428-021-01762-8
   Iachini T, 2016, J ENVIRON PSYCHOL, V45, P154, DOI 10.1016/j.jenvp.2016.01.004
   Jhan XD, 2022, IEEE T VIS COMPUT GR, V28, P3767, DOI 10.1109/TVCG.2022.3203107
   JONES SE, 1971, J SOC PSYCHOL, V84, P35, DOI 10.1080/00224545.1971.9918518
   Jording M, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00226
   Keller K, 2009, NEUROIMAGE, V47, P342, DOI 10.1016/j.neuroimage.2009.04.042
   Kelly DJ, 2007, PSYCHOL SCI, V18, P1084, DOI 10.1111/j.1467-9280.2007.02029.x
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Keogh E, 2001, P 2001 SIAM INT C DA, P1, DOI [DOI 10.1137/1.9781611972719.1, 10.1137/1.9781611972719.1]
   Khawar A., 2018, Journal of Arts and Social Sciences, VV, P125
   Knapp M. L., 2013, Nonverbal Communication in Human Interaction
   Knorr AG, 2016, J EXP PSYCHOL HUMAN, V42, P1332, DOI 10.1037/xhp0000223
   Koilias A, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1963
   Kyriakou M, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1729
   Lieberz KA, 2017, AGGRESSIVE BEHAV, V43, P460, DOI 10.1002/ab.21704
   Lin L, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119884
   Lynch SD, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P136, DOI 10.1109/VR.2018.8446180
   Maloney D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1074, DOI [10.1109/vr.2019.8798122, 10.1109/VR.2019.8798122]
   Marschner L, 2015, INT J PSYCHOPHYSIOL, V97, P85, DOI 10.1016/j.ijpsycho.2015.05.007
   Mayer KM, 2021, ATTEN PERCEPT PSYCHO, V83, P1752, DOI 10.3758/s13414-020-02217-6
   Mousas C, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P40, DOI 10.1109/VR50410.2021.00024
   Mousas C, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P726, DOI [10.1109/vr.2019.8798043, 10.1109/VR.2019.8798043]
   Mousas C, 2018, COMPUT HUM BEHAV, V86, P99, DOI 10.1016/j.chb.2018.04.036
   Naidoo S., 2000, Sch. Arts Sci.. Univ. Pennsylvania
   Narang S, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P91, DOI 10.1145/2993369.2993378
   Nilsson NC, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3180658
   Nummenmaa L, 2009, PSYCHOL SCI, V20, P1454, DOI 10.1111/j.1467-9280.2009.02464.x
   Nyström M, 2010, BEHAV RES METHODS, V42, P188, DOI 10.3758/BRM.42.1.188
   Olivier AH, 2012, GAIT POSTURE, V36, P399, DOI 10.1016/j.gaitpost.2012.03.021
   Patotskaya Y, 2023, COMPUT GRAPH-UK, V110, P162, DOI 10.1016/j.cag.2023.01.001
   Pejsa T, 2015, ACM T INTERACT INTEL, V5, DOI 10.1145/2724731
   Peters C, 2005, LECT NOTES ARTIF INT, V3661, P229
   Raimbaud P, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P281, DOI 10.1109/VR51125.2022.00047
   Redlick FP, 2001, VISION RES, V41, P213, DOI 10.1016/S0042-6989(00)00243-1
   Robb A, 2016, IEEE T VIS COMPUT GR, V22, P1336, DOI 10.1109/TVCG.2016.2518405
   Ruggiero G, 2017, PSYCHOL RES-PSYCH FO, V81, P1232, DOI 10.1007/s00426-016-0806-x
   Sacheli LM, 2015, SCI REP-UK, V5, DOI 10.1038/srep08507
   Sanz FA, 2015, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2015.7223327
   Senju A, 2008, BRAIN COGNITION, V67, P127, DOI 10.1016/j.bandc.2007.12.001
   Sicorello M, 2019, J CROSS CULT PSYCHOL, V50, P8, DOI 10.1177/0022022118798513
   Silva WS, 2018, GAIT POSTURE, V61, P294, DOI 10.1016/j.gaitpost.2018.01.028
   Stein N, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P493, DOI 10.1109/VR51125.2022.00069
   Usselsteijn W. A., 2013, The game experience questionnaire
   Vicaria IM, 2016, J NONVERBAL BEHAV, V40, P335, DOI 10.1007/s10919-016-0238-8
   Volonte M, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P39, DOI 10.1109/VRW55335.2022.00015
   Volonte M, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343118
   Volonte M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P293, DOI [10.1109/VR46266.2020.1581610451331, 10.1109/VR46266.2020.00-55]
   Volonte M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P25, DOI 10.1109/VR.2018.8446364
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   Wang CC, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P340, DOI [10.1109/VR51125.2022.00053, 10.1109/ISPDS56360.2022.9874172]
   Wong HK, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-87933-1
   Wu YX, 2014, IEEE T VIS COMPUT GR, V20, P626, DOI 10.1109/TVCG.2014.19
   ZEBROWITZ LA, 1993, J PERS SOC PSYCHOL, V65, P85, DOI 10.1037/0022-3514.65.1.85
   Zibrek K, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419985
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 99
TC 1
Z9 1
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6146
EP 6163
DI 10.1109/TVCG.2023.3329515
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000039
PM 37917527
DA 2024-11-06
ER

PT J
AU Le, TNH
   Huang, HG
   Chen, YR
   Lee, TY
AF Le, Thi-Ngoc-Hanh
   Huang, HuiGuang
   Chen, Yi-Ru
   Lee, Tong-Yee
TI Retargeting Video With an End-to-End Framework
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Streaming media; Image resolution; Visualization; Personal digital
   devices; Image segmentation; Distortion; Standards; Analyze video;
   deforming; grid movement; pixel movement; RETVI; video retargeting
AB Video holds significance in computer graphics applications. Because of the heterogeneous of digital devices, retargeting videos becomes an essential function to enhance user viewing experience in such applications. In the research of video retargeting, preserving the relevant visual content in videos, avoiding flicking, and processing time are the vital challenges. Extending image retargeting techniques to the video domain is challenging due to the high running time. Prior work of video retargeting mainly utilizes time-consuming preprocessing to analyze frames. Plus, being tolerant of different video content, avoiding important objects from shrinking, and the ability to play with arbitrary ratios are the limitations that need to be resolved in these systems requiring investigation. In this paper, we present an end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We eliminate the computational bottleneck in the conventional approaches by designing RETVI with two modules, content feature analyzer (CFA) and adaptive deforming estimator (ADE). The extensive experiments and evaluations show that our system outperforms previous work in quality and running time.
C1 [Le, Thi-Ngoc-Hanh; Huang, HuiGuang; Chen, Yi-Ru; Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Lee, TY (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM ngochanh.le1987@gmail.com; 604300468@qq.com; y2133256@gmail.com;
   tonylee@mail.ncku.edu.tw
OI HUIGUANG, HUANG/0009-0007-0597-1593; Le, Thi Ngoc
   Hanh/0000-0001-9667-9780
FU National Science and Technology Council, Republic of China (ROC), Taiwan
   [111-2221-E-006-112-MY3, 110-2221-E-006-135-MY3]
FX This work was supported in part by the National Science and Technology
   Council under Grants 111-2221-E-006-112-MY3 and 110-2221-E-006-135-MY3,
   Republic of China (ROC), Taiwan.
CR [Anonymous], 2023, Unity
   [Anonymous], 2022, Adobe after effects
   [Anonymous], 2022, Adobe express
   [Anonymous], 2023, Arcore
   Asheghi B, 2022, SIGNAL PROCESS, V195, DOI 10.1016/j.sigpro.2022.108496
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Cho D, 2017, IEEE I CONF COMP VIS, P4568, DOI 10.1109/ICCV.2017.488
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Fan XT, 2021, IEEE T CIRC SYST VID, V31, P4759, DOI 10.1109/TCSVT.2021.3054062
   Flexier, 2021, About us
   Furuta R, 2018, IEEE T CIRC SYST VID, V28, P1087, DOI 10.1109/TCSVT.2016.2620563
   Gallea R, 2014, IEEE T MULTIMEDIA, V16, P971, DOI 10.1109/TMM.2014.2305917
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Grundmann M, 2010, PROC CVPR IEEE, P569, DOI 10.1109/CVPR.2010.5540165
   Hashemzadeh M, 2019, SIGNAL PROCESS, V155, P233, DOI 10.1016/j.sigpro.2018.09.037
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kiess J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3231598
   Kim Y, 2019, IEEE T VIS COMPUT GR, V25, P3202, DOI 10.1109/TVCG.2018.2866106
   Kingma D.P., 2014, P INT C LEARNING REP
   Kopf Stephan., 2009, MM 09, P321
   Le TNH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572030
   Le TNH, 2024, IEEE T VIS COMPUT GR, V30, P3622, DOI 10.1109/TVCG.2023.3237739
   Lee SJ, 2020, IEEE T CIRC SYST VID, V30, P4434, DOI 10.1109/TCSVT.2020.2981652
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P1615, DOI 10.1109/TIP.2014.2305843
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin JX, 2019, IEEE INT CONF MULTI, P54, DOI 10.1109/ICMEW.2019.0-111
   Lin SS, 2016, IEEE T CIRC SYST VID, V26, P801, DOI 10.1109/TCSVT.2015.2409711
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Lin SS, 2013, IEEE T VIS COMPUT GR, V19, P1677, DOI 10.1109/TVCG.2013.75
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu D, 2016, MULTIMED TOOLS APPL, V75, P12465, DOI 10.1007/s11042-014-2304-8
   Liu Hao., 2003, P ACM INT C MULTIMED, P148, DOI [10.1145/957013.957045, DOI 10.1145/957013.957045]
   Liu S, 2018, IEEE T IMAGE PROCESS, V27, P5032, DOI 10.1109/TIP.2018.2836313
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Paszke A, 2019, Arxiv, DOI arXiv:1912.01703
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Song E, 2019, IEEE ACCESS, V7, P284, DOI 10.1109/ACCESS.2018.2885347
   Tan WM, 2020, IEEE T MULTIMEDIA, V22, P1730, DOI 10.1109/TMM.2019.2959925
   Tang F, 2020, IEEE T MULTIMEDIA, V22, P641, DOI 10.1109/TMM.2019.2932620
   Veed, About us
   Wang BT, 2014, IEEE J EM SEL TOP C, V4, P82, DOI 10.1109/JETCAS.2014.2298313
   Yan B, 2014, IEEE T MULTIMEDIA, V16, P272, DOI 10.1109/TMM.2013.2286112
   Yang JC, 2022, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR52688.2022.00768
   Zhang JY, 2014, IEEE T IMAGE PROCESS, V23, P797, DOI 10.1109/TIP.2013.2294541
   Zhang YB, 2016, INT CONF ACOUST SPEE, P1080, DOI 10.1109/ICASSP.2016.7471842
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
   Zhao MD, 2020, IEEE T IMAGE PROCESS, V29, P3582, DOI 10.1109/TIP.2019.2963380
   Zhou Y, 2021, IEEE T CIRC SYST VID, V31, P126, DOI 10.1109/TCSVT.2020.2977943
NR 57
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6164
EP 6176
DI 10.1109/TVCG.2023.3327825
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000019
PM 37883263
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Huang, K
   Zhang, FL
   Zhao, JH
   Li, YH
   Dodgson, N
AF Huang, Kun
   Zhang, Fang-Lue
   Zhao, Junhong
   Li, Yiheng
   Dodgson, Neil
TI 360<SUP>°</SUP> Stereo Image Composition With Depth Adaption
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Cameras; Stereo image processing; Videos;
   Estimation; Geometry; Image synthesis; Image composition; image
   synthesis; stereoscopic panoramic image; virtual reality
AB 360(degrees) images and videos have become an economic and popular way to provide VR experiences using real-world content. However, the manipulation of the stereo panoramic content remains less explored. In this paper, we focus on the 360(degrees) image composition problem, and develop a solution that can take an object from a stereo image pair and insert it at a given 3D position in a target stereo panorama, with well-preserved geometry information. Our method uses recovered 3D point clouds to guide the composited image generation. More specifically, we observe that using only a one-off operation to insert objects into equirectangular images will never produce satisfactory depth perception and generate ghost artifacts when users are watching the result from different view directions. Therefore, we propose a novel view-dependent projection method that segments the object in 3D spherical space with the stereo camera pair facing in that direction. A deep depth densification network is proposed to generate depth guidance for the stereo image generation of each view segment according to the desired position and pose of the inserted object. We finally merge the synthesized view segments and blend the objects into the target stereo 360(degrees) scene. A user study demonstrates that our method can provide good depth perception and removes ghost artifacts. The view-dependent solution is a potential paradigm for other content manipulation methods for 360(degrees) images and videos.
C1 [Huang, Kun; Zhang, Fang-Lue; Li, Yiheng; Dodgson, Neil] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
   [Zhao, Junhong] Victoria Univ Wellington, CMIC, Wellington 6012, New Zealand.
C3 Victoria University Wellington; Victoria University Wellington
RP Zhang, FL (corresponding author), Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
EM kun.huang@vuw.ac.nz; fanglue.zhang@vuw.ac.nz;
   junhong.jennifer@gmail.com; yiheng.li@vuw.ac.nz; neil.dodgson@vuw.ac.nz
OI Dodgson, Neil/0000-0001-7649-8528; , Kun/0000-0001-7812-4562; Zhao,
   Junhong/0000-0001-7031-3828; Li, Yiheng/0000-0001-7905-3217
FU Royal Society of New Zealand [MFP-20-VUW-180]
FX This work was supported by Marsden Fund Council managed by Royal Society
   of New Zealand under Grant MFP-20-VUW-180.
CR Athar S., 2023, P INT C AUT FAC GEST, P1
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Chen RS, 2023, IEEE T VIS COMPUT GR, V29, P3976, DOI 10.1109/TVCG.2022.3176832
   Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Cong WY, 2020, PROC CVPR IEEE, P8391, DOI 10.1109/CVPR42600.2020.00842
   Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963
   Dai YT, 2021, PROC CVPR IEEE, P6837, DOI 10.1109/CVPR46437.2021.00677
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Ding HH, 2022, IEEE T IMAGE PROCESS, V31, P2421, DOI 10.1109/TIP.2022.3155958
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Feng Q., 2020, J. WSCG, V28, P79
   Feng Q, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P664, DOI 10.1109/VR51125.2022.00087
   Fielding R., 1965, The Technique of Special Effects-Cinematography. WithIllustrations
   Germer Thomas, 2020, J. Open Source Softw., V5, P2481, DOI [DOI 10.21105/JOSS.02481, DOI 10.21105/JOSS.02481.20K]
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Ku J, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2018.00013
   Laga H, 2022, IEEE T PATTERN ANAL, V44, P1738, DOI 10.1109/TPAMI.2020.3032602
   Li JZZ, 2022, INT J COMPUT VISION, V130, P246, DOI 10.1007/s11263-021-01541-0
   Li JX, 2021, PROC CVPR IEEE, P10586, DOI 10.1109/CVPR46437.2021.01045
   Li YH, 2022, LECT NOTES COMPUT SC, V13695, P336, DOI 10.1007/978-3-031-19833-5_20
   Li ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6177, DOI 10.1109/ICCV48922.2021.00614
   Lil YJ, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P1, DOI 10.1109/VR51125.2022.00017
   Liu H, 2022, IEEE T IMAGE PROCESS, V31, P7389, DOI 10.1109/TIP.2022.3222918
   Luo SJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366201
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Mandikal P, 2019, Arxiv, DOI arXiv:1807.07796
   Martin D, 2022, IEEE T VIS COMPUT GR, V28, P2003, DOI 10.1109/TVCG.2022.3150502
   Medeiros D, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P21, DOI 10.1109/VR50410.2021.00022
   Mendiburu B., 2012, 3D movie making: stereoscopic digital cinema from script to screen
   Morioka H, 2016, P IEEE VIRT REAL ANN, P231, DOI 10.1109/VR.2016.7504738
   Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480
   Mortensen Eric N, 1995, ACM SIGGRAPH 1995 Papers, P191, DOI [DOI 10.1145/218380.218442, 10.1145/218380.218442]
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Smith A. R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P259, DOI 10.1145/237170.237263
   Su SY, 2021, ADV NEUR IN, V34
   Su YC, 2022, IEEE T PATTERN ANAL, V44, P8371, DOI 10.1109/TPAMI.2021.3113612
   Sun C, 2021, PROC CVPR IEEE, P2573, DOI 10.1109/CVPR46437.2021.00260
   Tang CZ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3211889
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   Tosi F, 2018, LECT NOTES COMPUT SC, V11210, P323, DOI 10.1007/978-3-030-01231-1_20
   Unger J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P141
   van den Heuvel FA, 1998, ISPRS J PHOTOGRAMM, V53, P354, DOI 10.1016/S0924-2716(98)00019-7
   Wang J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239460
   Wang NH, 2020, IEEE INT CONF ROBOT, P582, DOI 10.1109/ICRA40945.2020.9196975
   Wang OL, 2011, PITUITARY, 3RD EDITION, P47, DOI 10.1016/B978-0-12-380926-1.10003-3
   Wenger A, 2005, ACM T GRAPHIC, V24, P756, DOI 10.1145/1073204.1073258
   Wu X, 2020, COMPUT VIS MEDIA, V6, P215, DOI 10.1007/s41095-020-0168-6
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xian K, 2020, PROC CVPR IEEE, P608, DOI 10.1109/CVPR42600.2020.00069
   Xu JP, 2022, AAAI CONF ARTIF INTE, P2857
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Zhang L, 2002, J VISUAL COMP ANIMAT, V13, P225, DOI 10.1002/vis.291
   Zhang Y, 2022, IEEE ACCESS, V10, P43882, DOI 10.1109/ACCESS.2022.3168665
   Zhang Y, 2021, COMPUT GRAPH-UK, V96, P61, DOI 10.1016/j.cag.2021.03.005
   Zhao JH, 2021, IEEE T VIS COMPUT GR, V27, P4097, DOI 10.1109/TVCG.2021.3106497
   Zongker DE, 1999, COMP GRAPH, P205, DOI 10.1145/311535.311558
NR 62
TC 0
Z9 0
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6177
EP 6191
DI 10.1109/TVCG.2023.3327943
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000036
PM 37889815
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Ju, YK
   Shi, BX
   Chen, Y
   Zhou, HY
   Dong, JY
   Lam, KM
AF Ju, Yakun
   Shi, Boxin
   Chen, Yang
   Zhou, Huiyu
   Dong, Junyu
   Lam, Kin-Man
TI GR-PSN: Learning to Estimate Surface Normal and Reconstruct Photometric
   Stereo Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image reconstruction; Surface reconstruction; Lighting; Rendering
   (computer graphics); Training; Three-dimensional displays; Reflectivity;
   3D reconstruction; deep neural networks; photometric image
   reconstruction; photometric stereo; surface normal estimate
ID NETWORK
AB In this paper, we propose a novel method, namely GR-PSN, which learns surface normals from photometric stereo images and generates the photometric images under distant illumination from different lighting directions and surface materials. The framework is composed of two subnetworks, named GeometryNet and ReconstructNet, which are cascaded to perform shape reconstruction and image rendering in an end-to-end manner. ReconstructNet introduces additional supervision for surface-normal recovery, forming a closed-loop structure with GeometryNet. We also encode lighting and surface reflectance in ReconstructNet, to achieve arbitrary rendering. In training, we set up a parallel framework to simultaneously learn two arbitrary materials for an object, providing an additional transform loss. Therefore, our method is trained based on the supervision by three different loss functions, namely the surface-normal loss, reconstruction loss, and transform loss. We alternately input the predicted surface-normal map and the ground-truth into ReconstructNet, to achieve stable training for ReconstructNet. Experiments show that our method can accurately recover the surface normals of an object with an arbitrary number of inputs, and can re-render images of the object with arbitrary surface materials. Extensive experimental results show that our proposed method outperforms those methods based on a single surface recovery network and shows realistic rendering results on 100 different materials.
C1 [Ju, Yakun; Lam, Kin-Man] Hong Kong Polytech Univ, Dept Elect Engn, Hung Hom, Hong Kong, Peoples R China.
   [Shi, Boxin] Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
   [Shi, Boxin] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Chen, Yang] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Beijing 100190, Peoples R China.
   [Zhou, Huiyu] Univ Leicester, Sch Comp & Math Sci, Leicester LE1 7RH, England.
   [Dong, Junyu] Ocean Univ China, Sch Comp Sci & Technol, Qingdao 266005, Peoples R China.
C3 Hong Kong Polytechnic University; Peking University; Peking University;
   Tsinghua University; Tsinghua Shenzhen International Graduate School;
   University of Leicester; Ocean University of China
RP Lam, KM (corresponding author), Hong Kong Polytech Univ, Dept Elect Engn, Hung Hom, Hong Kong, Peoples R China.; Dong, JY (corresponding author), Ocean Univ China, Sch Comp Sci & Technol, Qingdao 266005, Peoples R China.
EM kelvin.yakun.ju@gmail.com; shiboxin@pku.edu.cn;
   cy21@mails.tsinghua.edu.cn; hz143@leicester.ac.uk; dongjunyu@ouc.edu.cn;
   enkmlam@polyu.edu.hk
RI ; Zhou, Huiyu/O-2692-2014; Ju, Yakun/JEP-0636-2023; Lam,
   Kin-Man/A-9352-2014
OI Dong, Junyu/0000-0001-7012-2087; Zhou, Huiyu/0000-0003-1634-9840; Ju,
   Yakun/0000-0003-4065-4108; Chen, Yang/0000-0001-6410-0922; Lam,
   Kin-Man/0000-0002-0422-8454
FU National Key R&D Program of China [2022ZD0117201]; National Key
   Scientific Instrument and Equipment Development Projects of China
   [41927805]; National Natural Science Foundation of China [62136001,
   62088102, 62372306]; Project of Strategic Importance Fund from The Hong
   Kong Polytechnic University [ZE1X]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2022ZD0117201, in part by the National Key Scientific
   Instrument and Equipment Development Projects of China under Grant
   41927805, in part by the National Natural Science Foundation of China
   under Grants 62136001, 62088102, and 62372306, and in part by the
   Project of Strategic Importance Fund from The Hong Kong Polytechnic
   University under Grant ZE1X.
CR Cao YL, 2022, OPT LASER ENG, V150, DOI 10.1016/j.optlaseng.2021.106838
   Chabert C.-F., 2006, SIGGRAPH 06 SKETCHES, P76
   Chandraker M, 2013, IEEE T PATTERN ANAL, V35, P2941, DOI 10.1109/TPAMI.2012.217
   Chen AP, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203192
   Chen GY, 2022, IEEE T PATTERN ANAL, V44, P129, DOI 10.1109/TPAMI.2020.3005397
   Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1
   Cheng KHM, 2019, IEEE T IMAGE PROCESS, V28, P1544, DOI 10.1109/TIP.2018.2875531
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Einarsson P., 2006, Rendering Techniques, P183
   Goldman DB, 2010, IEEE T PATTERN ANAL, V32, P1060, DOI 10.1109/TPAMI.2009.102
   Guanying Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P745, DOI 10.1007/978-3-030-58568-6_44
   Han TQ, 2015, IEEE T IMAGE PROCESS, V24, P4888, DOI 10.1109/TIP.2015.2471081
   Honzátko D, 2021, INT CONF 3D VISION, P394, DOI 10.1109/3DV53792.2021.00049
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ikehata S., 2022, arXiv
   Ikehata S, 2022, PROC CVPR IEEE, P12581, DOI 10.1109/CVPR52688.2022.01226
   Ikehata S, 2018, LECT NOTES COMPUT SC, V11219, P3, DOI 10.1007/978-3-030-01267-0_1
   Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280
   Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691
   Jin ZR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4315, DOI 10.1145/3474085.3475571
   Johnson M. K., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2553, DOI 10.1109/CVPR.2011.5995510
   Ju YK, 2024, IEEE T CIRC SYST VID, V34, P2512, DOI 10.1109/TCSVT.2023.3301930
   Ju YK, 2024, Arxiv, DOI arXiv:2212.08414
   Ju YK, 2022, INT J COMPUT VISION, V130, P3014, DOI 10.1007/s11263-022-01684-8
   Ju YK, 2022, COMPUT VIS MEDIA, V8, P105, DOI 10.1007/s41095-021-0223-y
   Ju YK, 2020, IEEE I C VI COM I PR, P411, DOI 10.1109/vcip49819.2020.9301860
   Ju YK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3096282
   Ju YK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P694
   Ju YK, 2021, IEEE T IMAGE PROCESS, V30, P3676, DOI 10.1109/TIP.2021.3064230
   Li JX, 2019, PROC CVPR IEEE, P7560, DOI 10.1109/CVPR.2019.00775
   Liu HY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3047917
   Liu YR, 2022, IMAGE VISION COMPUT, V118, DOI 10.1016/j.imavis.2021.104368
   Logothetis F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12737, DOI 10.1109/ICCV48922.2021.01252
   Luo JD, 2020, IEEE T VIS COMPUT GR, V26, P3434, DOI 10.1109/TVCG.2020.3023565
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McAuley S., 2012, P ACM SIGGRAPH COURS, P1
   Mirza M, 2014, Arxiv, DOI arXiv:1411.1784
   Miyazaki D, 2010, INT J COMPUT VISION, V86, P229, DOI 10.1007/s11263-009-0262-9
   Papadhimitri T, 2014, INT J COMPUT VISION, V107, P139, DOI 10.1007/s11263-013-0665-5
   Ren JJ, 2022, PROC CVPR IEEE, P12571, DOI 10.1109/CVPR52688.2022.01225
   Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66
   Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659
   Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222
   Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196
   SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103
   Taniai T, 2018, PR MACH LEARN RES, V80
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Tiwari A, 2022, LECT NOTES COMPUT SC, V13667, P129, DOI 10.1007/978-3-031-20071-7_8
   Tiwari A, 2022, INT CONF ACOUST SPEE, P2060, DOI 10.1109/ICASSP43922.2022.9746974
   Verbiest F, 2008, PROC CVPR IEEE, P2886
   Wang C, 2020, IEEE T PATTERN ANAL, V42, P1570, DOI 10.1109/TPAMI.2020.2986777
   Wang X, 2020, IEEE T IMAGE PROCESS, V29, P6032, DOI 10.1109/TIP.2020.2987176
   Wiles O., 2017, P BRIT MACH VIS C
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Yang XB, 2020, IEEE T VIS COMPUT GR, V26, P3446, DOI 10.1109/TVCG.2020.3023634
   Yao Z., 2020, P ADV NEUR INF PROC
   Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327
   Yuan GZQ, 2023, IEEE T VIS COMPUT GR, V29, P4906, DOI 10.1109/TVCG.2022.3193406
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang JB, 2023, IEEE T VIS COMPUT GR, V29, P3039, DOI 10.1109/TVCG.2022.3148245
   Zheng Q., 2020, Virtual Real. Intell. Hardw, V2, P213, DOI DOI 10.1016/J.VRIH.2020.03.001
   Zheng Q, 2019, IEEE I CONF COMP VIS, P8548, DOI 10.1109/ICCV.2019.00864
   Zheng Q, 2019, IEEE T IMAGE PROCESS, V28, P3177, DOI 10.1109/TIP.2019.2894963
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
NR 66
TC 8
Z9 8
U1 10
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6192
EP 6207
DI 10.1109/TVCG.2023.3329817
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000046
PM 37922172
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Lavoué,É
   Villenave, S
   Serna, A
   Didier, C
   Baert, P
   Lavoué, G
AF Lavoue, Elise
   Villenave, Sophie
   Serna, Audrey
   Didier, Clementine
   Baert, Patrick
   Lavoue, Guillaume
TI Influence of Scenarios and Player Traits on Flow in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; player traits; flow; scenario; game mechanics; Virtual
   reality; player traits; flow; scenario; game mechanics
ID SELF-DETERMINATION; ENVIRONMENTS; PERSONALITY; TECHNOLOGY; EXPERIENCE;
   IMMERSION; EQUATION; GAMES
AB Many studies have investigated how interpersonal differences between users influence their experience in Virtual Reality (VR) and it is now well recognized that user's subjective experiences and responses to the same VR environment can vary widely. In this study, we focus on player traits, which correspond to users' preferences for game mechanics, arguing that players react differently when experiencing VR scenarios. We developed three scenarios in the same VR environment that rely on different game mechanics, and evaluate the influence of the scenarios, the player traits and the time of practice of the VR environment on users' perceived flow. Our results show that 1) the type of scenario has an impact on specific dimensions of flow; 2) the scenarios have different effects on flow depending on the order they are performed, the flow preconditions being stronger when performed at last; 3) almost all dimensions of flow are influenced by the player traits, these influences depending on the scenario, 4) the Aesthetic trait has the most influences in the three scenarios. We finally discuss the findings and limitations of the present study that we believe have strong implications for the design of scenarios in VR experiences.
C1 [Lavoue, Elise] Univ Jean Moulin Lyon 3, IAE Lyon Sch Management, CNRS, LIRIS,UMR5205,INSA Lyon, F-69621 Villeurbanne, France.
   [Villenave, Sophie] Univ Lyon, Cent Lyon, CNRS, LIRIS,UMR5205,ENISE, F-42023 St Etienne, France.
   [Serna, Audrey] Univ Lyon, INSA Lyon, LIRIS, CNRS,UMR5205, F-69621 Villeurbanne, France.
   [Didier, Clementine] Univ Lyon, Cent Lyon, CNRS, LTDS,ENISE, F-69130 Ecully, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Universite Jean Moulin Lyon
   3; Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); Institut National des
   Sciences Appliquees de Lyon - INSA Lyon; Centre National de la Recherche
   Scientifique (CNRS); Centre National de la Recherche Scientifique
   (CNRS); Ecole Centrale de Lyon
RP Lavoué,É (corresponding author), Univ Jean Moulin Lyon 3, IAE Lyon Sch Management, CNRS, LIRIS,UMR5205,INSA Lyon, F-69621 Villeurbanne, France.
EM elise.lavoue@univ-lyon3.fr; fsophie.villenave@enise.fr;
   audrey.serna@insa-lyon.fr; clementine.didier@enise.fr;
   patrick.baert@enise.fr; guillaume.lavoue@enise.fr
OI Lavoue, Guillaume/0000-0003-3988-6702; Helfenstein-Didier,
   Clementine/0000-0002-3197-601X; Villenave, Sophie/0000-0002-6152-9800;
   SERNA, Audrey/0000-0003-1468-9761
FU LIRIS Laboratory; Agence nationale de la recherche [ANR-22-CE31-0023-03]
FX This work was supported in part by LIRIS Laboratory and in part by
   RENFORCE under Grant ANR-22-CE31-0023-03 project financed by the Agence
   nationale de la recherche.
CR Alsina I., 2005, PRESENCE 2005, P133
   Banos R, 1999, Cyberpsychol Behav, V2, P143, DOI 10.1089/cpb.1999.2.143
   Bartle R., 1996, Players Who Suit MUDs
   Bartle R. A., 2003, DESIGNING VIRTUAL WO
   Benitez J, 2020, INFORM MANAGE-AMSTER, V57, DOI 10.1016/j.im.2019.05.003
   Berta R, 2013, IEEE T COMP INTEL AI, V5, P164, DOI 10.1109/TCIAIG.2013.2260340
   Bian YL, 2016, PERS UBIQUIT COMPUT, V20, P821, DOI 10.1007/s00779-016-0953-5
   Csikszentmihalyi M., 1990, FLOW PSYCHOL OPTIMAL
   Deci EL, 2000, PSYCHOL INQ, V11, P227, DOI 10.1207/S15327965PLI1104_01
   DECI EL, 1985, J RES PERS, V19, P109, DOI 10.1016/0092-6566(85)90023-6
   Dewez D, 2019, INT SYM MIX AUGMENT, P123, DOI 10.1109/ISMAR.2019.00-12
   Faiola A, 2013, COMPUT HUM BEHAV, V29, P1113, DOI 10.1016/j.chb.2012.10.003
   Ferro LS, 2013, PROCEEDINGS OF THE 9TH AUSTRALASIAN CONFERENCE ON INTERACTIVE ENTERTAINMENT (IE 2013)
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Georgiou Y, 2017, INT J HUM-COMPUT ST, V98, P24, DOI 10.1016/j.ijhcs.2016.09.014
   Hair JF, 2011, J MARKET THEORY PRAC, V19, P139, DOI 10.2753/MTP1069-6679190202
   Hallifax S, 2019, CHI PLAY'19: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P559, DOI 10.1145/3311350.3347167
   Hamari J., 2014, Trans. Digit. Games Res. Assoc., V1, P29, DOI [DOI 10.26503/TODIGRA.V1I2.13, 10.26503/todigra.v1i2.13]
   Hamari J, 2014, COMPUT HUM BEHAV, V40, P133, DOI 10.1016/j.chb.2014.07.048
   Hassan L., 2020, P 53 HAW INT C SYST, P1196
   Heintz S., 2015, P 2015 ANN S COMP HU, P175, DOI DOI 10.1145/2793107.2793123
   Hult C. M., 2021, APrimeronPartial Least Squares Structural Equation Modeling (PLS-SEM)
   Iachini T, 2016, J ENVIRON PSYCHOL, V45, P154, DOI 10.1016/j.jenvp.2016.01.004
   Jackson SA, 2002, J SPORT EXERCISE PSY, V24, P133, DOI 10.1123/jsep.24.2.133
   Jackson SA, 2008, J SPORT EXERCISE PSY, V30, P561, DOI 10.1123/jsep.30.5.561
   Kim D, 2019, COMPUT HUM BEHAV, V93, P346, DOI 10.1016/j.chb.2018.12.040
   Kober SE, 2013, INT J HUM-COMPUT INT, V29, P13, DOI 10.1080/10447318.2012.668131
   Laarni J., 2004, P 7 ANN INT WORKSHOP, P88
   Lavoué É, 2021, INT J HUM-COMPUT ST, V154, DOI 10.1016/j.ijhcs.2021.102670
   Ling Y, 2013, COMPUT HUM BEHAV, V29, P1519, DOI 10.1016/j.chb.2012.12.010
   Moneta GB, 1996, J PERS, V64, P275, DOI 10.1111/j.1467-6494.1996.tb00512.x
   Murray CD, 2007, COMPUT HUM BEHAV, V23, P1347, DOI 10.1016/j.chb.2004.12.010
   Nacke LE, 2014, ENTERTAIN COMPUT, V5, P55, DOI 10.1016/j.entcom.2013.06.002
   Nicovich SG, 2005, J COMPUT-MEDIAT COMM, V10
   Oliveira W., 2020, P 53 HAW INT C SYST, P1226, DOI https://doi.org/10.24251/HICSS.2020.152
   Orji R, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174009
   Orji R, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1015, DOI 10.1145/3025453.3025577
   Orji R, 2014, USER MODEL USER-ADAP, V24, P453, DOI 10.1007/s11257-014-9149-8
   Pavlou PA, 2007, MIS QUART, V31, P105
   Rheinberg R. Vollmeyer, 2003, Die Erfassung des Flow-Erlebens, DOI [10.23668/psycharchives.8590.39S.A., DOI 10.23668/PSYCHARCHIVES.8590.39S.A]
   Santos I. I., 2018, P BRAZ S COMP GAM DI, P1586
   Sarstedt C., 2021, Partial Least Squares StructuralEquation Modeling, P1
   Sas C, 2003, PRESENCE-TELEOP VIRT, V12, P523, DOI 10.1162/105474603322761315
   Shin D, 2018, COMPUT HUM BEHAV, V78, P64, DOI 10.1016/j.chb.2017.09.012
   SLATER M, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P90, DOI 10.1109/VRAIS.1993.380793
   Suh A, 2018, COMPUT HUM BEHAV, V86, P77, DOI 10.1016/j.chb.2018.04.019
   Sweetser D., 2012, P AUSTR INT C INT EN
   Sweetser P., 2005, ACM COMPUT ENTERTAIN, V3, DOI DOI 10.1145/1077246.1077253
   Tondello GF, 2019, LECT NOTES COMPUT SC, V11747, P375, DOI 10.1007/978-3-030-29384-0_23
   Tondello GF, 2019, CHI PLAY'19: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P365, DOI 10.1145/3311350.3347185
   Tondello GF, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P329, DOI 10.1145/3116595.3116629
   Tondello GF, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P229, DOI 10.1145/2967934.2968082
   Weibel D, 2010, CYBERPSYCH BEH SOC N, V13, P251, DOI 10.1089/cyber.2009.0171
   Yang XZ, 2019, J EDUC COMPUT RES, V57, P846, DOI 10.1177/0735633118770800
   Yee N, 2006, CYBERPSYCHOL BEHAV, V9, P772, DOI 10.1089/cpb.2006.9.772
NR 55
TC 0
Z9 0
U1 9
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6208
EP 6221
DI 10.1109/TVCG.2023.3332261
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000045
PM 37956017
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Yuan, YZ
   Wu, Y
   Fan, XL
   Gong, MG
   Ma, WP
   Miao, QG
AF Yuan, Yongzhe
   Wu, Yue
   Fan, Xiaolong
   Gong, Maoguo
   Ma, Wenping
   Miao, Qiguang
TI EGST: Enhanced Geometric Structure Transformer for Point Cloud
   Registration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geometric structure descriptors; point cloud registration; structure
   consistency; transformer; Geometric structure descriptors; point cloud
   registration; structure consistency; transformer
ID HISTOGRAMS
AB We explore the effect of geometric structure descriptors on extracting reliable correspondences and obtaining accurate registration for point cloud registration. The point cloud registration task involves the estimation of rigid transformation motion in unorganized point cloud, hence it is crucial to capture the contextual features of the geometric structure in point cloud. Recent coordinates-only methods ignore numerous geometric information in the point cloud which weaken ability to express the global context. We propose Enhanced Geometric Structure Transformer to learn enhanced contextual features of the geometric structure in point cloud and model the structure consistency between point clouds for extracting reliable correspondences, which encodes three explicit enhanced geometric structures and provides significant cues for point cloud registration. More importantly, we report empirical results that Enhanced Geometric Structure Transformer can learn meaningful geometric structure features using none of the following: (i) explicit positional embeddings, (ii) additional feature exchange module such as cross-attention, which can simplify network structure compared with plain Transformer. Extensive experiments on the synthetic dataset and real-world datasets illustrate that our method can achieve competitive results.
C1 [Yuan, Yongzhe; Wu, Yue; Miao, Qiguang] Xidian Univ, Minist Educ, Sch Comp Sci & Technol, Key Lab Collaborat Intelligence Syst, Xian 710071, Peoples R China.
   [Fan, Xiaolong; Gong, Maoguo] Xidian Univ, Minist Educ, Sch Elect Engn, Key Lab Collaborat Intelligence Syst, Xian 710071, Peoples R China.
   [Ma, Wenping] Xidian Univ, Sch Artificial Intelligence, Xian, Peoples R China.
C3 Xidian University; Xidian University; Xidian University
RP Wu, Y (corresponding author), Xidian Univ, Minist Educ, Sch Comp Sci & Technol, Key Lab Collaborat Intelligence Syst, Xian 710071, Peoples R China.
EM yyz@stu.xidian.edu.cn; ywu@xidian.edu.cn; xiaolongfan@outlook.com;
   gong@ieee.org; wpma@mail.xidian.edu.cn; qgmiao@mail.xidian.edu.cn
RI Wu, Yue/GPX-9074-2022
OI Wu, Yue/0000-0002-3459-5079; Yuan, Yongzhe/0009-0002-9206-1283
FU National Natural Science Foundation of China [62276200, 62036006];
   Natural Science Basic Research Plan in Shaanxi Province of China
   [2022JM-327]; CAAI-Huawei MINDSPORE Academic Open Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62276200 and 62036006, in part by the
   Natural Science Basic Research Plan in Shaanxi Province of China under
   Grant 2022JM-327, and in part by the CAAI-Huawei MINDSPORE Academic Open
   Fund.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Chen C, 2019, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR.2019.00513
   Chen H., 2022, P ACM SIGGRAPH C, P1
   Cheng WT, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2623488
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Du ZJ, 2022, IEEE T VIS COMPUT GR, V28, P1745, DOI 10.1109/TVCG.2020.3028218
   Engel J, 2015, IEEE INT C INT ROBOT, P1935, DOI 10.1109/IROS.2015.7353631
   Fu KX, 2021, PROC CVPR IEEE, P8889, DOI [10.1109/CVPR46437.2021.00878, 10.1109/TPAMI.2022.3204713]
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1
   Guo YL, 2013, INT C COMM SIG PROC
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Lei H, 2017, IEEE T IMAGE PROCESS, V26, P3614, DOI 10.1109/TIP.2017.2700727
   Li C., 2021, ARXIV
   Li L, 2023, IEEE T VIS COMPUT GR, V29, P3368, DOI 10.1109/TVCG.2022.3160005
   Lucey S, 2013, IEEE T PATTERN ANAL, V35, P1383, DOI 10.1109/TPAMI.2012.220
   Ma LF, 2022, INT J COMPUT ASS RAD, V17, P1543, DOI 10.1007/s11548-022-02671-7
   Papadopoulo T, 2000, LECT NOTES COMPUT SC, V1842, P554
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Rusu RB, 2008, IAS-10: INTELLIGENT AUTONOMOUS SYSTEMS 10, P119, DOI 10.3233/978-1-58003-887-8-119
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Salvi J, 2004, PATTERN RECOGN, V37, P827, DOI 10.1016/j.patcog.2003.10.002
   Sarode V., 2019, arXiv
   SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI [10.1145/1877808.1877821, DOI 10.1145/1877808.1877821]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiaoshui Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11363, DOI 10.1109/CVPR42600.2020.01138
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Yang JQ, 2017, PATTERN RECOGN, V65, P175, DOI 10.1016/j.patcog.2016.11.019
   Yew ZJ, 2020, PROC CVPR IEEE, P11821, DOI 10.1109/CVPR42600.2020.01184
   Yu H, 2021, ADV NEUR IN, V34
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang J., 2014, P ROBOTICS SCI SYSTE, V2, P1, DOI [10.15607/RSS.2014.X.007, DOI 10.15607/RSS.2014.X.007]
   Zhou Q-Y, 2018, ARXIV
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
NR 46
TC 15
Z9 15
U1 10
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6222
EP 6234
DI 10.1109/TVCG.2023.3329578
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000033
PM 37971922
DA 2024-11-06
ER

PT J
AU Yang, MX
   Svirsky, Y
   Cheng, ZL
   Sharf, A
AF Yang, Mingxin
   Svirsky, Yonatan
   Cheng, Zhanglin
   Sharf, Andrei
TI Self-Supervised Fragment Alignment With Gaps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Generative adversarial networks; Neural networks;
   Visualization; Training; Noise measurement; Image restoration; Fragment
   registration; fragment alignment; deep learning
AB Image alignment and registration methods typically rely on visual correspondences across common regions and boundaries to guide the alignment process. Without them, the problem becomes significantly more challenging. Nevertheless, in real world, image fragments may be corrupted with no common boundaries and little or no overlap. In this work, we address the problem of learning the alignment of image fragments with gaps (i.e., without common boundaries or overlapping regions). Our setting is unsupervised, having only the fragments at hand with no ground truth to guide the alignment process. This is usually the situation in the restoration of unique archaeological artifacts such as frescoes and mosaics. Hence, we suggest a self-supervised approach utilizing self-examples which we generate from the existing data and then feed into an adversarial neural network. Our idea is that available information inside fragments is often sufficiently rich to guide their alignment with good accuracy. Following this observation, our method splits the initial fragments into sub-fragments yielding a set of aligned pieces. Thus, sub-fragmentation allows exposing new alignment relations and revealing inner structures and feature statistics. In fact, the new sub-fragments construct true and false alignment relations between fragments. We feed this data to a spatial transformer GAN which learns to predict the alignment between fragments gaps. We test our technique on various synthetic datasets as well as large scale frescoes and mosaics. Results demonstrate our method's capability to learn the alignment of deteriorated image fragments in a self-supervised manner, by examining inner image statistics for both synthetic and real data.
C1 [Yang, Mingxin; Cheng, Zhanglin] Chinese Acad Sci, Shenzhen Inst Adv Technol SIAT, Shenzhen Key Lab Visual Comp & Analyt VisuCA, Shenzhen 518055, Peoples R China.
   [Svirsky, Yonatan; Sharf, Andrei] Bengurion Univ, Beer Sheva 84105, Israel.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS
RP Cheng, ZL (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol SIAT, Shenzhen Key Lab Visual Comp & Analyt VisuCA, Shenzhen 518055, Peoples R China.
EM yangmingxin18@mails.ucas.edu.cn; svirskyy@post.bgu.ac.il;
   zhanglin.cheng@gmail.com; asharf@gmail.com
RI Cheng, Zhanglin/AAP-1760-2021; Sharf, Andrei/F-1370-2012
OI Cheng, Zhanglin/0000-0002-3360-2679; Sharf, Andrei/0000-0002-3963-4508
FU NSFC [U21A20515, 61972388]; National Key R&D Program of China
   [2022ZD0160801]; Shenzhen Science and Technology Program
   [GJHZ20210705141402008]
FX This work was supported in part by NSFC under Grants U21A20515 and
   61972388, in part by National Key R&D Program of China under Grant
   2022ZD0160801, and in part by Shenzhen Science and Technology Program
   under Grant GJHZ20210705141402008.
CR Altman T., 1989, Applied Artificial Intelligence, V3, P453, DOI 10.1080/08839518908949937
   Anirudh R, 2018, Arxiv, DOI arXiv:1805.07281
   Arandjelovic R, 2019, Arxiv, DOI arXiv:1905.11369
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Bora A., 2018, P INT C LEARN REPR, P1
   Bridger D, 2020, PROC CVPR IEEE, P3523, DOI 10.1109/CVPR42600.2020.00358
   Brown BJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360683
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Cao SJ, 2010, IEEE INT CON MULTI, P358, DOI 10.1109/ICME.2010.5582544
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Cho TS, 2010, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2010.5540212
   da Gama Leitao H. C., 2000, P BRIT MACH VIS C, P1
   Deever A, 2012, IEEE IMAGE PROC, P233, DOI 10.1109/ICIP.2012.6466838
   Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Derech N, 2018, Arxiv, DOI arXiv:1812.10553
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dondi P, 2020, PATTERN RECOGN LETT, V138, P631, DOI 10.1016/j.patrec.2020.09.015
   Gallagher AC, 2012, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2012.6247699
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508373
   Huang H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366198
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Justino E, 2006, FORENSIC SCI INT, V160, P140, DOI 10.1016/j.forsciint.2005.09.001
   KOLLER D., 2006, Bullettino Della Commissione Archeologica Comunale di Roma, V15, P103
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li R, 2022, IEEE T IMAGE PROCESS, V31, P513, DOI 10.1109/TIP.2021.3120052
   Lin CH, 2018, PROC CVPR IEEE, P9455, DOI 10.1109/CVPR.2018.00985
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Marande W, 2007, SCIENCE, V318, P415, DOI 10.1126/science.1148033
   MARQUES M., 2009, P ACM S APPL COMP, P893
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Paikin G, 2015, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR.2015.7299116
   Pajot A., 2019, P INT C LEARN REPR, P1
   Papaodysseus C, 2002, IEEE T SIGNAL PROCES, V50, P1277, DOI 10.1109/TSP.2002.1003053
   Paumard MM, 2020, IEEE T IMAGE PROCESS, V29, P3569, DOI 10.1109/TIP.2019.2963378
   Paumard MM, 2018, IEEE IMAGE PROC, P1018, DOI 10.1109/ICIP.2018.8451094
   Paumard MM, 2018, LECT NOTES COMPUT SC, V11210, P155, DOI 10.1007/978-3-030-01231-1_10
   Pintus R, 2016, COMPUT GRAPH FORUM, V35, P4, DOI 10.1111/cgf.12668
   Poleg Y., 2012, P IEEE INT C COMP PH, P1
   Pomeranz D, 2011, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2011.5995331
   Remez T, 2018, LECT NOTES COMPUT SC, V11211, P39, DOI 10.1007/978-3-030-01234-2_3
   Sholomon D, 2016, LECT NOTES COMPUT SC, V9887, P170, DOI 10.1007/978-3-319-44781-0_21
   Sholomon D, 2013, PROC CVPR IEEE, P1767, DOI 10.1109/CVPR.2013.231
   Sivapriya V., 2018, P INT S AUT ROB CONS, P1
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Toler-Franklin C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866207
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JQ, 2009, PROCEEDINGS OF 2009 CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE & SYSTEM DYNAMICS, VOL 7, P1
   Wei C, 2019, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2019.00201
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zelnik-Manor L, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P121
NR 53
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6235
EP 6246
DI 10.1109/TVCG.2023.3330859
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000024
PM 37938966
DA 2024-11-06
ER

PT J
AU Baker, AH
   Pinard, A
   Hammerling, DM
AF Baker, Allison H.
   Pinard, Alexander
   Hammerling, Dorit M.
TI On a Structural Similarity Index Approach for Floating-Point Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image coding; Meteorology; Loss measurement; Data models; Indexes; Data
   visualization; Costs; Climate simulation data; compression;
   floating-point data; structural similarity index
ID IMAGE QUALITY ASSESSMENT; COMPRESSION
AB Data visualization is typically a critical component of post-processing analysis workflows for floating-point output data from large simulation codes, such as global climate models. For example, images are often created from the raw data as a means for evaluation against a reference dataset or image. While the popular Structural Similarity Index Measure (SSIM) is a useful tool for such image comparisons, generating large numbers of images can be costly when simulation data volumes are substantial. In fact, computational cost considerations motivated our development of an alternative to the SSIM, which we refer to as the Data SSIM (DSSIM). The DSSIM is conceptually similar to the SSIM, but can be applied directly to the floating-point data as a means of assessing data quality. We present the DSSIM in the context of quantifying differences due to lossy compression on large volumes of simulation data from a popular climate model. Bypassing image creation results in a sizeable performance gain for this case study. In addition, we show that the DSSIM is useful in terms of avoiding plot-specific (but data-independent) choices that can affect the SSIM. While our work is motivated by and evaluated with climate model output data, the DSSIM may prove useful for other applications involving large volumes of simulation data.
C1 [Baker, Allison H.] Natl Ctr Atmospher Res, Boulder, CO 80305 USA.
   [Pinard, Alexander; Hammerling, Dorit M.] Colorado Sch Mines, Golden, CO 80401 USA.
C3 National Center Atmospheric Research (NCAR) - USA; Colorado School of
   Mines
RP Baker, AH (corresponding author), Natl Ctr Atmospher Res, Boulder, CO 80305 USA.
EM abaker@ucar.edu; apinard@mines.edu; hammerling@mines.edu
OI Hammerling, Dorit/0000-0003-3583-3611; Baker,
   Allison/0000-0003-2436-7838
CR [Anonymous], 2016, NCAR COMMAND LANGUAG, DOI DOI 10.5065/D6WD3XH5
   [Anonymous], 2012, Int. J. Comput. Appl
   Baker AH, 2019, COMPUT GRAPH FORUM, V38, P517, DOI 10.1111/cgf.13707
   Baker Allison H., 2017, High Performance Computing. ISC High Performance 2017 International Workshops DRBSD, ExaComm, HCPM, HPC-IODC, IWOPH, IXPUG, P^3MA, VHPC, Visualization at Scale, WOPSSS. Revised Selected Papers: LNCS 10524, P30, DOI 10.1007/978-3-319-67630-2_3
   Baker A. H., 2014, P 23 INT S HIGH PERF, P203, DOI DOI 10.1145/2600212.2600217
   Dosselmann R, 2011, SIGNAL IMAGE VIDEO P, V5, P81, DOI 10.1007/s11760-009-0144-1
   Elson Phil, 2023, Zenodo, DOI 10.5281/ZENODO.1182735
   Gaudeau Y, 2014, IEEE IMAGE PROC, P501, DOI 10.1109/ICIP.2014.7025100
   Georgiev VT, 2013, J DIGIT IMAGING, V26, P427, DOI 10.1007/s10278-012-9538-7
   Hamman Joseph., 2018, P EGU GEN ASS C, V20, page, P12146
   Hübbe N, 2013, LECT NOTES COMPUT SC, V7905, P343, DOI 10.1007/978-3-642-38750-0_26
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Hurrell JW, 2013, B AM METEOROL SOC, V94, P1339, DOI 10.1175/BAMS-D-12-00121.1
   Jones EL, 2016, ECOL INDIC, V70, P67, DOI 10.1016/j.ecolind.2016.05.051
   Kay JE, 2015, B AM METEOROL SOC, V96, P1333, DOI 10.1175/BAMS-D-13-00255.1
   Kowalik-Urbaniak I, 2014, PROC SPIE, V9037, DOI 10.1117/12.2043196
   Kuhn Michael, 2016, [Supercomputing Frontiers and Innovations, Supercomputing Frontiers and Innovations], V3, P75
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lindstrom P, 2014, IEEE T VIS COMPUT GR, V20, P2674, DOI 10.1109/TVCG.2014.2346458
   Nilsson Jim, 2020, arXiv
   Pinard A., 2021, Tech. Rep. NCAR/TN-570 STR, DOI [10.5065/4q49-t141, DOI 10.5065/4Q49-T141]
   Pinard A., 2021, Tech. Rep. NCAR/TN-568 STR, DOI [10.5065/jc2r-5289, DOI 10.5065/JC2R-5289]
   Poppick A, 2020, COMPUT GEOSCI-UK, V145, DOI 10.1016/j.cageo.2020.104599
   Price-Whelan AM, 2018, ASTRON J, V156, DOI 10.3847/1538-3881/aabc4f
   Razaak Manzoor, 2013, 2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013), P6, DOI 10.1109/HealthCom.2013.6720628
   Robitaille TP, 2013, ASTRON ASTROPHYS, V558, DOI 10.1051/0004-6361/201322068
   Sawant S., 2013, Compressed image quality measurement
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   Venkataramanan AK, 2021, IEEE ACCESS, V9, P28872, DOI 10.1109/ACCESS.2021.3056504
   Veras R, 2020, IEEE T VIS COMPUT GR, V26, P749, DOI 10.1109/TVCG.2019.2934432
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE SIGNAL PROC MAG, V28, P137, DOI 10.1109/MSP.2011.942295
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wegener A, 2010, IEEE SIGNAL PROC MAG, V27, P125, DOI 10.1109/MSP.2010.936781
   Woodring J., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P31, DOI 10.1109/LDAV.2011.6092314
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zhang L, 2012, IEEE IMAGE PROC, P1477, DOI 10.1109/ICIP.2012.6467150
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 40
TC 2
Z9 2
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6261
EP 6274
DI 10.1109/TVCG.2023.3332843
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000040
PM 37966929
OA Bronze
DA 2024-11-06
ER

PT J
AU Chen, JT
   Huang, QY
   Wang, CB
   Li, CH
AF Chen, Juntong
   Huang, Qiaoyun
   Wang, Changbo
   Li, Chenhui
TI SenseMap: Urban Performance Visualization and Analytics Via Semantic
   Textual Similarity
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Density map; point of interest; semantic textual similarity; urban data;
   visual analytics; visualization design; Density map; point of interest;
   semantic textual similarity; urban data; visual analytics; visualization
   design
ID VISUAL EXPLORATION; POINTS; MAP
AB As urban populations grow, effectively accessing urban performance measures such as livability and comfort becomes increasingly important due to their significant socioeconomic impacts. While Point of Interest (POI) data has been utilized for various applications in location-based services, its potential for urban performance analytics remains unexplored. In this article, we present SenseMap, a novel approach for analyzing urban performance by leveraging POI data as a semantic representation of urban functions. We quantify the contribution of POIs to different urban performance measures by calculating semantic textual similarities on our constructed corpus. We propose Semantic-adaptive Kernel Density Estimation which takes into account POIs' influential areas across different Traffic Analysis Zones and semantic contributions to generate semantic density maps for measures. We design and implement a feature-rich, real-time visual analytics system for users to explore the urban performance of their surroundings. Evaluations with human judgment and reference data demonstrate the feasibility and validity of our method. Usage scenarios and user studies demonstrate the capability, usability and explainability of our system.
C1 [Chen, Juntong; Huang, Qiaoyun; Wang, Changbo; Li, Chenhui] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200050, Peoples R China.
C3 East China Normal University
RP Wang, CB; Li, CH (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200050, Peoples R China.
EM jtchen@stu.ecnu.edu.cn; qyhuang@stu.ecnu.edu.cn; cbwang@cs.ecnu.edu.cn;
   chli@cs.ecnu.edu.cn
RI Li, Chenhui/AAR-3682-2020; Chen, Juntong/KBV-8278-2024
OI Li, Chenhui/0000-0001-9835-2650; Chen, Juntong/0000-0001-9343-4032;
   Huang, Qiaoyun/0009-0007-4425-4984; Wang, Changbo/0000-0001-8940-6418
FU NSFC [61802128, 62072183]; Shanghai Committee of Science and Technology,
   China [22511104600]; Yangtze River Delta Science and Technology
   Innovation Community Project [23002400400]
FX This work was supported in part by NSFC under Grants 61802128 and
   62072183, in part by the Shanghai Committee of Science and Technology,
   China under Grant 22511104600, and in part by Yangtze River Delta
   Science and Technology Innovation Community Project, China under Grant
   23002400400.
CR Abualigah L. M. Q., 2019, Stud. Comput. Intell., V816, P61
   Al-Dohuki S, 2017, IEEE T VIS COMPUT GR, V23, P11, DOI 10.1109/TVCG.2016.2598416
   Bansal P. P., 1972, Metallography, V5, P97, DOI 10.1016/0026-0800(72)90048-1
   Ben Aouicha M, 2018, SOFT COMPUT, V22, P1855, DOI 10.1007/s00500-016-2438-x
   Cesme B, 2017, TRANSPORT RES REC, P45, DOI 10.3141/2605-04
   Chen JL, 2016, LECT NOTES COMPUT SC, V9927, P130, DOI 10.1007/978-3-319-45738-3_9
   Chen NK, 2017, PAC ECON REV, V22, P293, DOI 10.1111/1468-0106.12231
   Chen W, 2018, IEEE T VIS COMPUT GR, V24, P2636, DOI 10.1109/TVCG.2017.2758362
   Chokhachian A, 2020, SUSTAIN CITIES SOC, V53, DOI 10.1016/j.scs.2019.101952
   Deng Liu, 2019, ISPRS INT J GEO-INF, V8, DOI DOI 10.3390/ijgi8060283
   Deng ZK, 2023, COMPUT VIS MEDIA, V9, P3, DOI 10.1007/s41095-022-0275-7
   Deng ZK, 2022, IEEE T VIS COMPUT GR, V28, P1051, DOI 10.1109/TVCG.2021.3114875
   Deng ZK, 2020, IEEE T VIS COMPUT GR, V26, P800, DOI 10.1109/TVCG.2019.2934670
   Feng ZZ, 2021, IEEE T VIS COMPUT GR, V27, P828, DOI 10.1109/TVCG.2020.3030469
   Frew T, 2016, LAND USE POLICY, V50, P239, DOI 10.1016/j.landusepol.2015.10.007
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gao S., 2021, Urban Informatics, P503, DOI DOI 10.1007/978-981-15-8983-628
   Gao S, 2017, T GIS, V21, P446, DOI 10.1111/tgis.12289
   GEOS Developers, 2023, GEOS: Geos::Operation::Valid::MakeValid Class Reference
   Giller P. S., 1984, Community Structure and the Niche, P112
   Guo FZ, 2019, ACM T INTERACT INTEL, V9, DOI 10.1145/3182187
   Hogräfer M, 2020, COMPUT GRAPH FORUM, V39, P647, DOI 10.1111/cgf.14031
   Hu YJ, 2019, ANN AM ASSOC GEOGR, V109, P1052, DOI 10.1080/24694452.2018.1535886
   Huang JZ, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P3029, DOI 10.1145/3534678.3539021
   Huang ZS, 2020, IEEE T VIS COMPUT GR, V26, P1256, DOI 10.1109/TVCG.2019.2934671
   Jacobs J., 1958, Exploding Metropolis, V168, P124
   Jo J, 2019, IEEE T VIS COMPUT GR, V25, P470, DOI 10.1109/TVCG.2018.2865141
   Kim JR, 2016, INT AREA STUD REV, V19, P131, DOI 10.1177/2233865915581432
   Kim S, 2018, IEEE T VIS COMPUT GR, V24, P1287, DOI 10.1109/TVCG.2017.2666146
   Lee C, 2020, IEEE T VIS COMPUT GR, V26, P3133, DOI 10.1109/TVCG.2019.2922597
   Li CH, 2022, IEEE T VIS COMPUT GR, V28, P1062, DOI 10.1109/TVCG.2021.3114762
   Li CH, 2018, IEEE T VIS COMPUT GR, V24, P1381, DOI 10.1109/TVCG.2017.2668409
   Li CL, 2018, VIS INFORM, V2, P50, DOI 10.1016/j.visint2018.04.006
   Liang XJ, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/8269274
   Lin L, 2010, HEALTH PLACE, V16, P339, DOI 10.1016/j.healthplace.2009.11.002
   Liu DY, 2019, IEEE T VIS COMPUT GR, V25, P1, DOI 10.1109/TVCG.2018.2865018
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Long Y, 2019, LANDSCAPE URBAN PLAN, V191, DOI 10.1016/j.landurbplan.2019.103612
   Ma SM, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P635, DOI 10.18653/v1/P17-2100
   Maciejewski R, 2011, IEEE T VIS COMPUT GR, V17, P440, DOI 10.1109/TVCG.2010.82
   McKenzie G., 2017, 13 INT C SPATIAL INF
   McKenzie G, 2024, CARTOGR GEOGR INF SC, V51, P583, DOI 10.1080/15230406.2023.2176930
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Miranda F, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376399
   Narechania A, 2021, IEEE T VIS COMPUT GR, V27, P369, DOI 10.1109/TVCG.2020.3030378
   Nyunt MSZ, 2015, INT J BEHAV NUTR PHY, V12, DOI 10.1186/s12966-015-0276-3
   Oppermann M, 2021, IEEE T VIS COMPUT GR, V27, P495, DOI 10.1109/TVCG.2020.3030387
   Qiu WS, 2022, LANDSCAPE URBAN PLAN, V221, DOI 10.1016/j.landurbplan.2022.104358
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Renhe Jiang, 2021, ACM/IMS Transactions on Data Science, V2, DOI 10.1145/3416914
   Rodrigue J.P., 2020, The Geography of Transport Systems, DOI [10.4324/9780429346323, DOI 10.4324/9780429346323]
   Saeed U, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14148755
   Sanh V, 2020, Arxiv, DOI [arXiv:1910.01108, 10.48550/ARXIV.1910.01108]
   Shen LX, 2023, IEEE T VIS COMPUT GR, V29, P3121, DOI 10.1109/TVCG.2022.3148007
   Shen QM, 2018, IEEE T VIS COMPUT GR, V24, P1004, DOI 10.1109/TVCG.2017.2744159
   Shi KF, 2020, J CLEAN PROD, V255, DOI 10.1016/j.jclepro.2020.120245
   Shin D, 2023, IEEE T VIS COMPUT GR, V29, P1799, DOI 10.1109/TVCG.2021.3131824
   Silverman B. W., 1986, Density Estimation for Statistics and Data Analysis
   Sinoara RA, 2019, KNOWL-BASED SYST, V163, P955, DOI 10.1016/j.knosys.2018.10.026
   Song Kaitao, 2020, Advances in Neural Information Processing Systems, V33
   Song SC, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581067
   Southworth M, 2005, J URBAN PLAN DEV, V131, P246, DOI 10.1061/(ASCE)0733-9488(2005)131:4(246)
   Sun ZH, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10050339
   Tsukagoshi H, 2021, ACL-IJCNLP 2021: THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 2, P411
   Tu W, 2020, COMPUT ENVIRON URBAN, V80, DOI 10.1016/j.compenvurbsys.2019.101428
   Vibriyanti D., 2022, P U LAMP INT C SOC S, P394
   Voigt H, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P348
   Wang XS, 2019, ACCIDENT ANAL PREV, V125, P249, DOI 10.1016/j.aap.2019.02.014
   Wang YD, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7040130
   Weng D, 2019, IEEE T VIS COMPUT GR, V25, P459, DOI 10.1109/TVCG.2018.2865126
   Wieting J, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4344
   Wilson JQ., 2017, Social, Ecological and Environmental Theories of Crime, P169
   Wolch JR, 2014, LANDSCAPE URBAN PLAN, V125, P234, DOI 10.1016/j.landurbplan.2014.01.017
   Wu AY, 2022, IEEE T VIS COMPUT GR, V28, P5049, DOI 10.1109/TVCG.2021.3099002
   Wu WC, 2016, IEEE T VIS COMPUT GR, V22, P935, DOI 10.1109/TVCG.2015.2467194
   Yu BW, 2020, IEEE T VIS COMPUT GR, V26, P1, DOI 10.1109/TVCG.2019.2934668
   Zeng W, 2017, IEEE T INTELL TRANSP, V18, P2271, DOI 10.1109/TITS.2016.2639320
   Zhai W, 2019, COMPUT ENVIRON URBAN, V74, P1, DOI 10.1016/j.compenvurbsys.2018.11.008
   Zhang MD, 2023, IEEE T VIS COMPUT GR, V29, P2067, DOI 10.1109/TVCG.2021.3140153
   Zheng SQ, 2019, NAT HUM BEHAV, V3, P237, DOI 10.1038/s41562-018-0521-2
   Zhu MF, 2019, IEEE T INTELL TRANSP, V20, P3981, DOI 10.1109/TITS.2019.2901117
NR 81
TC 3
Z9 3
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6275
EP 6290
DI 10.1109/TVCG.2023.3333356
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000025
PM 37971923
DA 2024-11-06
ER

PT J
AU Gu, LP
   Yan, XF
   Cui, P
   Gong, L
   Xie, HR
   Wang, FL
   Qin, J
   Wei, MQ
AF Gu, Lipeng
   Yan, Xuefeng
   Cui, Peng
   Gong, Lina
   Xie, Haoran
   Wang, Fu Lee
   Qin, Jing
   Wei, Mingqiang
TI PointSee: Image Enhances Point Cloud
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D object detection; feature enhancement; multi-modal fusion; PointSee;
   3D object detection; feature enhancement; multi-modal fusion; PointSee
AB There is a prevailing trend towards fusing multi-modal information for 3D object detection (3OD). However, challenges related to computational efficiency, plug-and-play capabilities, and accurate feature alignment have not been adequately addressed in the design of multi-modal fusion networks. In this paper, we present PointSee, a lightweight, flexible, and effective multi-modal fusion solution to facilitate various 3OD networks by semantic feature enhancement of point clouds (e.g., LiDAR or RGB-D data) assembled with scene images. Beyond the existing wisdom of 3OD, PointSee consists of a hidden module (HM) and a seen module (SM): HM decorates point clouds using 2D image information in an offline fusion manner, leading to minimal or even no adaptations of existing 3OD networks; SM further enriches the point clouds by acquiring point-wise representative semantic features, leading to enhanced performance of existing 3OD networks. Besides the new architecture of PointSee, we propose a simple yet efficient training strategy, to ease the potential inaccurate regressions of 2D object detection networks. Extensive experiments on the popular outdoor/indoor benchmarks show quantitative and qualitative improvements of our PointSee over thirty-five state-of-the-art methods.
C1 [Gu, Lipeng; Yan, Xuefeng; Gong, Lina; Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210016, Peoples R China.
   [Gu, Lipeng; Gong, Lina; Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Shenzhen Inst Res, Shenzhen 518055, Peoples R China.
   [Yan, Xuefeng] Collaborat Innovat Ctr Novel Software Technol & In, Nanjing 210000, Peoples R China.
   [Cui, Peng] Dalian Naval Acad, Inst Software & Simulat, Dalian 116018, Peoples R China.
   [Xie, Haoran] Lingnan Univ, Dept Comp & Decis Sci, Tuen Mun, Hong Kong, Peoples R China.
   [Wang, Fu Lee] Hong Kong Metropolitan Univ, Sch Sci & Technol, Ho Man Tin, Hong Kong, Peoples R China.
   [Qin, Jing] Polytech Univ Hong Kong, Sch Nursing, Hung Hom, Hong Kong, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics; Dalian Naval Academy; Lingnan University;
   Hong Kong Metropolitan University
RP Yan, XF (corresponding author), Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210016, Peoples R China.
EM glp1224@163.com; yxf@nuaa.edu.cn; cphkzy@126.com; gonglina@nuaa.edu.cn;
   hrxie@ln.edu.hk; pwang@hkmu.edu.hk; harry.qin@polyu.edu.hk;
   mingqiang.wei@gmail.com
RI Xie, Haoran/ADP-8087-2022; Yan, xuefeng/JGL-6667-2023; Wang,
   Fu/AAD-9782-2021; Xie, Haoran/AFS-3515-2022
OI Xie, Haoran/0000-0003-0965-3617; Gong, Lina/0000-0002-5272-6706; Wang,
   Fu Lee/0000-0002-3976-0053; Gu, Lipeng/0000-0003-2323-2317
FU National Natural Science Foundation of China [T2322012, 62172218,
   62032011]; Shenzhen Science and Technology Program
   [JCYJ20220818103401003, JCYJ20220530172403007]; National Defense Basic
   Scientific Research Program of China [JCKY2020605C003]; Guangdong Basic
   and Applied Basic Research Foundation [2022A1515010170]; General
   Research Fund of Hong Kong Research Grants Council [15218521]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants T2322012, 62172218,and 62032011, in
   part by Shenzhen Science and Technology Program under Grants
   JCYJ20220818103401003 and JCYJ20220530172403007, in part by the National
   Defense Basic Scientific Research Program of China under Grant
   JCKY2020605C003, in part by Guangdong Basic and Applied Basic Research
   Foundation under Grant 2022A1515010170, and in part by the General
   Research Fund of Hong Kong Research Grants Council under Grant 15218521.
CR Bai ZW, 2022, IEEE INT VEH SYM, P1366, DOI 10.1109/IV51971.2022.9827461
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen C, 2022, AAAI CONF ARTIF INTE, P221
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen WY, 2022, NEUROCOMPUTING, V494, P23, DOI 10.1016/j.neucom.2022.04.075
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Chen ZH, 2022, PROCEEDINGS OF THE THIRTY-FIRST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2022, P827
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gwak J., 2020, P 16 EUR C COMP VI 4, V12349, P297
   He CH, 2022, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR52688.2022.00823
   He CH, 2020, PROC CVPR IEEE, P11870, DOI 10.1109/CVPR42600.2020.01189
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li X, 2022, LECT NOTES COMPUT SC, V13698, P691, DOI 10.1007/978-3-031-19839-7_40
   Li YW, 2022, PROC CVPR IEEE, P17161, DOI 10.1109/CVPR52688.2022.01667
   Li ZC, 2021, PROC CVPR IEEE, P7542, DOI 10.1109/CVPR46437.2021.00746
   Lin CM, 2022, IEEE T INTELL TRANSP, V23, P18040, DOI 10.1109/TITS.2022.3154537
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2929, DOI 10.1109/ICCV48922.2021.00294
   Liu Z, 2023, IEEE T PATTERN ANAL, V45, P8324, DOI 10.1109/TPAMI.2022.3228806
   Liu ZJ, 2023, IEEE INT CONF ROBOT, P2774, DOI 10.1109/ICRA48891.2023.10160968
   Mahmoud A, 2023, IEEE WINT CONF APPL, P663, DOI 10.1109/WACV56688.2023.00073
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qian R, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108796
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Ren Z, 2016, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2016.169
   Rukhovich D, 2022, LECT NOTES COMPUT SC, V13670, P477, DOI 10.1007/978-3-031-20080-9_28
   Shi SS, 2020, PROC CVPR IEEE, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shin K, 2019, IEEE INT VEH SYM, P2510, DOI 10.1109/IVS.2019.8813895
   Sindagi VA, 2019, IEEE INT CONF ROBOT, P7276, DOI [10.1109/ICRA.2019.8794195, 10.1109/icra.2019.8794195]
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466
   Wang CW, 2021, PROC CVPR IEEE, P11789, DOI 10.1109/CVPR46437.2021.01162
   Wang HA, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P2539, DOI 10.1109/ICRA46639.2022.9811699
   Wang YJ, 2023, INT J COMPUT VISION, V131, P2122, DOI 10.1007/s11263-023-01784-z
   Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI [10.1109/iros40897.2019.8968513, 10.1109/IROS40897.2019.8968513]
   Wang ZT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462219
   Xie Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3692, DOI 10.1109/ICCV48922.2021.00369
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang ZT, 2020, PROC CVPR IEEE, P11037, DOI 10.1109/CVPR42600.2020.01105
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Yoo JH, 2020, SELECTED PAPERS FROM THE NINETEENTH BIENNIAL IEEE CONFERENCE ON ELECTROMAGNETIC FIELD COMPUTATION (IEEE CEFC 2020), DOI [10.1109/CEFC46938.2020.9451336, 10.1007/978-3-030-58583-9_43]
   Zeng YH, 2022, PROC CVPR IEEE, P17151, DOI 10.1109/CVPR52688.2022.01666
   Zhang YA, 2022, PROC CVPR IEEE, P898, DOI 10.1109/CVPR52688.2022.00098
   Zhang YF, 2022, PROC CVPR IEEE, P18931, DOI 10.1109/CVPR52688.2022.01838
   Zhang ZH, 2022, IEEE INT C INTELL TR, P369, DOI 10.1109/ITSC55140.2022.9922104
   Zhang ZQ, 2020, CICTP 2020: ADVANCED TRANSPORTATION TECHNOLOGIES AND DEVELOPMENT-ENHANCING CONNECTIONS, P311, DOI [10.1007/978-3-030-58610-2_19, 10.1061/9780784482933.027]
   Zheng W, 2021, AAAI CONF ARTIF INTE, V35, P3555
   Zhou C, 2023, PROC CVPR IEEE, P5166, DOI 10.1109/CVPR52729.2023.00500
   Zhou SC, 2023, PROC CVPR IEEE, P5116, DOI 10.1109/CVPR52729.2023.00495
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zuo LY, 2021, IEEE INT C INTELL TR, P2746, DOI 10.1109/ITSC48978.2021.9564764
NR 72
TC 0
Z9 0
U1 9
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6291
EP 6308
DI 10.1109/TVCG.2023.3331779
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000006
PM 37948146
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Liu, XC
   Wu, YC
   Hall, P
AF Liu, Xiao-Chang
   Wu, Yu-Chen
   Hall, Peter
TI Painterly Style Transfer With Learned Brush Strokes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Non-photorealistic rendering; style transfer; Non-photorealistic
   rendering; style transfer
AB Real-world paintings are made, by artists, using brush strokes as the rendering primitive to depict semantic content. The bulk of the Neural Style Transfer (NST) is known transferring style using texture patches, not strokes. The output looks like the content image, but some are traced over using the style texture: it does not look painterly. We adopt a very different approach that uses strokes. Our contribution is to analyse paintings to learn stroke families-that is, distributions of strokes based on their shape (a dot, straight lines, curved arcs, etc.). When synthesising a new output, these distributions are sampled to ensure the output is painted with the correct style of stroke. Consequently, our output looks more "painterly" than NST output based on texture. Furthermore, where strokes are placed is an important contributing factor in determining output quality, and we have also addressed this aspect. Humans place strokes to emphasize salient semantically meaningful image content. Conventional NST uses a content loss premised on filter responses that is agnostic to salience. We show that replacing that loss with one based on the language-image model benefits the output through greater emphasis of salient content.
C1 [Liu, Xiao-Chang; Wu, Yu-Chen; Hall, Peter] Univ Bath, Dept Comp Sci, Bath BA2 7AY, England.
C3 University of Bath
RP Liu, XC (corresponding author), Univ Bath, Dept Comp Sci, Bath BA2 7AY, England.
EM xl2546@bath.ac.uk; yw3283@bath.ac.uk; p.m.hall@bath.ac.uk
RI Wu, Yuchen/AFB-8756-2022
OI Wu, Yu-Chen/0009-0000-8664-2992; Hall, Peter/0009-0006-5699-5483
FU China Scholarship Council [201906200059]
FX This work was supported in part by the China Scholarship Council under
   Grant 201906200059. Recommended for acceptance by L.-Y.Wei.
CR Berezhnoy IE, 2009, MACH VISION APPL, V20, P1, DOI 10.1007/s00138-007-0098-7
   Brooks S, 2007, IEEE T VIS COMPUT GR, V13, P1041, DOI 10.1109/TVCG.2007.1025
   Brooks T, 2023, PROC CVPR IEEE, P18392, DOI 10.1109/CVPR52729.2023.01764
   Chan C, 2022, PROC CVPR IEEE, P7905, DOI 10.1109/CVPR52688.2022.00776
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen Y.-L., 2016, P BRIT MACH VIS C, P1
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   Collomosse JP, 2006, INT J ARTIF INTELL T, V15, P551, DOI 10.1142/S0218213006002813
   Collomosse JP, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P122, DOI 10.1109/EGUK.2002.1011281
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Di Blasi G, 2005, VISUAL COMPUT, V21, P373, DOI 10.1007/s00371-005-0292-4
   Dumoulin V., 2016, Preprint, P8, DOI [10.48550/arXiv.1610.07629, DOI 10.48550/ARXIV.1610.07629]
   Finkelstein A., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P261, DOI 10.1145/192161.192223
   Frans Kevin., 2022, ADV NEURAL INFORM PR, V35, P5207
   Freeman WT, 2003, ACM T GRAPHIC, V22, P33, DOI 10.1145/588272.588277
   Fu YF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3429742
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gooch B., 2002, P 2 INT S NONPH AN R, P83, DOI DOI 10.1145/508530.508545>
   Gooch B., 2001, Non-Photorealistic Rendering
   Gravesen J, 1997, COMP GEOM-THEOR APPL, V8, P13, DOI 10.1016/0925-7721(95)00054-2
   Ha D., 2018, INT C LEARN REPR ICL
   Haeberli P., 1990, Computer Graphics, V24, P207, DOI 10.1145/97880.97902
   Han FZ, 2023, IEEE T VIS COMPUT GR, V29, P1371, DOI 10.1109/TVCG.2021.3114308
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Hertzmann A, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P47, DOI 10.1109/CGI.2001.934657
   Hertzmann A., 2010, P 8 INT S NONPH AN R, P147
   Hertzmann A., 2000, NPAR, P7
   Höllein L, 2022, PROC CVPR IEEE, P6188, DOI 10.1109/CVPR52688.2022.00610
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang ZW, 2019, IEEE I CONF COMP VIS, P8708, DOI 10.1109/ICCV.2019.00880
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Champandard AJ, 2016, Arxiv, DOI arXiv:1603.01768
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   Kim Sunnie S. Y., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P246, DOI 10.1007/978-3-030-58574-7_15
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kotovenko D, 2021, PROC CVPR IEEE, P12191, DOI 10.1109/CVPR46437.2021.01202
   Kurzman L, 2019, IEEE INT CONF COMP V, P3189, DOI 10.1109/ICCVW.2019.00396
   Kwon G, 2022, PROC CVPR IEEE, P18041, DOI 10.1109/CVPR52688.2022.01753
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Lang K., 2015, P WORKSH NONPH AN RE, P203
   LANSDOWN J, 1995, IEEE COMPUT GRAPH, V15, P29, DOI 10.1109/38.376610
   Lecoutre A., 2017, AS C MACH LEARN, P327
   Li J, 2012, IEEE T PATTERN ANAL, V34, P1159, DOI 10.1109/TPAMI.2011.203
   Liao YS, 2022, IEEE T IMAGE PROCESS, V31, P1911, DOI 10.1109/TIP.2022.3149237
   Lindemeier T, 2013, COMPUT GRAPH-UK, V37, P293, DOI 10.1016/j.cag.2013.01.005
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6578, DOI 10.1109/ICCV48922.2021.00653
   Liu X.-C., 2017, P S NONPH AN REND, P1
   Liu XC, 2021, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR46437.2021.00370
   Lu JW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185542
   Lüddecke T, 2022, PROC CVPR IEEE, P7076, DOI 10.1109/CVPR52688.2022.00695
   Mo HR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459833
   Mould D., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P20
   Putri T, 2017, INT CONF IMAG VIS
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Radford A, 2021, PR MACH LEARN RES, V139
   Salisbury M. P., 1994, P 21 ANN C COMP GRAP, P393
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Schaldenbrand P, 2023, IEEE INT CONF ROBOT, P11712, DOI 10.1109/ICRA48891.2023.10160702
   Schaldenbrand P, 2021, AAAI CONF ARTIF INTE, V35, P505
   Secord A., 2002, P 2 INT S NONPHOTORE, P37, DOI [DOI 10.1145/508530.5085372, DOI 10.1145/508535.508537]
   Shaheen S, 2016, COMPUT GRAPH FORUM, V35, P73, DOI 10.1111/cgf.12733
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh J, 2022, LECT NOTES COMPUT SC, V13676, P685, DOI 10.1007/978-3-031-19787-1_39
   Singh J, 2021, PROC CVPR IEEE, P16382, DOI 10.1109/CVPR46437.2021.01612
   Song YZ, 2013, IEEE T VIS COMPUT GR, V19, P1252, DOI 10.1109/TVCG.2013.13
   Tong ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1035, DOI 10.1145/3503161.3547759
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Vinker Y, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530068
   Wang MY, 2014, IEEE T VIS COMPUT GR, V20, P1451, DOI 10.1109/TVCG.2014.2303984
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winkenbach G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P91, DOI 10.1145/192161.192184
   Yan CR, 2008, IEEE T VIS COMPUT GR, V14, P468, DOI 10.1109/TVCG.2007.70440
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zang Y, 2013, J COMPUT SCI TECH-CH, V28, P762, DOI 10.1007/s11390-013-1375-8
   Zhao HH, 2020, VISUAL COMPUT, V36, P1307, DOI 10.1007/s00371-019-01726-2
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou ZX, 2021, PROC CVPR IEEE, P15684, DOI 10.1109/CVPR46437.2021.01543
NR 84
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6309
EP 6320
DI 10.1109/TVCG.2023.3332950
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000032
PM 37966930
DA 2024-11-06
ER

PT J
AU Ruan, SL
   Guan, Q
   Griffin, P
   Mao, Y
   Wang, Y
AF Ruan, Shaolun
   Guan, Qiang
   Griffin, Paul
   Mao, Ying
   Wang, Yong
TI <i>QuantumEyes</i>: Towards Better Interpretability of Quantum Circuits
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Quantum computing; Visualization; Quantum circuit; Quantum state; Logic
   gates; Qubit; Quantum algorithm; Data visualization; interpretability;
   quantum circuits; quantum computing
ID DISCRETE LOGARITHMS; TRIANGLE GEOMETRY; ALGORITHMS; VISUALIZATION;
   REPRESENTATION; DESIGN; STATE; 2D; 3D
AB Quantum computing offers significant speedup compared to classical computing, which has led to a growing interest among users in learning and applying quantum computing across various applications. However, quantum circuits, which are fundamental for implementing quantum algorithms, can be challenging for users to understand due to their underlying logic, such as the temporal evolution of quantum states and the effect of quantum amplitudes on the probability of basis quantum states. To fill this research gap, we propose QuantumEyes, an interactive visual analytics system to enhance the interpretability of quantum circuits through both global and local levels. For the global-level analysis, we present three coupled visualizations to delineate the changes of quantum states and the underlying reasons: a Probability Summary View to overview the probability evolution of quantum states; a State Evolution View to enable an in-depth analysis of the influence of quantum gates on the quantum states; a Gate Explanation View to show the individual qubit states and facilitate a better understanding of the effect of quantum gates. For the local-level analysis, we design a novel geometrical visualization dandelion chart to explicitly reveal how the quantum amplitudes affect the probability of the quantum state. We thoroughly evaluated QuantumEyes as well as the novel dandelion chart integrated into it through two case studies on different types of quantum algorithms and in-depth expert interviews with 12 domain experts. The results demonstrate the effectiveness and usability of our approach in enhancing the interpretability of quantum circuits.
C1 [Ruan, Shaolun; Griffin, Paul; Wang, Yong] Singapore Management Univ, Singapore 178902, Singapore.
   [Guan, Qiang] Kent State Univ, Kent, OH 44240 USA.
   [Mao, Ying] Fordham Univ, New York, NY 10458 USA.
C3 Singapore Management University; University System of Ohio; Kent State
   University; Kent State University Kent; Kent State University Salem;
   Fordham University
RP Wang, Y (corresponding author), Singapore Management Univ, Singapore 178902, Singapore.
EM slruan.2021@phdcs.smu.edu.sg; qguan@kent.edu; paulgriffin@smu.edu.sg;
   ymao41@fordham.edu; yongwang@smu.edu.sg
RI GRIFFIN, Paul/L-9582-2016; Griffin, Paul/L-4696-2014
OI Griffin, Paul/0000-0002-1656-421X; Ruan, Shaolun/0000-0002-6163-9786;
   Mao, Ying/0000-0002-4484-4892; Griffin, Paul/0000-0003-2294-5980
FU Lee Kong Chian Fellowship by Singapore Management University
FX This work was supported by Lee Kong Chian Fellowship awarded to Yong
   Wang by Singapore Management University.
CR Aharonov D., 2003, A simple proof that toffoli and hadamard are quantum universal
   Altepeter J. B., 2009, VOLS, V1-5, P2395
   Amini F, 2015, IEEE T VIS COMPUT GR, V21, P122, DOI 10.1109/TVCG.2014.2329308
   [Anonymous], 2021, Quantum fourier transform
   [Anonymous], 2016, Quirk
   [Anonymous], IBM Qiskit
   [Anonymous], Q-Sphere
   [Anonymous], Google Cirq
   [Anonymous], 2017, Qiskit
   [Anonymous], 2021, Quantum computing statistics: Benefits and negative effects
   [Anonymous], 2022, Torchquantum
   [Anonymous], 2023, Quantum code visualizer
   [Anonymous], Lattice surgery
   Arute F, 2019, NATURE, V574, P505, DOI 10.1038/s41586-019-1666-5
   Ash-Saki A, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317888
   Bardin JC, 2021, IEEE J MICROW, V1, P403, DOI 10.1109/JMW.2020.3034071
   Bengtsson K., Geometry of Quantum States: An Intro-duction to Quantum Entanglement
   Biamonte J, 2017, NATURE, V549, P195, DOI 10.1038/nature23474
   BLOCH F, 1946, PHYS REV, V70, P460, DOI 10.1103/PhysRev.70.460
   Buluta I, 2009, SCIENCE, V326, P108, DOI 10.1126/science.1177838
   Cambridge U.K., 2017, Visualizing 2-qubit entanglement
   Castelvecchi D, 2017, NATURE, V543, P159, DOI 10.1038/nature.2017.21585
   Chernega VN, 2019, EUR PHYS J D, V73, DOI 10.1140/epjd/e2018-90487-9
   Chernega VN, 2017, J RUSS LASER RES, V38, P416, DOI 10.1007/s10946-017-9662-4
   Chernega VN, 2017, J RUSS LASER RES, V38, P141, DOI 10.1007/s10946-017-9628-6
   CLEVELAND WS, 1987, J ROY STAT SOC A STA, V150, P192, DOI 10.2307/2981473
   Coppersmith D., 2002, An approximate fourier transform useful in quantum factoring
   DEUTSCH D, 1985, P ROY SOC LOND A MAT, V400, P97, DOI 10.1098/rspa.1985.0070
   FEYNMAN RP, 1986, FOUND PHYS, V16, P507, DOI 10.1007/BF01886518
   Galambos M., 2012, Int. J. Adv. Syst. Meas., V5
   Grover L. K., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P212, DOI 10.1145/237814.237866
   Hallgren S, 2007, J ACM, V54, DOI 10.1145/1206035.1206039
   Hassija V, 2020, IET QUANTUM COMMUN, V1, P42, DOI 10.1049/iet-qtc.2020.0027
   Hey T, 1999, COMPUT CONTROL ENG J, V10, P105, DOI 10.1049/cce:19990303
   Karafyllidis IG, 2003, QUANTUM INF PROCESS, V2, P271, DOI 10.1023/B:QINP.0000020076.36114.13
   Lamy JB, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL 3: IVAPP, P155, DOI 10.5220/0007247801550163
   Li A, 2023, ACM T QUANTUM COMPUT, V4, DOI 10.1145/3550488
   Lin SY, 2018, 2018 IEEE SCIENTIFIC VISUALIZATION CONFERENCE (SCIVIS), P37, DOI 10.1109/SciVis.2018.8823602
   Mäkelä H, 2010, PHYS SCRIPTA, VT140, DOI 10.1088/0031-8949/2010/T140/014054
   Miller M, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON QUANTUM COMPUTING AND ENGINEERING (QCE 2021) / QUANTUM WEEK 2021, P378, DOI 10.1109/QCE52317.2021.00057
   Munzner T, 2009, IEEE T VIS COMPUT GR, V15, P921, DOI 10.1109/TVCG.2009.111
   Narayanan A., 1997, P IEE C QUANT COMP T, P1
   Orus R., 2019, REV PHYS, V4, DOI DOI 10.1016/J.REVIP.2019.100028
   Piattini M., 2020, QANSWER 20 QUANTUM S, P23
   Politi A, 2009, SCIENCE, V325, P1221, DOI 10.1126/science.1173731
   Preskill J, 2018, QUANTUM-AUSTRIA, V2, DOI 10.22331/q-2018-08-06-79
   Ruan Shaolun, 2023, IEEE Transactions on Visualization and Computer Graphics, V29, P462, DOI 10.1109/TVCG.2022.3209455
   Ruan SL, 2023, COMPUT GRAPH FORUM, V42, P247, DOI 10.1111/cgf.14827
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   SHOR PW, 1994, AN S FDN CO, P124
   Shor PW, 1997, SIAM J COMPUT, V26, P1484, DOI 10.1137/S0036144598347011
   Steane A, 1998, REP PROG PHYS, V61, P117, DOI 10.1088/0034-4885/61/2/002
   Tao ZW, 2017, I C VIRTUAL REALITY, P360, DOI 10.1109/ICVRV.2017.00082
   Tory M, 2006, IEEE T VIS COMPUT GR, V12, P2, DOI 10.1109/TVCG.2006.17
   Toyama FM, 2013, QUANTUM INF PROCESS, V12, P1897, DOI 10.1007/s11128-012-0498-0
   Valiev KA, 2005, PHYS-USP+, V48, P1, DOI 10.1070/PU2005v048n01ABEH002024
   Van Dam W, 2006, SIAM J COMPUT, V36, P763, DOI 10.1137/S009753970343141X
   van de Wetering J., 2020, arXiv
   Wen Z, 2024, IEEE T VIS COMPUT GR, V30, P573, DOI 10.1109/TVCG.2023.3327148
   Wie CR, 2019, Arxiv, DOI arXiv:1403.8069
   Wie CR, 2020, PHYSICS-BASEL, V2, P383, DOI 10.3390/physics2030021
   Wille R, 2021, PROCEEDINGS OF THE 2021 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION (DATE 2021), P768, DOI 10.23919/DATE51398.2021.9474236
   Williams M. M., 2021, PhD thesis,
   YAO ACC, 1993, AN S FDN CO, P352
NR 64
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6321
EP 6333
DI 10.1109/TVCG.2023.3332999
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000011
PM 37966932
OA Green Published, Green Accepted
DA 2024-11-06
ER

PT J
AU Hussain, R
   Chessa, M
   Solari, F
AF Hussain, Razeen
   Chessa, Manuela
   Solari, Fabio
TI Improving Depth Perception in Immersive Media Devices by Addressing
   Vergence-Accommodation Conflict
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Media; Visualization; Convergence; Three-dimensional displays; Rendering
   (computer graphics); Lenses; Deconvolution; Depth perception;
   depth-of-field; immersive media; inverse blurring; reaching task;
   space-variant technique; vergence-accommodation conflict; virtual
   reality; wiener deconvolution
ID GAZE-CONTINGENT; REALITY; TARGET; VIDEO
AB Recently, immersive media devices have seen a boost in popularity. However, many problems still remain. Depth perception is a crucial part of how humans behave and interact with their environment. Convergence and accommodation are two physiological mechanisms that provide important depth cues. However, when humans are immersed in virtual environments, they experience a mismatch between these cues. This mismatch causes users to feel discomfort while also hindering their ability to fully perceive object distances. To address the conflict, we have developed a technique that encompasses inverse blurring into immersive media devices. For the inverse blurring, we utilize the classical Wiener deconvolution approach by proposing a novel technique that is applied without the need for an eye-tracker and implemented in a commercial immersive media device. The technique's ability to compensate for the vergence-accommodation conflict was verified through two user studies aimed at reaching and spatial awareness, respectively. The two studies yielded a statistically significant 36% and 48% error reduction in user performance to estimate distances, respectively. Overall, the work done demonstrates how visual stimuli can be modified to allow users to achieve a more natural perception and interaction with the virtual environment.
C1 [Hussain, Razeen; Chessa, Manuela; Solari, Fabio] Univ Genoa, Dept Informat Bioengn Robot & Syst Engn, I-16126 Genoa, Italy.
C3 University of Genoa
RP Solari, F (corresponding author), Univ Genoa, Dept Informat Bioengn Robot & Syst Engn, I-16126 Genoa, Italy.
EM razeen.hussain@edu.unige.it; manuela.chessa@unige.it;
   fabio.solari@unige.it
RI Solari, Fabio/O-4729-2016; Chessa, Manuela/O-4628-2016; Hussain,
   Razeen/JZT-3081-2024
OI Solari, Fabio/0000-0002-8111-0409; Chessa, Manuela/0000-0003-3098-5894;
   Hussain, Razeen/0000-0002-7579-5069
FU Interreg Alcotra projects PRO-SOL We-Pro [4298]; CLIP E-Sante [4793]
FX This work was supported by Interreg Alcotra projects PRO-SOL We-Pro (n.
   4298) and CLIP E-Sante (n. 4793).
CR Akeley K, 2004, ACM T GRAPHIC, V23, P804, DOI 10.1145/1015706.1015804
   Aksit K, 2019, IEEE T VIS COMPUT GR, V25, P1928, DOI 10.1109/TVCG.2019.2898781
   Aksit K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130892
   [Anonymous], 2014, P ACM S APPL PERC NE
   Arefin MS, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P731, DOI 10.1109/VRW52623.2021.00248
   Ballestin G, 2021, IEEE ACCESS, V9, P64828, DOI 10.1109/ACCESS.2021.3075780
   Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700
   Bos P.J., 2016, SID Digest, P354
   Brown M., 1963, P IEEE COMP SOC C CO, P1956
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Chakravarthula P, 2021, IEEE T VIS COMPUT GR, V27, P4194, DOI 10.1109/TVCG.2021.3106433
   Chakravarthula P, 2018, IEEE T VIS COMPUT GR, V24, P2906, DOI 10.1109/TVCG.2018.2868532
   Cholewiak SA, 2018, J VISION, V18, DOI 10.1167/18.9.1
   Cholewiak SA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130815
   Cook T, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P394, DOI 10.1109/VR.2018.8446058
   Cutting J. E., 1995, Perception of space and motion, P69, DOI DOI 10.1016/B978-012240530-3/50005-5
   Daum SO, 2009, ATTEN PERCEPT PSYCHO, V71, P1127, DOI 10.3758/APP.71.5.1127
   Davis T., 2008, Confidence region radius
   Day M, 2009, J VISION, V9, DOI 10.1167/9.10.5
   Diez PS, 2019, VISION RES, V164, P62, DOI 10.1016/j.visres.2019.07.005
   Ebner C, 2022, IEEE T VIS COMPUT GR, V28, P2256, DOI 10.1109/TVCG.2022.3150504
   Efron B., 1994, An Introduction to the Bootstrap, DOI 10.1007/978-1-4899-4541-9
   Gerschutz B., 2019, P DES SOC INT C ENG, P1893, DOI [10.1017/dsi.2019.195, DOI 10.1017/DSI.2019.195]
   González R, 2016, PROC IEEE-PES
   Güzel AH, 2023, BIOMED OPT EXPRESS, V14, DOI 10.1364/BOE.485776
   Held RT, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731057
   Heron G, 2001, VISION RES, V41, P507, DOI 10.1016/S0042-6989(00)00282-0
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Hussain Razeen, 2020, 2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS), P71, DOI 10.1109/IPAS50080.2020.9334947
   Hussain R, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21124006
   Iwai D, 2015, IEEE T VIS COMPUT GR, V21, P462, DOI 10.1109/TVCG.2015.2391861
   Kimura Y, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P695, DOI [10.1109/VRW50115.2020.00198, 10.1109/VRW50115.2020.00-77]
   Kleder M., 2004, SEP - an algorithm for converting covariance to spherical error probable
   Klein E, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P107, DOI 10.1109/VR.2009.4811007
   Konrad Robert, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073594
   Kramida G, 2016, IEEE T VIS COMPUT GR, V22, P1912, DOI 10.1109/TVCG.2015.2473855
   Kress BC, 2017, PROC SPIE, V10335, DOI 10.1117/12.2270017
   Laffont PY, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 EMERGING TECHNOLOGIES, DOI 10.1145/3214907.3214925
   Lanman D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508366
   Maiello G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0140230
   Maiello G, 2014, J VISION, V14, DOI 10.1167/14.8.13
   Mantiuk RK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459831
   March J, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555405
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   O'Haver ThomasC., 2016, PRAGMATIC INTRO SIGN
   Oshima K, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P173, DOI 10.1109/3DUI.2016.7460049
   Padmanaban N, 2017, P NATL ACAD SCI USA, V114, P2183, DOI 10.1073/pnas.1617251114
   Reichelt S, 2010, PROC SPIE, V7690, DOI 10.1117/12.850094
   Ren DW, 2020, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR42600.2020.00340
   Renner RS, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2724716
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Rompapas Damien Constantine, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040022
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Starck JL, 2002, PUBL ASTRON SOC PAC, V114, P1051, DOI 10.1086/342606
   TUCKER J, 1979, AM J OPTOM PHYS OPT, V56, P490
   Vera-Diaz FA, 2010, VISION RES, V50, P1452, DOI 10.1016/j.visres.2010.04.013
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watt SJ, 2005, J VISION, V5, P834, DOI 10.1167/5.10.7
   Weast RAT, 2018, CONSCIOUS COGN, V64, P121, DOI 10.1016/j.concog.2018.02.013
   Wickens T. D., 2001, Elementary signal detection theory
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Xu JJ, 2015, VISION RES, V115, P1, DOI 10.1016/j.visres.2015.05.020
   Zandbergen P. A., 2008, Transactions in GIS, V12, P103, DOI 10.1111/j.1467-9671.2008.01088.x
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang L, 2006, ACM T GRAPHIC, V25, P907, DOI 10.1145/1141911.1141974
NR 68
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6334
EP 6346
DI 10.1109/TVCG.2023.3331902
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000010
PM 37956016
OA Green Submitted, Bronze
DA 2024-11-06
ER

PT J
AU Kostic, Z
   Dumas, C
   Pratt, S
   Beyer, J
AF Kostic, Zona
   Dumas, Catherine
   Pratt, Sarah
   Beyer, Johanna
TI Exploring Mid-Air Hand Interaction in Data Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Space exploration; Instruments;
   Vocabulary; Faces; Three-dimensional displays; Mid-air hand gestures;
   touchless interaction; embedded interaction; design space; elicitation
   study; user study; expert study; data visualization
ID INFORMATION VISUALIZATION; DATA EXPLORATION; MANIPULATION; GESTURES;
   SPEECH; TOUCH; MODEL
AB Interacting with data visualizations without an instrument or touch surface is typically characterized by the use of mid-air hand gestures. While mid-air expressions can be quite intuitive for interacting with digital content at a distance, they frequently lack precision and necessitate a different way of expressing users' data-related intentions. In this work, we aim to identify new designs for mid-air hand gesture manipulations that can facilitate instrument-free, touch-free, and embedded interactions with visualizations, while utilizing the three-dimensional (3D) interaction space that mid-air gestures afford. We explore mid-air hand gestures for data visualization by searching for natural means to interact with content. We employ three studies-an Elicitation Study, a User Study, and an Expert Study, to provide insight into the users' mental models, explore the design space, and suggest considerations for future mid-air hand gesture design. In addition to forming strong associations with physical manipulations, we discovered that mid-air hand gestures can: promote space-multiplexed interaction, which allows for a greater degree of expression; play a functional role in visual cognition and comprehension; and enhance creativity and engagement. We further highlight the challenges that designers in this field may face to help set the stage for developing effective gestures for a wide range of touchless interactions with visualizations.
C1 [Kostic, Zona; Beyer, Johanna] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Dumas, Catherine; Pratt, Sarah] Simmons Univ, Sch Lib & Informat Sci, Boston, MA 02115 USA.
   [Dumas, Catherine; Pratt, Sarah] SUNY Albany, Albany, NY 12207 USA.
C3 Harvard University; Simmons University; State University of New York
   (SUNY) System; University at Albany, SUNY
RP Kostic, Z (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM zona@ieee.org; catherine.dumas2@simmons.edu; sarah.pratt@simmons.edu;
   jbeyer@seas.harvard.edu
OI Beyer, Johanna/0000-0002-3505-9171; Kostic, Zona/0000-0003-1757-6260
CR Ackad C, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1227, DOI 10.1145/2750858.2807532
   Aigner Roland, 2012, Microsoft Research TechReport MSR-TR-2012-111 2, MSR-TR-2012-111, V2, P30
   Anderson F, 2013, P SIGCHI C HUM FACT, P1109, DOI DOI 10.1145/2470654.2466143
   [Anonymous], 2005, Proc. of the 18th Annual ACM Symposium on User interface Software and Technology (UIST'05), DOI [10.1145/1095034.1095041, DOI 10.1145/1095034.1095041]
   Aromaa Susanna, 2018, Proceedings of the ACM on Human-Computer Interaction, V2, DOI 10.1145/3229088
   Arslan C, 2019, PROCEEDINGS OF THE 2019 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2019), P1101, DOI 10.1145/3322276.3322284
   Aslan I, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P135, DOI 10.1145/3242969.3242979
   Austin CR, 2020, CARTOGR GEOGR INF SC, V47, P214, DOI 10.1080/15230406.2019.1696232
   Avery Jeff., 2014, P 27 ANN ACM S USER, P595, DOI DOI 10.1145/2642918.2647352
   Baur D., 2012, Proceedings of the 2012 ACM international conference on Interactive tabletops and surfaces, P255, DOI DOI 10.1145/2396636.2396675
   Besançon L, 2021, COMPUT GRAPH FORUM, V40, P293, DOI 10.1111/cgf.14189
   Bilius LB, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580929
   Blascheck T, 2019, IEEE T VIS COMPUT GR, V25, P630, DOI 10.1109/TVCG.2018.2865142
   Block F., 2010, P 6 ACM NORD C HUM C, P603, DOI [10.1145/1868914.1868984, DOI 10.1145/1868914.1868984]
   Brasier E, 2020, INT SYM MIX AUGMENT, P332, DOI 10.1109/ISMAR50242.2020.00060
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Cabreira AT, 2015, BRITISH HCI 2015, P257, DOI 10.1145/2783446.2783599
   Cafaro F, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174167
   Chan E, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3403, DOI 10.1145/2858036.2858589
   Chen X.A., 2014, P 27 ANN ACM S USER, P519, DOI DOI 10.1145/2642918.2647392
   Claes Sandy., 2015, Proceedings of the 4th International Symposium on Pervasive Displays (PerDis'15), P201, DOI DOI 10.1145/2757710.2757733
   Cordeil M, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P71, DOI 10.1145/3126594.3126613
   Courtoux E, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445404
   Danielescu A, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501962
   Dimara E, 2022, IEEE T VIS COMPUT GR, V28, P1128, DOI 10.1109/TVCG.2021.3114813
   Dimara E, 2020, IEEE T VIS COMPUT GR, V26, P119, DOI 10.1109/TVCG.2019.2934283
   Dourish P., 2001, Where the Action is: The Foundations of Embodied In- teraction, DOI [10.7551/mit-press/7221.001.0001, DOI 10.7551/MIT-PRESS/7221.001.0001]
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Freeman D, 2012, 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT HUMAN COMPUTER INTERACTION (IHCI 2012)
   Gallo L., 2013, P SIGGRAPH AS TECH B, P1, DOI [10.1145/2542355.2542390, DOI 10.1145/2542355.2542390]
   Grandhi SA, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P821
   Han J, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ACM ISS 2017), P210, DOI 10.1145/3132272.3134134
   Hegde S, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P262, DOI [10.1109/ISMAR-Adjunct.2016.0090, 10.1109/ISMAR-Adjunct.2016.81]
   Hoffmann F, 2019, MUM 2019: 18TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA, DOI 10.1145/3365610.3365624
   Hosseini M, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581420
   Huang YJ, 2017, IEEE PAC VIS SYMP, P41, DOI 10.1109/PACIFICVIS.2017.8031577
   Isenberg T., 2012, P CHI WORKSH 3 DIM C, P53
   Jakobsen MR, 2013, IEEE T VIS COMPUT GR, V19, P2386, DOI 10.1109/TVCG.2013.166
   Jang J, 2020, IEEE T HAPTICS, V13, P116, DOI 10.1109/TOH.2020.2966605
   Jansen Y., 2014, PhD dissertation,
   Jansen Y, 2013, IEEE T VIS COMPUT GR, V19, P2396, DOI 10.1109/TVCG.2013.134
   Karam, 2005, TAXONOMY GESTURES HU
   Kim JS, 2007, ELEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P15
   Kondo B, 2014, IEEE T VIS COMPUT GR, V20, P2003, DOI 10.1109/TVCG.2014.2346250
   Kopsel Anne, 2015, ACM Interactions, V22, P44, DOI 10.1145/2803169
   Lee B, 2012, IEEE T VIS COMPUT GR, V18, P2689, DOI 10.1109/TVCG.2012.204
   Lee Jinha., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '13, P189
   Lee T, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P145
   Liu ZC, 2010, IEEE T VIS COMPUT GR, V16, P999, DOI 10.1109/TVCG.2010.177
   Lu W, 2016, IEEE SIGNAL PROC LET, V23, P1188, DOI 10.1109/LSP.2016.2590470
   McNeill D., 1992, HAND MIND WHAT GESTU, DOI [DOI 10.2307/1576015, 10.2307/1576015]
   Moghaddam ArasBalali., 2011, Proceedings of the 2011 Annual Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA'11, P2347, DOI [10.1145/1979742.1979934, DOI 10.1145/1979742.1979934]
   Morris Meredith Ringel, 2014, Interactions, V21, P40, DOI DOI 10.1145/2591689
   Morsella E, 2004, AM J PSYCHOL, V117, P411, DOI 10.2307/4149008
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Nancel M, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P177
   Nielsen M, 2003, LECT NOTES ARTIF INT, V2915, P409
   Norman Don, 2010, Interactions, V17, DOI 10.1145/1836216.1836228
   Norman D. A., 1986, User Centered System Design: New Per- spectives on Human-Computer Interaction, DOI DOI 10.5555/576915
   Perin Charles., 2014, Direct Manipulation for Information Visualization
   Pham T, 2018, DIS 2018: PROCEEDINGS OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P227, DOI 10.1145/3196709.3196719
   Pousman Z., 2006, P WORK C ADV VIS INT, P67, DOI DOI 10.1145/1133265.1133277
   Ren G, 2013, IEEE COMPUT GRAPH, V33, P47, DOI 10.1109/MCG.2013.15
   Rzeszotarski JM, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P897, DOI 10.1145/2556288.2557231
   Sabir K, 2013, 2013 IEEE SYMPOSIUM ON BIOLOGICAL DATA VISUALIZATION (BIOVIS), P49, DOI 10.1109/BioVis.2013.6664346
   Saket B, 2020, IEEE T VIS COMPUT GR, V26, P482, DOI 10.1109/TVCG.2019.2934534
   Saket B, 2018, IEEE T VIS COMPUT GR, V24, P1316, DOI 10.1109/TVCG.2017.2680452
   Saket B, 2017, IEEE T VIS COMPUT GR, V23, P331, DOI 10.1109/TVCG.2016.2598839
   Saktheeswaran A, 2020, IEEE T VIS COMPUT GR, V26, P2168, DOI 10.1109/TVCG.2020.2970512
   Satriadi KA, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P593, DOI [10.1109/vr.2019.8798340, 10.1109/VR.2019.8798340]
   Schmidt S., 2010, Proc. ACM Conference on Interactive Tabletops and Surfaces (ITS), P113, DOI DOI 10.1145/1936652.1936673
   Schön D, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581461
   Seyed Teddy, 2012, P 2012 ACM INT C INT, P41, DOI [10.1145/2396636.2396643, DOI 10.1145/2396636.2396643]
   Shimon SSA, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3822, DOI 10.1145/2858036.2858385
   Spittle B, 2023, IEEE T VIS COMPUT GR, V29, P3900, DOI 10.1109/TVCG.2022.3174805
   Stachl T., 2021, Elicitation and evaluation of mid-air hand gestures for global earth observation data presented on large public displays, DOI [10.34726/hss.2021.90363, DOI 10.34726/HSS.2021.90363]
   Taher F, 2017, IEEE T VIS COMPUT GR, V23, P451, DOI 10.1109/TVCG.2016.2598498
   Thompson J, 2018, AVI'18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, DOI 10.1145/3206505.3206519
   Tuddenham P, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2223
   van de Camp Florian, 2013, Distributed, Ambient, and Pervasive Interactions. First International Conference, DAPI 2013. Held as Part of HCI International 2013. Proceedings: LNCS 8028, P78, DOI 10.1007/978-3-642-39351-8_9
   Vatavu RD, 2022, ACM T COMPUT-HUM INT, V29, DOI 10.1145/3476101
   Vatavu RD, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1325, DOI 10.1145/2702123.2702223
   Veras Rafael, 2021, P 2021 CHI C HUM FAC, DOI 10.1145/3411764.3445546
   Villarreal-Narvaez S, 2020, PROCEEDINGS OF THE 2020 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2020), P855, DOI 10.1145/3357236.3395511
   Vogiatzidakis Panagiotis, 2018, Multimodal Technologies and Interaction, V2, DOI 10.3390/mti2040065
   Vuletic T, 2019, INT J HUM-COMPUT ST, V129, P74, DOI 10.1016/j.ijhcs.2019.03.011
   Wagner J, 2021, IEEE T VIS COMPUT GR, V27, P2513, DOI 10.1109/TVCG.2021.3067759
   Walny J, 2012, IEEE T VIS COMPUT GR, V18, P2779, DOI 10.1109/TVCG.2012.275
   Willett W., 2014, P EUR C VIS, DOI [10.2312/eurovisshort.20141161, DOI 10.2312/EUROVISSHORT.20141161]
   Williams AS, 2020, IEEE T VIS COMPUT GR, V26, P3479, DOI 10.1109/TVCG.2020.3023566
   Wobbrock J.O., 2005, CHI 05 EXTENDED ABST, DOI [DOI 10.1145/1056808.1057043, 10.1145/1056808.1057043]
   Wobbrock JO, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1083
   Xia HJ, 2022, ACM T COMPUT-HUM INT, V29, DOI 10.1145/3503537
   Xu W, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P204, DOI [10.1109/VR46266.2020.1581268453415, 10.1109/VR46266.2020.00-65]
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
NR 95
TC 0
Z9 0
U1 8
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6347
EP 6364
DI 10.1109/TVCG.2023.3332647
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000020
PM 37963001
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Boorboor, S
   Kim, Y
   Hu, P
   Moses, JM
   Colle, BA
   Kaufman, AE
AF Boorboor, Saeed
   Kim, Yoonsang
   Hu, Ping
   Moses, Josef M.
   Colle, Brian A.
   Kaufman, Arie E.
TI Submerse: Visualizing Storm Surge Flooding Simulations in Immersive
   Display Ecologies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Floods; Data visualization; Three-dimensional displays; Cameras;
   Rendering (computer graphics); Data models; Solid modeling; Camera
   navigation; flooding simulation; immersive visualization; mixed reality
ID INFORMATION VISUALIZATION; REALITY; LEVEL
AB We present Submerse, an end-to-end framework for visualizing flooding scenarios on large and immersive display ecologies. Specifically, we reconstruct a surface mesh from input flood simulation data and generate a to-scale 3D virtual scene by incorporating geographical data such as terrain, textures, buildings, and additional scene objects. To optimize computation and memory performance for large simulation datasets, we discretize the data on an adaptive grid using dynamic quadtrees and support level-of-detail based rendering. Moreover, to provide a perception of flooding direction for a time instance, we animate the surface mesh by synthesizing water waves. As interaction is key for effective decision-making and analysis, we introduce two novel techniques for flood visualization in immersive systems: (1) an automatic scene-navigation method using optimal camera viewpoints generated for marked points-of-interest based on the display layout, and (2) an AR-based focus+context technique using an aux display system. Submerse is developed in collaboration between computer scientists and atmospheric scientists. We evaluate the effectiveness of our system and application by conducting workshops with emergency managers, domain experts, and concerned stakeholders in the Stony Brook Reality Deck, an immersive gigapixel facility, to visualize a superstorm flooding scenario in New York City.
C1 [Boorboor, Saeed; Kim, Yoonsang; Hu, Ping; Kaufman, Arie E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Moses, Josef M.; Colle, Brian A.] SUNY Stony Brook, Sch Marine & Atmospher Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   State University of New York (SUNY) System; Stony Brook University
RP Boorboor, S (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM sboorboor@cs.stonybrook.edu; yoonsakim@cs.stonybrook.edu;
   pihu@cs.stonybrook.edu; josef.moses@stonybrook.edu;
   brian.colle@stonybrook.edu; ari@cs.stonybrook.edu
RI Moses, Josef/JOZ-5319-2023; Boorboor, Saeed/ABI-7739-2020
OI Kim, Yoonsang/0009-0006-2341-3862; Moses, Josef/0000-0001-5177-6085;
   Colle, Brian/0000-0002-4546-5472; Boorboor, Saeed/0000-0001-6644-5983
FU NSF [OAC1919752, ICER1940302, IIS2107224]
FX This work was supported in part by NSF Grants OAC1919752, ICER1940302,
   and IIS2107224.
CR Andrews C, 2011, INFORM VISUAL, V10, P341, DOI 10.1177/1473871611415997
   Arbel T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P248, DOI 10.1109/ICCV.1999.791227
   Ball R, 2007, COMPUT GRAPH-UK, V31, P380, DOI 10.1016/j.cag.2007.01.029
   Baricevic D, 2012, INT SYM MIX AUGMENT, P197, DOI 10.1109/ISMAR.2012.6402557
   Beaudouin-Lafon M, 2011, IHM'11: 23EME CONFERENCE FRANCOPHONE SUR L'INTERACTION HOMME-MACHINE
   Bonaventura X, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20050370
   Colle B. A., 2023, RISK PERC PREP STORM
   Colle BA, 2015, J MAR SCI ENG, V3, P428, DOI 10.3390/jmse3020428
   Cornel D, 2019, COMPUT GRAPH FORUM, V38, P25, DOI 10.1111/cgf.13669
   Cornel D, 2015, COMPUT GRAPH FORUM, V34, P331, DOI 10.1111/cgf.12645
   Davis J, 2002, DISPLAYS, V23, P205, DOI 10.1016/S0141-9382(02)00039-2
   emdat, DAT: The international disasters database
   Feixas M, 1999, COMPUT GRAPH FORUM, V18, pC95, DOI 10.1111/1467-8659.00331
   Field CB, 2014, CLIMATE CHANGE 2014: IMPACTS, ADAPTATION, AND VULNERABILITY, PT A: GLOBAL AND SECTORAL ASPECTS, P1
   Ganser C, 2006, P IEEE VIRT REAL ANN, P253, DOI 10.1109/VR.2006.86
   github, Unity, "WebRTC for unity
   Hill A., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P239, DOI 10.1109/ISMAR.2011.6092395
   Hinsinger D., 2002, P 2002 ACM SIGGRAPH, P161, DOI DOI 10.1145/545261.545288
   Hu P, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356490
   Huang H, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.13008
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kim B, 2009, PROC SPIE, V7248, DOI 10.1117/12.810965
   Laha B, 2012, IEEE T VIS COMPUT GR, V18, P597, DOI 10.1109/TVCG.2012.42
   Langner R., 2018, IEEE VIS
   Langner R, 2019, IEEE T VIS COMPUT GR, V25, P608, DOI 10.1109/TVCG.2018.2865235
   Leedal D, 2010, J FLOOD RISK MANAG, V3, P140, DOI 10.1111/j.1753-318X.2010.01063.x
   Leskens JG, 2017, MITIG ADAPT STRAT GL, V22, P307, DOI 10.1007/s11027-015-9651-2
   Liang QH, 2011, INT J NUMER METH FL, V66, P537, DOI 10.1002/fld.2266
   Lin LQ, 2022, LECT NOTES COMPUT SC, V13668, P93, DOI 10.1007/978-3-031-20074-8_6
   Malik S., 2005, P 18 ANN ACM S US IN, DOI [10.1145/1095034.1095042, DOI 10.1145/1095034.1095042]
   Mol JM, 2022, JUDGM DECIS MAK, V17, P189
   Morris M. R., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1201
   MURPHY AH, 1993, WEATHER FORECAST, V8, P281, DOI 10.1175/1520-0434(1993)008<0281:WIAGFA>2.0.CO;2
   National Oceanic and Atmospheric Administration, COASTAL FLOOD VIEWER
   Nishimoto A, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357579
   NYC Planning, BYTES of the BIG APPLE
   Open street maps, Open StreetMap
   Oyshi M. T., 2022, P EG WORKSH VIS ENV, P27
   Papadopoulos C, 2015, IEEE COMPUT GRAPH, V35, P33, DOI 10.1109/MCG.2014.80
   Reipschlager P, 2021, IEEE T VIS COMPUT GR, V27, P1182, DOI 10.1109/TVCG.2020.3030460
   Siddhpuria S, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173747
   Stoev SL, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P545, DOI 10.1109/VISUAL.2002.1183826
   Thon S, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P65, DOI 10.1109/CGI.2000.852321
   Unity Technologies, Unity3D
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Vuckovic M, 2022, INFORMATION, V13, DOI 10.3390/info13010007
   Wobbrock JO, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1083
NR 48
TC 3
Z9 3
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6365
EP 6377
DI 10.1109/TVCG.2023.3332511
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000044
PM 37966931
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhao, GH
   Orlosky, J
   Gabbard, J
   Kiyokawa, K
AF Zhao, Guanghan
   Orlosky, Jason
   Gabbard, Joseph
   Kiyokawa, Kiyoshi
TI HazARdSnap: Gazed-Based Augmentation Delivery for Safe Information
   Access While Cycling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hazards; Pedestrians; Roads; Resists; Computer vision; Real-time
   systems; Navigation; Augmented reality; cycling; eye tracking; object
   detection and user interfaces; safety
ID BICYCLE; BEHAVIOR; REALITY
AB During cycling activities, cyclists often monitor a variety of information such as heart rate, distance, and navigation using a bike-mounted phone or cyclocomputer. In many cases, cyclists also ride on sidewalks or paths that contain pedestrians and other obstructions such as potholes, so monitoring information on a bike-mounted interface can slow the cyclist down or cause accidents and injury. In this article, we present HazARdSnap, an augmented reality-based information delivery approach that improves the ease of access to cycling information and at the same time preserves the user's awareness of hazards. To do so, we implemented real-time outdoor hazard detection using a combination of computer vision and motion and position data from a head mounted display (HMD). We then developed an algorithm that snaps information to detected hazards when they are also viewed so that users can simultaneously view both rendered virtual cycling information and the real-world cues such as depth, position, time to hazard, and speed that are needed to assess and avoid hazards. Results from a study with 24 participants that made use of real-world cycling and virtual hazards showed that both HazARdSnap and forward-fixed augmented reality (AR) user interfaces (UIs) can effectively help cyclists access virtual information without having to look down, which resulted in fewer collisions (51% and 43% reduction compared to baseline, respectively) with virtual hazards.
C1 [Zhao, Guanghan; Orlosky, Jason] Osaka Univ, Osaka 5650871, Japan.
   [Orlosky, Jason] Augusta Univ, Augusta, GA 30912 USA.
   [Gabbard, Joseph] Virginia Tech, Blacksburg, VA 24061 USA.
   [Kiyokawa, Kiyoshi] NAIST, Nara 6300192, Japan.
C3 Osaka University; University System of Georgia; Augusta University;
   Virginia Polytechnic Institute & State University; Nara Institute of
   Science & Technology
RP Zhao, GH (corresponding author), Osaka Univ, Osaka 5650871, Japan.
EM zhao.guanghan@lab.ime.cmc.osaka-u.ac.jp; jasonorlosky@gmail.com;
   jgabbard@vt.edu; kiyo@is.naist.jp
OI Kiyokawa, Kiyoshi/0000-0003-2260-1707; Zhao,
   Guanghan/0000-0002-1216-9685
FU ONRG [N62909-18-1-2036]
FX This work was supported in part by ONRG, under grant N62909-18-1-2036.
CR Berge SH, 2022, TRANSPORT RES F-TRAF, V84, P33, DOI 10.1016/j.trf.2021.11.013
   Chatterjee S, 2020, COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION), P426, DOI 10.1145/3395035.3425207
   Chen J, 2004, P IEEE VIRT REAL ANN, P181, DOI 10.1109/VR.2004.1310072
   Chong S, 2010, ACCIDENT ANAL PREV, V42, P290, DOI 10.1016/j.aap.2009.08.006
   Chuanqi305, 2017, Caffe implementation of Google mobilenet SSD detec- tion network
   Dancu A., 2015, Proceedings of the 2015 International Conference on Interactive Tabletops Surfaces, ITS '15, P151
   De Waard D, 2014, TRANSPORT RES F-TRAF, V22, P196, DOI 10.1016/j.trf.2013.12.003
   Falanga Davide, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5774, DOI 10.1109/ICRA.2017.7989679
   Gavrila D., 2000, PROC EUROPEAN C COMP, P37, DOI [10.1007/3-540-45053-X-3., DOI 10.1007/3-540-45053-X-3, DOI 10.1007/3-540-45053-X]
   Ghiurau Florin-Timotei, 2020, UIST '20: 33rd Annual ACM Symposium on User Interface Software and Technology, P62, DOI 10.1145/3379350.3416186
   Ginters E, 2019, PROCEDIA COMPUT SCI, V149, P167, DOI 10.1016/j.procs.2019.01.120
   Hariyono J, 2014, SCI WORLD J, DOI 10.1155/2014/196415
   Kita-Kyushu City, 2022, Bicycle rules and etiquette many don't know about
   Kyoto Police Office, 2022, Rules and penalties on riding bicycle
   Labayrade R, 2002, IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS, P646
   Langlois S, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1571, DOI 10.1109/ITSC.2016.7795767
   League of American Cyclists, 2012, State bike laws
   Li YT, 2018, IET COMMUN, V12, P751, DOI 10.1049/iet-com.2017.0502
   Liang X, 2021, TRANSPORT RES A-POL, V153, P115, DOI 10.1016/j.tra.2021.09.003
   Lindemann P, 2017, AUTOMOTIVEUI'17: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P83, DOI 10.1145/3131726.3131754
   Matsunaga N, 2018, IEEE SYS MAN CYBERN, P1915, DOI 10.1109/SMC.2018.00331
   Matviienko A, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517560
   Nataprawira J., 2020, P INT WORKSH INN S I, P97
   Oliveira W, 2018, LECT NOTES COMPUT SC, V11112, P52, DOI 10.1007/978-3-319-99426-0_5
   Orlosky J, 2014, MOB COMPUT COMMUN RE, V18, P20, DOI 10.1145/2636242.2636246
   Orlosky J, 2013, INT SYM MIX AUGMENT, P281, DOI 10.1109/ISMAR.2013.6671805
   Park HS, 2013, ETRI J, V35, P1038, DOI 10.4218/etrij.13.2013.0041
   Rateke T., 2019, Revista de Informtica Terica e Aplicada, V26, P50
   Redmon J., 2018, arXiv, DOI 10.48550/arXiv.1804.02767
   Ren J, 2021, AM J EPIDEMIOL, V190, P37, DOI 10.1093/aje/kwaa164
   Scaramuzza D., 2020, Autonomous, agile micro drones: Perception, learning, and control
   Shashua A, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P1
   Terzano K, 2013, ACCIDENT ANAL PREV, V57, P87, DOI 10.1016/j.aap.2013.04.007
   Uchiya T., 2021, P INT C INT NETW COL, P209
   van der Horst ARA, 2014, ACCIDENT ANAL PREV, V62, P358, DOI 10.1016/j.aap.2013.04.005
   van Lopik K, 2020, APPL ERGON, V82, DOI 10.1016/j.apergo.2019.102928
   Vansteenkiste P, 2015, ACCIDENT ANAL PREV, V78, P8, DOI 10.1016/j.aap.2015.02.010
   Vansteenkiste P, 2013, ACCIDENT ANAL PREV, V51, P222, DOI 10.1016/j.aap.2012.11.025
   von Sawitzky T, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3383022
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang Tianyu., 2012, P 12 WORKSHOP MOBILE, P1, DOI 10.1145/2162081.2162089
   Wenbo Lan, 2018, 2018 IEEE International Conference on Mechatronics and Automation (ICMA), P1547, DOI 10.1109/ICMA.2018.8484698
   Wickens C.D., 1991, Multiple-task performance, P3, DOI [10.1201/9781003069447-2, DOI 10.1201/9781003069447-2]
   Yeo D, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376787
NR 44
TC 2
Z9 2
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6378
EP 6389
DI 10.1109/TVCG.2023.3333336
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000015
PM 37971924
DA 2024-11-06
ER

PT J
AU Pont, M
   Tierny, J
AF Pont, Mathieu
   Tierny, Julien
TI Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Encoding; Neural networks; Visualization; Measurement; Data analysis;
   Stability analysis; Dimensionality reduction; Topological data analysis;
   ensemble data; merge trees; persistence diagrams
ID MORSE COMPLEXES; VISUAL ANALYSIS; ENSEMBLES; UNCERTAINTY; VARIABILITY;
   ALGORITHMS; EXTRACTION; ASSIGNMENT; DISTANCES; POINTS
AB This article presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts (Pont et al. 2023) at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding (Pont et al. 2023). First, we apply MT-WAE to merge tree compression, by concisely representing them with their coordinates in the final layer of our auto-encoder. Second, we document an application to dimensionality reduction, by exploiting the latent space of our auto-encoder, for the visual analysis of ensemble data. We illustrate the versatility of our framework by introducing two penalty terms, to help preserve in the latent space both the Wasserstein distances between merge trees, as well as their clusters. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used for reproducibility.
C1 [Pont, Mathieu; Tierny, Julien] Sorbonne Univ, CNRS, F-75005 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Sorbonne Universite
RP Pont, M (corresponding author), Sorbonne Univ, CNRS, F-75005 Paris, France.
EM mathieu.pont@lip6.fr; julien.tierny@sorbonne-universite.fr
OI Pont, Mathieu/0000-0002-0037-0314
FU European Commission [ERC-2019-COG"TORI", 863464]; European Research
   Council (ERC) [863464] Funding Source: European Research Council (ERC)
FX This work was supported in part by European Commission under Grant
   ERC-2019-COG"TORI"(ref. 863464, https://erctori.github.io/).
CR Acharya A, 2015, IEEE PAC VIS SYMP, P271, DOI 10.1109/PACIFICVIS.2015.7156387
   Adams H, 2017, J MACH LEARN RES, V18
   Anderson KL, 2018, LECT NOTES COMPUT SC, V11083, P67, DOI 10.1007/978-3-030-00755-3_8
   Anirudh R, 2016, IEEE COMPUT SOC CONF, P1023, DOI 10.1109/CVPRW.2016.132
   Athawale T. M., 2019, arXiv
   Ayachit U., 2015, P 1 WORKSH IN SIT IN, P25, DOI [10.1145/2828612. 2828624, DOI 10.1145/2828612.2828624, 10.1145/2828612.2828624]
   Bauer U., 2014, P 16 WORKSH ALG ENG, P31, DOI [10.1137/1.9781611973198.4, DOI 10.1137/1.9781611973198.4]
   Bauer U., 2014, P 30 ANN S COMP GEOM, P464
   Beketayev K., 2014, TopoinVis III, P151, DOI 10.1007/978-3-319-04099-8
   BERTSEKAS DP, 1981, MATH PROGRAM, V21, P152, DOI 10.1007/BF01584237
   Biasotti S, 2008, THEOR COMPUT SCI, V392, P5, DOI 10.1016/j.tcs.2007.10.018
   Bin Masood T., 2019, P TOP METH DAT AN VI, VIII
   Bock A, 2018, IEEE T VIS COMPUT GR, V24, P812, DOI 10.1109/TVCG.2017.2743980
   Bollen B, 2022, Arxiv, DOI arXiv:2110.05631
   Bollen B, 2023, IEEE T VIS COMPUT GR, V29, P1168, DOI 10.1109/TVCG.2022.3209395
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Bremer PT, 2011, IEEE T VIS COMPUT GR, V17, P1307, DOI 10.1109/TVCG.2010.253
   Bremer PT, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P139, DOI 10.1109/VISUAL.2003.1250365
   Brown N, 2021, PROCEEDINGS OF URGENTHPC 2021: THE THIRD INTERNATIONAL WORKSHOP ON HPC FOR URGENT DECISION MAKING, P36, DOI 10.1109/UrgentHPC54802.2021.00010
   Bruel Gabrielsson Rickard, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P1069, DOI 10.1109/ICMLA.2019.00180
   Bubenik P, 2015, J MACH LEARN RES, V16, P77
   Carr H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P497, DOI 10.1109/VISUAL.2004.96
   Carr H, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P918
   Carr HA, 2016, SYMP LARG DATA ANAL, P75, DOI 10.1109/LDAV.2016.7874312
   Carrière M, 2017, PR MACH LEARN RES, V70
   De Floriani L, 2015, COMPUT GRAPH FORUM, V34, P761, DOI 10.1111/cgf.12596
   Diggle P., 2002, ANAL LONGITUDINAL DA, DOI DOI 10.2307/2983303
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Edelsbrunner H, 2009, Computational Topology An Introduction
   Favelier G, 2019, IEEE T VIS COMPUT GR, V25, P1152, DOI 10.1109/TVCG.2018.2864432
   Ferstl F, 2016, COMPUT GRAPH FORUM, V35, P221, DOI 10.1111/cgf.12898
   Ferstl F, 2016, IEEE T VIS COMPUT GR, V22, P767, DOI 10.1109/TVCG.2015.2467204
   Günther D, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12359
   Gueunet C., 2019, P EUR S PAR GRAPH VI, P27
   Gueunet C, 2019, IEEE T PARALL DISTR, V30, P1889, DOI 10.1109/TPDS.2019.2898436
   Guillou P, 2024, IEEE T VIS COMPUT GR, V30, P1897, DOI 10.1109/TVCG.2023.3238008
   Gyulassy A, 2014, COMPUT GRAPH FORUM, V33, P51, DOI 10.1111/cgf.12361
   Gyulassy A, 2019, IEEE T VIS COMPUT GR, V25, P1183, DOI 10.1109/TVCG.2018.2864848
   Gyulassy A, 2016, IEEE T VIS COMPUT GR, V22, P916, DOI 10.1109/TVCG.2015.2467432
   Heine C, 2016, COMPUT GRAPH FORUM, V35, P643, DOI 10.1111/cgf.12933
   Hensel F, 2021, FRONT ARTIF INTELL, V4, DOI 10.3389/frai.2021.681108
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hu X., 2021, P INT C LEARN REPR
   Hu XY, 2019, ADV NEUR IN, V32
   Hummel M, 2013, IEEE T VIS COMPUT GR, V19, P2743, DOI 10.1109/TVCG.2013.141
   Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199
   Kasten J, 2011, IEEE T VIS COMPUT GR, V17, P2080, DOI 10.1109/TVCG.2011.249
   Kerber Michael, 2017, Journal of Experimental Algorithmics (JEA), V22, P1, DOI 10.1145/3064175
   Khrulkov V, 2018, PR MACH LEARN RES, V80
   Kim K., 2020, P 34 INT C NEUR INF
   Kingma D.P., 2014, P INT C LEARNING REP
   Kruskal JB., 1978, SAGE U PAPER SERIES, DOI DOI 10.4135/9781412985130
   Lacombe T, 2018, ADV NEUR IN, V31
   Li B., 2014, P EUR WORKSH 3D OBJ
   Li M., 2023, P TOP METH DAT AN VI, VIII
   Lohfink AP, 2020, COMPUT GRAPH FORUM, V39, P343, DOI 10.1111/cgf.13985
   Maadasamy S, 2012, INT C HIGH PERFORM
   Maljovec D, 2016, IEEE PAC VIS SYMP, P64, DOI 10.1109/PACIFICVIS.2016.7465252
   Mirzargar M, 2014, IEEE T VIS COMPUT GR, V20, P2654, DOI 10.1109/TVCG.2014.2346455
   Monge G., 1781, MEMOIRE THEORIE DEBL
   Moor M., 2020, INT C MACH LEARN, P7045
   Morozov D., 2014, P TOP METH DAT AN VI, VIII
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Nauleau F, 2022, SYMP LARG DATA ANAL, P50, DOI 10.1109/LDAV57265.2022.9966403
   Olejniczak M, 2023, PHYS CHEM CHEM PHYS, V25, P5942, DOI 10.1039/d2cp05893f
   Olejniczak M, 2020, INT J QUANTUM CHEM, V120, DOI 10.1002/qua.26133
   Organizers, 2004, The IEEE SciVis Contest
   Parsa S, 2013, DISCRETE COMPUT GEOM, V49, P864, DOI 10.1007/s00454-013-9511-3
   Paszke A, 2019, ADV NEUR IN, V32
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Pont M, 2023, IEEE T VIS COMPUT GR, V29, P1573, DOI 10.1109/TVCG.2022.3215001
   Pont M, 2022, IEEE T VIS COMPUT GR, V28, P291, DOI 10.1109/TVCG.2021.3114839
   Potter K, 2009, INT CONF DAT MIN WOR, P233, DOI 10.1109/ICDMW.2009.55
   Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106
   Robins V, 2016, PHYSICA D, V334, P99, DOI 10.1016/j.physd.2016.03.007
   Robins V, 2011, IEEE T PATTERN ANAL, V33, P1646, DOI 10.1109/TPAMI.2011.95
   Saikia H, 2014, COMPUT GRAPH FORUM, V33, P41, DOI 10.1111/cgf.12360
   Sanyal J, 2010, IEEE T VIS COMPUT GR, V16, P1421, DOI 10.1109/TVCG.2010.181
   Shivashankar N, 2016, IEEE T VIS COMPUT GR, V22, P1745, DOI 10.1109/TVCG.2015.2452919
   Shivashankar N, 2012, COMPUT GRAPH FORUM, V31, P965, DOI 10.1111/j.1467-8659.2012.03089.x
   Sisouk K, 2023, Arxiv, DOI arXiv:2304.14852
   Soler M, 2019, SYMP LARG DATA ANAL, P62, DOI [10.1109/ldav48142.2019.8944365, 10.1109/LDAV48142.2019.8944365]
   Sousbie T, 2011, MON NOT R ASTRON SOC, V414, P384, DOI 10.1111/j.1365-2966.2011.18395.x
   Sridharamurthy R, 2020, IEEE T VIS COMPUT GR, V26, P1518, DOI 10.1109/TVCG.2018.2873612
   Stucki N., 2023, PMLR, P32698
   Tierny J, 2018, IEEE T VIS COMPUT GR, V24, P832, DOI 10.1109/TVCG.2017.2743938
   Tolstikhin I., 2018, 6 INT C LEARNING REP
   Turner K, 2014, DISCRETE COMPUT GEOM, V52, P44, DOI 10.1007/s00454-014-9604-7
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vidal J, 2020, IEEE T VIS COMPUT GR, V26, P151, DOI 10.1109/TVCG.2019.2934256
   Wetzels F, 2022, 2022 IEEE WORKSHOP ON TOPOLOGICAL DATA ANALYSIS AND VISUALIZATION (TOPOINVIS 2022), P29, DOI 10.1109/TopoInVis57755.2022.00010
   Wetzels F, 2022, COMPUT GRAPH FORUM, V41, P367, DOI 10.1111/cgf.14547
   Whitaker RT, 2013, IEEE T VIS COMPUT GR, V19, P2713, DOI 10.1109/TVCG.2013.143
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Yan L, 2021, COMPUT GRAPH FORUM, V40, P599, DOI 10.1111/cgf.14331
   Yan L, 2020, IEEE T VIS COMPUT GR, V26, P832, DOI 10.1109/TVCG.2019.2934242
   Zhang KZ, 1996, ALGORITHMICA, V15, P205, DOI 10.1007/BF01975866
   Zhou S., 2021, P INT C LEARN REPR
NR 98
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6390
EP 6406
DI 10.1109/TVCG.2023.3334755
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000037
PM 38015696
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Luo, JD
   Zhao, NX
   Li, WB
   Richardt, C
AF Luo, Jundan
   Zhao, Nanxuan
   Li, Wenbin
   Richardt, Christian
TI CRefNet: Learning Consistent Reflectance Estimation With a
   Decoder-Sharing Transformer
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Reflectivity; Transformers; Estimation; Image reconstruction; Task
   analysis; Image decomposition; Decoding; Intrinsic image decomposition;
   multi-task learning; rectified gradient filter; reflectance consistency;
   self-supervised learning
ID INTRINSIC IMAGE DECOMPOSITION; RETINEX; VIDEO
AB We present CRefNet, a hybrid transformer-convolutional deep neural network for consistent reflectance estimation in intrinsic image decomposition. Estimating consistent reflectance is particularly challenging when the same material appears differently due to changes in illumination. Our method achieves enhanced global reflectance consistency via a novel transformer module that converts image features to reflectance features. At the same time, this module also exploits long-range data interactions. We introduce reflectance reconstruction as a novel auxiliary task that shares a common decoder with the reflectance estimation task, and which substantially improves the quality of reconstructed reflectance maps. Finally, we improve local reflectance consistency via a new rectified gradient filter that effectively suppresses small variations in predictions without any overhead at inference time. Our experiments show that our contributions enable CRefNet to predict highly consistent reflectance maps and to outperform the state of the art by 10% WHDR.
C1 [Luo, Jundan; Li, Wenbin; Richardt, Christian] Univ Bath, Dept Comp Sci, Bath BA2 7AY, England.
   [Zhao, Nanxuan] Adobe Res, San Jose, CA 95110 USA.
   [Richardt, Christian] Meta, Pittsburgh, PA 15222 USA.
C3 University of Bath; Adobe Systems Inc.
RP Richardt, C (corresponding author), Univ Bath, Dept Comp Sci, Bath BA2 7AY, England.; Richardt, C (corresponding author), Meta, Pittsburgh, PA 15222 USA.
EM jundanluo22@gmail.com; nanxuanzhao@gmail.com; wenbin.li@bath.edu;
   christian@richardt.name
RI Li, Wenbin/ABD-9849-2021
OI ZHAO, Nanxuan/0000-0002-4007-2776; Li, Wenbin/0000-0002-5593-2599; Luo,
   Jundan/0000-0002-3336-034X; Richardt, Christian/0000-0001-6716-9845
FU China Scholarship Council; An EPSRC-UKRI Innovation Fellowship
   [EP/S001050/1]; EPSRC Grant CAMERA [EP/M023281/1, EP/T022523/1]
FX This work was supported in part by the China Scholarship Council,an
   EPSRC-UKRI Innovation Fellowship under Grant EP/S001050/1 and in partby
   EPSRC Grant CAMERA under Grants EP/M023281/1 and
   EP/T022523/1.Recommended for acceptance by V. Popescu.
CR Alshammari N, 2018, IEEE INT VEH SYM, P1027, DOI 10.1109/IVS.2018.8500664
   Baldi P., 2012, JMLR WORKSHOP C P, P37
   Bank D., 2020, arXiv
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Barrow Harry G., 1978, computer vision systems, P3
   Baslamisli AS, 2021, J OPT SOC AM A, V38, DOI 10.1364/JOSAA.414682
   Baslamisli AS, 2018, LECT NOTES COMPUT SC, V11210, P289, DOI 10.1007/978-3-030-01231-1_18
   Beigpour S., 2018, J. Perceptual Imag., V1, P1
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476
   Brempong EA, 2022, IEEE COMPUT SOC CONF, P4174, DOI 10.1109/CVPRW56347.2022.00462
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37
   Chen TQ, 2016, Arxiv, DOI arXiv:1604.06174
   Chu XX, 2021, ADV NEUR IN
   Das P, 2022, PROC CVPR IEEE, P19758, DOI 10.1109/CVPR52688.2022.01917
   Dosovitskiy A., 2021, ICLR
   Drew MS, 1999, PATTERN RECOGN, V32, P1369, DOI 10.1016/S0031-3203(98)00168-X
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fairchild M. D., 2013, Color appearance models, DOI 10.1002/9781118653128
   Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932
   FUNT BV, 1992, LECT NOTES COMPUT SC, V588, P124
   Garces E, 2022, INT J COMPUT VISION, V130, P836, DOI 10.1007/s11263-021-01563-8
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Jin YY, 2023, AAAI CONF ARTIF INTE, P1069
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kim S, 2016, LECT NOTES COMPUT SC, V9912, P143, DOI 10.1007/978-3-319-46484-8_9
   Kingma D.P., 2014, P INT C LEARNING REP
   Kovacs B, 2017, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2017.97
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lettry L, 2018, COMPUT GRAPH FORUM, V37, P409, DOI 10.1111/cgf.13578
   Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P381, DOI 10.1007/978-3-030-01219-9_23
   Li ZQ, 2021, PROC CVPR IEEE, P7186, DOI 10.1109/CVPR46437.2021.00711
   Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin T.-H., 2022, P INT JOINT C ART IN, P1166
   Liu A., 2020, ECCV
   Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo JD, 2020, IEEE T VIS COMPUT GR, V26, P3434, DOI 10.1109/TVCG.2020.3023565
   Meka A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3374753
   Meka A, 2017, IEEE T VIS COMPUT GR, V23, P2447, DOI 10.1109/TVCG.2017.2734425
   Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342
   Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915
   Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192
   Park N., 2022, P INT C LEARN REPR
   Paszke A, 2019, ADV NEUR IN, V32
   Qian YL, 2021, IEEE WINT CONF APPL, P3168, DOI 10.1109/WACV48630.2021.00321
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranganathan A, 2013, IEEE INT CONF ROBOT, P3791, DOI 10.1109/ICRA.2013.6631110
   Rematas K, 2016, PROC CVPR IEEE, P4508, DOI 10.1109/CVPR.2016.488
   Riba E, 2020, IEEE WINT CONF APPL, P3663, DOI 10.1109/WACV45572.2020.9093363
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rother M., 2011, ADV NEURAL INFORM PR, P765
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, 10.48550/arXiv.1706.05098, DOI 10.48550/ARXIV.1706.05098]
   Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869
   Shakeri M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P466, DOI 10.1109/IROS.2016.7759095
   Shen JB, 2011, PROC CVPR IEEE
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincent P., 2008, P 25 TH INT C MACHIN, P1096, DOI DOI 10.1145/1390156.1390294
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12518, DOI 10.1109/ICCV48922.2021.01231
   Wang ZJ, 2019, IEEE INT CONF COMP V, P4310, DOI 10.1109/ICCVW.2019.00531
   Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Yan S, 2018, LECT NOTES COMPUT SC, V11214, P155, DOI 10.1007/978-3-030-01249-6_10
   Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135
   Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186
   Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327
   Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77
   Zhou H, 2019, IEEE I CONF COMP VIS, P7819, DOI 10.1109/ICCV.2019.00791
   Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396
   Zhu JS, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555407
   Zhu R, 2022, PROC CVPR IEEE, P2812, DOI 10.1109/CVPR52688.2022.00284
   Zollhöfer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887
   Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52
NR 92
TC 2
Z9 2
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6407
EP 6420
DI 10.1109/TVCG.2023.3337870
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000026
PM 38039167
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Chubarau, A
   Zhao, YY
   Rao, RBY
   Nowrouzezahrai, D
   Kry, PG
AF Chubarau, Andrei
   Zhao, Yangyang
   Rao, Ruby
   Nowrouzezahrai, Derek
   Kry, Paul G.
TI Cone-Traced Supersampling With Subpixel Edge Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Image edge detection; Shape; Image
   reconstruction; Real-time systems; Image quality; Geometry; Signed
   distance fields; cone tracing; antialiasing; rendering; computer
   graphics
ID GEOMETRIC-METHOD
AB While signed distance fields (SDFs) in theory offer infinite level of detail, they are typically rendered using the sphere tracing algorithm at finite resolutions, which causes the common rasterized image synthesis problem of aliasing. Most existing optimized antialiasing solutions rely on polygon mesh representations; SDF-based geometry can only be directly antialiased with the computationally expensive supersampling or with post-processing filters that may produce undesirable blurriness and ghosting. In this work, we present cone-traced supersampling (CTSS), an efficient and robust spatial antialiasing solution that naturally complements the sphere tracing algorithm, does not require casting additional rays per pixel or offline pre-filtering, and can be easily implemented in existing real-time SDF renderers. CTSS performs supersampling along the traced ray near surfaces with partial visibility - object contours - identified by evaluating cone intersections within a pixel's view frustum. We further introduce subpixel edge reconstruction (SER), a technique that extends CTSS to locate and resolve complex pixels with geometric edges in relatively flat regions, which are otherwise undetected by cone intersections. Our combined solution relies on a specialized sampling strategy to minimize the number of shading computations and correlates sample visibility to aggregate the samples. With comparable antialiasing quality at significantly lower computational cost, CTSS is a reliable practical alternative to conventional supersampling.
C1 [Chubarau, Andrei; Nowrouzezahrai, Derek; Kry, Paul G.] McGill Univ, Montreal, PQ H3A 0G4, Canada.
   [Chubarau, Andrei; Zhao, Yangyang; Rao, Ruby; Kry, Paul G.] Huawei Technol Canada, Markham, ON L3R 5A4, Canada.
C3 McGill University; Huawei Technologies
RP Chubarau, A (corresponding author), McGill Univ, Montreal, PQ H3A 0G4, Canada.
EM andrei.chubarau@mail.mcgill.ca; yangyang.zhao2@huawei.com;
   ruby.rao@huawei.com; derek@cim.mcgill.ca; kry@cs.mcgill.ca
RI Zhao, Jack/LDF-4197-2024
OI Nowrouzezahrai, Derek/0000-0002-4279-1774; Kry,
   Paul/0000-0003-4176-6857; Chubarau, Andrei/0000-0002-1566-485X
CR Akeley K., 1993, Computer Graphics Proceedings, P109, DOI 10.1145/166117.166131
   Amanatides J., 1984, Computers & Graphics, V18, P129
   Balint C., 2018, EG 2018 - Short Papers
   Bálint C, 2021, ACTA CYBERN, V25, P171, DOI 10.14232/actacyb.290007
   Ban R., 2019, Eurographics 2019-Short Papers
   Carpenter L., 1984, Computers & Graphics, V18, P103
   Catmull E. E., 1974, A subdivision algorithm for computer display of curved surfaces
   Chubarau A., 2023, P GRAPHICS INTERFACE
   Chubarau A., 2023, Interactive demo for cone-traced supersampling (CTSS)
   Cook R.L., 1987, ACM SIGGRAPH Computer Graphics, P95, DOI [DOI 10.1145/37402.37414, DOI 10.1145/37401.37414]
   Crassin C., 2011, ACM SIGGRAPH 2011 Talks, DOI [10.1145/2037826.2037853, DOI 10.1145/2037826.2037853]
   Crassin C, 2015, PROCEEDINGS - I3D 2015, P109, DOI 10.1145/2699276.2699285
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Davies T, 2021, Arxiv, DOI arXiv:2009.09808
   de Araújo BR, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2732197
   Edelsten A., 2019, TRULY NEXT GEN ADDIN
   Enderton E., 2015, ACM SIGGRAPH 2015 TA, DOI [10.1145/2775280.2792515, DOI 10.1145/2775280.2792515]
   Epic Games, 2021, UNREAL ENGINE
   Galin E, 2020, COMPUT GRAPH FORUM, V39, P545, DOI 10.1111/cgf.13951
   Genetti J, 1998, COMPUT GRAPH FORUM, V17, P29, DOI 10.1111/1467-8659.00214
   Gomes A., 2009, Implicit Curves and Surfaces: Mathematics, Data Structures and Algorithms, V1st
   Hart J. C., 1989, Computer Graphics, V23, P289, DOI 10.1145/74334.74363
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Heitz E., 2012, P EUR C HIGH PERF GR, P125
   Hermanns Lukas, 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P308, DOI 10.1007/978-3-319-39907-2_29
   Jeremias P., 2013, ACM SIGGRAPH 2013 Computer Animation Festival, P1, DOI 10.1145/2503541.2503644
   Jiang Y, 2022, Arxiv, DOI arXiv:1912.07109
   Jimenez J, 2012, COMPUT GRAPH FORUM, V31, P355, DOI 10.1111/j.1467-8659.2012.03014.x
   Kaplanyan Anton S, 2016, P HIGH PERFORMANCE G, P151, DOI DOI 10.2312/HPG.20161201
   Karis Brian., 2014, High Quality Temporal Supersampling
   Knoll A, 2009, COMPUT GRAPH FORUM, V28, P26, DOI 10.1111/j.1467-8659.2008.01189.x
   Linietsky J., 2021, Godot engine
   Lorensen WE., 1998, Seminal graphics: pioneering efforts that shaped the field; SIGGRAPH 98; celebrating 25 years of discovery; a publication of ACM SIGGRAPH, P347, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422, 10.1145/280811.281026, DOI 10.1145/280811.281026]
   Lottes T., 2009, FXAA
   Malan H., 2010, GPU Pro-Advanced Rendering Techniques, P265, DOI [10.1201/b10648-21, DOI 10.1201/B10648-21]
   Marrs A, 2018, HIGH-PERFORMANCE GRAPHICS 2018, DOI 10.1145/3231578.3231579
   Matthaus G.Chajdas., 2011, P S INT 3D GRAPH GAM, P15, DOI DOI 10.1145/1944745.1944748
   Media Molecule, 2020, Dreams
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Olano Marc, 2010, P 2010 ACM SIGGRAPH, P181, DOI DOI 10.1145/1730804.1730834
   Osher S., 2004, LEVEL SET METHODS DY
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Reeves William T., 1987, SIGGRAPH COMPUT GRAP, V21, P283, DOI [10.1145/37401.37435, DOI 10.1145/37402.37435]
   Reshetov Alexander, 2009, P C HIGH PERF GRAPH, P109, DOI 10.1145/1572769.1572787
   Rubinstein R.Y., 2016, Simulation and the Monte Carlo method
   Salvi M., 2012, P ACM SIGGRAPH S INT, P159, DOI [10.1145/2159616.2159643, DOI 10.1145/2159616.2159643]
   Second Order, 2018, Claybook game
   Takikawa T, 2021, PROC CVPR IEEE, P11353, DOI 10.1109/CVPR46437.2021.01120
   Unity Technologies, 2021, Unity
   Wand M, 2003, PROC GRAPH INTERF, P139
   Wang YH, 2015, PROCEEDINGS OF 2015 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P33, DOI 10.1109/GSIS.2015.7301822
   Williams L., 1983, Computer Graphics, V17, P1, DOI 10.1145/964967.801126
   Xiao K, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190850
   Xiao L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392376
   Yang L, 2020, COMPUT GRAPH FORUM, V39, P607, DOI 10.1111/cgf.14018
   Yang L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618481
   Young P., 2006, Technical Report
NR 58
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6421
EP 6432
DI 10.1109/TVCG.2023.3343166
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000007
PM 38096100
DA 2024-11-06
ER

PT J
AU Jiao, XH
   Lv, CL
   Yi, R
   Zhao, JL
   Pan, ZK
   Wu, ZK
   Liu, YJ
AF Jiao, Xianhe
   Lv, Chenlei
   Yi, Ran
   Zhao, Junli
   Pan, Zhenkuan
   Wu, Zhongke
   Liu, Yong-Jin
TI MSL-Net: Sharp Feature Detection Network for 3D Point Clouds
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Shape; Point cloud compression; Image edge
   detection; Feature detection; Three-dimensional displays; Manifolds;
   Sharp feature; 3D point cloud; intrinsic neighbor; multi-scale Laplace
   network
ID EXTRACTION
AB As a significant geometric feature of 3D point clouds, sharp features play an important role in shape analysis, 3D reconstruction, registration, localization, etc. Current sharp feature detection methods are still sensitive to the quality of the input point cloud, and the detection performance is affected by random noisy points and non-uniform densities. In this paper, using the prior knowledge of geometric features, we propose a Multi-scale Laplace Network (MSL-Net), a new deep-learning-based method based on an intrinsic neighbor shape descriptor, to detect sharp features from 3D point clouds. First, we establish a discrete intrinsic neighborhood of the point cloud based on the Laplacian graph, which reduces the error of local implicit surface estimation. Then, we design a new intrinsic shape descriptor based on the intrinsic neighborhood, combined with enhanced normal extraction and cosine-based field estimation function. Finally, we present the backbone of MSL-Net based on the intrinsic shape descriptor. Benefiting from the intrinsic neighborhood and shape descriptor, our MSL-Net has simple architecture and is capable of establishing accurate feature prediction that satisfies the manifold distribution while avoiding complex intrinsic metric calculations. Extensive experimental results demonstrate that with the multi-scale structure, MSL-Net has a strong analytical ability for local perturbations of point clouds. Compared with state-of-the-art methods, our MSL-Net is more robust and accurate.
C1 [Jiao, Xianhe; Zhao, Junli; Pan, Zhenkuan] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
   [Lv, Chenlei] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Yi, Ran] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Wu, Zhongke] Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100190, Peoples R China.
C3 Qingdao University; Shenzhen University; Shanghai Jiao Tong University;
   Beijing Normal University; Tsinghua University
RP Zhao, JL (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.; Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100190, Peoples R China.
EM jiaoxianhe@gmail.com; chenleilv@mail.bnu.edu.cn; ranyi@sjtu.edu.cn;
   zjl@qdu.edu.cn; zkpan@qdu.edu.cn; zwu@bnu.edu.cn;
   liuyongjin@tsinghua.edu.cn
RI zhou, rui/KZT-9353-2024; Yi, Ran/AAU-6636-2021
OI Yi, Ran/0000-0003-1858-3358; Zhao, Junli/0000-0002-2034-6426
FU National Natural Science Foundation of China [62172247, 62302297,
   61972041, 61772294, 61702293]; Beijing Natural Science Foundation
   [L222008]; Natural Science Foundation of Shandong Province
   [ZR2019LZH002]; Young Elite Scientists Sponsorship Program by CAST
   [2022QNRC001]; Shanghai Sailing Program [22YF1420300]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172247, 62302297, 61972041, 61772294,
   and 61702293, in part by Beijing Natural Science Foundation underGrant
   L222008, in part by the Natural Science Foundation of Shandong Province
   under Grant ZR2019LZH002, Young Elite Scientists Sponsorship Program by
   CAST under Grant 2022QNRC001, and in part by Shanghai Sailing Program
   under Grant 22YF1420300.
CR [Anonymous], 2003, Comput. Lab. Tech. Rep.
   [Anonymous], 2015, ISPRS J. Photogrammetry Remote Sens., V102, P172
   Ben-Shabat S., P 16 EUR C COMP VIS
   Cascade O., 2011, Open cascade technology
   Cherenkova K., 2023, P IEEE CVF C COMP VI, P2726
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Demarsin K, 2007, COMPUT AIDED DESIGN, V39, P276, DOI 10.1016/j.cad.2006.12.005
   Dunbar D, 2006, ACM T GRAPHIC, V25, P503, DOI 10.1145/1141911.1141915
   Edirimuni de Silva, IEEE Trans.Vis. Comput. Graph., DOI [10.1109/TVCG.2023.3263866.25T., DOI 10.1109/TVCG.2023.3263866.25T]
   Farin G.E., 2002, Curves and surfaces for CAGD, a practical guide
   Feng YF, 2023, COMPUT AIDED DESIGN, V157, DOI 10.1016/j.cad.2022.103468
   Hackel T, 2017, ISPRS J PHOTOGRAMM, V130, P231, DOI 10.1016/j.isprsjprs.2017.05.012
   Hackel T, 2016, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2016.178
   Himeur CE, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3481804
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   Li B, 2010, COMPUT GRAPH-UK, V34, P94, DOI 10.1016/j.cag.2010.01.004
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loizou M, 2020, COMPUT GRAPH FORUM, V39, P183, DOI 10.1111/cgf.14078
   Lv CL, 2023, IEEE T PATTERN ANAL, V45, P3274, DOI 10.1109/TPAMI.2022.3185644
   Lv CL, 2024, IEEE T VIS COMPUT GR, V30, P3196, DOI 10.1109/TVCG.2022.3227970
   Lv CL, 2021, IEEE T IMAGE PROCESS, V30, P7241, DOI 10.1109/TIP.2021.3104174
   Matveev A, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530140
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Raina P, 2019, COMPUT GRAPH-UK, V78, P37, DOI 10.1016/j.cag.2018.11.003
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Wang X., 2020, ARXIV
   Wang Y, 2013, COMPUT AIDED DESIGN, V45, P1333, DOI 10.1016/j.cad.2013.06.003
   Weber Christopher, 2010, Proceedings of the Shape Modeling International (SMI 2010), P175, DOI 10.1109/SMI.2010.32
   Xia SB, 2017, IEEE GEOSCI REMOTE S, V14, P1288, DOI 10.1109/LGRS.2017.2707467
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Zeng J, 2020, IEEE T IMAGE PROCESS, V29, P3474, DOI 10.1109/TIP.2019.2961429
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhang J, 2019, IEEE T VIS COMPUT GR, V25, P1693, DOI 10.1109/TVCG.2018.2827998
   Zhao T, 2023, COMPUT AIDED GEOM D, V103, DOI 10.1016/j.cagd.2023.102204
   Zhu XY, 2023, PROC CVPR IEEE, P13601, DOI 10.1109/CVPR52729.2023.01307
NR 40
TC 0
Z9 0
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6433
EP 6446
DI 10.1109/TVCG.2023.3346907
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000049
PM 38145513
DA 2024-11-06
ER

PT J
AU Ulmer, A
   Angelini, M
   Fekete, JD
   Kohlhammer, J
   May, T
AF Ulmer, Alex
   Angelini, Marco
   Fekete, Jean-Daniel
   Kohlhammer, Joern
   May, Thorsten
TI A Survey on Progressive Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Convergence; Visual analytics; Taxonomy; Surveys;
   Rendering (computer graphics); Task analysis; Progressive visual
   analytics; progressive visualization; taxonomy; state-of-the-art report;
   survey
ID OF-THE-ART; VISUAL ANALYTICS; INFORMATION VISUALIZATION; EXPLORATION;
   MODEL; SCATTERPLOTS
AB Currently, growing data sources and long-running algorithms impede user attention and interaction with visual analytics applications. Progressive visualization (PV) and visual analytics (PVA) alleviate this problem by allowing immediate feedback and interaction with large datasets and complex computations, avoiding waiting for complete results by using partial results improving with time. Yet, creating a progressive visualization requires more effort than a regular visualization but also opens up new possibilities, such as steering the computations towards more relevant parts of the data, thus saving computational resources. However, there is currently no comprehensive overview of the design space for progressive visualization systems. We surveyed the related work of PV and derived a new taxonomy for progressive visualizations by systematically categorizing all PV publications that included visualizations with progressive features. Progressive visualizations can be categorized by well-known visualization taxonomies, but we also found that progressive visualizations can be distinguished by the way they manage their data processing, data domain, and visual update. Furthermore, we identified key properties such as uncertainty, steering, visual stability, and real-time processing that are significantly different with progressive applications. We also collected evaluation methodologies reported by the publications and conclude with statistical findings, research gaps, and open challenges.
C1 [Ulmer, Alex; Kohlhammer, Joern; May, Thorsten] Fraunhofer IGD, D-64283 Darmstadt, Germany.
   [Ulmer, Alex; Kohlhammer, Joern] Tech Univ Darmstadt, D-64289 Darmstadt, Germany.
   [Angelini, Marco] Link Campus Univ, I-00165 Rome, Italy.
   [Angelini, Marco] Sapienza Univ Rome, I-00185 Rome, Italy.
   [Fekete, Jean-Daniel] Univ Paris Saclay, CNRS, Inria Saclay, F-91190 Gif Sur Yvette, France.
C3 Technical University of Darmstadt; Sapienza University Rome; Universite
   Paris Cite; Universite Paris Saclay; Centre National de la Recherche
   Scientifique (CNRS)
RP Ulmer, A (corresponding author), Fraunhofer IGD, D-64283 Darmstadt, Germany.
EM alex.ulmer@igd.fraunhofer.de; angelini@dis.uniroma1.it;
   jean-daniel.fekete@inria.fr; joern.kohlhammer@igd.fraunhofer.de;
   thorsten.may@igd.fraunhofer.de
RI Fekete, Jean-Daniel/N-9175-2018
OI Fekete, Jean-Daniel/0000-0003-3770-8726; Ulmer,
   Alex/0009-0008-3778-8184; Angelini, Marco/0000-0001-9051-6972; May,
   Thorsten/0000-0001-8027-2687; Kohlhammer, Jorn/0000-0003-1706-8979
FU MUR PRIN2022 [202248FWFS]
FX This work was supported in part by the MUR PRIN2022 under Grant
   202248FWFS "Discount quality for responsible data science:
   Human-in-the-Loop for quality data" within the NextGenerationEU
   Programme.
CR Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   Angelini M., 2013, Proc. of the EuroVis Workshop on Visual Analytics (EuroVA13), P13, DOI [10.2312/PE.EuroVAST.EuroVA13.013-017, DOI 10.2312/PE.EUROVAST.EUROVA13.013-017]
   Angelini M, 2018, INFORMATICS-BASEL, V5, DOI 10.3390/informatics5030031
   Angelini M, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 3, P335, DOI 10.5220/0006269703350341
   Angelini Marco, 2019, P 10 INT EUROVIS WOR, P25, DOI [10.2312/eurova.20191120, DOI 10.2312/EUROVA.20191120]
   Armstrong S, 2019, BDCAT'19: PROCEEDINGS OF THE 6TH IEEE/ACM INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING, APPLICATIONS AND TECHNOLOGIES, P1, DOI 10.1145/3365109.3368793
   Badam SK, 2017, COMPUT GRAPH FORUM, V36, P491, DOI 10.1111/cgf.13205
   Beck F, 2017, COMPUT GRAPH FORUM, V36, P133, DOI 10.1111/cgf.12791
   Behrisch M, 2016, COMPUT GRAPH FORUM, V35, P693, DOI 10.1111/cgf.12935
   Bi CK, 2019, J VISUAL-JAPAN, V22, P641, DOI 10.1007/s12650-019-00555-8
   Borodin A., 1998, Online Computation and Competitive Analysis
   Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173
   Callahan SP, 2006, IEEE T VIS COMPUT GR, V12, P1307, DOI 10.1109/TVCG.2006.171
   Chen X, 2022, IEEE T VIS COMPUT GR, V28, P593, DOI 10.1109/TVCG.2021.3114880
   Cui Z, 2019, INFORM VISUAL, V18, P251, DOI 10.1177/1473871618806555
   Demir I, 2013, COMPUT GRAPH FORUM, V32, P21, DOI 10.1111/cgf.12089
   Duke D, 2006, IEEE T VIS COMPUT GR, V12, P973, DOI 10.1109/TVCG.2006.145
   Elmqvist N, 2010, IEEE T VIS COMPUT GR, V16, P439, DOI 10.1109/TVCG.2009.84
   Fekete J.-D., 2019, DAGSTUHL REPORTS, V8, P1, DOI [DOI 10.4230/DAGREP.8.10.1, 10.4230/DAGREP.8.10.1]
   Fekete JD, 2016, Arxiv, DOI arXiv:1607.05162
   Fisher D., 2012, P SIGCHI C HUM FACT, P1673, DOI DOI 10.1145/2207676.2208294
   Fisher D, 2012, IEEE COMPUT GRAPH, V32, P55, DOI 10.1109/MCG.2012.48
   Frey S, 2017, IEEE T VIS COMPUT GR, V23, P921, DOI 10.1109/TVCG.2016.2599042
   Frey S, 2014, IEEE T VIS COMPUT GR, V20, P2397, DOI 10.1109/TVCG.2014.2346319
   Frishman Y, 2008, IEEE T VIS COMPUT GR, V14, P727, DOI 10.1109/TVCG.2008.11
   Giachelle F., 2019, P IT INF RETR WORKSH, P2
   Gilbert JM, 1999, IEEE INFOCOM SER, P1291, DOI 10.1109/INFCOM.1999.752147
   Gintautas D., 2013, Methods and applications, DOI DOI 10.1007/978-1-4419-0236-8
   Glueck M, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P561, DOI 10.1145/2556288.2557195
   Griethe H., 2006, SIMVIS, P143
   Harrison C, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1549
   Hayashi Aki, 2013, 2013 17th International Conference on Information Visualisation, P435, DOI 10.1109/IV.2013.57
   Heinrich J, 2011, COMPUT GRAPH FORUM, V30, P653, DOI 10.1111/j.1467-8659.2011.01914.x
   Hellerstein J. M., 1997, SIGMOD Record, V26, P171, DOI 10.1145/253262.253291
   Hellerstein JM, 1999, COMPUTER, V32, P51, DOI 10.1109/2.781635
   Hetzler EG, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P89, DOI 10.1109/INFVIS.2005.1532133
   Höllt T, 2016, COMPUT GRAPH FORUM, V35, P171, DOI 10.1111/cgf.12893
   Hografer M., 2022, P INT EUROVIS WORKSH, P49, DOI [10.2312/eurova.20221079, DOI 10.2312/EUROVA.20221079]
   Hogräfer M, 2024, IEEE T VIS COMPUT GR, V30, P4809, DOI 10.1109/TVCG.2023.3278084
   Hogräfer M, 2022, ACM T INTEL SYST TEC, V13, DOI 10.1145/3531229
   Im Jean-Francois, 2013, 2013 IEEE International Conference on Big Data, P25, DOI 10.1109/BigData.2013.6691710
   Jena A, 2020, IEEE PAC VIS SYMP, P201, DOI 10.1109/PacificVis48177.2020.1014
   Jo J, 2021, IEEE T VIS COMPUT GR, V27, P3109, DOI 10.1109/TVCG.2019.2962404
   Jo J, 2020, IEEE T VIS COMPUT GR, V26, P1347, DOI 10.1109/TVCG.2018.2869149
   Jo J, 2017, IEEE PAC VIS SYMP, P131, DOI 10.1109/PACIFICVIS.2017.8031587
   Kamarianakis M., 2022, arXiv
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P1, DOI 10.1109/2945.981847
   Kesavan SP, 2020, IEEE PAC VIS SYMP, P206, DOI 10.1109/PacificVis48177.2020.9280
   Kim H, 2017, AAAI CONF ARTIF INTE, P1001
   Ko H.-K., 2020, Proc. EuroVis, P133, DOI DOI 10.2312/EVS.20201061
   Kraak M.-J., 2020, Cartography: Visualization of Geospatial Data, DOI [10.1201/9780429464195, DOI 10.1201/9780429464195]
   Kwon BC, 2017, IEEE COMPUT GRAPH, V37, P100, DOI 10.1109/MCG.2017.6
   Lam H, 2012, IEEE T VIS COMPUT GR, V18, P1520, DOI 10.1109/TVCG.2011.279
   LAUR D, 1991, COMP GRAPH, V25, P285, DOI 10.1145/127719.122748
   Li JPK, 2020, IEEE T VIS COMPUT GR, V26, P1151, DOI 10.1109/TVCG.2019.2934537
   Liu DY, 2019, IEEE T VIS COMPUT GR, V25, P1, DOI 10.1109/TVCG.2018.2865018
   Liu SX, 2016, IEEE T VIS COMPUT GR, V22, P2451, DOI 10.1109/TVCG.2015.2509990
   Liu ZC, 2014, IEEE T VIS COMPUT GR, V20, P2122, DOI 10.1109/TVCG.2014.2346452
   Liu ZY, 2002, COMPUT GRAPH-UK, V26, P209, DOI 10.1016/S0097-8493(02)00052-3
   Luebke D., 2003, Level of Detail for 3D Graph, DOI [10.1016/B978-155860838-2/50000-5, DOI 10.1016/B978-155860838-2/50000-5]
   Machiraju R, 2001, MASSIVE COMP, V2, P257
   May T., 2003, P SIM VIS, P89
   Micallef Luana, 2019, IN PRESS, P19, DOI [10.2312/evs.20191164, DOI 10.2312/EVS.20191164]
   Moritz D, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2904, DOI 10.1145/3025453.3025456
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002
   Nielsen J., 2009, Powers of 10: Time scales in user experience
   Nobre C, 2019, COMPUT GRAPH FORUM, V38, P807, DOI 10.1111/cgf.13728
   Padilla L., 2021, Uncertainty Visualization, P1, DOI [DOI 10.1002/9781118445112.STAT08296, 10.1002/ 9781118445112.stat08296]
   Pang A., 1994, Proceedings. Visualization '94 (Cat. No.94CH35707), P302, DOI 10.1109/VISUAL.1994.346305
   Patil Ameya, 2023, IEEE Trans Vis Comput Graph, V29, P407, DOI 10.1109/TVCG.2022.3209426
   Pezzotti N, 2016, COMPUT GRAPH FORUM, V35, P21, DOI 10.1111/cgf.12878
   Pezzotti N, 2018, IEEE T VIS COMPUT GR, V24, P98, DOI 10.1109/TVCG.2017.2744358
   Pezzotti N, 2017, IEEE T VIS COMPUT GR, V23, P1739, DOI 10.1109/TVCG.2016.2570755
   Procopio M, 2022, IEEE T VIS COMPUT GR, V28, P3093, DOI 10.1109/TVCG.2021.3051013
   Procopio M, 2019, INFORMATICS-BASEL, V6, DOI 10.3390/informatics6010014
   Rahman S, 2017, PROC VLDB ENDOW, V10, P1262, DOI 10.14778/3137628.3137637
   Raveneau V., 2018, P VIS DAT SCI
   Rosenbaum R, 2012, IEEE PAC VIS SYMP, P25, DOI 10.1109/PacificVis.2012.6183570
   Rosenbaum R, 2009, LECT NOTES COMPUT SC, V5876, P71, DOI 10.1007/978-3-642-10520-3_7
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Schulz HJ, 2016, IEEE T VIS COMPUT GR, V22, P1830, DOI 10.1109/TVCG.2015.2462356
   Schwank J., 2017, P ADV US US EXP, P495, DOI [10.1007/978-3-319-60492-347, DOI 10.1007/978-3-319-60492-347]
   Shilpika, 2022, IEEE T VIS COMPUT GR, V28, P2338, DOI 10.1109/TVCG.2022.3165348
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Siddiqui F, 2021, COMPUT GRAPH FORUM, V40, P411, DOI 10.1111/cgf.14317
   Sondag M, 2020, IEEE PAC VIS SYMP, P111, DOI 10.1109/PacificVis48177.2020.7614
   Song D., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P126, DOI 10.1109/VISUAL.1993.398860
   Stolper CD, 2014, IEEE T VIS COMPUT GR, V20, P1653, DOI 10.1109/TVCG.2014.2346574
   Tao Y, 2023, J VISUAL-JAPAN, V26, P367, DOI 10.1007/s12650-022-00879-y
   Tensorflow, 2016, Tensorflow Playground
   Turkay C, 2017, IEEE T VIS COMPUT GR, V23, P131, DOI 10.1109/TVCG.2016.2598470
   Ulmer A, 2021, COMPUT GRAPH FORUM, V40, P37, DOI 10.1111/cgf.14287
   Ulmer A, 2019, IEEE SYM VIS CYB SEC, DOI 10.1109/vizsec48167.2019.9161633
   van Ham F, 2009, IEEE T VIS COMPUT GR, V15, P953, DOI 10.1109/TVCG.2009.108
   vanLiere R, 1997, FUTURE GENER COMP SY, V12, P441, DOI 10.1016/S0167-739X(96)00029-5
   Ventocilla E, 2020, COMM COM INF SC, V1182, P203, DOI 10.1007/978-3-030-41590-7_9
   Vidal J, 2021, IEEE T VIS COMPUT GR, V27, P2833, DOI 10.1109/TVCG.2021.3060500
   Vidal J, 2020, IEEE T VIS COMPUT GR, V26, P151, DOI 10.1109/TVCG.2019.2934256
   von Landesberger T, 2011, COMPUT GRAPH FORUM, V30, P1719, DOI 10.1111/j.1467-8659.2011.01898.x
   Wald A., 1947, SEQUENTIAL ANAL
   Wang SR, 2011, APPL MECH MATER, V40-41, P948, DOI 10.4028/www.scientific.net/AMM.40-41.948
   Wang Yunhai, 2023, Proceedings of the ACM on Management of Data, V1, DOI 10.1145/3589290
   Williams M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P57, DOI 10.1109/INFVIS.2004.60
   Wong PC, 2003, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2003, PROCEEDINGS, P97
   Xiong YL, 2018, LECT NOTES COMPUT SC, V11215, P489, DOI 10.1007/978-3-030-01252-6_29
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Zgraggen E, 2017, IEEE T VIS COMPUT GR, V23, P1977, DOI 10.1109/TVCG.2016.2607714
   Zhang YH, 2023, J VISUAL-JAPAN, V26, P423, DOI 10.1007/s12650-022-00883-2
   Zhao HQ, 2017, J VISUAL LANG COMPUT, V43, P42, DOI 10.1016/j.jvlc.2017.05.004
NR 110
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6447
EP 6467
DI 10.1109/TVCG.2023.3346641
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000043
PM 38145517
OA Green Submitted, Bronze
DA 2024-11-06
ER

PT J
AU Wang, M
   Zhou, JC
   Feng, WQ
   Jiang, YZ
   Serrano, A
AF Wang, Miao
   Zhou, Jin-Chao
   Feng, Wei-Qi
   Jiang, Yu-Zhu
   Serrano, Ana
TI Studying the Effect of Material and Geometry on Perceptual Outdoor
   Illumination
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Geometry; Scattering; Sun; Solid modeling; Reflectivity;
   Estimation; Perceptual outdoor illumination; geometry; material
ID SURFACE REFLECTANCE; VISUAL-PERCEPTION; CONSTANCY
AB Understanding and modeling perceived properties of sky-dome illumination is an important but challenging problem due to the interplay of several factors such as the materials and geometries of the objects present in the scene being observed. Existing models of sky-dome illumination focus on the physical properties of the sky. However, these parametric models often do not align well with the properties perceived by a human observer. In this work, drawing inspiration from the Hosek-Wilkie sky-dome model, we investigate the perceptual properties of outdoor illumination. For this purpose, we perform a large-scale user study via crowdsourcing to collect a dataset of perceived illumination properties (scattering, glare, and brightness) for different combinations of geometries and materials under a variety of outdoor illuminations, totaling 5,000 distinct images. We perform a thorough statistical analysis of the collected data which reveals several interesting effects. For instance, our analysis shows that when there are objects in the scene made of rough materials, the perceived scattering of the sky increases. Furthermore, we utilize our extensive collection of images and their corresponding perceptual attributes to train a predictor. This predictor, when provided with a single image as input, generates an estimation of perceived illumination properties that align with human perceptual judgments. Accurately estimating perceived illumination properties can greatly enhance the overall quality of integrating virtual objects into real scene photographs. Consequently, we showcase various applications of our predictor. For instance, we demonstrate its utility as a luminance editing tool for showcasing virtual objects in outdoor scenes.
C1 [Wang, Miao; Zhou, Jin-Chao; Feng, Wei-Qi; Jiang, Yu-Zhu] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Miao] Zhongguancun Lab, Beijing 100094, Peoples R China.
   [Serrano, Ana] Univ Zaragoza, Zaragoza 50018, Spain.
C3 Beihang University; Zhongguancun Laboratory; University of Zaragoza
RP Wang, M (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM miaow@buaa.edu.cn; zhoujinchao@buaa.edu.cn; fengweiqi@buaa.edu.cn;
   20373779@buaa.edu.cn; anase@unizar.es
RI Jiang, Yuzhu/R-3246-2017; Serrano Pacheu, Ana Belen/ABC-3358-2021
OI Serrano Pacheu, Ana Belen/0000-0002-7796-3177
FU National Natural Science Foundation of China [62372025, 61932003];
   Fundamental Research Funds for the Central Universities; Spanish Agencia
   Estatal de Investigacion [PID2022-141539NB-I00]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62372025 and 61932003, and in part by
   the Fundamental Research Funds for the Central Universities. The work of
   Ana Serrano was supported by the Spanish Agencia Estatal de
   Investigacion under Grant PID2022-141539NB-I00.
CR Adams WJ, 2018, J VISION, V18, DOI 10.1167/18.13.4
   Agresti A, 2011, Categorical data analysis
   Anderson BL, 2011, CURR BIOL, V21, pR978, DOI 10.1016/j.cub.2011.11.022
   Brossier P., 2004, P 30 ANN INT COMP MU, P1
   Bruneton E, 2008, COMPUT GRAPH FORUM, V27, P1079, DOI 10.1111/j.1467-8659.2008.01245.x
   Bulthoff H. H., 2005, ACM Transactions on Applied Perception, V2, P346, DOI DOI 10.1145/1077399.1077409
   Chadwick AC, 2015, VISION RES, V109, P221, DOI 10.1016/j.visres.2014.10.026
   Chen B, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555406
   Chen B, 2021, VISUAL COMPUT, V37, P2975, DOI 10.1007/s00371-021-02227-x
   Christensen R. H. B., 2018, J STAT SOFTW, V35
   Cunningham DouglasW., 2011, Experimental Design: From User Studies to Psychophysics
   Doroudi S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2623, DOI 10.1145/2858036.2858268
   Fleming RW, 2017, ANNU REV VIS SCI, V3, P365, DOI 10.1146/annurev-vision-102016-061429
   Fleming RW, 2015, VISION RES, V115, P157, DOI 10.1016/j.visres.2015.08.006
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Gardner MA, 2017, Arxiv, DOI arXiv:1704.00090
   Gilchrist A, 1999, PSYCHOL REV, V106, P795, DOI 10.1037/0033-295X.106.4.795
   Gkioulekas I, 2015, PROC CVPR IEEE, P5528, DOI 10.1109/CVPR.2015.7299192
   Guo J, 2020, IEEE T VIS COMPUT GR, V26, P1476, DOI 10.1109/TVCG.2018.2872709
   Havran V, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hold-Geoffroy Y, 2019, PROC CVPR IEEE, P6920, DOI 10.1109/CVPR.2019.00709
   Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255
   Hosek L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185591
   Jakob W., 2022, MITSUBA 3 RENDERER
   Jakob Wenzel, 2010, Mitsuba Renderer
   Kingma D.P., 2014, P INT C LEARNING REP
   Lagunas M, 2021, J VISION, V21, DOI 10.1167/jov.21.2.2
   Lagunas M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323036
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Marlow PJ, 2013, J VISION, V13, DOI 10.1167/13.14.2
   Marlow PJ, 2012, CURR BIOL, V22, P1909, DOI 10.1016/j.cub.2012.08.009
   MCCULLAGH P, 1980, J ROY STAT SOC B MET, V42, P109
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Motoyoshi I, 2012, VISION RES, V53, P30, DOI 10.1016/j.visres.2011.11.010
   Nunnally J. C., 1994, Psychometric theory, V3
   Olkkonen M, 2010, J VISION, V10, DOI 10.1167/10.9.5
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Schwartz G, 2020, IEEE T PATTERN ANAL, V42, P1981, DOI 10.1109/TPAMI.2019.2907850
   Serrano A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459813
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivasan PP, 2020, PROC CVPR IEEE, P8077, DOI 10.1109/CVPR42600.2020.00810
   Storrs KR, 2021, NAT HUM BEHAV, V5, P1402, DOI 10.1038/s41562-021-01097-6
   Sun TC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275026
   Todd JT, 2004, PSYCHOL SCI, V15, P33, DOI 10.1111/j.0963-7214.2004.01501006.x
   Toscani M, 2019, I-PERCEPTION, V10, DOI 10.1177/2041669519884335
   Toscani M, 2017, VISION RES, V131, P82, DOI 10.1016/j.visres.2016.12.004
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   Welinder P., 2010, ADV NEURAL INFORM PR, V23, P1, DOI DOI 10.5555/2997046.2997166
   Xu HY, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0261-2
   Xu JP, 2022, AAAI CONF ARTIF INTE, P2857
   Yu PP, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-021-3282-4
   Zhan FN, 2022, IEEE T IMAGE PROCESS, V31, P2268, DOI 10.1109/TIP.2022.3151997
   Zhang F, 2020, J VISION, V20, DOI 10.1167/jov.20.4.13
   Zhang J, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-1523-9
NR 56
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6468
EP 6480
DI 10.1109/TVCG.2023.3347560
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000035
PM 38150337
DA 2024-11-06
ER

PT J
AU Zhang, ZY
   Chen, JL
   Fu, HB
   Zhao, JJ
   Chen, SY
   Gao, L
AF Zhang, Zhaoyang
   Chen, Junliang
   Fu, Hongbo
   Zhao, Jianjun
   Chen, Shu-Yu
   Gao, Lin
TI Text2Face: Text-Based Face Generation With Geometry and Appearance
   Control
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Faces; Geometry; Feature extraction; Semantics; Pipelines; Nose; Image
   synthesis; Image generation; face editing; sketching interface;
   text-based user interaction
AB Recent years have witnessed the emergence of various techniques proposed for text-based human face generation and manipulation. Such methods, targeting bridging the semantic gap between text and visual contents, provide users with a deft hand to turn ideas into visuals via text interface and enable more diversified multimedia applications. However, due to the flexibility of linguistic expressiveness, the mapping from sentences to desired facial images is clearly many-to-many, causing ambiguities during text-to-face generation. To alleviate these ambiguities, we introduce a local-to-global framework with two graph neural networks (one for geometry and the other for appearance) embedded to model the inter-dependency among facial parts. This is based upon our key observation that the geometry and appearance attributes among different facial components are not mutually independent, i.e., the combinations of part-level facial features are not arbitrary and thus do not conform to a uniform distribution. By learning from the dataset distribution and enabling recommendations given partial descriptions of human faces, these networks are highly suitable for our text-to-face task. Our method is capable of generating high-quality attribute-conditioned facial images from text. Extensive experiments have confirmed the superiority and usability of our method over the prior art.
C1 [Zhang, Zhaoyang] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
   [Chen, Shu-Yu; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Chen, Shu-Yu; Gao, Lin] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Chen, Junliang; Zhao, Jianjun] Beijing Film Acad, Dept Film & TV Technol, Beijing 100088, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Kowloon, Hong Kong, Peoples R China.
C3 Yale University; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; City University of Hong Kong
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.; Gao, L (corresponding author), Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
EM zhaoyang.zhang@yale.edu; juneleungchan@gmail.com; hongbofu@cityu.edu.hk;
   zhaojianjun@bfa.edu.cn; chenshuyu@ict.ac.cn; gaolin@ict.ac.cn
RI Zhang, Zhaoyang/LBH-8145-2024; Zhao, Jianjun/ABE-2301-2021
OI FU, Hongbo/0000-0002-0284-726X; Zhang, Zhaoyang/0000-0002-8892-1191;
   CHEN, JUNLIANG/0009-0002-3812-4597
FU National Natural Science Foundation of China [62061136007, 62102403];
   Beijing Municipal Natural Science Foundation for Distinguished Young
   Scholars [JQ21013]; Beijing Municipal Science and Technology Commission
   [Z231100005923031]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 62061136007, 62102403 in part by the Beijing
   Municipal Natural Science Foundation for Distinguished Young Scholars
   under Grant JQ21013, and in part by Beijing Municipal Science and
   Technology Commission under Grant Z231100005923031.
CR Abdal R, 2020, Arxiv, DOI arXiv:2008.02401
   Agostinelli A., 2023, arXiv
   Alibaba, 2020, ABOUT US
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Cheng Jun, 2020, CVPR, P10911
   Face++, 2020, ABOUT US
   Härkönen E, 2020, ADV NEUR IN, V33
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZQ, 2023, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR52729.2023.00589
   Jiang YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13779, DOI 10.1109/ICCV48922.2021.01354
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2020, ADV NEUR IN, V33
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   2022, Arxiv, DOI arXiv:2209.15352
   Kwon G, 2022, Arxiv, DOI arXiv:2112.00374
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Ma YY, 2023, Arxiv, DOI arXiv:2303.09319
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Michel O, 2022, PROC CVPR IEEE, P13482, DOI 10.1109/CVPR52688.2022.01313
   microsoft, 2020, About us
   Montani Ines, 2023, Zenodo
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Pinkney J., 2022, arXiv
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, arXiv, DOI DOI 10.48550/ARXIV.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Rombach R., 2021, arXiv
   Ruiz N, 2023, PROC CVPR IEEE, P22500, DOI 10.1109/CVPR52729.2023.02155
   Sangkloy P, 2022, LECT NOTES COMPUT SC, V13698, P251, DOI 10.1007/978-3-031-19839-7_15
   seeprettyface, 2020, ABOUT US
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SmartClick, 2021, ABOUT US
   Sun JX, 2022, PROC CVPR IEEE, P18666, DOI 10.1109/CVPR52688.2022.01813
   Tan ZT, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392488
   Tewari A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417803
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Vinker Y, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530068
   Wah C., 2011, CNSTR2011001 CAL I T
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677
   Xia WH, 2021, Arxiv, DOI arXiv:2104.08910
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xiao CF, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480502
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang DC, 2023, IEEE-ACM T AUDIO SPE, V31, P1720, DOI 10.1109/TASLP.2023.3268730
   Zhu B., 2020, arXiv
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 53
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6481
EP 6492
DI 10.1109/TVCG.2023.3349050
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000023
PM 38165798
DA 2024-11-06
ER

PT J
AU Wan, TJ
   Wei, YS
   Shi, RK
   Shen, JX
   Kristensson, PO
   Atkinson, K
   Liang, HN
AF Wan, Tingjie
   Wei, Yushi
   Shi, Rongkai
   Shen, Junxiao
   Kristensson, Per Ola
   Atkinson, Katie
   Liang, Hai-Ning
TI Design and Evaluation of Controller-Based Raycasting Methods for
   Efficient Alphanumeric and Special Character Entry in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Keyboards; Layout; Standards; Switches; Touch sensitive screens; Task
   analysis; Electronic mail; Virtual reality; text entry; keyboard layout;
   alphanumeric and special character entry; mode-switching; user study
AB Alphanumeric and special characters are essential during text entry. Text entry in virtual reality (VR) is usually performed on a virtual Qwerty keyboard to minimize the need to learn new layouts. As such, entering capitals, symbols, and numbers in VR is often a direct migration from a physical/touchscreen Qwerty keyboard-that is, using the mode-switching keys to switch between different types of characters and symbols. However, there are inherent differences between a keyboard in VR and a physical/touchscreen keyboard, and as such, a direct adaptation of mode-switching via switch keys may not be suitable for VR. The high flexibility afforded by VR opens up more possibilities for entering alphanumeric and special characters using the Qwerty layout. In this work, we designed two controller-based raycasting text entry methods for alphanumeric and special characters input (Layer-ButtonSwitch and Key-ButtonSwitch) and compared them with two other methods (Standard Qwerty Keyboard and Layer-PointSwitch) that were derived from physical and soft Qwerty keyboards. We explored the performance and user preference of these four methods via two user studies (one short-term and one prolonged use), where participants were instructed to input text containing alphanumeric and special characters. Our results show that Layer-ButtonSwitch led to the highest statistically significant performance, followed by Key-ButtonSwitch and Standard Qwerty Keyboard, while Layer-PointSwitch had the slowest speed. With continuous practice, participants' performance using Key-ButtonSwitch reached that of Layer-ButtonSwitch. Further, the results show that the key-level layout used in Key-ButtonSwitch led users to parallel mode switching and character input operations because this layout showed all characters on one layer. We distill three recommendations from the results that can help guide the design of text entry techniques for alphanumeric and special characters in VR.
C1 [Wan, Tingjie; Wei, Yushi; Shi, Rongkai] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215000, Peoples R China.
   [Wan, Tingjie; Wei, Yushi; Shi, Rongkai; Atkinson, Katie] Univ Liverpool, Dept Comp Sci, Liverpool L69 3BX, England.
   [Shen, Junxiao; Kristensson, Per Ola] Univ Cambridge, Dept Engn, Cambridge CB2 1TN, England.
   [Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou 215000, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; University of Liverpool; University
   of Cambridge; Xi'an Jiaotong-Liverpool University
RP Liang, HN (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou 215000, Peoples R China.
EM tingjie.wan20@student.xjtlu.edu.cn; yushi.wei21@student.xjtlu.edu.cn;
   rongkai.shi19@student.xjtlu.edu.cn; js2283@cam.ac.uk; pok21@cam.ac.uk;
   katie@liverpool.ac.uk; haining.liang@xjtlu.edu.cn
OI Liang, Hai-Ning/0000-0003-3600-8955; Shen, Junxiao/0000-0002-1552-4689;
   wan, tingjie/0009-0003-0237-9587; Shi, Rongkai/0000-0001-8845-6034; Wei,
   Yushi/0000-0002-6003-0557; Kristensson, Per Ola/0000-0002-7139-871X
FU Suzhou Municipal Key Laboratory for Intelligent Virtual Engineering
   [SZS2022004]; National Natural Science Foundation of China [62272396];
   Xi'an Jiaotong-Liverpool University [RDF-17-01-54]
FX This work was supported in part by Suzhou Municipal Key Laboratory for
   Intelligent Virtual Engineering under Grant SZS2022004, in part by the
   National Natural Science Foundation of China under Grant 62272396, and
   in part by Xi'an Jiaotong-Liverpool University under Grant RDF-17-01-54.
CR Argelaguet F, 2013, COMPUT GRAPH-UK, V37, P121, DOI 10.1016/j.cag.2012.12.003
   Arif AS, 2009, IEEE TIC-STH 09: 2009 IEEE TORONTO INTERNATIONAL CONFERENCE: SCIENCE AND TECHNOLOGY FOR HUMANITY, P100, DOI 10.1109/TIC-STH.2009.5444533
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Boletsis C, 2019, TECHNOLOGIES, V7, DOI 10.3390/technologies7020031
   Boletsis Costas., 2019, INT J VIRTUAL REALIT, V19, P2, DOI DOI 10.20870/IJVR.2019.19.3.2917
   Brooke J., 1995, USABILITY EVAL IND, P189
   Chen SB, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312762
   Cuaresma Justin., 2013, Proceedings of the International Conference on Multimedia and Human-Computer Interaction-MHCI. Citeseer, P126
   Dresner E, 2010, COMMUN THEOR, V20, P249, DOI 10.1111/j.1468-2885.2010.01362.x
   Dudley JJ, 2019, INT SYM MIX AUGMENT, P289, DOI 10.1109/ISMAR.2019.00027
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Francis W. Nelson, 1979, Brown Corpus Manual
   George D., 2019, IBM SPSS STAT 26 STE, DOI DOI 10.4324/9780429056765
   Grubert J, 2018, IEEE COMPUT GRAPH, V38, P125, DOI 10.1109/MCG.2018.2875609
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P151, DOI 10.1109/VR.2018.8446250
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P159, DOI 10.1109/VR.2018.8446059
   Gupta A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300244
   Hart C.L., 2007, N AM J PSYCHOL, V9, P201
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Henderson Jay., 2020, 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services. MobileHCI20, P1, DOI DOI 10.1145/3379503.3403549
   HIRSCH RS, 1970, J APPL PSYCHOL, V54, P484, DOI 10.1037/h0030143
   Jiang HY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P692, DOI [10.1109/VR46266.2020.1581236395562, 10.1109/VR46266.2020.00-12]
   Johnson B. A., 1995, Graduate Res. Papers
   Johnson J., 1989, SIGCHI Bulletin, V20, P38, DOI 10.1145/67243.67248
   Kim W, 2022, APPL ERGON, V104, DOI 10.1016/j.apergo.2022.103819
   Knierim P, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173919
   Kristensson PO, 2015, COMPUTER, V48, P84, DOI 10.1109/MC.2015.185
   Kristensson PO, 2009, AI MAG, V30, P85, DOI 10.1609/aimag.v30i4.2269
   Lu Xueshi, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P815, DOI 10.1145/3472749.3474788
   Lu XS, 2020, INT SYM MIX AUGMENT, P344, DOI 10.1109/ISMAR50242.2020.00061
   Lyddy F, 2014, J COMPUT-MEDIAT COMM, V19, P546, DOI 10.1111/jcc4.12045
   Ma J, 2014, P IEEE S SECUR PRIV, P689, DOI 10.1109/SP.2014.50
   Ma XY, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P263, DOI 10.1145/3172944.3172988
   MacKenzie IS, 2001, BEHAV INFORM TECHNOL, V20, P411, DOI 10.1080/01449290110089561
   Ogitani T, 2018, INT CON ADV INFO NET, P342, DOI 10.1109/AINA.2018.00059
   Park C, 2019, PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS '19), P361, DOI 10.1145/3343055.3360752
   Pham DM, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364265
   Pick S, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P109, DOI 10.1109/3DUI.2016.7460039
   Prabaswari A.D., 2019, IOP Conference Series: Materials Science and Engineering, V528, DOI [DOI 10.1088/1757-899X/528/1/012018, 10.1088/1757-899X/528/1/012018]
   Rajanna V, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204541
   Schneider D, 2019, IEEE T VIS COMPUT GR, V25, P3190, DOI 10.1109/TVCG.2019.2932239
   Sears A, 2003, INT J HUM-COMPUT INT, V16, P163, DOI 10.1207/S15327590IJHC1602_03
   Sellen A. J., 1992, Human-Computer Interaction, V7, P141, DOI 10.1207/s15327051hci0702_1
   Shi R., 2023, Proc. ACM Hum.- Comput. Interact., V7, DOI DOI 10.1145/3591129
   Shi RK, 2021, INT SYM MIX AUGMENT, P118, DOI 10.1109/ISMAR52148.2021.00026
   Smith J., 2019, P 45 GRAPH INT C P G, DOI [10.20380/G12019.20, DOI 10.20380/G12019.20]
   Smith J, 2020, PROCEEDINGS OF THE WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES AVI 2020, DOI 10.1145/3399715.3399850
   Song ZM, 2022, INT SYM MIX AUGMENT, P864, DOI 10.1109/ISMAR55827.2022.00105
   Soukoreff R. W., 2003, P SIGCHI C HUM FACT, P113, DOI DOI 10.1145/642611.642632
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Stark L, 2015, SOC MEDIA SOC, V1, DOI 10.1177/2056305115604853
   Surale HB, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300426
   Thompson GB, 2009, COGN NEUROPSYCHOL, V26, P50, DOI 10.1080/02643290802200838
   Tominaga K, 2021, 5TH ASIAN CHI SYMPOSIUM PROCEEDINGS, P25, DOI 10.1145/3429360.3468174
   Vertanen K., 2011, P 13 INT C HUM COMP, P295, DOI [DOI 10.1145/2037373.2037418, 10.1145/2037373.2037418]
   Wan TJ, 2024, VIRTUAL REAL-LONDON, V28, DOI 10.1007/s10055-023-00902-z
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Xu WG, 2019, INT SYM MIX AUGMENT, P279, DOI 10.1109/ISMAR.2019.00026
   Xu WG, 2019, IEEE T VIS COMPUT GR, V25, P1991, DOI 10.1109/TVCG.2019.2898736
   Yamada H., 1980, J. Inf. Process., V2, P175
   Yildirim Caglar, 2020, HCI International 2020 - Late Breaking Papers. Virtual and Augmented Reality. 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12428), P450, DOI 10.1007/978-3-030-59990-4_33
   Yildirim C, 2023, INT J HUM-COMPUT INT, V39, P3815, DOI 10.1080/10447318.2022.2107330
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
   Yu DF, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445343
   Yu DF, 2018, IEEE T VIS COMPUT GR, V24, P2927, DOI 10.1109/TVCG.2018.2868581
NR 65
TC 5
Z9 5
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6493
EP 6506
DI 10.1109/TVCG.2024.3349428
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000009
PM 38170655
DA 2024-11-06
ER

PT J
AU Bujack, R
   Caffrey, E
   Teti, E
   Turton, TL
   Rogers, DH
   Miller, J
AF Bujack, Roxana
   Caffrey, Elektra
   Teti, Emily
   Turton, Terece L.
   Rogers, David H.
   Miller, Jonah
TI Efficient Computation of Geodesics in Color Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Color space; colormaps; geodesics; Riemannian; line element; color
   perception; human cognition; Color space; colormaps; geodesics;
   Riemannian; line element; color perception; human cognition
ID SURFACE; SHAPE
AB Although scientists agree that a perceptual color space is not Euclidean and color difference measures, such as CIELAB's Delta E-2000, model these aspects of color perception, colormaps are still mostly evaluated through piecewise linear interpolation in a Euclidean color space. In a non-Euclidean setting, the piecewise linear interpolation of a colormap through control points translates to finding shortest paths. Alternatively, a smooth interpolation can be generalized to finding the straightest path. Both approaches are difficult to solve and are compute intensive. We compare the 11 most promising optimization algorithms for the computation of a geodesic either as the shortest or as the straightest path to find the most efficient one to use for colormap interpolation in real-world applications. For two control points, the zero curvature algorithms excelled, especially the 2D relaxation method. For multiple control points, only the mimimal curvature algorithms can produce smooth curves, amongst which the 1D relaxation method performed best.
C1 [Bujack, Roxana] Los Alamos Natl Lab, Appl Comp Sci, Los Alamos, NM 87545 USA.
   [Caffrey, Elektra; Rogers, David H.] Los Alamos Natl Lab, CCS3, Los Alamos, NM 87545 USA.
   [Teti, Emily] Los Alamos Natl Lab, NEN 1, Los Alamos, NM 87545 USA.
   [Turton, Terece L.] Los Alamos Natl Lab, CCS 7 Data Sci Scale Team, Los Alamos, NM 87545 USA.
   [Miller, Jonah] Los Alamos Natl Lab, CCS 2, Los Alamos, NM 87545 USA.
C3 United States Department of Energy (DOE); Los Alamos National
   Laboratory; United States Department of Energy (DOE); Los Alamos
   National Laboratory; United States Department of Energy (DOE); Los
   Alamos National Laboratory; United States Department of Energy (DOE);
   Los Alamos National Laboratory; United States Department of Energy
   (DOE); Los Alamos National Laboratory
RP Bujack, R (corresponding author), Los Alamos Natl Lab, Appl Comp Sci, Los Alamos, NM 87545 USA.
EM bujack@lanl.gov; elektracaffrey@gmail.com; esteti@lanl.gov;
   tlturton@lanl.gov; dhr@lanl.gov; jonahm@lanl.gov
OI Bujack, Roxana/0000-0002-5479-3726; Turton, Terece/0000-0003-4345-7783;
   Miller, Jonah/0000-0001-6432-7860
FU LANL [LA-UR-21-24634]; Los Alamos National Laboratory through Laboratory
   Directed Research and Development Program [20200512ECR]; National
   Nuclear Security Administration (NNSA) Advanced Simulation and Computing
   (ASC) Program
FX This work was supported in part by LANL under Grant LA-UR-21-24634, in
   part by Los Alamos National Laboratory through Laboratory Directed
   Research and Development Program under Grant 20200512ECR, and in part by
   the National Nuclear Security Administration (NNSA) Advanced Simulation
   and Computing (ASC) Program.
CR Ahrens J., 2005, Vis. Handb., P717, DOI 10.1016/B978-012387582-2/50038-1
   Bishop RL., 1964, Trans Am Math Soc, V112, P508
   Bondy JA., 2008, Graph Theory
   Boyd J.P., 2013, Chebyshev and Fourier Spectral Methods, V2nd
   Bujack Roxana, 2022, Proc Natl Acad Sci U S A, V119, pe2119753119, DOI 10.1073/pnas.2119753119
   Bujack R, 2018, 2018 IEEE SCIENTIFIC VISUALIZATION CONFERENCE (SCIVIS), P32, DOI 10.1109/SciVis.2018.8823772
   Bujack R, 2018, IEEE T VIS COMPUT GR, V24, P923, DOI 10.1109/TVCG.2017.2743978
   Commission Internationale de lEclairage, 2004, Colorimetry, V3rd
   Crane K., 2020, arXiv, DOI DOI 10.48550/ARXIV.2007.10430
   Ebner F, 1998, P SOC PHOTO-OPT INS, V3300, P107, DOI 10.1117/12.298269
   Fechner G.T., 1889, ELEMENTE PSYCHOPHYSI, V2nd
   Friele L., 1961, Farbe, V10, P193
   Geem ZW, 2001, SIMULATION, V76, P60, DOI 10.1177/003754970107600201
   Guild J., Philos. Trans. Roy.Soc. London
   Judd D. B., 1968, Palette, V29, P4
   Kanai T, 2001, COMPUT AIDED DESIGN, V33, P801, DOI 10.1016/S0010-4485(01)00097-5
   Koenderink J, 2018, I-PERCEPTION, V9, DOI 10.1177/2041669518803971
   Lanthier M., 1997, Proceedings of the Thirteenth Annual Symposium on Computational Geometry, P274, DOI 10.1145/262839.262984
   Laugwitz D., 2014, Differential and Riemannian Geometry
   LEVKOWITZ H, 1992, IEEE COMPUT GRAPH, V12, P72, DOI 10.1109/38.135886
   Liu BQ, 2017, COMPUT AIDED DESIGN, V90, P105, DOI 10.1016/j.cad.2017.05.022
   MacAdam DL, 1942, J OPT SOC AM, V32, P247, DOI 10.1364/JOSA.32.000247
   MACADAM DL, 1963, J OPT SOC AM, V53, P754, DOI 10.1364/JOSA.53.000754
   Miller JM, 2017, CLASSICAL QUANT GRAV, V34, DOI 10.1088/1361-6382/34/1/015003
   Moreland K, 2009, LECT NOTES COMPUT SC, V5876, P92, DOI 10.1007/978-3-642-10520-3_9
   Nardini P, 2021, COMPUT GRAPH FORUM, V40, P361, DOI 10.1111/cgf.14313
   Nardini P, 2021, IEEE T VIS COMPUT GR, V27, P3048, DOI 10.1109/TVCG.2019.2961674
   Newsam G., 1991, P THEOR NUM ASP GEOM, P179
   Ortega J.M., 2000, Classics in Applied Mathematics, V30
   OWEN DB, 1965, TECHNOMETRICS, V7, P78, DOI 10.2307/1266136
   Pant DR, 2013, COLOR RES APPL, V38, P259, DOI 10.1002/col.20751
   Pant DR, 2012, COLOR RES APPL, V37, P429, DOI 10.1002/col.20710
   Peyré G, 2009, COMPUT METH APPL SCI, V13, P29
   PHAM B, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P202, DOI 10.1109/VISUAL.1990.146383
   Riemann B., 1854, Konigliche Gesellschaft der Wissenschaften und der GeorgAugustus-Universitat Gottingen, V13
   ROBERTSON PK, 1986, IEEE COMPUT GRAPH, V6, P24, DOI 10.1109/MCG.1986.276688
   Rogowitz BE, 1998, IEEE SPECTRUM, V35, P52, DOI 10.1109/6.736450
   Sauer R., 1970, Differenzengeometrie
   Schrodinger E, 1920, ANN PHYS-BERLIN, V63, P481
   Smart S, 2020, IEEE T VIS COMPUT GR, V26, P1215, DOI 10.1109/TVCG.2019.2934284
   STILES WS, 1946, P PHYS SOC LOND, V58, P41, DOI 10.1088/0959-5309/58/1/305
   Stoer J., 1980, Introduction to numerical analysis, V1993
   TRUMBO BE, 1981, AM STAT, V35, P220, DOI 10.2307/2683294
   von Helmholtz H., 1883, Wissenschaftliche Abhandlungen, V2
   Vos JJ, 2006, CLIN EXP OPTOM, V89, P348, DOI 10.1111/j.1444-0938.2006.00091.x
   VOS JJ, 1972, VISION RES, V12, P1327, DOI 10.1016/0042-6989(72)90181-2
   VOS JJ, 1979, COLOR RES APPL, V4, P208, DOI 10.1002/col.5080040406
   WAINER H, 1980, AM STAT, V34, P81, DOI 10.2307/2684111
   Wright W.D., 1929, T OPTICAL SOC, V30, P141, DOI [10.1088/1475-4878/30/4/301, DOI 10.1088/1475-4878/30/4/301]
   Xie Q, 2013, IEEE I CONF COMP VIS, P865, DOI 10.1109/ICCV.2013.112
   Yuan N, 2023, IEEE T VIS COMPUT GR, V29, P1951, DOI 10.1109/TVCG.2021.3135021
   Zeyen M, 2018, 2018 IEEE SCIENTIFIC VISUALIZATION CONFERENCE (SCIVIS), P11, DOI 10.1109/SciVis.2018.8823597
NR 52
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6507
EP 6519
DI 10.1109/TVCG.2023.3346673
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000005
PM 38145515
DA 2024-11-06
ER

PT J
AU Coscia, A
   Endert, A
AF Coscia, Adam
   Endert, Alex
TI KnowledgeVIS: Interpreting Language Models by Comparing
   Fill-in-the-Blank Prompts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Predictive models; Task analysis; Analytical models; Transformers;
   Semantics; Visual analytics; Adaptation models; language models;
   prompting; interpretability; machine learning
AB Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVIS, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVIS reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVIS with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.
C1 [Coscia, Adam; Endert, Alex] Georgia Inst Technol, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Coscia, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM acoscia6@gatech.edu; endert@gatech.edu
RI Coscia, Adam/GUO-9221-2022
OI Coscia, Adam/0000-0002-0429-9295
FU NSF [IIS-1750474, DRL-2247790]
FX This work was supported by NSF under Grants IIS-1750474 and DRL-2247790.
CR Abid A, 2021, AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P298, DOI 10.1145/3461702.3462624
   Atanasova P, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P3256
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bederson B. B., 2000, UIST. Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology, P217, DOI 10.1145/354401.354782
   Beltagy I, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3615
   Bender EM, 2021, PROCEEDINGS OF THE 2021 ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2021, P610, DOI 10.1145/3442188.3445922
   Brown T.B., 2020, Advances in neural information processing systems, V33, P1877
   Clark K, 2019, BLACKBOXNLP WORKSHOP ON ANALYZING AND INTERPRETING NEURAL NETWORKS FOR NLP AT ACL 2019, P276, DOI 10.18653/v1/w19-4828
   Collins Christopher, 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P91, DOI 10.1109/VAST.2009.5333443
   DeRose JF, 2021, IEEE T VIS COMPUT GR, V27, P1160, DOI 10.1109/TVCG.2020.3028976
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Elazar Y, 2021, T ASSOC COMPUT LING, V9, P1012, DOI 10.1162/tacl_a_00410
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Hoover B, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020): SYSTEM DEMONSTRATIONS, P187
   Jain S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P3543
   Jiang ZB, 2020, T ASSOC COMPUT LING, V8, P423, DOI 10.1162/tacl_a_00324
   Jin Q, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2567
   Lau J.H., 2011, P 49 ANN M ASS COMP, P1536
   Liu PF, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3560815
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Mikolov T, 2013, Arxiv, DOI arXiv:1301.3781
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mullner D., 2011, arXiv, DOI [10.48550/ARXIV.1109.2378, DOI 10.48550/ARXIV.1109.2378]
   Nozza D, 2022, PROCEEDINGS OF THE SECOND WORKSHOP ON LANGUAGE TECHNOLOGY FOR EQUALITY, DIVERSITY AND INCLUSION (LTEDI 2022), P26
   Nozza D, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P2398
   OLSEN KA, 1993, INFORM PROCESS MANAG, V29, P69, DOI 10.1016/0306-4573(93)90024-8
   Petroni F, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2463
   Qin GH, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5203
   Roberts A, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P5418
   Rogers A, 2020, T ASSOC COMPUT LING, V8, P842, DOI 10.1162/tacl_a_00349
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Sanh V, 2020, Arxiv, DOI [arXiv:1910.01108, 10.48550/ARXIV.1910.01108]
   Shin T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4222
   Srivastava A., 2023, Trans. Mach. Learn. Res, P1
   Strobelt H, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P96
   Strobelt Hendrik, 2023, IEEE Trans Vis Comput Graph, V29, P1146, DOI 10.1109/TVCG.2022.3209479
   Strobelt H, 2019, IEEE T VIS COMPUT GR, V25, P353, DOI 10.1109/TVCG.2018.2865044
   Tenney I, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P107
   Vaswani A., 2017, Advances in neural information processing systems
   Vig J, 2019, PROCEEDINGS OF THE 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, (ACL 2019), P37
   Wang CH, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P253
   Wang Z. J., 2021, P 1 WORKSHOP BRIDGIN, P47, DOI [DOI 10.48550/ARXIV.2103.04044, 10.48550/arXiv.2103.04044]
   Wang ZJ, 2021, ACL-IJCNLP 2021: THE JOINT CONFERENCE OF THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING: PROCEEDINGS OF THE SYSTEM DEMONSTRATIONS, P132
   Wang ZJ, 2021, IEEE T VIS COMPUT GR, V27, P1396, DOI 10.1109/TVCG.2020.3030418
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Warstadt A, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2877
   Wolf T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P38
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Yi J. S., 2005, Information Visualization, V4, P239, DOI 10.1057/palgrave.ivs.9500099
   Yu Gu, 2022, ACM Transactions on Computing and Healthcare, V3, DOI 10.1145/3458754
   Zhong Z, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5017
NR 53
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6520
EP 6532
DI 10.1109/TVCG.2023.3346713
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000002
PM 38145514
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Yu, D
   Lau, M
   Gao, L
   Fu, HB
AF Yu, Deng
   Lau, Manfred
   Gao, Lin
   Fu, Hongbo
TI Sketch Beautification: Learning Part Beautification and Structure
   Refinement for Sketches of Man-Made Objects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Training; Interpolation; Manifolds; Layout; Image
   reconstruction; Geometry; Sketch assembly; sketch beautification; sketch
   implicit representation
AB We present a novel freehand sketch beautification method, which takes as input a freely drawn sketch of a man-made object and automatically beautifies it both geometrically and structurally. Beautifying a sketch is challenging because of its highly abstract and heavily diverse drawing manner. Existing methods are usually confined to their limited training samples and thus cannot beautify freely drawn sketches with both geometric and structural variations. To address this challenge, we adopt a divide-and-combine strategy. Specifically, we first parse an input sketch into semantic components, beautify individual components by a learned part beautification module based on part-level implicit manifolds, and then reassemble the beautified components through a structure beautification module. With this strategy, our method can go beyond the training samples and handle novel freehand sketches. We demonstrate the effectiveness of our system with extensive experiments and a perceptual study.
C1 [Yu, Deng; Lau, Manfred; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100045, Peoples R China.
   [Gao, Lin] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
C3 City University of Hong Kong; Chinese Academy of Sciences; Institute of
   Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Lau, M; Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
EM dengyucn@gmail.com; manfred.lau@gmail.com; gaolin@ict.ac.cn;
   fuplus@gmail.com
OI YU, Deng/0000-0003-2343-3688; FU, Hongbo/0000-0002-0284-726X
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China under Grants CityU [11212119, 11205420, 11206319]; Chow Sang Sang
   Group Research Fund/Donation; Centre for Applied Computing and
   Interactive Media (ACIM) of the School of Creative Media, CityU
FX This work was supported in part by the Research Grants Council of the
   Hong Kong Special Administrative Region, China under Grants CityU
   11212119, 11206319, and 11205420, in part by Chow Sang Sang Group
   Research Fund/Donation, and in part by the Centre for Applied Computing
   and Interactive Media (ACIM) of the School of Creative Media, CityU.
   Recommended for acceptance by T. Weyrich.
CR [Anonymous], 2018, INT C LEARN REPR ICL
   Bae SH, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P151
   Bhunia A. K., 2022, arXiv
   Bhunia AK, 2021, PROC CVPR IEEE, P5668, DOI 10.1109/CVPR46437.2021.00562
   Bouaziz S., 2014, EUROGRAPHICS TUTORIA
   Chen JX, 2018, LECT NOTES COMPUT SC, V11217, P624, DOI 10.1007/978-3-030-01261-8_37
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Dai GX, 2017, AAAI CONF ARTIF INTE, P4002
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Delanoy J, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203197
   Deng Y, 2021, PROC CVPR IEEE, P10281, DOI 10.1109/CVPR46437.2021.01015
   Dixon D, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P897
   Dutagaci H., 2010, P ACM WORKSH 3D OBJ, P45, DOI [10.1145/1877808.1877819, DOI 10.1145/1877808.1877819]
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Fiser J, 2016, COMPUT GRAPH-UK, V56, P46, DOI 10.1016/j.cag.2016.02.003
   Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Gryaditskaya Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356533
   Huang Z, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2661229.2661280, 10.1145/2661228.2661280]
   Igarashi Takeo., 2007, ACM SIGGRAPH 2007 CO, P18, DOI DOI 10.1145/1281500.1281529
   Jaderberg M, 2015, ADV NEUR IN, V28
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Li CJ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530133
   Li CJ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417807
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Li L, 2021, IEEE T VIS COMPUT GR, V27, P3745, DOI 10.1109/TVCG.2020.2987626
   Li L, 2019, IEEE COMPUT GRAPH, V39, P38, DOI 10.1109/MCG.2018.2884192
   Liu CX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201314
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Orbay G, 2011, IEEE T VIS COMPUT GR, V17, P694, DOI 10.1109/TVCG.2010.105
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paszke A, 2019, ADV NEUR IN, V32
   Pavlidis T., 1985, Computer Graphics, V19, P225, DOI 10.1145/325165.325240
   Ribeiro LSF, 2020, PROC CVPR IEEE, P14141, DOI 10.1109/CVPR42600.2020.01416
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231
   Schor N, 2019, IEEE I CONF COMP VIS, P8758, DOI 10.1109/ICCV.2019.00885
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Smirnov D., 2020, P INT C LEARN REPR
   Sorkine O., 2004, P S GEOM PROC, P175, DOI [DOI 10.1145/1057432.1057456, 10.]
   Su QK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174236
   Su QK, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601202
   Su WC, 2017, SIGGRAPH ASIA 2017 POSTERS (SA'17), DOI 10.1145/3145690.3145744
   Sutherland L. E., 1964, Simulation, V2, pR
   Van Mossel DP, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459777
   Wang B., 2005, P 6 ACM SIGCHI NZ CH, P15
   Wang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275025
   Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091
   Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820
   Xu P, 2018, PROC CVPR IEEE, P8090, DOI 10.1109/CVPR.2018.00844
   Yan C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417784
   Yang LM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450284
   Ye H, 2022, IEEE T VIS COMPUT GR, V28, P2809, DOI 10.1109/TVCG.2020.3049006
   Yu D, 2021, IEEE T CIRC SYST VID, V31, P1738, DOI 10.1109/TCSVT.2020.3015279
   [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
   Zitnick CL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461985
NR 64
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6533
EP 6546
DI 10.1109/TVCG.2023.3346995
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000041
PM 38145518
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhang, G
   Gao, XB
   Meng, HY
   Pang, Y
   Nie, XX
AF Zhang, Guo
   Gao, Xinbo
   Meng, Hongying
   Pang, Yu
   Nie, Xixi
TI A Self-Supervised Network-Based Smoke Removal and Depth Estimation for
   Monocular Endoscopic Videos
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Estimation; Convolution; Generators; Training; Three-dimensional
   displays; Laparoscopes; Videos; CycleGAN; de-endoscopic smoke; HR- net;
   monocular depth estimation; self-supervised learning
ID 3D RECONSTRUCTION; SEGMENTATION; STEREO
AB In minimally invasive surgery videos, label-free monocular laparoscopic depth estimation is challenging due to smoke. For this reason, we propose a self-supervised collaborative network-based depth estimation method with smoke-removal for monocular endoscopic video, which is decomposed into two steps of smoke-removal and depth estimation. In the first step, we develop a de-endoscopic smoke for cyclic GAN (DS-cGAN) to mitigate the smoke components at different concentrations. The designed generator network comprises sharpened guide encoding module (SGEM), residual dense bottleneck module (RDBM) and refined upsampling convolution module (RUCM), which restores more detailed organ edges and tissue structures. In the second step, high resolution residual U-Net (HRR-UNet) consisting of a DepthNet and two PoseNets is designed to improve the depth estimation accuracy, and adjacent frames are used for camera self-motion estimation. In particular, the proposed method requires neither manual labeling nor patient computed tomography scans during the training and inference phases. Experimental studies on the laparoscopic data set of the Hamlyn Centre show that our method can effectively achieve accurate depth information after net smoking in real surgical scenes while preserving the blood vessels, contours and textures of the surgical site. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods in effectiveness and achieves a frame rate of 94.45fps in real time, making it a promising clinical application.
C1 [Zhang, Guo] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Zhang, Guo] Southwest Med Univ, Sch Med Informat & Engn, Luzhou 646000, Peoples R China.
   [Gao, Xinbo; Nie, Xixi] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Meng, Hongying] Brunel Univ London, Dept Elect & Elect Engn, London UB8 PH, England.
   [Pang, Yu] Chongqing Univ Posts & Telecommun, Sch Optoelect Engn, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Southwest Medical
   University; Chongqing University of Posts & Telecommunications; Brunel
   University; Chongqing University of Posts & Telecommunications
RP Nie, XX (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.; Pang, Y (corresponding author), Chongqing Univ Posts & Telecommun, Sch Optoelect Engn, Chongqing 400065, Peoples R China.
EM zhangguo@swmu.edu.cn; gaoxb@cqupt.edu.cn; Hongying.Meng@brunel.ac.uk;
   pangyu@cqupt.edu.cn; s170201012@stu.cqupt.edu.cn
RI ; Meng, Hongying/O-5192-2014
OI Zhang, Guo/0000-0001-8165-2411; Meng, Hongying/0000-0002-8836-1382
FU National Natural Science Foundation of China [62036007, U21A20447,
   61971079]; Chongqing Innovation Group Project [cstc2020jcyj-cxttX0002];
   Science and Technology Research Program of Chongqing Municipal Education
   Commission [KJZD-k202000604]; Sichuan Regional Innovation Cooperation
   Program [2020YFQ0025]; Chongqing Special Project on Technological
   Innovation and Applied Development [cstc2021jscxgksbx0051]; Chongqing
   Natural Science Foundation [CSTB2023NSCQ-LZX0064]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62036007, U21A20447, and 61971079, in part by
   Chongqing Innovation Group Project under Grant cstc2020jcyj-cxttX0002,
   inpart by the Science and Technology Research Program of Chongqing
   Municipal Education Commission under Grant KJZD-k202000604, in part by
   Sichuan Regional Innovation Cooperation Program under Grant 2020YFQ0025,
   in part by Chongqing Special Project on Technological Innovation and
   Applied Development under Grant cstc2021jscxgksbx0051, and in part by
   Chongqing Natural Science Foundation under Grant CSTB2023NSCQ-LZX0064.
CR Antal B, 2016, PECCS: PROCEEDINGS OF THE 6TH INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND EMBEDDED COMPUTING AND COMMUNICATION SYSTEMS, P116, DOI 10.5220/0006008001160121
   Besse F, 2014, INT J COMPUT VISION, V110, P2, DOI 10.1007/s11263-013-0653-9
   Bor-Sheng Huang W. L. H. K., 2022, P IEEE INT C AC SPEE, P4668
   Chen L, 2020, IEEE T MED IMAGING, V39, P1615, DOI 10.1109/TMI.2019.2953717
   Coimbra MT, 2006, IEEE T CIRC SYST VID, V16, P628, DOI 10.1109/TCSVT.2006.873158
   do Nascimento MG, 2019, IEEE I CONF COMP VIS, P5147, DOI 10.1109/ICCV.2019.00525
   Dong JH, 2021, IEEE T CIRC SYST VID, V31, P2020, DOI 10.1109/TCSVT.2020.3016058
   Dudhane A, 2019, IEEE COMPUT SOC CONF, P2014, DOI 10.1109/CVPRW.2019.00253
   Engel D, 2024, IEEE T VIS COMPUT GR, V30, P3981, DOI 10.1109/TVCG.2023.3245305
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Hsia CH, 2016, IEEE SENS J, V16, P4521, DOI 10.1109/JSEN.2016.2542259
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu MX, 2012, MED IMAGE ANAL, V16, P597, DOI 10.1016/j.media.2010.11.002
   Hu X., 2022, P IEEE ACM T COMP BI, P1
   Hu ZS, 2021, IEEE ENG MED BIO, P3070, DOI 10.1109/EMBC46164.2021.9629657
   Huang JC, 2022, IEEE T CIRC SYST VID, V32, P6044, DOI 10.1109/TCSVT.2022.3155626
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Kezheng Lin W. C., 2022, Laser Optoelectron.Prog., V59
   Khened M, 2019, MED IMAGE ANAL, V51, P21, DOI 10.1016/j.media.2018.10.004
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li WJ, 2022, PHYS MED BIOL, V67, DOI 10.1088/1361-6560/ac6724
   Liu XT, 2020, IEEE T MED IMAGING, V39, P1438, DOI 10.1109/TMI.2019.2950936
   Luo HL, 2019, HEALTHC TECHNOL LETT, V6, P154, DOI 10.1049/htl.2019.0063
   Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mahmoud N, 2017, Arxiv, DOI arXiv:1705.09107
   Mahmoud N, 2017, LECT NOTES COMPUT SC, V10170, P72, DOI 10.1007/978-3-319-54057-3_7
   Mao AH, 2023, IEEE T VIS COMPUT GR, V29, P1785, DOI 10.1109/TVCG.2021.3131712
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Penza V, 2018, INT J MED ROBOT COMP, V14, DOI 10.1002/rcs.1926
   Penza V, 2016, INT J COMPUT ASS RAD, V11, P197, DOI 10.1007/s11548-015-1276-0
   Qiu L, 2020, INT J INTELL ROBOT, V4, P252, DOI 10.1007/s41315-020-00127-2
   Salazar-Colores S, 2020, IEEE ACCESS, V8, P208898, DOI 10.1109/ACCESS.2020.3038437
   Shao SW, 2021, IEEE INT CONF ROBOT, P7159, DOI 10.1109/ICRA48506.2021.9561508
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Singh H, 2022, IEEE T SYST MAN CY-S, V52, P2275, DOI 10.1109/TSMC.2021.3049402
   Tao R, 2023, IEEE T MED ROBOT BIO, V5, P42, DOI 10.1109/TMRB.2023.3237867
   Vijayanarasimhan S., 2017, Sfm-net: Learning of structure and motion from video
   Wang CC, 2019, PROC SPIE, V10949, DOI 10.1117/12.2507822
   Wang CC, 2018, PROC SPIE, V10576, DOI 10.1117/12.2297398
   Yan PL, 2018, 2018 4TH ANNUAL INTERNATIONAL CONFERENCE ON NETWORK AND INFORMATION SYSTEMS FOR COMPUTERS (ICNISC 2018), P160, DOI 10.1109/ICNISC.2018.00039
   Yang XB, 2020, IEEE T VIS COMPUT GR, V26, P3446, DOI 10.1109/TVCG.2020.3023634
   Yang ZX, 2021, LECT NOTES COMPUT SC, V12722, P337, DOI 10.1007/978-3-030-80432-9_26
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang G, 2022, FRONT PHYSIOL, V13, DOI 10.3389/fphys.2022.994343
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 1
Z9 1
U1 12
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6547
EP 6559
DI 10.1109/TVCG.2023.3347438
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000017
PM 38153833
DA 2024-11-06
ER

PT J
AU Chaves-de-Plaza, NF
   Mody, P
   Staring, M
   van Egmond, R
   Vilanova, A
   Hildebrandt, K
AF Chaves-de-Plaza, Nicolas F.
   Mody, Prerak
   Staring, Marius
   van Egmond, Rene
   Vilanova, Anna
   Hildebrandt, Klaus
TI Inclusion Depth for Contour Ensembles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Uncertainty; Feature extraction; Data
   models; Computational modeling; Semantic segmentation; Uncertainty
   visualization; contours; ensemble summarization; depth statistics
ID NONPARAMETRIC MODELS; VISUAL ANALYSIS; UNCERTAINTY; VISUALIZATION;
   VARIABILITY
AB Ensembles of contours arise in various applications like simulation, computer-aided design, and semantic segmentation. Uncovering ensemble patterns and analyzing individual members is a challenging task that suffers from clutter. Ensemble statistical summarization can alleviate this issue by permitting analyzing ensembles' distributional components like the mean and median, confidence intervals, and outliers. Contour boxplots, powered by Contour Band Depth (CBD), are a popular non-parametric ensemble summarization method that benefits from CBD's generality, robustness, and theoretical properties. In this work, we introduce Inclusion Depth (ID), a new notion of contour depth with three defining characteristics. First, ID is a generalization of functional Half-Region Depth, which offers several theoretical guarantees. Second, ID relies on a simple principle: the inside/outside relationships between contours. This facilitates implementing ID and understanding its results. Third, the computational complexity of ID scales quadratically in the number of members of the ensemble, improving CBD's cubic complexity. This also in practice speeds up the computation enabling the use of ID for exploring large contour ensembles or in contexts requiring multiple depth evaluations like clustering. In a series of experiments on synthetic data and case studies with meteorological and segmentation data, we evaluate ID's performance and demonstrate its capabilities for the visual analysis of contour ensembles.
C1 [Chaves-de-Plaza, Nicolas F.; van Egmond, Rene; Hildebrandt, Klaus] Delft Univ Technol, NL-2628 CD Delft, Netherlands.
   [Chaves-de-Plaza, Nicolas F.; Mody, Prerak] Holland PTC, NL-2629 JH Delft, Netherlands.
   [Mody, Prerak] Leiden Univ, Med Ctr, Dept Radiol, NL-2333 ZA Leiden, Netherlands.
   [Staring, Marius] Leiden Univ, Med Ctr, Dept Radiol, Dept Radiat Oncol, NL-2333 ZA Leiden, Netherlands.
   [Vilanova, Anna] TU Eindhoven, NL-5612 AZ Eindhoven, Netherlands.
C3 Delft University of Technology; Leiden University - Excl LUMC; Leiden
   University; Leiden University Medical Center (LUMC); Leiden University -
   Excl LUMC; Leiden University; Leiden University Medical Center (LUMC);
   Eindhoven University of Technology
RP Chaves-de-Plaza, NF (corresponding author), Delft Univ Technol, NL-2628 CD Delft, Netherlands.
EM n.f.chavesdeplaza@tudelft.nl; p.p.mody@lumc.nl; m.staring@lumc.nl;
   R.vanEgmond@tudelft.nl; a.vilanova@tue.nl; k.a.hildebrandt@tudelft.nl
RI Mody, Prerak/HGC-2285-2022; VanEgmond, Rene/KOC-9788-2024
OI Chaves-de-Plaza, Nicolas F./0000-0003-4971-3151; Van Egmond,
   Rene/0000-0001-5015-0670; Hildebrandt, Klaus/0000-0002-9196-3923;
   Vilanova, Anna/0000-0002-1034-737X; Mody, Prerak/0000-0001-9697-2258
FU Varian, a Siemens Healthineers Company, through the HollandPTC-Varian
   Consortium [2019022]; Surcharge for Top Consortia for Knowledge and
   Innovation (TKIs) from the Ministry of Economic Affairs and Climate
FX This work was supported in part by Varian, a Siemens Healthineers
   Company, through the HollandPTC-Varian Consortium under Grant 2019022,
   and in part by the Surcharge for Top Consortia for Knowledge and
   Innovation (TKIs) from the Ministry of Economic Affairs and Climate. We
   also thank Rudiger Westermann and Keving Holhein for sharing with us
   ECMWF's meteorological forecasting data.
CR Abdar M, 2021, INFORM FUSION, V76, P243, DOI 10.1016/j.inffus.2021.05.008
   Athawale T, 2016, IEEE T VIS COMPUT GR, V22, P777, DOI 10.1109/TVCG.2015.2467958
   Demir I., 2016, P S VIS NEW YORK NY, P1, DOI DOI 10.1145/3002151.3002165
   Ferstl F, 2016, COMPUT GRAPH FORUM, V35, P221, DOI 10.1111/cgf.12898
   Ferstl F, 2016, IEEE T VIS COMPUT GR, V22, P767, DOI 10.1109/TVCG.2015.2467204
   Gao C, 2020, BERNOULLI, V26, P1139, DOI 10.3150/19-BEJ1144
   Jörnsten R, 2004, J MULTIVARIATE ANAL, V90, P67, DOI 10.1016/S0047-259X(04)00027-2
   Kumpf A, 2018, IEEE T VIS COMPUT GR, V24, P109, DOI 10.1109/TVCG.2017.2745178
   Leutbecher M, 2008, J COMPUT PHYS, V227, P3515, DOI 10.1016/j.jcp.2007.02.014
   LIU RY, 1990, ANN STAT, V18, P405, DOI 10.1214/aos/1176347507
   López-Pintado S, 2011, COMPUT STAT DATA AN, V55, P1679, DOI 10.1016/j.csda.2010.10.024
   López-Pintado S, 2009, J AM STAT ASSOC, V104, P718, DOI 10.1198/jasa.2009.0108
   Ma B, 2019, IEEE T VIS COMPUT GR, V25, P1091, DOI 10.1109/TVCG.2018.2864815
   Mirzargar M, 2018, COMPUT GRAPH FORUM, V37, P13, DOI 10.1111/cgf.13397
   Pöthkow K, 2013, COMPUT GRAPH FORUM, V32, P131, DOI 10.1111/cgf.12100
   Pöthkow K, 2011, IEEE T VIS COMPUT GR, V17, P1393, DOI 10.1109/TVCG.2010.247
   Raj M, 2016, IEEE COMPUT GRAPH, V36, P60, DOI 10.1109/MCG.2015.70
   Raudaschl PF, 2017, MED PHYS, V44, P2020, DOI 10.1002/mp.12197
   Renard F, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-69920-0
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanyal J, 2010, IEEE T VIS COMPUT GR, V16, P1421, DOI 10.1109/TVCG.2010.181
   Stephenson DB, 2000, TELLUS A, V52, P300, DOI 10.1034/j.1600-0870.2000.d01-5.x
   Sun Y, 2012, STAT, V1, P68, DOI 10.1002/sta4.8
   Sun Y, 2011, J COMPUT GRAPH STAT, V20, P316, DOI 10.1198/jcgs.2011.09224
   Tukey J. W., 1975, P INT C MATH, V2, P523
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P2853, DOI 10.1109/TVCG.2018.2853721
   Whitaker RT, 2013, IEEE T VIS COMPUT GR, V19, P2713, DOI 10.1109/TVCG.2013.143
   Zhang MD, 2023, IEEE T VIS COMPUT GR, V29, P2067, DOI 10.1109/TVCG.2021.3140153
   Zuo YJ, 2000, ANN STAT, V28, P461, DOI 10.1214/aos/1016218226
NR 29
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6560
EP 6571
DI 10.1109/TVCG.2024.3350076
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000047
PM 38194372
DA 2024-11-06
ER

PT J
AU Scully-Allison, C
   Lumsden, I
   Williams, K
   Bartels, J
   Taufer, M
   Brink, S
   Bhatele, A
   Pearce, O
   Isaacs, KE
AF Scully-Allison, Connor
   Lumsden, Ian
   Williams, Katy
   Bartels, Jesse
   Taufer, Michela
   Brink, Stephanie
   Bhatele, Abhinav
   Pearce, Olga
   Isaacs, Katherine E.
TI Design Concerns for Integrated Scripting and Interactive Visualization
   in Notebook Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational notebooks; exploratory data analysis; hybrid
   visualization-scripting; interactive data analysis; visualization
   design; Computational notebooks; exploratory data analysis; hybrid
   visualization-scripting; interactive data analysis; visualization design
ID TOOLS
AB Interactive visualization can support fluid exploration but is often limited to predetermined tasks. Scripting can support a vast range of queries but may be more cumbersome for free-form exploration. Embedding interactive visualization in scripting environments, such as computational notebooks, provides an opportunity to leverage the strengths of both direct manipulation and scripting. We investigate interactive visualization design methodology, choices, and strategies under this paradigm through a design study of calling context trees used in performance analysis, a field which exemplifies typical exploratory data analysis workflows with Big Data and hard to define problems. We first produce a formal task analysis assigning tasks to graphical or scripting contexts based on their specificity, frequency, and suitability. We then design a notebook-embedded interactive visualization and validate it with intended users. In a follow-up study, we present participants with multiple graphical and scripting interaction modes to elicit feedback about notebook-embedded visualization design, finding consensus in support of the interaction model. We report and reflect on observations regarding the process and design implications for combining visualization and scripting in notebooks.
C1 [Scully-Allison, Connor; Isaacs, Katherine E.] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
   [Lumsden, Ian] Univ Tennessee, Knoxville, TN 37996 USA.
   [Taufer, Michela] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
   [Williams, Katy] Davidson Coll, Math & Comp Sci, Davidson, NC 28035 USA.
   [Bartels, Jesse] Rincon Res, Tucson, AZ 85711 USA.
   [Brink, Stephanie; Pearce, Olga] Larence Livermore Natl Lab, Livermore, CA 94550 USA.
   [Bhatele, Abhinav] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Bhatele, Abhinav] Univ Maryland, Parallel Software & Syst Grp, College Pk, MD 20742 USA.
C3 Utah System of Higher Education; University of Utah; University of
   Tennessee System; University of Tennessee Knoxville; University of
   Tennessee System; University of Tennessee Knoxville; Davidson College;
   University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park
RP Scully-Allison, C (corresponding author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
EM cscullyallison@sci.utah.edu; ilumsden@utk.edu; kawilliams@davidson.edu;
   jessebartels@email.arizona.edu; taufer@utk.edu; brink2@llnl.gov;
   bhatele@cs.umd.edu; pearce8@llnl.gov; kisaacs@sci.utah.edu
OI Williams, Katy/0000-0003-0864-1446; Lumsden, Ian/0000-0003-0009-5487;
   Isaacs, Katherine/0000-0002-9947-928X; Bhatele,
   Abhinav/0000-0003-3069-3701; Pearce, Olga/0000-0002-1904-9627; Bartels,
   Jesse/0009-0000-1859-3404; Taufer, Michela/0000-0002-0031-6377; Brink,
   Stephanie/0000-0002-1458-8453
FU U.S. Department of Energy by Lawrence Livermore National Laboratory
   [DE-AC52-07NA27344]; United States Department of Defense through DTIC
   [FA8075-14-D-002-007]; National Science Foundation [NSF IIS-1844573,
   IIS-2324465]; Department of Energy [DE-SC0022044, DE-SC0024635,
   LLNL-JRNL-859074]; U.S. Department of Energy (DOE) [DE-SC0024635,
   DE-SC0022044] Funding Source: U.S. Department of Energy (DOE)
FX This work was supported in part by the U.S. Department of Energy by
   Lawrence Livermore National Laboratory under Grant DE-AC52-07NA27344, in
   part the United States Department of Defense through DTIC under Grant
   FA8075-14-D-002-007, in part by National Science Foundation under Grants
   NSF IIS-1844573 and IIS-2324465, and in part by the Department of Energy
   under Grants DE-SC0022044 and DE-SC0024635 and LLNL-JRNL-859074.
CR Adamoli A, 2010, SOFTVIS 2010: PROCEEDINGS OF THE 2010 INTERNATIONAL SYMPOSIUM ON SOFTWARE VISUALIZATION, P73
   Adar E., 2006, Conference on Human Factors in Computing Systems. CHI2006, P791
   Adhianto L, 2010, CONCURR COMP-PRACT E, V22, P685, DOI 10.1002/cpe.1553
   Ahn DH, 2009, PROCEEDINGS OF THE CONFERENCE ON HIGH PERFORMANCE COMPUTING NETWORKING, STORAGE AND ANALYSIS
   Allaire J, 2021, RMARKDOWN DYNAMIC DO
   [Anonymous], OBSERVABLE
   Bell R, 2003, LECT NOTES COMPUT SC, V2790, P17
   Bergel A., 2017, Programming and Performance Visualization Tools, P233
   Bezemer CP, 2015, 2015 22ND INTERNATIONAL CONFERENCE ON SOFTWARE ANALYSIS, EVOLUTION, AND REENGINEERING (SANER), P535, DOI 10.1109/SANER.2015.7081872
   Bhatele A, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356219
   Bigelow A, 2017, IEEE T VIS COMPUT GR, V23, P481, DOI 10.1109/TVCG.2016.2598609
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Carbrera A., 2021, Website: Creating reactive Jupyter widgets with svelte
   Chattopadhyay S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376729
   Childs H., 2012, HIGH PERFORMANCE VIS, P395
   Clarke DJB, 2021, PATTERNS, V2, DOI 10.1016/j.patter.2021.100213
   DeRose L, 2007, LECT NOTES COMPUT SC, V4641, P150
   Ellson J, 2002, LECT NOTES COMPUT SC, V2265, P483
   Epperson W., 2023, IEEE Trans. Vis.ization Comput. Graph., V30, P197
   Faust R, 2022, 2022 IEEE VISUALIZATION IN DATA SCIENCE (VDS 2022), P37, DOI 10.1109/VDS57266.2022.00009
   Francoise Jules, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P39, DOI 10.1145/3472749.3474734
   Geimer M, 2012, LECT NOTES COMPUT SC, V7134, P463
   Guedj B, 2018, J MACH LEARN RES, V18
   Guo G, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581236
   Hartmann B, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P91, DOI 10.1145/1449715.1449732
   Hempel B, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P281, DOI 10.1145/3332165.3347925
   Nguyen HT, 2021, IEEE T VIS COMPUT GR, V27, P2455, DOI 10.1109/TVCG.2019.2953746
   Huu Tan Nguyen, 2016, 2016 Third Workshop on Visual Performance Analysis (VPA), P25, DOI 10.1109/VPA.2016.009
   Isaacs KE, 2019, IEEE T VIS COMPUT GR, V25, P2804, DOI 10.1109/TVCG.2018.2859974
   Kery Mary Beth, 2020, P 33 ANN ACM S US IN, DOI DOI 10.1145/3379337.3415842
   Kesavan SP, 2023, IEEE T VIS COMPUT GR, V29, P1691, DOI 10.1109/TVCG.2021.3129414
   Kluyver T, 2016, POSITIONING AND POWER IN ACADEMIC PUBLISHING: PLAYERS, AGENTS AND AGENDAS, P87, DOI 10.3233/978-1-61499-649-1-87
   KNUTH DE, 1984, COMPUT J, V27, P97, DOI 10.1093/comjnl/27.2.97
   Kunen AJ., 2015, KRIPKE-A Massively Parallel Transport Mini-App
   Lin S, 2010, SOFTVIS 2010: PROCEEDINGS OF THE 2010 INTERNATIONAL SYMPOSIUM ON SOFTWARE VISUALIZATION, P103
   LLNL, 2020, Tech. Rep. LLNL-TR-490254
   Lumsden I, 2022, P IEEE INT C E-SCI, P256, DOI 10.1109/eScience55777.2022.00039
   Madadhain J, 2005, J STAT SOFTW, V10, P1
   Malony A. D., 2014, OSTI Tech. Rep.
   Meyer M, 2020, IEEE T VIS COMPUT GR, V26, P87, DOI 10.1109/TVCG.2019.2934539
   Moret P., 2010, P WOSP SIPEW INT C P, P63
   Ono J., 2020, Notebookjs
   Pandey A, 2022, IEEE T VIS COMPUT GR, V28, P3563, DOI 10.1109/TVCG.2021.3064037
   Ono JP, 2021, COMPUT SCI ENG, V23, P99, DOI 10.1109/MCSE.2021.3052619
   Scully-Allison C., 2019, Roundtrip
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Shrestha Nischal, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P198, DOI 10.1145/3472749.3474744
   Sivaraman V, 2022, IUI'22: 27TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P418, DOI 10.1145/3490099.3511137
   The pandas development team, 2023, Zenodo
   Vaithilingam P, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P563, DOI 10.1145/3332165.3347944
   Wang ZJ, 2022, Arxiv, DOI arXiv:2205.03963
   Weidendorfer J, 2004, LECT NOTES COMPUT SC, V3038, P440
   Xenopoulos Peter, 2023, IEEE Trans Vis Comput Graph, V29, P853, DOI 10.1109/TVCG.2022.3209489
   Xie C, 2019, IEEE T VIS COMPUT GR, V25, P215, DOI 10.1109/TVCG.2018.2865026
   Yifan Wu, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P152, DOI 10.1145/3379337.3415851
NR 56
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6572
EP 6585
DI 10.1109/TVCG.2024.3354561
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000012
PM 38236684
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zheng, ZT
   Gao, XF
   Pan, ZR
   Li, W
   Wang, PS
   Wang, GP
   Wu, K
AF Zheng, Zhongtian
   Gao, Xifeng
   Pan, Zherong
   Li, Wei
   Wang, Peng-Shuai
   Wang, Guoping
   Wu, Kui
TI Visual-Preserving Mesh Repair
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Maintenance engineering; Faces; Visualization; Manifolds; Robustness;
   Pipelines; Ray tracing; Geometry processing; mesh repairing
ID POLYGONAL MODELS; SIMPLIFICATION
AB Mesh repair is a long-standing challenge in computer graphics and related fields. Converting defective meshes into watertight manifold meshes can greatly benefit downstream applications such as geometric processing, simulation, fabrication, learning, and synthesis. In this work, by assuming the model is visually correct, we first introduce three visual measures for visibility, orientation, and openness, based on ray-tracing. We then present a novel mesh repair framework incorporating visual measures with several critical steps, i.e., open surface closing, face reorientation, and global optimization, to effectively repair meshes with defects (e.g., gaps, holes, self-intersections, degenerate elements, and inconsistent orientations) and preserve visual appearances. Our method reduces unnecessary mesh complexity without compromising geometric accuracy or visual quality while preserving input attributes such as UV coordinates for rendering. We evaluate our approach on hundreds of models randomly selected from ShapeNet and Thingi10K, demonstrating its effectiveness and robustness compared to existing approaches.
C1 [Zheng, Zhongtian; Wang, Peng-Shuai; Wang, Guoping] Peking Univ, Beijing 100871, Peoples R China.
   [Gao, Xifeng; Pan, Zherong; Li, Wei; Wu, Kui] LightSpeed Studios, Irvine, CA 92618 USA.
C3 Peking University
RP Wang, GP (corresponding author), Peking Univ, Beijing 100871, Peoples R China.; Wu, K (corresponding author), LightSpeed Studios, Irvine, CA 92618 USA.
EM zhengzhongtian@pku.edu.cn; gxf.xisha@gmail.com;
   zherong.pan.usa@gmail.com; liwei@shanghaitech.edu.cn;
   wangps@hotmail.com; wgp@pku.edu.cn; walker.kui.wu@gmail.com
RI Wang, Peng-Shuai/AAK-6216-2021; wang, guoping/KQU-3394-2024
OI Pan, Zherong/0000-0001-9348-526X; Gao, Xifeng/0000-0003-0829-7075; Wang,
   Peng-Shuai/0000-0001-9700-8188
FU National Key R&D Program of China [2022YFB3303400, 2021YFF0500901,
   2023YFB3309000]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2022YFB3303400, 2021YFF0500901 and 2023YFB3309000.
CR Andújar C, 2002, ACM T GRAPHIC, V21, P88, DOI 10.1145/508357.508359
   Attene M, 2018, RAPID PROTOTYPING J, V24, P855, DOI 10.1108/RPJ-11-2016-0185
   Attene M, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431214
   Attene M, 2014, GRAPH MODELS, V76, P658, DOI 10.1016/j.gmod.2014.09.002
   Attene M, 2010, VISUAL COMPUT, V26, P1393, DOI 10.1007/s00371-010-0416-3
   Attene M, 2009, COMPUT AIDED GEOM D, V26, P850, DOI 10.1016/j.cagd.2009.06.002
   Bauchet JP, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3376918
   Bischoff S, 2005, ACM T GRAPHIC, V24, P1332, DOI 10.1145/1095878.1095883
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Campen M, 2010, COMPUT GRAPH FORUM, V29, P397, DOI 10.1111/j.1467-8659.2009.01609.x
   Chang Angel X., 2015, arXiv
   Cherchi G, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417818
   Chu L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356507
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Diazzi L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480564
   Dutre P., 2003, Global illumination compendium
   Furukawa R, 2007, LECT NOTES COMPUT SC, V4844, P206
   Gao X., 2022, P ACM SIGGRAPH 2022
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guéziec A, 2001, IEEE T VIS COMPUT GR, V7, P136, DOI 10.1109/2945.928166
   Guo TQ, 2006, PROCEEDINGS OF 2006 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P4370
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Held M, 2001, ALGORITHMICA, V30, P563, DOI 10.1007/s00453-001-0028-4
   Hétroy F, 2011, COMPUT AIDED DESIGN, V43, P101, DOI 10.1016/j.cad.2010.09.012
   Hornung A., 2006, SGP 06, P41, DOI DOI 10.2312/SGP/SGP06/041-050
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   Hu YX, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392385
   Hu YX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201353
   Huang JW, 2020, Arxiv, DOI arXiv:2005.11621
   Jacobson A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461916
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Ju T, 2009, J COMPUT SCI TECH-CH, V24, P19, DOI 10.1007/s11390-009-9206-7
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Labatut P, 2009, COMPUT GRAPH FORUM, V28, P2275, DOI 10.1111/j.1467-8659.2009.01530.x
   Masuda T, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P1003
   Murali T. M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P155, DOI 10.1145/253284.253326
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Oomes S, 1997, LECT NOTES COMPUT SC, V1252, P349
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Podolak S., 2005, P 3 EUR S GEOM PROC
   Portaneri C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530152
   Rossignac J., 1999, P 5 ACM S SOL MOD AP, P31
   Spillmann M., 2006, VISION MODELING VISU, P9
   Takayama A., 2014, J. Comput. Graph. Techn., V3
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Wang BL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392426
   Wu K, 2022, COMPUT GRAPH FORUM, V41, P205, DOI 10.1111/cgf.14669
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
   Zhou QN, 2016, Arxiv, DOI arXiv:1605.04797
NR 49
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6586
EP 6597
DI 10.1109/TVCG.2023.3348829
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000004
PM 38163301
DA 2024-11-06
ER

PT J
AU Schuster, R
   Gregory, K
   Möller, T
   Koesten, L
AF Schuster, Regina
   Gregory, Kathleen
   Moeller, Torsten
   Koesten, Laura
TI "Being Simple on Complex Issues" - Accounts on Visual Data Communication
   About Climate Change
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; climate change; experts; laypeople; sensemaking;
   understandability; takeaway messages; uncertainty; design decisions
ID VISUALIZATION; SCIENCE; SENSEMAKING; EMOTIONS; IMAGERY; DESIGN; CHARTS
AB Data visualizations play a critical role in both communicating scientific evidence about climate change and in stimulating engagement and action. To investigate how visualizations can be better utilized to communicate the complexities of climate change to different audiences, we conducted interviews with 17 experts in the fields of climate change, data visualization, and science communication, as well as with 12 laypersons. Besides questions about climate change communication and various aspects of data visualizations, we also asked participants to share what they think is the main takeaway message for two exemplary climate change data visualizations. Through a thematic analysis, we observe differences regarding the included contents, the length and abstraction of messages, and the sensemaking process between and among the participant groups. On average, experts formulated shorter and more abstract messages, often referring to higher-level conclusions rather than specific details. We use our findings to reflect on design decisions for creating more effective visualizations, particularly in news media sources geared toward lay audiences. We hereby discuss the adaption of contents according to the needs of the audience, the trade-off between simplification and accuracy, as well as techniques to make a visualization attractive.
C1 [Schuster, Regina; Koesten, Laura] Univ Vienna, Fac Comp Sci, A-1090 Vienna, Austria.
   [Gregory, Kathleen] Leiden Univ, Ctr Sci & Technol Studies CWTS, NL-2300 Leiden, Netherlands.
   [Moeller, Torsten] Univ Vienna, Fac Comp Sci, A-1090 Vienna, Austria.
   [Moeller, Torsten] Data Sci Res Network, A-1090 Vienna, Austria.
C3 University of Vienna; Leiden University - Excl LUMC; Leiden University;
   University of Vienna
RP Schuster, R (corresponding author), Univ Vienna, Fac Comp Sci, A-1090 Vienna, Austria.
EM regina.maria.veronika.schuster@univie.ac.at;
   k.m.gregory@cwts.leidenuniv.nl; torsten.moeller@univie.ac.at;
   laura.koesten@univie.ac.at
RI Gregory, Kathleen/KQT-9546-2024
OI Moller, Torsten/0000-0003-1192-0710; Schuster,
   Regina/0000-0002-1702-9083; Koesten, Laura/0000-0003-4110-1759; Gregory,
   Kathleen/0000-0001-5475-8632
FU Vienna Science and Technology Fund (WWTF)
FX This work was supported by the Vienna Science and Technology Fund (WWTF)
   under Grant 10.47379/ICT20065.
CR Adar E, 2021, IEEE T VIS COMPUT GR, V27, P946, DOI 10.1109/TVCG.2020.3030375
   [Anonymous], 2021, The Guardian
   [Anonymous], Tagesschau Instagram Profile
   [Anonymous], 2008, DIGITAL EARTH SUMMIT
   Ballantyne AG, 2016, CLIMATIC CHANGE, V134, P73, DOI 10.1007/s10584-015-1533-9
   Bateman S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2573
   Bayes R, 2023, ENVIRON COMMUN, V17, P16, DOI 10.1080/17524032.2020.1805343
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Boy J, 2014, IEEE T VIS COMPUT GR, V20, P1963, DOI 10.1109/TVCG.2014.2346984
   Brosch T, 2021, CURR OPIN BEHAV SCI, V42, P15, DOI 10.1016/j.cobeha.2021.02.001
   Bttinger Michael., 2020, Foundations of Data Visualization, P297, DOI [DOI 10.1007/978-3-030-34444-3, 10.1007/978-3-030-34444-316, DOI 10.1007/978-3-030-34444-316]
   Burns A., 2022, Communicative information visualizations: How to make data more understandable by the general public
   Burns A, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581524
   Burns A, 2024, IEEE T VIS COMPUT GR, V30, P3427, DOI 10.1109/TVCG.2022.3231716
   Burns A, 2020, 2020 IEEE WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2020), P19, DOI 10.1109/BELIV51497.2020.00010
   Burns A, 2022, IEEE T VIS COMPUT GR, V28, P4515, DOI 10.1109/TVCG.2021.3092680
   Corner A., 2018, Principles for effective communication and public engagement on climate change: a handbook for IPCC authors
   Deng W, 2016, COGNITIVE PSYCHOL, V91, P24, DOI 10.1016/j.cogpsych.2016.09.002
   Dudman K, 2021, CLIMATIC CHANGE, V168, DOI 10.1007/s10584-021-03186-x
   Feldman L, 2018, RISK ANAL, V38, P585, DOI 10.1111/risa.12868
   Ferreira M, 2021, C&C'21: PROCEEDINGS OF THE 13TH CONFERENCE ON CREATIVITY AND COGNITION, DOI 10.1145/3450741.3466774
   Fischer H, 2020, ENVIRON RES LETT, V15, DOI 10.1088/1748-9326/abbc3c
   Fischer H, 2018, INT J ENV RES PUB HE, V15, DOI 10.3390/ijerph15050875
   Fischhoff B, 2013, P NATL ACAD SCI USA, V110, P14033, DOI 10.1073/pnas.1213273110
   Fish C, 2019, CARTOGRAPHICA, V55, P69, DOI 10.3138/cart-2019-0019
   Fish CS, 2020, CARTOGR GEOGR INF SC, V47, P492, DOI 10.1080/15230406.2020.1774421
   Franconeri SL, 2021, PSYCHOL SCI PUBL INT, V22, P110, DOI 10.1177/15291006211051956
   Gaulkin T., 2021, Why the bad news in the IPCC report is good news for visual learners
   Giuseffi K, 2019, CAMB HANDB PSYCHOL, P836
   Harold J., 2017, Enhancing the accessibility of climate change data visuals: Recommendations to the IPCC and guidance for researchers
   Harold J, 2020, CLIMATIC CHANGE, V158, P255, DOI 10.1007/s10584-019-02537-z
   Harold J, 2016, NAT CLIM CHANGE, V6, P1080, DOI [10.1038/nclimate3162, 10.1038/NCLIMATE3162]
   He HA, 2024, IEEE T VIS COMPUT GR, V30, P1435, DOI 10.1109/TVCG.2023.3326917
   Head B.W., 2008, Public Policy, V3, P101, DOI DOI 10.1007/978-3-030-94580-0
   Hinds PJ, 2001, J APPL PSYCHOL, V86, P1232, DOI 10.1037/0021-9010.86.6.1232
   Holder E, 2024, IEEE T VIS COMPUT GR, V30, P1446, DOI 10.1109/TVCG.2023.3326512
   Hopke JE, 2018, SOC MEDIA SOC, V4, DOI 10.1177/2056305118782687
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2231, DOI 10.1109/TVCG.2011.255
   IPCC, About the IPCC
   IPCC, 2021, AR6 WGI SPM Basic slide deck with figures
   Johansson J, 2010, IEEE INT CONF INF VI, P156, DOI 10.1109/IV.2010.32
   Joslyn S, 2021, J EXP PSYCHOL-APPL, V27, P473, DOI 10.1037/xap0000347
   Kause A, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12072955
   Kekeya J., 2020, Con- temporary PNG Stud., V24, P86
   Kennedy H, 2018, SOCIOLOGY, V52, P830, DOI 10.1177/0038038516674675
   Kong HK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174012
   LaDonna Kori A, 2021, J Grad Med Educ, V13, P607, DOI 10.4300/JGME-D-21-00752.1
   Lee BS, 2020, IEEE COMPUT GRAPH, V40, P82, DOI 10.1109/MCG.2020.2968244
   Lee S., 2017, Ph.D. dissertation
   Lee S, 2017, IEEE T VIS COMPUT GR, V23, P551, DOI 10.1109/TVCG.2016.2598920
   Lee S, 2016, IEEE T VIS COMPUT GR, V22, P499, DOI 10.1109/TVCG.2015.2467195
   Lee-Robbins Elsie, 2023, IEEE Trans Vis Comput Graph, V29, P1, DOI 10.1109/TVCG.2022.3209500
   Lundgard A, 2022, IEEE T VIS COMPUT GR, V28, P1073, DOI 10.1109/TVCG.2021.3114770
   Malakis S, 2013, APPL ERGON, V44, P327, DOI 10.1016/j.apergo.2012.09.003
   Mann M., 2012, The hockey stick and the climate wars: Dispatches from the front lines
   Markman A., 2018, Why people aren't motivated to address climate change
   Masson-Delmotte V.P., 2021, Contribution of working group I to the sixth assessment report of the intergovernmental panel on climate change
   McGrath M., 2021, Climate change: Five things we have learned from the IPCC report
   McMahon R, 2016, CLIMATIC CHANGE, V138, P369, DOI 10.1007/s10584-016-1758-2
   McMahon R, 2015, CLIMATIC CHANGE, V133, P141, DOI 10.1007/s10584-015-1473-4
   Metag J, 2016, SCI COMMUN, V38, P197, DOI 10.1177/1075547016635181
   Morelli A, 2021, CLIMATIC CHANGE, V168, DOI 10.1007/s10584-021-03171-4
   Morini F, 2024, IEEE T VIS COMPUT GR, V30, P1413, DOI 10.1109/TVCG.2023.3327185
   O'Neill S, 2020, CLIMATIC CHANGE, V163, P9, DOI 10.1007/s10584-019-02504-8
   O'Neill SJ, 2014, WIRES CLIM CHANGE, V5, P73, DOI 10.1002/wcc.249
   Park JH, 2018, IEEE COMPUT GRAPH, V38, P67, DOI 10.1109/MCG.2018.2879066
   Patt A, 2000, J RISK UNCERTAINTY, V21, P45, DOI 10.1023/A:1026517309871
   Peck EM, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300474
   Peluso D.A., 2007, EXPERTISE OUT CONTEX, P113, DOI DOI 10.4324/9780203810088
   Pew ResearchCenter, 2021, In response to climate change, citizens in advanced economies are willing to alter how they live and work
   Pidcock R, 2021, CLIMATIC CHANGE, V168, DOI 10.1007/s10584-021-03230-w
   Pousman Z, 2007, IEEE T VIS COMPUT GR, V13, P1145, DOI 10.1109/TVCG.2007.70541
   @ quarks | Instagram, 2022, So hat sich die globale Temperatur der Erde entwickelt
   Quispel A, 2016, INFORM VISUAL, V15, P238, DOI 10.1177/1473871615606478
   Robson C., 2002, Real World Research: A Resource for Social Scientists and Practitioner-Researchers (Regional surveys of the world)
   Rothermich K, 2021, PERS INDIV DIFFER, V168, DOI 10.1016/j.paid.2020.110304
   RUSSELL DM, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P269
   Samuelson W., 1988, Journal of Risk and Uncertainty, V1, P7, DOI [DOI 10.1007/BF00055564, 10.1007/BF00055564]
   Schneider B, 2012, WIRES CLIM CHANGE, V3, P185, DOI 10.1002/wcc.162
   Schuster R, 2023, Arxiv, DOI [arXiv:2305.04030, 10.48550/arXiv.2305.04030, DOI 10.48550/ARXIV.2305.04030]
   Sprague D, 2012, INFORM VISUAL, V11, P106, DOI 10.1177/1473871611433710
   Waite R, 2021, State of climate action 2021: systems transformations required to limit global warming to 1.5C
   Wang SS, 2018, WIRES CLIM CHANGE, V9, DOI 10.1002/wcc.509
   Wessler H, 2016, INT J PRESS/POLIT, V21, P423, DOI 10.1177/1940161216661848
   Williams R., 2017, The Non-Designer's Presentation Book: Principles for Ef- fective Presentation Design
   Windhager F., 2019, P 7 WORKSH VIS ENV S, P1
   Wozniak A., 2020, inResearch Handbook on Communicating Climate Change, P131
   Xiong C, 2022, IEEE T VIS COMPUT GR, V28, P955, DOI 10.1109/TVCG.2021.3114823
NR 88
TC 0
Z9 0
U1 6
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6598
EP 6611
DI 10.1109/TVCG.2024.3352282
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000038
PM 38271163
OA Green Submitted, hybrid
DA 2024-11-06
ER

PT J
AU Fan, PM
   Thanyadit, S
   Pong, TC
AF Fan, Pak Ming
   Thanyadit, Santawat
   Pong, Ting-Chuen
TI VisTA-LIVE: A Visualization Tool for Assessment of Laboratories in
   Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Education technology; extended reality; computer-assisted instruction;
   virtual laboratory; Education technology; extended reality;
   computer-assisted instruction; virtual laboratory
AB A Virtual Reality Laboratory (VR Lab) experiment refers to an experiment session that is being conducted in the virtual environment through Virtual Reality (VR) and aims to deliver procedural knowledge to students similar to that in a physical lab environment. While VR Lab is becoming more popular among education institutes as a learning tool for students, existing designs are mostly considered from a student's perspective. Instructors could only receive limited information on how the students are performing and could not provide useful feedback to aid the students' learning and evaluate their performance. This motivated us to create VisTA-LIVE: a Visualization Tool for Assessment of Laboratories In Virtual Environments. In this article, we present in detail the design thinking approach that was applied to create VisTA-LIVE. The tool is deployed in an Extended Reality (XR) environment, and we report the evaluation results with domain experts and discuss issues related to monitoring and assessing a live VR lab session which lay potential directions for future work. We also describe how the resulting design of the tool could be used as a reference for other education developers who wish to develop similar applications.
C1 [Fan, Pak Ming; Pong, Ting-Chuen] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Thanyadit, Santawat] King Mongkuts Univ Technol Thonburi, Bangkok 10140, Thailand.
C3 Hong Kong University of Science & Technology; King Mongkuts University
   of Technology Thonburi
RP Fan, PM (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM leofan@connect.ust.hk; santawat.t@esicplus.co.th; tcpong@ust.hk
OI Fan, Pak Ming/0000-0003-1259-7572
FU Research Grant Council of the Hong Kong SAR GRF [16209621]
FX This work was supported by the Research Grant Council of the Hong Kong
   SAR GRF under Grant 16209621.
CR A. V. Lab, 2021, Bring virtual software training to a new level
   Ansar Mifzala, 2023, Emerging IT/ICT and AI Technologies Affecting Society. Lecture Notes in Networks and Systems (478), P161, DOI 10.1007/978-981-19-2940-3_11
   Arjun S, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529777
   Bach B., 2014, EuroVis-STARs, DOI [10.2312/eurovisstar.20141171, DOI 10.2312/EUROVISSTAR.20141171, 10.2312/EUROVISSTAR.20141171]
   Binti Sulaiman Shahida, 2020, 2020 International Conference on Computational Intelligence (ICCI), P287, DOI 10.1109/ICCI51257.2020.9247819
   Blaga AD, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376554
   Broussard DM, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P284, DOI 10.1109/VRW52623.2021.00058
   Council D., 2019, The double diamond: A universally accepted depic- tion of the design process design council
   ESRI, 2023, How emerging hot spot analysis works
   Fan P. M., 2020, P ACM S SPAT US INT
   for the Future of Education, 2022, Learning architecture design with virtual reality
   Gao H., 2021, P CHI C HUM FACT COM
   Genz F, 2021, LECT NOTES COMPUT SC, V12980, P462, DOI 10.1007/978-3-030-87595-4_34
   Geoengineer.org, 2019, Unconfined compression test
   Gupta S, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P411, DOI 10.1145/3316782.3324016
   H. P. 1. of Design at Stanford University, 2016, Our impact
   H. P. I. of Design at Stanford University, 2020, Design thinking boot- camp
   H. P. L. of Design at Stanford University, 2010, An introduction to design thinking process guide
   Han Y, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519627
   Jin Q, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517542
   Jones D., 2019, P 17 INT C VIRT REAL
   Kaur R, 2019, PROC FRONT EDUC CONF, DOI [10.1109/Humanoids43949.2019.9035020, 10.1109/fie43999.2019.9028452]
   Kelly JW, 2022, IEEE T VIS COMPUT GR, V28, P2037, DOI 10.1109/TVCG.2022.3150475
   Khatkhatay M. A., 2021, Ben Shneiderman's visualiza- tion mantra
   Kim H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1325, DOI [10.1109/vr.2019.8798106, 10.1109/VR.2019.8798106]
   Krekhov Andrey, 2020, CHI PLAY '20: Proceedings of the Annual Symposium on Computer-Human Interaction in Play, P561, DOI 10.1145/3410404.3414247
   Labster, 2023, Labster
   Lino H, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489957
   McSbeery T, 2016, ACM SIGGRAPH 2016 VR VILLAGE (SIGGRAPH '16), DOI 10.1145/2929490.2949592
   Microsoft, 2023, Turn your data into immediate impact
   Miller MR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74486-y
   Moore AG, 2021, INT SYM MIX AUGMENT, P221, DOI 10.1109/ISMAR52148.2021.00037
   Narciso D, 2020, IEEE INT C BIOINF BI, P813, DOI 10.1109/BIBE50027.2020.00138
   OptiTrack, 2023, Optitrack motion capture systems
   Osborn A., 2012, Applied Imagination-Principles and Procedures of Creative Writing
   Pai Y. S., 2017, P ACM SIGGRAPH POST
   Qlik, 2023, Go from passive to active analytics
   Sallaberry LH, 2021, PROCEEDINGS OF SYMPOSIUM ON VIRTUAL AND AUGMENTED REALITY, SVR 2021, P14, DOI 10.1145/3488162.3488207
   Sararit N., 2017, P 22 INT C INT US IN, P117
   Sherwin Katie., 2014, Progress indicators make a slow system less insufferable
   Simon Herbert., 1996, The Sciences of the Artificial, V1
   Sonntag D., 2015, P 4 INT S PERV DISPL, p269 270
   Sprints G.D., 2016, Share and engage with the design sprint community
   T. U. of Hong Kong, 2020, About the VR lab project
   Thanyadit Santawat, 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3512983
   Thanyadit S, 2019, INT SYM MIX AUGMENT, P258, DOI 10.1109/ISMAR.2019.00023
   Vazquez C., 2021, P ACM SIGGRAPH IMM P
   VRLabacademy, 2023, vrlabacademy
   Wang Z, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489874
   Weissker Tim, 2021, IEEE Transactions on Visualization and Computer Graphics, V27, P2524, DOI 10.1109/TVCG.2021.3067756
   xAPI.com, 2023, Xapi.com homepage: What is xAPI (the experience API)
NR 51
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6813
EP 6825
DI 10.1109/TVCG.2023.3341079
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600014
PM 38079370
DA 2024-11-06
ER

PT J
AU Zhang, QJ
   Hou, JH
AF Zhang, Qijian
   Hou, Junhui
TI PointVST: Self-Supervised Pre-Training for 3D Point Clouds via
   View-Specific Point-to-Image Translation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D point clouds; cross-modal; multi-view images; pre-training;
   self-supervised learning; 3D point clouds; cross-modal; multi-view
   images; pre-training; self-supervised learning
AB The past few years have witnessed the great success and prevalence of self-supervised representation learning within the language and 2D vision communities. However, such advancements have not been fully migrated to the field of 3D point cloud learning. Different from existing pre-training paradigms designed for deep point cloud feature extractors that fall into the scope of generative modeling or contrastive learning, this paper proposes a translative pre-training framework, namely PointVST, driven by a novel self-supervised pretext task of cross-modal translation from 3D point clouds to their corresponding diverse forms of 2D rendered images. More specifically, we begin with deducing view-conditioned point-wise embeddings through the insertion of the viewpoint indicator, and then adaptively aggregate a view-specific global codeword, which can be further fed into subsequent 2D convolutional translation heads for image generation. Extensive experimental evaluations on various downstream task scenarios demonstrate that our PointVST shows consistent and prominent performance superiority over current state-of-the-art approaches as well as satisfactory domain transfer capability.
C1 [Zhang, Qijian; Hou, Junhui] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Hou, JH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM qijizhang3-c@my.cityu.edu.hk; jh.hou@cityu.edu.hk
OI Hou, Junhui/0000-0003-3431-2021; ZHANG, Qijian/0000-0003-4723-6136
FU Hong Kong Research Grants Council [11219422, 11202320]
FX This work was supported by Hong Kong Research Grants Council under
   Grants 11219422 and 11202320.
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Afham M, 2022, PROC CVPR IEEE, P9892, DOI 10.1109/CVPR52688.2022.00967
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Chang A.X., 2015, Technical Report
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3183, DOI 10.1109/TIP.2019.2957935
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Devlin J., 2019, P N AM CHAPT ASS COM
   Du B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3133, DOI 10.1145/3474085.3475458
   Feng WQ, 2021, PROC CVPR IEEE, P10292, DOI 10.1109/CVPR46437.2021.01016
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gidaris S, 2018, INT C LEARN REPR
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo Z., 2020, ADV NEURAL INF PROCE, V33, P21271
   Han ZZ, 2019, IEEE I CONF COMP VIS, P10441, DOI 10.1109/ICCV.2019.01054
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Hassani K, 2019, IEEE I CONF COMP VIS, P8159, DOI 10.1109/ICCV.2019.00825
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho T, 2023, PROC CVPR IEEE, P8349, DOI 10.1109/CVPR52729.2023.00807
   Hu QY, 2022, IEEE T PATTERN ANAL, V44, P8338, DOI 10.1109/TPAMI.2021.3083288
   Huang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6515, DOI 10.1109/ICCV48922.2021.00647
   Jing L., 2022, P INT C LEARN REPR, P9
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Katz S, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276407, 10.1145/1239451.1239475]
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li Q, 2023, PROC CVPR IEEE, P13591, DOI 10.1109/CVPR52729.2023.01306
   Li Y., 2018, Proc. Adv. Neural Inf. Process. Syst., P826
   Liu H, 2022, LECT NOTES COMPUT SC, V13662, P657, DOI 10.1007/978-3-031-20086-1_38
   Liu X, 2023, IEEE T KNOWL DATA EN, V35, P857, DOI 10.1109/TKDE.2021.3090866
   Liu XH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P989, DOI 10.1145/3343031.3350960
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Metzer G, 2022, COMPUT GRAPH FORUM, V41, P461, DOI 10.1111/cgf.14487
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Oord A. v. d., 2018, ARXIV
   Pang Y, 2022, LECT NOTES COMPUT SC, V13662, P604, DOI 10.1007/978-3-031-20086-1_35
   Poursaeed O, 2020, INT CONF 3D VISION, P1018, DOI 10.1109/3DV50981.2020.00112
   Qi C.R., 2017, P 31 INT C NEUR INF, P5105, DOI DOI 10.1109/CVPR.2017.16
   Qian GC, 2022, ADV NEUR IN
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Rao YM, 2020, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR42600.2020.00542
   Ravi Nikhila, 2020, arXiv
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sanghi Aditya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P626, DOI 10.1007/978-3-030-58526-6_37
   Sauder J, 2019, ADV NEUR IN, V32
   Sharma C, 2020, ADV NEUR IN, V33
   Sharma G, 2022, LECT NOTES COMPUT SC, V13662, P550, DOI 10.1007/978-3-031-20086-1_32
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI [10.1109/WACV45572.2020.9093430, 10.1109/wacv45572.2020.9093430]
   Tan WK, 2020, IEEE COMPUT SOC CONF, P797, DOI 10.1109/CVPRW50498.2020.00109
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470
   Valsesia D., 2018, INT C LEARN REPR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovic P., 2019, INT C LEARN REPR
   Wang B., 2022, arXiv
   Wang HC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9762, DOI 10.1109/ICCV48922.2021.00964
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu QG, 2022, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR52688.2022.00536
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Xu R, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592129
   Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Zeng YM, 2021, PROC CVPR IEEE, P6048, DOI 10.1109/CVPR46437.2021.00599
   Zhang Q., IEEE T MULTIMEDIA, DOI [10.1109/TMM.2023.3286981, DOI 10.1109/TMM.2023.3286981]
   Zhang RY, 2022, ADV NEUR IN
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
NR 83
TC 1
Z9 1
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6900
EP 6912
DI 10.1109/TVCG.2023.3345353
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600012
PM 38127600
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Hurter, C
   Rogowitz, B
   Truong, G
   Andry, T
   Romat, H
   Gardy, L
   Amini, F
   Riche, NH
AF Hurter, Christophe
   Rogowitz, Bernice
   Truong, Guillaume
   Andry, Tiffany
   Romat, Hugo
   Gardy, Ludovic
   Amini, Fereshteh
   Riche, Nathalie Henry
TI Memory Recall for Data Visualizations in Mixed Reality, Virtual Reality,
   3D and 2D
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualisation; recall; virtual reality; mixed reality; locomotion;
   human memory; perception; Data visualisation; recall; virtual reality;
   mixed reality; locomotion; human memory; perception
ID PERFORMANCE; PALACE; VIEWS
AB This article explores how the ability to recall information in data visualizations depends on the presentation technology. Participants viewed 10 Isotype visualizations on a 2D screen, in 3D, in Virtual Reality (VR) and in Mixed Reality (MR). To provide a fair comparison between the three 3D conditions, we used LIDAR to capture the details of the physical rooms, and used this information to create our textured 3D models. For all environments, we measured the number of visualizations recalled and their order (2D) or spatial location (3D, VR, MR). We also measured the number of syntactic and semantic features recalled. Results of our study show increased recall and greater richness of data understanding in the MR condition. Not only did participants recall more visualizations and ordinal/spatial positions in MR, but they also remembered more details about graph axes and data mappings, and more information about the shape of the data. We discuss how differences in the spatial and kinesthetic cues provided in these different environments could contribute to these results, and reasons why we did not observe comparable performance in the 3D and VR conditions.
C1 [Hurter, Christophe; Truong, Guillaume] Univ Toulouse, ENAC, F-31000 Toulouse, France.
   [Hurter, Christophe] IPAL Singapore Int Res Lab Artif Intelligence, Singapore 138632, Singapore.
   [Rogowitz, Bernice] Visual Perspect Res, Ossining, NY 10562 USA.
   [Andry, Tiffany] Univ Libre Bruxelles, B-1050 Brussels, Belgium.
   [Romat, Hugo; Amini, Fereshteh; Riche, Nathalie Henry] Microsoft Corp, MSR, Redmond, WA 98052 USA.
   [Gardy, Ludovic] CNRS, CERCO, F-67075 Paris, France.
C3 Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de
   Toulouse; Ecole Nationale de l'Aviation Civile (ENAC); Universite Libre
   de Bruxelles; Microsoft; Centre National de la Recherche Scientifique
   (CNRS)
RP Hurter, C (corresponding author), Univ Toulouse, ENAC, F-31000 Toulouse, France.; Hurter, C (corresponding author), IPAL Singapore Int Res Lab Artif Intelligence, Singapore 138632, Singapore.
EM christophe.hurter@enac.fr; bernice.e.rogowitz@gmail.com;
   guillaume.truong@enac.fr; tiffany.andry@uclouvain.be;
   romathugo@microsoft.com; ludovic.gardy@cnrs.fr;
   fereshteh.amini@microsoft.com; nath@microsoft.com
RI Hurter, Christophe/IAM-1546-2023
OI Andry, Tiffany/0009-0000-1139-5931; Hurter,
   Christophe/0000-0003-4318-6717
FU European Union [101114765]; CODA- Control leradaptive Digital Assistant
   [101114838]; Trustworthy Intelligent System for Remote Digital Tower
   (TRUSTY) [HORIZON-SESAR-2022-DES-ER-01]; National Research Foundation,
   Prime Minister's Office, Singapore
FX This work was supported in part by European Union's Horizon 2020 within
   the framework of the SESAR 2020 research and innovation program, under
   Grant 101114765 for the project in part by 'CODA- Control leradaptive
   Digital Assistant' under Grant 101114838 for the project in part by
   'Trustworthy Intelligent System for Remote Digital Tower (TRUSTY),'
   under Grant HORIZON-SESAR-2022-DES-ER-01. The work of Christophe Hurter
   extends his gratitude to the National Research Foundation, Prime
   Minister's Office, Singapore, for their support under the Campus for
   Research Excellence and Technological Enterprise (CREATE) programme,
   specifically for the DESCARTES programme
CR Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Alper BE, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1825, DOI 10.1145/2556288.2557112
   Andry T., 2021, P CHI C HUM FACT COM, DOI [10.1145/3411764.3445739, DOI 10.1145/3411764.3445739]
   Araiza-Alba P, 2021, COMPUT EDUC, V164, DOI 10.1016/j.compedu.2020.104121
   Atkinson R.C., 1968, PSYCHOL LEARN MOTIV, V2, P89, DOI DOI 10.1016/S0079-7421(08)60422-3
   BADDELEY A, 1992, Science (Washington D C), V255, P556, DOI 10.1016/j.cub.2009.12.014
   Bateman S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2573
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Brooks BM, 1999, MEMORY, V7, P65, DOI 10.1080/741943713
   Caplan H., 1954, RHETORICA AD HERENNI
   Chevalier F, 2013, IEEE T VIS COMPUT GR, V19, P2426, DOI 10.1109/TVCG.2013.210
   de la Peña N, 2010, PRESENCE-TELEOP VIRT, V19, P291, DOI 10.1162/PRES_a_00005
   Di Luca M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445319
   Ebbinghaus H, 2013, ANN NEUROSCI, V20, P155, DOI 10.5214/ans.0972.7531.200408
   ElSayed NAM, 2015, 2015 BIG DATA VISUAL ANALYTICS (BDVA)
   Friedrich T, 2021, PROCEEDINGS OF 27TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2021, DOI 10.1145/3489849.3489888
   Gaunet F, 2001, COGNITIVE BRAIN RES, V11, P409, DOI 10.1016/S0926-6410(01)00013-1
   Ghani S., 2011, Proceedings of the Conference on Graphics Interface, P175
   Gibson J. J., 1976, Philosophy Phenomenological Res., V37, P238
   GIBSON JJ, 1950, AM J PSYCHOL, V63, P367, DOI 10.2307/1418003
   Gibson JJ., 1979, ECOLOGICAL APPROACH
   Haroz S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1191, DOI 10.1145/2702123.2702275
   Jeandrain A., 2001, Journal of Interactive Advertising, V2, P2, DOI [10.1080/15252019.2001.10722053, DOI 10.1080/15252019.2001.10722053]
   Krokos E, 2019, VIRTUAL REAL-LONDON, V23, P1, DOI 10.1007/s10055-018-0346-3
   Larson JS, 2005, INT J RES MARK, V22, P395, DOI 10.1016/j.ijresmar.2005.09.005
   Lee B, 2021, IEEE T VIS COMPUT GR, V27, P1095, DOI 10.1109/TVCG.2020.3030435
   Legge ELG, 2012, ACTA PSYCHOL, V141, P380, DOI 10.1016/j.actpsy.2012.09.002
   Lim KYT, 2020, BRIT J EDUC TECHNOL, V51, P673, DOI 10.1111/bjet.12904
   Liu ZC, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173697
   Luria A. R., 1987, The Mind of a Mnemonist: A Little Book about a Vast Memory, With a New Foreword by Jerome S. Bruner
   Maguire EA, 2003, NAT NEUROSCI, V6, P90, DOI 10.1038/nn988
   MAYER RE, 1994, J EDUC PSYCHOL, V86, P389, DOI 10.1037/0022-0663.86.3.389
   McCormack J, 2018, LECT NOTES COMPUT SC, V11190, P57, DOI 10.1007/978-3-030-01388-2_3
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/0033-295X.101.2.343
   Moacdieh N., 2014, PROC HUMAN FACTORS E, P1516, DOI [10.1177/1541931214581316, DOI 10.1177/1541931214581316]
   Nader V., 2008, Otto Neurath: The Language of the Global Polis
   Neurath O., 1936, Department of Typography and Graphic Communication
   Norman D. A., 1999, Interactions, V6, P38, DOI 10.1145/301153.301168
   Patnaik B, 2019, IEEE T VIS COMPUT GR, V25, P726, DOI 10.1109/TVCG.2018.2865237
   Qu ZN, 2018, IEEE T VIS COMPUT GR, V24, P468, DOI 10.1109/TVCG.2017.2744198
   Riche NH., 2018, Data-driven storytelling, DOI [10.1201/9781315281575, DOI 10.1201/9781315281575]
   Robbins Phillip., 2008, The Cambridge Handbook of Situated Cognition
   Robertson G., 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P153, DOI 10.1145/288392.288596
   Roettl J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200724
   Romat H, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376348
   Ruddle R. A., 2013, The Effect of Translational and Rotational BodyBased Information on Navigation, P99, DOI [10.1007/978--1-4419-8432-6_5, DOI 10.1007/978-1-4419-8432-6_5]
   Saket B, 2015, COMPUT GRAPH FORUM, V34, P441, DOI 10.1111/cgf.12656
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Shapiro L., 2019, Embodied Cognition
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Smith SA, 2019, PSYCHON B REV, V26, P1213, DOI 10.3758/s13423-019-01605-w
   Tan D.S., 2002, CHI 02 EXTENDED ABST, P806
   TULVING E, 1990, SCIENCE, V247, P301, DOI 10.1126/science.2296719
   Tulving E., 1993, Current Directions in Psychological Science, V2, P67, DOI [10.1111/1467-8721.ep10770899, DOI 10.1111/1467-8721.EP10770899]
   Vicki Bruce P. H., 2006, Remembering Faces
   Vieilledent S, 2003, COGNITIVE BRAIN RES, V16, P238, DOI 10.1016/S0926-6410(02)00279-3
   Vindenes J, 2018, LECT NOTES COMPUT SC, V10850, P205, DOI 10.1007/978-3-319-95270-3_16
   Waller D., 2013, Human Walking in Virtual Environments, P26
   Wang Y, 2016, INT CONF SOFTW ENG, P422, DOI 10.1109/ICSESS.2016.7883100
   Willett W, 2017, IEEE T VIS COMPUT GR, V23, P461, DOI 10.1109/TVCG.2016.2598608
   Wilson M, 2002, PSYCHON B REV, V9, P625, DOI 10.3758/BF03196322
   Wilson M, 2001, PSYCHON B REV, V8, P44, DOI 10.3758/BF03196138
   Xia HJ, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P853, DOI 10.1145/3242587.3242597
   Xia HJ, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173797
   Yang FM, 2021, IEEE T VIS COMPUT GR, V27, P4359, DOI 10.1109/TVCG.2020.3009003
   Yates Frances, 1966, ART MEMORY
   Zhang JE, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376172
   Zhao H., 2005, P HUM FACT COMP SYST, P1905
NR 69
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6691
EP 6706
DI 10.1109/TVCG.2023.3336588
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600007
PM 38498758
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Yazgan, M
   Sahillioglu, Y
AF Yazgan, Misranur
   Sahillioglu, Yusuf
TI A Partition Based Method for Spectrum-Preserving Mesh Simplification
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mesh simplification; spectrum-preserving; Laplace-Beltrami spectrum;
   partitioning; Mesh simplification; spectrum-preserving; Laplace-Beltrami
   spectrum; partitioning
AB The majority of the simplification methods focus on preserving the appearance of the mesh, ignoring the spectral properties of the differential operators derived from the mesh. The spectrum of the Laplace-Beltrami operator is essential for a large subset of applications in geometry processing. Coarsening a mesh without considering its spectral properties might result in incorrect calculations on the simplified mesh. Given a 3D triangular mesh, this article aims to simplify the mesh using edge collapses, while focusing on preserving the spectral properties of the associated cotangent Laplace-Beltrami operator. Unlike the existing spectrum-preserving coarsening methods, we consider solely the eigenvalues of the operator in order to preserve the spectrum. The presented method is partition based, that is the input mesh is divided into smaller patches which are simplified individually. We evaluate our method on a variety of meshes, by using functional maps and quantitative norms, to measure how well the eigenvalues and eigenvectors of the Laplace-Beltrami operator computed on the input mesh are maintained by the output mesh. We demonstrate that the achieved spectrum preservation is at least as effective as the existing spectral coarsening methods.
C1 [Yazgan, Misranur; Sahillioglu, Yusuf] Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkiye.
C3 Middle East Technical University
RP Sahillioglu, Y (corresponding author), Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkiye.
EM yazganmisra@gmail.com; ysahillioglu@gmail.com
FU TUBITAK [EEEAG-119E572]
FX This work was supported in part by TUBITAK under Grant EEEAG-119E572.
CR Chen HL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417789
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cheng XY, 2018, J NUMBER THEORY, V185, P48, DOI 10.1016/j.jnt.2017.09.021
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cosmo L, 2019, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2019.00771
   Dubrovina A, 2011, ADV DATA SCI ADAPT, V3, P203, DOI 10.1142/S1793536911000829
   Edelstein M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392447
   Garey M. R., 1979, Computers and intractability. A guide to the theory of NP-completeness
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   Hoppe H., 1993, P 20 C COMP GRAPH IN, P26
   Hu JX, 2017, IEEE T VIS COMPUT GR, V23, P721, DOI 10.1109/TVCG.2016.2598790
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Karypis G., 1998, "A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices, V38
   Keros AD, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591544
   Lescoat T, 2020, COMPUT GRAPH FORUM, V39, P315, DOI 10.1111/cgf.13932
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805971
   Liu H, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323000
   Liu HTD, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592403
   Liu R, 2007, COMPUT GRAPH FORUM, V26, P385, DOI 10.1111/j.1467-8659.2007.01061.x
   Nasikun A, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13496
   Öztireli AC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866190
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Ponchio F., 2009, THESIS
   Qiu Y., 2018, "Spectra. A library for large scale eigenvalue problems
   Ren J, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14359
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M., 2005, P ACM S SOL PHYS MOD, P106
   Rustamov R. M., 2007, P S GEOM PROC, V257, P225, DOI 10.1145/1281991.1282022
   Sahillio glu Y., 2020, Vis. Comput., V36, P1721
   Sahillio glu Y., 2022, ACM Trans. Graph., V42, P1
   Sahillioglu Y, 2011, COMPUT GRAPH FORUM, V30, P1461, DOI 10.1111/j.1467-8659.2011.02020.x
   Schonsheck SC, 2021, J SCI COMPUT, V86, DOI 10.1007/s10915-020-01390-y
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   WELSH DJA, 1967, COMPUT J, V10, P85, DOI 10.1093/comjnl/10.1.85
   Yan JQ, 2004, IEEE T VIS COMPUT GR, V10, P142, DOI 10.1109/TVCG.2004.1260766
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
NR 41
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6839
EP 6850
DI 10.1109/TVCG.2023.3341610
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600006
PM 38090863
DA 2024-11-06
ER

PT J
AU Li, JY
   Pan, XK
   Huang, G
   Zhang, ZY
   Wang, N
   Bao, HJ
   Zhang, GF
AF Li, Jinyu
   Pan, Xiaokun
   Huang, Gan
   Zhang, Ziyang
   Wang, Nan
   Bao, Hujun
   Zhang, Guofeng
TI RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in
   Dynamic Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Degenerate motion; dynamic environment; RANSAC; SLAM; VIO; Degenerate
   motion; dynamic environment; RANSAC; SLAM; VIO
ID MONOCULAR SLAM; KALMAN FILTER; TRACKING; VERSATILE
AB It is typically challenging for visual or visual-inertial odometry systems to handle the problems of dynamic scenes and pure rotation. In this work, we design a novel visual-inertial odometry (VIO) system called RD-VIO to handle both of these two problems. First, we propose an IMU-PARSAC algorithm which can robustly detect and match keypoints in a two-stage process. In the first state, landmarks are matched with new keypoints using visual and IMU measurements. We collect statistical information from the matching and then guide the intra-keypoint matching in the second stage. Second, to handle the problem of pure rotation, we detect the motion type and adapt the deferred-triangulation technique during the data-association process. We make the pure-rotational frames into the special subframes. When solving the visual-inertial bundle adjustment, they provide additional constraints to the pure-rotational motion. We evaluate the proposed VIO system on public datasets and online comparison. Experiments show the proposed RD-VIO has obvious advantages over other methods in dynamic environments.
C1 [Li, Jinyu; Pan, Xiaokun; Huang, Gan; Zhang, Ziyang; Bao, Hujun; Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Wang, Nan] SenseTime Res, Hangzhou 311215, Peoples R China.
C3 Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM mail@jinyu.li; xkpan@zju.edu.cn; huanggan@zju.edu.cn;
   zhangzion@zju.edu.cn; wangnan@sensetime.com; baohujun@zju.edu.cn;
   zhangguofeng@zju.edu.cn
RI Zhao, Gang/JMC-6248-2023; , panxkun/JCE-7904-2023; Zhang,
   Ziyang/AAF-4011-2019
OI Bao, Hujun/0000-0002-2662-0334; Huang, Gan/0000-0001-8515-2721; Zhang,
   Ziyang/0009-0004-4169-2282; Zhang, Guofeng/0000-0001-5661-8430; Li,
   Jinyu/0000-0002-5206-8600; Pan, Xiaokun/0000-0002-7438-1665
FU NSF of China [61932003]
FX This work was supported in part by the NSF of China under Grant
   61932003.
CR Bao HJ, 2022, IEEE T VIS COMPUT GR, V28, P2212, DOI 10.1109/TVCG.2022.3150495
   Bescos B, 2021, IEEE ROBOT AUTOM LET, V6, P5191, DOI 10.1109/LRA.2021.3068640
   Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   Bloesch M, 2017, INT J ROBOT RES, V36, P1053, DOI 10.1177/0278364917728574
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Chen DP, 2021, INT SYM MIX AUGMENT, P275, DOI 10.1109/ISMAR52148.2021.00043
   Civera J, 2008, IEEE T ROBOT, V24, P932, DOI 10.1109/TRO.2008.2003276
   Cortés S, 2018, LECT NOTES COMPUT SC, V11214, P425, DOI 10.1007/978-3-030-01249-6_26
   Cui LY, 2019, IEEE ACCESS, V7, P166528, DOI 10.1109/ACCESS.2019.2952161
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321
   Geneva P, 2020, IEEE INT CONF ROBOT, P4666, DOI [10.1109/ICRA40945.2020.9196524, 10.1109/icra40945.2020.9196524]
   He YJ, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041159
   Huai Z, 2022, INT J ROBOT RES, V41, P667, DOI 10.1177/0278364919853361
   Jinyu L., 2019, VIRTUAL REALITY INTE, V1, P386, DOI [10.1016/j.vrih.2019.07.002, DOI 10.1016/J.VRIH.2019.07.002]
   Jinyu Li, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11859), P283, DOI 10.1007/978-3-030-31726-3_24
   Johari MM, 2023, PROC CVPR IEEE, P17408, DOI 10.1109/CVPR52729.2023.01670
   Kaneko M, 2018, IEEE COMPUT SOC CONF, P371, DOI 10.1109/CVPRW.2018.00063
   Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813
   Li H, 2024, IEEE T VIS COMPUT GR, V30, P1743, DOI 10.1109/TVCG.2022.3225844
   Li MY, 2012, IEEE INT CONF ROBOT, P828, DOI 10.1109/ICRA.2012.6225229
   Li PL, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P11, DOI 10.1109/ISMAR.2017.18
   Liu JH, 2022, IEEE ROBOT AUTOM LET, V7, P9573, DOI 10.1109/LRA.2022.3191193
   Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024
   Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nilsson JO, 2014, INT C INDOOR POSIT, P24, DOI 10.1109/IPIN.2014.7275464
   Owada S., 2008, P 6 INT S NONPHOTORE, P65
   Pirchheim C, 2013, INT SYM MIX AUGMENT, P229, DOI 10.1109/ISMAR.2013.6671783
   Qiang F  ..., 2020, arXiv
   Qin T, 2019, Arxiv, DOI arXiv:1901.03638
   Qin T, 2019, Arxiv, DOI arXiv:1901.03642
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Qin T, 2017, IEEE INT C INT ROBOT, P4225, DOI 10.1109/IROS.2017.8206284
   Ram K, 2021, IEEE INT C INT ROBOT, P9198, DOI 10.1109/IROS51168.2021.9636522
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Song S, 2022, IEEE ROBOT AUTOM LET, V7, P11523, DOI 10.1109/LRA.2022.3203231
   Tan W, 2013, INT SYM MIX AUGMENT, P209, DOI 10.1109/ISMAR.2013.6671781
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Teed Z, 2021, ADV NEUR IN, V34
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   von Stumberg L, 2022, IEEE ROBOT AUTOM LET, V7, P1408, DOI 10.1109/LRA.2021.3140129
   Stumberg L, 2018, IEEE INT CONF ROBOT, P2510, DOI 10.1109/ICRA.2018.8462905
   Wang HY, 2023, PROC CVPR IEEE, P13293, DOI 10.1109/CVPR52729.2023.01277
   Xiaochen Q., 2020, Chin. J. Aeronaut., V33, P3359
   Yang XR, 2022, INT SYM MIX AUGMENT, P499, DOI 10.1109/ISMAR55827.2022.00066
   Ye WC, 2023, PROC CVPR IEEE, P9579, DOI 10.1109/CVPR52729.2023.00924
   Yousif K., 2015, INTELL IND SYST, V1, P289, DOI [10.1007/s40903-015-0032-7, DOI 10.1007/S40903-015-0032-7]
   Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691
   Yun DS, 2014, I C INF COMM TECH CO, P609, DOI 10.1109/ICTC.2014.6983225
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
NR 53
TC 1
Z9 1
U1 15
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6941
EP 6955
DI 10.1109/TVCG.2024.3353263
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600017
PM 38215333
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Liu, PX
   Pan, TY
   Lin, HS
   Chu, HK
   Hu, MC
AF Liu, Pin-Xuan
   Pan, Tse-Yu
   Lin, Hsin-Shih
   Chu, Hung-Kuo
   Hu, Min-Chun
TI VisionCoach: Design and Effectiveness Study on VR Vision Training for
   Basketball Passing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Sports; Task analysis; Visualization; Machine vision;
   Trajectory; Systematics; Sports VR; vision training; basketball VR;
   computer-aided training
ID SPORTS VISION; FIELD
AB Vision Training is important for basketball players to effectively search for teammates who has wide-open opportunities to shoot, observe the defenders around the wide-open teammates and quickly choose a proper way to pass the ball to the most suitable one. We develop an immersive virtual reality (VR) system called VisionCoach to simulate the player's viewing perspective and generate three designed systematic vision training tasks to benefit the cultivating procedure. By recording the player's eye gazing and dribbling video sequence, the proposed system can analyze the vision-related behavior to understand the training effectiveness. To demonstrate the proposed VR training system can facilitate the cultivation of vision ability, we recruited 14 experienced players to participate in a 6-week between-subject study, and conducted a study by comparing the most frequently used 2D vision training method called Vision Performance Enhancement (VPE) program with the proposed system. Qualitative experiences and quantitative training results are reported to show that the proposed immersive VR training system can effectively improve player's vision ability in terms of gaze behavior and dribbling stability. Furthermore, training in the VR-VisionCoach Condition can transfer the learned abilities to real scenario more easily than training in the 2D-VPE Condition.
C1 [Liu, Pin-Xuan; Chu, Hung-Kuo; Hu, Min-Chun] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [Pan, Tse-Yu] Natl Taiwan Univ Sci & Technol, Grad Inst AI Cross Disciplinary Tech, Taipei 10607, Taiwan.
   [Lin, Hsin-Shih] Natl Cheng Kung Univ, Phys Educ Off, Tainan 70101, Taiwan.
C3 National Tsing Hua University; National Taiwan University of Science &
   Technology; National Cheng Kung University
RP Hu, MC (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.; Pan, TY (corresponding author), Natl Taiwan Univ Sci & Technol, Grad Inst AI Cross Disciplinary Tech, Taipei 10607, Taiwan.
EM s110062534@m110.nthu.edu.tw; typan@mail.ntust.edu.tw;
   sevenfour@mail.ncku.edu.tw; hkchu@cs.nthu.edu.tw; anitahu@cs.nthu.edu.tw
RI Hu, Min-Chun/AAX-1721-2020
FU National Science and Technology Council, Taiwan
   [NSTC-111-2221-E-007-065-MY3, NSTC-111-2823-8-007-002,
   NSTC-110-2221-E-007-061-MY3]
FX This work was supported in part by the National Science and Technology
   Council, Taiwan, under Grants NSTC-111-2221-E-007-065-MY3,
   NSTC-111-2823-8-007-002, and NSTC-110-2221-E-007-061-MY3.
CR [Anonymous], 2023, Visual concepts, "NBA2K
   Appelbaum L. G., 2016, Athletic Training Sports Health Care, V8, P163
   Appelbaum LG, 2018, INT REV SPORT EXER P, V11, P160, DOI 10.1080/1750984X.2016.1266376
   Appelbaum LG, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00276
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Brooke J., 1996, Usability Eval. Ind./Taylor Fr, V189, P4, DOI DOI 10.1201/9781498710411-35
   Carrasco M, 2001, P NATL ACAD SCI USA, V98, P5363, DOI 10.1073/pnas.081074098
   Chu XT, 2022, IEEE T VIS COMPUT GR, V28, P118, DOI 10.1109/TVCG.2021.3114861
   Clark JF, 2015, JOVE-J VIS EXP, DOI 10.3791/52648
   Düking P, 2018, FRONT PHYSIOL, V9, DOI 10.3389/fphys.2018.00128
   Holliday J., 2013, "Effect of stroboscopic vision training on dynamic visual acuity scores: Nike vapor strobe eyewear, P1
   Hülsdünker T, 2019, INT J SPORT PHYSIOL, V14, P343, DOI 10.1123/ijspp.2018-0302
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kioumourtzoglou E, 1998, PERCEPT MOTOR SKILL, V86, P771, DOI 10.2466/pms.1998.86.3.771
   Kirschen DG, 2011, EYE CONTACT LENS, V37, P127, DOI 10.1097/ICL.0b013e3182126a08
   Klostermann A, 2020, FRONT SPORTS ACT LIV, V1, DOI 10.3389/fspor.2019.00066
   Krasich K, 2016, J MOTOR BEHAV, V48, P401, DOI 10.1080/00222895.2015.1113918
   Laby DM, 2021, OPTOMETRY VISION SCI, V98, P723, DOI 10.1097/OPX.0000000000001729
   Laurent E, 2006, VIS COGN, V13, P247, DOI 10.1080/13506280544000020
   Lin T., 2021, P ACM C HUM FACT COM, P13, DOI [DOI 10.1145/3411764.34456492, 10.1145/3411764.34456492,9, DOI 10.1145/3411764.34456492,9, 10.1145/3411764.3445649, DOI 10.1145/3411764.3445649]
   LLC Senaptec, 2023, "Senaptec strobe eyewear
   M&S Technologies, 2023, "Sports vision performance
   Microsoft, 2023, "Azure cloud cognitive service
   PuttView, 2023, "Puttview outdoor
   S. Labs, 2023, "Strivr-Immersive training solutions
   Saredakis D, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.00096
   Scharfen HE, 2019, APPL COGNITIVE PSYCH, V33, P843, DOI 10.1002/acp.3526
   Schwab S, 2012, J SPORT SCI MED, V11, P624
   Senaptec LLC, 2023, "Senaptec sensory station
   Sports E, 2023, "Eon sports VR
   StarVR Corporation, 2023, "Starvr one
   Stinson C, 2014, IEEE T VIS COMPUT GR, V20, P606, DOI 10.1109/TVCG.2014.23
   Tsai WL, 2022, IEEE T VIS COMPUT GR, V28, P2970, DOI 10.1109/TVCG.2020.3046326
   Ultraleap, 2023, "Gemini
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Visionup, 2023, "Visionup strobe glasses
   Vizual edge, 2023, "Sports vision performance training-Visual fitness
   WELFORD AT, 1960, ERGONOMICS, V3, P189, DOI 10.1080/00140136008930484
   Wilkins L, 2015, PERCEPT MOTOR SKILL, V121, P57, DOI 10.2466/22.25.PMS.121c11x0
   WILLIAMS LJ, 1982, HUM FACTORS, V24, P683, DOI 10.1177/001872088202400605
   Williams SE, 2011, J SPORT EXERCISE PSY, V33, P416, DOI 10.1123/jsep.33.3.416
   Ye SN, 2021, IEEE T VIS COMPUT GR, V27, P860, DOI 10.1109/TVCG.2020.3030392
NR 42
TC 0
Z9 0
U1 9
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6665
EP 6677
DI 10.1109/TVCG.2023.3335312
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F2M7M
UT WOS:001308219500002
PM 38015694
DA 2024-11-06
ER

PT J
AU Jung, S
   Shin, D
   Jeon, H
   Choe, K
   Seo, J
AF Jung, Seokweon
   Shin, Donghwa
   Jeon, Hyeon
   Choe, Kiroong
   Seo, Jinwook
TI MoNetExplorer: A Visual Analytics System for Analyzing Dynamic Networks
   With Temporal Network Motifs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; Measurement; Size measurement; Windows; Time
   measurement; Data visualization; Task analysis; Dynamic networks;
   interactive network slicing; temporal network motifs; visual analytics
ID PATTERNS; TIME; ANIMATION; FLOW
AB Partitioning a dynamic network into subsets (i.e., snapshots) based on disjoint time intervals is a widely used technique for understanding how structural patterns of the network evolve. However, selecting an appropriate time window (i.e., slicing a dynamic network into snapshots) is challenging and time-consuming, often involving a trial-and-error approach to investigating underlying structural patterns. To address this challenge, we present MoNetExplorer, a novel interactive visual analytics system that leverages temporal network motifs to provide recommendations for window sizes and support users in visually comparing different slicing results. MoNetExplorer provides a comprehensive analysis based on window size, including (1) a temporal overview to identify the structural information, (2) temporal network motif composition, and (3) node-link-diagram-based details to enable users to identify and understand structural patterns at various temporal resolutions. To demonstrate the effectiveness of our system, we conducted a case study with network researchers using two real-world dynamic network datasets. Our case studies show that the system effectively supports users to gain valuable insights into the temporal and structural aspects of dynamic networks.
C1 [Jung, Seokweon; Jeon, Hyeon; Choe, Kiroong; Seo, Jinwook] Seoul Natl Univ, Seoul 08826, South Korea.
   [Shin, Donghwa] Kwangwoon Univ, Seoul 01897, South Korea.
C3 Seoul National University (SNU); Kwangwoon University
RP Seo, J (corresponding author), Seoul Natl Univ, Seoul 08826, South Korea.; Shin, D (corresponding author), Kwangwoon Univ, Seoul 01897, South Korea.
EM swjung@hcil.snu.ac.kr; dhshin@kw.ac.kr; hj@hcil.snu.ac.kr;
   krchoe@hcil.snu.ac.kr; jseo@snu.ac.kr
OI Jeon, Hyeon/0000-0002-9659-2922; Choe, Kiroong/0000-0002-6084-2539;
   Jung, Seokweon/0009-0002-5326-765X; Shin, DongHwa/0000-0001-9460-809X
FU National Research Foundation of Korea (NRF) - Korean government (MSIT)
   [2023R1A2C200520911]; Supreme Prosecutors' Office of the Republic of
   Korea - Ministry of Science and ICT [SPO2023A1202digitalB]; Research
   Grant of Kwangwoon University
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korean government(MSIT) under Grant
   2023R1A2C200520911, in part by the Supreme Prosecutors' Office of the
   Republic of Korea grant funded by the Ministry of Science and ICT under
   Grant SPO2023A1202digitalB, and in part by the Research Grant of
   Kwangwoon University in 2023.
CR Arleo A, 2022, COMPUT GRAPH FORUM, V41, P226, DOI 10.1111/cgf.14615
   Bach B, 2015, COMPUT GRAPH FORUM, V34, P31, DOI 10.1111/cgf.12615
   Bach B, 2016, IEEE T VIS COMPUT GR, V22, P559, DOI 10.1109/TVCG.2015.2467851
   Bach B, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P877, DOI 10.1145/2556288.2557010
   Beck F, 2017, COMPUT GRAPH FORUM, V36, P133, DOI 10.1111/cgf.12791
   Boyandin I, 2012, COMPUT GRAPH FORUM, V31, P1005, DOI 10.1111/j.1467-8659.2012.03093.x
   Cakmak E, 2022, 2022 IEEE VISUALIZATION IN DATA SCIENCE (VDS 2022), P17, DOI 10.1109/VDS57266.2022.00007
   Cakmak E, 2020, 2020 IEEE VISUALIZATION IN DATA SCIENCE (VDS 2020), P32, DOI 10.1109/VDS51726.2020.00008
   Cakmak E, 2021, IEEE T VIS COMPUT GR, V27, P517, DOI 10.1109/TVCG.2020.3030398
   Chen H, 2018, LECT NOTES COMPUT SC, V11282, P463, DOI 10.1007/978-3-030-04414-5_33
   Chiappori A, 2022, STUD COMPUT INTELL, V1015, P566, DOI 10.1007/978-3-030-93409-5_47
   Crawford J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0195993
   Crnovrsanin T, 2021, IEEE T VIS COMPUT GR, V27, P539, DOI 10.1109/TVCG.2020.3030385
   Cui WW, 2014, IEEE PAC VIS SYMP, P121, DOI 10.1109/PacificVis.2014.48
   Dang TN, 2016, COMPUT GRAPH FORUM, V35, P61, DOI 10.1111/cgf.12882
   Furmanova K, 2020, INFORM VISUAL, V19, P114, DOI 10.1177/1473871619878085
   Gao ZQ, 2022, PROC INT CONF DATA, P2656, DOI 10.1109/ICDE53745.2022.00244
   Gehrke J., 2003, ACM SIGKDD Explor. Newslett., P149
   Gratzl S, 2013, IEEE T VIS COMPUT GR, V19, P2277, DOI 10.1109/TVCG.2013.173
   Halkidi M, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P187, DOI 10.1109/ICDM.2001.989517
   Hulovatyy Y, 2015, BIOINFORMATICS, V31, P171, DOI 10.1093/bioinformatics/btv227
   Jeon H, 2022, IEEE T VIS COMPUT GR, V28, P551, DOI 10.1109/TVCG.2021.3114833
   Jurgens T.-C., 2012, P INT AAAI C WEB SOC, P162
   Kerracher N, 2015, IEEE T VIS COMPUT GR, V21, P1160, DOI 10.1109/TVCG.2015.2424889
   Klimt B, 2004, LECT NOTES COMPUT SC, V3201, P217
   KOrman G., 2021, J. Statist. Mech: Theory Exp.
   Kovanen L, 2013, P NATL ACAD SCI USA, V110, P18070, DOI 10.1073/pnas.1307941110
   Kovanen L, 2011, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2011/11/P11005
   Krings G, 2012, EPJ DATA SCI, V1, DOI 10.1140/epjds4
   Latapy M, 2018, SOC NETW ANAL MIN, V8, DOI 10.1007/s13278-018-0537-7
   Lee A, 2021, IEEE T VIS COMPUT GR, V27, P528, DOI 10.1109/TVCG.2020.3030446
   Lee A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300360
   Lei D, 2020, TRANSPORT RES C-EMER, V120, DOI 10.1016/j.trc.2020.102810
   Linhares CDG, 2023, IEEE T VIS COMPUT GR, V29, P203, DOI 10.1109/TVCG.2022.3209477
   Liu PH, 2023, IEEE T KNOWL DATA EN, V35, P945, DOI 10.1109/TKDE.2021.3077495
   Liu PH, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-05217-8
   Liu Y, 2010, 2010 IEEE International Conference on Data Mining, P911, DOI [DOI 10.1109/ICDM.2010.35, 10.1109/ICDM.2010.35]
   Masuda N, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-37534-2
   McInnes L, 2017, INT CONF DAT MIN WOR, P33, DOI 10.1109/ICDMW.2017.12
   Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824
   Moody J, 2005, AM J SOCIOL, V110, P1206, DOI 10.1086/421509
   Moriano P, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40137-0
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Paranjape A, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P601, DOI 10.1145/3018661.3018731
   Peel L, 2015, AAAI CONF ARTIF INTE, P2914
   Ponciano JR, 2021, COMPUT GRAPH-UK, V97, P170, DOI 10.1016/j.cag.2021.04.006
   Priebe C. Y., 2005, Computational & Mathematical Organization Theory, V11, P229, DOI 10.1007/s10588-005-5378-z
   Schieber TA, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13928
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Sulo T., 2010, P 8 WORKSHOP MINING, P127, DOI DOI 10.1145/1830252.1830269
   Troidi J., 2023, IEEE Trans. Vis. Comput. Graph.
   Uddin S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13640-5
   Wall E, 2019, IEEE T VIS COMPUT GR, V25, P491, DOI 10.1109/TVCG.2018.2865146
   Wang Y, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P241, DOI 10.1109/visual.2019.8933748
   Xie L., 2020, P INT C ADV VIS INT, P1
NR 55
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6725
EP 6739
DI 10.1109/TVCG.2023.3337396
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F2M7M
UT WOS:001308219500003
PM 38019635
DA 2024-11-06
ER

PT J
AU Yu, D
   Xiao, CF
   Lau, M
   Fu, HB
AF Yu, Deng
   Xiao, Chufeng
   Lau, Manfred
   Fu, Hongbo
TI Sketch2Stress: Sketching With Structural Stress Awareness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Force; Stress; Shape; Three-dimensional displays; Image reconstruction;
   Fabrication; Task analysis; Sketching; sketch-based image synthesis;
   digital fabrication
AB In the process of product design and digital fabrication, the structural analysis of a designed prototype is a fundamental and essential step. However, such a step is usually invisible or inaccessible to designers at the early sketching phase. This limits the user's ability to consider a shape's physical properties and structural soundness. To bridge this gap, we introduce a novel approach Sketch2Stress that allows users to perform structural analysis of desired objects at the sketching stage. This method takes as input a 2D freehand sketch and one or multiple locations of user-assigned external forces. With the specially-designed two-branch generative-adversarial framework, it automatically predicts a normal map and a corresponding structural stress map distributed over the user-sketched underlying object. In this way, our method empowers designers to easily examine the stress sustained everywhere and identify potential problematic regions of their sketched object. Furthermore, combined with the predicted normal map, users are able to conduct a region-wise structural analysis efficiently by aggregating the stress effects of multiple forces in the same direction. Finally, we demonstrate the effectiveness and practicality of our system with extensive experiments and user studies.
C1 [Yu, Deng; Xiao, Chufeng; Lau, Manfred; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Lau, M; Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Kowloon, Hong Kong, Peoples R China.
EM dengyucn@gmail.com; chufengxiao@outlook.com; manfred.lau@gmail.com;
   fuplus@gmail.com
OI FU, Hongbo/0000-0002-0284-726X; Xiao, Chufeng/0000-0001-6749-0161; YU,
   Deng/0000-0003-2343-3688
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [11212119, 11206319, 11205420]; Centre for Applied Computing and
   Interactive Media (ACIM) of the School of Creative Media, CityU
FX This work was supported in part by the Research Grants Council of the
   Hong Kong Special Administrative Region, China under Grants CityU
   11212119, 11206319, and 11205420, and in part by the Centre for Applied
   Computing and Interactive Media (ACIM) of the School of Creative Media,
   CityU.
CR CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chang A. X., Shapenet
   Delanoy J, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203197
   Du D., 2020, IEEE T VISUALIZATION
   Dumas J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766984
   Fang GX, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417834
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Gryaditskaya Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356533
   Guillard B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13003, DOI 10.1109/ICCV48922.2021.01278
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Hughes T. J. R., 1987, Linear static and dynamic finite element analysis
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiao JB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3072, DOI 10.1145/3394171.3413588
   Johnson G., 2012, CHI 12 EXTENDED ABST, P1079
   Langlois T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982436
   Li CL, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3238250
   Lu L, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601168
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Meng C., ICLR 2021
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Miki M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766888
   Panetta J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073649
   Paszke A., NeurIPS2019, V32
   Pr ́evost R., 2013, ACM Trans-actions on Graphics (TOG), V32, P1
   Ramesh A., 2022, arXiv, DOI DOI 10.48550/ARXIV.2204.06125
   Schumacher C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766926
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   Smirnov D., ICLR 2021
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song Y, 2021, Arxiv, DOI arXiv:2011.13456
   Stava O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185544
   Stutz D, 2020, INT J COMPUT VISION, V128, P1162, DOI 10.1007/s11263-018-1126-y
   Su WC, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203186
   Telea A, 2011, LECT NOTES COMPUT SC, V6671, P393, DOI 10.1007/978-3-642-21569-8_34
   Ulu E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073626
   Umetani N., 2013, SIGGRAPH ASIA, DOI DOI 10.1145/2542355.2542361
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508382
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Yao MJ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818064
   Yu D, 2023, Arxiv, DOI arXiv:2306.05832
   Yu D, 2021, IEEE T CIRC SYST VID, V31, P1738, DOI 10.1109/TCSVT.2020.3015279
   Zehnder J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925888
   Zhang S.-H., CVPR 2021, P6012
   Zhou J, 2023, Arxiv, DOI arXiv:2309.05946
   Zhou QN, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461967
NR 49
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6851
EP 6865
DI 10.1109/TVCG.2023.3342119
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F2M7M
UT WOS:001308219500001
PM 38090862
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhou, P
   Shi, LQ
   Liu, XY
   Jin, J
   Zhang, YT
   Hou, JH
AF Zhou, Ping
   Shi, Langqing
   Liu, Xiaoyang
   Jin, Jing
   Zhang, Yuting
   Hou, Junhui
TI Light Field Depth Estimation via Stitched Epipolar Plane Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Estimation; Three-dimensional displays; Image resolution; Tensors; Light
   fields; Learning systems; Uncertainty; Depth estimation; light field;
   occlusion; stitched -EPI; texture-less region
ID MULTIVIEW; OCCLUSION; NETWORK
AB Depth estimation is a fundamental problem in light field processing. Epipolar-plane image (EPI)-based methods often encounter challenges such as low accuracy in slope computation due to discretization errors and limited angular resolution. Besides, existing methods perform well in most regions but struggle to produce sharp edges in occluded regions and resolve ambiguities in texture-less regions. To address these issues, we propose the concept of stitched-EPI (SEPI) to enhance slope computation. SEPI achieves this by shifting and concatenating lines from different EPIs that correspond to the same 3D point. Moreover, we introduce the half-SEPI algorithm, which focuses exclusively on the non-occluded portion of lines to handle occlusion. Additionally, we present a depth propagation strategy aimed at improving depth estimation in texture-less regions. This strategy involves determining the depth of such regions by progressing from the edges towards the interior, prioritizing accurate regions over coarse regions. Through extensive experimental evaluations and ablation studies, we validate the effectiveness of our proposed method. The results demonstrate its superior ability to generate more accurate and robust depth maps across all regions compared to state-of-the-art methods.
C1 [Zhou, Ping; Shi, Langqing; Liu, Xiaoyang; Zhang, Yuting] Southeast Univ, Sch Biol Sci & Med Engn, Nanjing 211189, Jiangsu, Peoples R China.
   [Jin, Jing; Hou, Junhui] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Southeast University - China; City University of Hong Kong
RP Zhou, P (corresponding author), Southeast Univ, Sch Biol Sci & Med Engn, Nanjing 211189, Jiangsu, Peoples R China.; Hou, JH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM capzhou@163.com; slq13072020073@126.com; xyliu20@seu.edu.cn;
   jingjin25-c@my.cityu.edu.hk; yt.zhang1@outlook.com; jh.hou@cityu.edu.hk
RI Liu, Xiaoyang/JHU-2009-2023
OI Hou, Junhui/0000-0003-3431-2021
FU National Natural Science Foundation of China [52071075, 11572087]; Hong
   Kong Research Grants Council [11218121]; Hong Kong Innovation and
   Technology Fund [MHP/117/21]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 52071075 and 11572087, in part by the
   Hong Kong Research Grants Council under Grant 11218121, and in part by
   Hong Kong Innovation and Technology Fund under Grant MHP/117/21.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ai W, 2019, OPT EXPRESS, V27, P24793, DOI 10.1364/OE.27.024793
   Barone S, 2012, OPT LASER ENG, V50, P380, DOI 10.1016/j.optlaseng.2011.10.019
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   Cai ZW, 2019, OPT EXPRESS, V27, P13532, DOI 10.1364/OE.27.013532
   Chen C, 2014, PROC CVPR IEEE, P1518, DOI 10.1109/CVPR.2014.197
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524
   Chuchvara A, 2020, IEEE T IMAGE PROCESS, V29, P2492, DOI 10.1109/TIP.2019.2959233
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dorst L., 1984, IEEE Trans. Pattern Anal. Mach. Intell., VPAMI-6
   Fiss J, 2014, IEEE INT CONF COMPUT
   Guo CL, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102829
   Han K, 2022, IEEE T PATTERN ANAL, V44, P8022, DOI 10.1109/TPAMI.2021.3105523
   Heber Stefan, 2013, Energy Minimization Methods in Computer Vision and Pattern Recognition. 9th International Conference, EMMCVPR 2013. Proceedings. LNCS 8081, P66, DOI 10.1007/978-3-642-40395-8_6
   Heber S, 2014, LECT NOTES COMPUT SC, V8694, P751
   Heber S, 2017, IEEE I CONF COMP VIS, P2271, DOI 10.1109/ICCV.2017.247
   Heber S, 2016, PROC CVPR IEEE, P3746, DOI 10.1109/CVPR.2016.407
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Jin J, 2022, IEEE T IMAGE PROCESS, V31, P2216, DOI 10.1109/TIP.2022.3154288
   Khan N., 2021, P BRIT MACH VIS C NO, P19
   Khan N, 2021, PROC CVPR IEEE, P8908, DOI 10.1109/CVPR46437.2021.00880
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Kim C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024224
   Kim HS, 2012, OPT EXPRESS, V20, P23755, DOI 10.1364/OE.20.023755
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Lin HT, 2015, IEEE I CONF COMP VIS, P3451, DOI 10.1109/ICCV.2015.394
   Lin PH, 2017, J IMAGING, V3, DOI 10.3390/jimaging3020017
   Lourenco R, 2022, EUR W VIS INF PROCES, DOI 10.1109/EUVIP53989.2022.9922672
   Mishiba K, 2020, IEEE T IMAGE PROCESS, V29, P4232, DOI 10.1109/TIP.2020.2970814
   Nakamura J, 2013, APPL PHYS EXPRESS, V6, DOI 10.7567/APEX.6.022501
   Ng R., 2005, Light field photography with a hand-held plenoptic camera, DOI DOI 10.1145/3097571
   Peng JY, 2018, INT CONF 3D VISION, P295, DOI 10.1109/3DV.2018.00042
   Sahin E, 2016, OPT LETT, V41, P998, DOI 10.1364/OL.41.000998
   Sajadi B, 2010, COMPUT GRAPH FORUM, V29, P1063, DOI 10.1111/j.1467-8659.2009.01676.x
   Sheng H, 2018, PATTERN RECOGN, V74, P587, DOI 10.1016/j.patcog.2017.09.010
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Stanford Lytro Light Field Archive, 2016, About us
   Strecke M, 2017, PROC CVPR IEEE, P2529, DOI 10.1109/CVPR.2017.271
   Strutz T., 2010, Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond)
   Suzuki T, 2016, IEEE IMAGE PROC, P1444, DOI 10.1109/ICIP.2016.7532597
   Tao MW, 2017, IEEE T PATTERN ANAL, V39, P546, DOI 10.1109/TPAMI.2016.2554121
   Tao MW, 2015, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2015.7298804
   Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89
   Tian JD, 2017, IEEE I CONF COMP VIS, P2420, DOI 10.1109/ICCV.2017.263
   Tsai YJ, 2020, AAAI CONF ARTIF INTE, V34, P12095
   Uliyar M, 2013, IEEE IMAGE PROC, P1, DOI 10.1109/ICIP.2013.6738001
   Wang TC, 2015, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2015.398
   Wang YQ, 2023, IEEE T PATTERN ANAL, V45, P425, DOI 10.1109/TPAMI.2022.3152488
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wanner S., 2013, Vis., Model. Vis., V13, P225
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44
   Williem, 2018, IEEE T PATTERN ANAL, V40, P2484, DOI 10.1109/TPAMI.2017.2746858
   Williem, 2016, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2016.476
   Wu GC, 2017, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR.2017.178
   Wu GC, 2019, IEEE T IMAGE PROCESS, V28, P3261, DOI 10.1109/TIP.2019.2895463
   Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yu JY, 2017, IEEE MULTIMEDIA, V24, P104, DOI 10.1109/MMUL.2017.24
   Yücer K, 2016, INT CONF 3D VISION, P249, DOI 10.1109/3DV.2016.33
   Zhang K, 2014, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2014.206
   Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007
   Zhang YB, 2017, IEEE T CIRC SYST VID, V27, P739, DOI 10.1109/TCSVT.2016.2555778
   Zhu H, 2017, IEEE J-STSP, V11, P965, DOI 10.1109/JSTSP.2017.2730818
   Zhu K, 2019, IEEE T PATTERN ANAL, V41, P1131, DOI 10.1109/TPAMI.2018.2827049
   Zhu Y., 2009, P AS PAC C INF PROC
   Ziegler R, 2007, COMPUT GRAPH FORUM, V26, P435, DOI 10.1111/j.1467-8659.2007.01066.x
NR 70
TC 0
Z9 0
U1 3
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6866
EP 6879
DI 10.1109/TVCG.2023.3344132
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F2M7M
UT WOS:001308219500005
PM 38113148
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Cao, Y
   Meng, XQ
   Mok, PY
   Lee, TY
   Liu, XT
   Li, P
AF Cao, Yu
   Meng, Xiangqiao
   Mok, P. Y.
   Lee, Tong-Yee
   Liu, Xueting
   Li, Ping
TI AnimeDiffusion: Anime Diffusion Colorization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Training; Task analysis; Faces; Image synthesis;
   Computational modeling; Noise reduction; Line drawing colorization;
   diffusion models; reference-based colorization; conditional GAN
AB Being essential in animation creation, colorizing anime line drawings is usually a tedious and time-consuming manual task. Reference-based line drawing colorization provides an intuitive way to automatically colorize target line drawings using reference images. The prevailing approaches are based on generative adversarial networks (GANs), yet these methods still cannot generate high-quality results comparable to manually-colored ones. In this article, a new AnimeDiffusion approach is proposed via hybrid diffusions for the automatic colorization of anime face line drawings. This is the first attempt to utilize the diffusion model for reference-based colorization, which demands a high level of control over the image synthesis process. To do so, a hybrid end-to-end training strategy is designed, including phase 1 for training diffusion model with classifier-free guidance and phase 2 for efficiently updating color tone with a target reference colored image. The model learns denoising and structure-capturing ability in phase 1, and in phase 2, the model learns more accurate color information. Utilizing our hybrid training strategy, the network convergence speed is accelerated, and the colorization performance is improved. Our AnimeDiffusion generates colorization results with semantic correspondence and color consistency. In addition, the model has a certain generalization performance for line drawings of different line styles. To train and evaluate colorization methods, an anime face line drawing colorization benchmark dataset, containing 31,696 training data and 579 testing data, is introduced and shared. Extensive experiments and user studies have demonstrated that our proposed AnimeDiffusion outperforms state-of-the-art GAN-based methods and another diffusion-based model, both quantitatively and qualitatively.
C1 [Cao, Yu; Mok, P. Y.] Hong Kong Polytech Univ, Sch Fash & Text, Hong Kong, Peoples R China.
   [Meng, Xiangqiao; Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Meng, Xiangqiao; Li, Ping] Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
   [Mok, P. Y.] Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
   [Liu, Xueting] St Francis Univ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University; Hong
   Kong Polytechnic University; National Cheng Kung University; Saint
   Francis University Hong Kong
RP Mok, PY (corresponding author), Hong Kong Polytech Univ, Sch Fash & Text, Hong Kong, Peoples R China.; Li, P (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.; Li, P (corresponding author), Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
EM yu-daniel.cao@connect.polyu.hk; xiangqiao.meng@connect.polyu.hk;
   tracy.mok@polyu.edu.hk; tonylee@mail.ncku.edu.tw; tliu@cihe.edu.hk;
   p.li@polyu.edu.hk
RI Li, Ping/AAO-2019-2020; Liu, Xueting/AAG-9648-2019
OI Li, Ping/0000-0002-1503-0240; Liu, Xueting/0000-0002-0868-5353; Cao, Yu
   (Daniel)/0000-0002-9761-0723; Mok, Tracy/0000-0002-0635-5318
FU Innovation and Technology Commission of Hong Kong [ITP/028/21TP];
   National Science and Technology Council, Taiwan
   [110-2221-E-006-135-MY3]; Hong Kong Polytechnic University [P0044520,
   P0048387, P0042740, P0035358, P0043906, P0030419]
FX This work was supported in part by the Innovation and Technology
   Commission of Hong Kong under Grant ITP/028/21TP, in part by the
   National Science and Technology Council under Grant
   110-2221-E-006-135-MY3, Taiwan, and in part by The Hong Kong Polytechnic
   University under Grants P0044520, P0048387, P0042740, P0035358,
   P0043906, and P0030419.
CR Akita K, 2020, COMPUT GRAPH FORUM, V39, P601, DOI 10.1111/cgf.14171
   AnonymousCommunity D., 2021, Danbooru2020: A large-scale crowdsourced and tagged anime illustration dataset
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao MD, 2023, IEEE I CONF COMP VIS, P22503, DOI 10.1109/ICCV51070.2023.02062
   Cao Y, 2023, IEEE INT CON MULTI, P1637, DOI 10.1109/ICME55011.2023.00282
   Chen SY, 2022, IEEE T VIS COMPUT GR, V28, P1198, DOI 10.1109/TVCG.2020.3009949
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Ci YZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1536, DOI 10.1145/3240508.3240661
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dong X, 2022, IEEE T VIS COMPUT GR, V28, P1469, DOI 10.1109/TVCG.2020.3022480
   Fang FM, 2020, IEEE T VIS COMPUT GR, V26, P2931, DOI 10.1109/TVCG.2019.2908363
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Ho J, 2022, J MACH LEARN RES, V23, P1
   Huang ZY, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591548
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Kim H, 2019, IEEE I CONF COMP VIS, P9055, DOI 10.1109/ICCV.2019.00915
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Li XY, 2022, IEEE T VIS COMPUT GR, V28, P2938, DOI 10.1109/TVCG.2021.3049419
   Li ZK, 2022, LECT NOTES COMPUT SC, V13677, P579, DOI 10.1007/978-3-031-19790-1_35
   Li ZS, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3585002
   Liu XT, 2022, COMPUT VIS MEDIA, V8, P135, DOI 10.1007/s41095-021-0228-6
   Meng C., 2021, P INT C LEARN REPR, P1
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., arXiv:2204.06125, P2022
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia Chitwan, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530757
   Saharia C., 2022, Proc. Adv. Neural Inf. Process. Syst.
   Shi M, 2023, IEEE T VIS COMPUT GR, V29, P2965, DOI 10.1109/TVCG.2022.3146000
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song J., 2021, P INT C LEARN REPR
   Sykora D, 2009, COMPUT GRAPH FORUM, V28, P599, DOI 10.1111/j.1467-8659.2009.01400.x
   Varga D, 2017, PROCEEDINGS OF THE 15TH INTERNATIONAL WORKSHOP ON CONTENT-BASED MULTIMEDIA INDEXING (CBMI), DOI 10.1145/3095713.3095742
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Xia MH, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555432
   Xiao TH, 2022, INT J COMPUT VISION, V130, P1293, DOI 10.1007/s11263-022-01602-y
   Xiao Y, 2022, IEEE T VIS COMPUT GR, V28, P1557, DOI 10.1109/TVCG.2020.3021510
   Xu XQ, 2023, Arxiv, DOI arXiv:2305.16223
   Yang BX, 2023, IEEE T MULTIMEDIA, V25, P8657, DOI 10.1109/TMM.2023.3239182
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhou XR, 2021, PROC CVPR IEEE, P11460, DOI 10.1109/CVPR46437.2021.01130
   Zou CQ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356561
NR 44
TC 1
Z9 1
U1 8
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6956
EP 6969
DI 10.1109/TVCG.2024.3357568
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F2M7M
UT WOS:001308219500004
PM 38261497
DA 2024-11-06
ER

PT J
AU Lin, YQ
   Chen, LC
   Huang, HB
   Ma, CY
   Han, XG
   Cui, SG
AF Lin, Yiqun
   Chen, Lichang
   Huang, Haibin
   Ma, Chongyang
   Han, Xiaoguang
   Cui, Shuguang
TI Task-Aware Sampling Layer for Point-Wise Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D vision; point cloud; point sampling; point cloud segmentation; 3D
   vision; point cloud; point sampling; point cloud segmentation
AB Sampling, grouping, and aggregation are three important components in the multi-scale analysis of point clouds. In this paper, we present a novel data-driven sampler learning strategy for point-wise analysis tasks. Unlike the widely used sampling technique, Farthest Point Sampling (FPS), we propose to learn sampling and downstream applications jointly. Our key insight is that uniform sampling methods like FPS are not always optimal for different tasks: sampling more points around boundary areas can make the point-wise classification easier for segmentation. Towards this end, we propose a novel sampler learning strategy that learns sampling point displacement supervised by task-related ground truth information and can be trained jointly with the underlying tasks. We further demonstrate our methods in various point-wise analysis tasks, including semantic part segmentation, point cloud completion, and keypoint detection. Our experiments show that jointly learning of the sampler and task brings better performance than using FPS in various point-based networks.
C1 [Lin, Yiqun; Han, Xiaoguang; Cui, Shuguang] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Chen, Lichang] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
   [Huang, Haibin; Ma, Chongyang] Kuaishou Technol, Beijing 100085, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); University of Pittsburgh
RP Han, XG (corresponding author), Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
EM lyq211003@gmail.com; bobchen23@pitt.edu; jackiehuanghaibin@gmail.com;
   chongyangm@gmail.com; hanxiaoguang@cuhk.edu.cn; shuguangcui@cuhk.edu.cn
RI Huang, Haibin/HHZ-1901-2022; Cui, Shuguang/D-4677-2014
OI Han, Xiaoguang/0000-0003-0162-3296
FU Key Area R&D Program of Guangdong Province [2018B030338001]; National
   Key R&D Program of China [2018YFB1800800]; Shenzhen Outstanding Talents
   Training Fund; Guangdong Research Project [2017ZT07X152]
FX This work was supported in part by the Key Area R&D Program of Guangdong
   Province under Grant 2018B030338001, in part by the National Key R&D
   Program of China under Grant 2018YFB1800800, in part by Shenzhen
   Outstanding Talents Training Fund, and in part by Guangdong Research
   Project under Grant 2017ZT07X152.
CR Atzmon M, 2018, Arxiv, DOI arXiv:1803.10091
   Behl A, 2017, IEEE I CONF COMP VIS, P2593, DOI 10.1109/ICCV.2017.281
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dovrat O, 2019, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR.2019.00287
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Gong BC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12468, DOI 10.1109/ICCV48922.2021.01226
   Graham B, 2017, Arxiv, DOI arXiv:1706.01307
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Groh Fabian., 2018, P AS C COMP VIS
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457
   Huang Z., 2020, PF-Net: Point Fractal Network for 3D Point Cloud Completion
   Lang I, 2020, PROC CVPR IEEE, P7575, DOI 10.1109/CVPR42600.2020.00760
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li YY, 2018, ADV NEUR IN, V31
   Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435
   Loizou M, 2020, COMPUT GRAPH FORUM, V39, P183, DOI 10.1111/cgf.14078
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Nie Yinyu, 2020, ADV NEUR IN, V33
   Pomares A, 2018, MED C CONTR AUTOMAT, P400, DOI 10.1109/MED.2018.8442569
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qi CR, 2018, Arxiv, DOI [arXiv:1711.08488, 10.48550/arXiv.1711.08488]
   Rambach J, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P103, DOI 10.1109/ISMAR-Adjunct.2017.42
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi SS, 2020, PROC CVPR IEEE, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Te G., 2018, Regularized graph CNN for point cloud segmentation
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang C, 2018, Arxiv, DOI arXiv:1803.05827
   Wang PS, 2021, AAAI CONF ARTIF INTE, V35, P2773
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang Y., 2020, "PFCNN: Convolutional neural networks on 3D surfaces using parallel frames
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yin KX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201288
   You Y, 2020, Arxiv, DOI arXiv:2002.12687
   Yu FG, 2019, PROC CVPR IEEE, P9483, DOI 10.1109/CVPR.2019.00972
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Yue XY, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P458, DOI 10.1145/3206025.3206080
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
NR 53
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6612
EP 6624
DI 10.1109/TVCG.2022.3171794
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600016
PM 35503827
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Xu, TC
   Ren, XH
   Yang, JL
   Sheng, B
   Wu, EH
AF Xu, Tianchen
   Ren, Xiaohua
   Yang, Jiale
   Sheng, Bin
   Wu, Enhua
TI Efficient Binocular Rendering of Volumetric Density Fields With Coupled
   Adaptive Cube-Map Ray Marching for Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Binocular views; density field; global illumination; ray marching;
   real-time rendering; virtual reality; volume rendering; Binocular views;
   density field; global illumination; ray marching; real-time rendering;
   virtual reality; volume rendering
ID GPU
AB Creating visualizations of multiple volumetric density fields is demanding in virtual reality (VR) applications, which often include divergent volumetric density distributions mixed with geometric models and physics-based simulations. Real-time rendering of such complex environments poses significant challenges for rendering quality and performance. This article presents a novel scheme for efficient real-time rendering of varying translucent volumetric density fields with global illumination (GI) effects on high-resolution binocular VR displays. Our scheme proposes creative solutions to address three challenges involved in the target problem. First, to tackle the doubled heavy workloads of binocular ray marching, we explore the anti-aliasing principles and more advanced potentials of ray marching on interior cube-map faces, and propose a coupled ray-marching technique that converges to multi-resolution cube maps with interleaved adaptive sampling. Second, we devise a fully dynamic ambient GI approximation method that leverages spherical-harmonics (SH) transform information of the phase function to reduce the huge amount of ray sampling required for GI while ensuring fidelity. The method catalyzes spatial ray-marching reuse and adaptive temporal accumulation. Third, we deploy a two-phase ray-tracing algorithm with a tiled k-buffer to achieve fast processing of order-independent transparency (OIT) for multiple volume instances. Consequently, high-quality and high-performance real-time dynamic volume rendering can be achieved under constrained budgets controlled by developers. As our solution supports mixed mesh-volume rendering, the test results prove the practical usefulness of our approach for high-resolution binocular VR rendering on hybrid multi-volumetric and geometric environments.
C1 [Xu, Tianchen; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab CS, Beijing 100190, Peoples R China.
   [Xu, Tianchen; Wu, Enhua] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Xu, Tianchen; Yang, Jiale] Adv Micro Devices AMD Inc, Shanghai 201210, Peoples R China.
   [Ren, Xiaohua] Tencent, Multimedia Res Ctr, Shenzhen 518054, Peoples R China.
   [Yang, Jiale; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Wu, Enhua] Univ Macau, FST, Taipa, Macao, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Tencent;
   Shanghai Jiao Tong University; University of Macau
RP Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab CS, Beijing 100190, Peoples R China.
EM tianchenx@outlook.com; xiaohua_ren@126.com; yangjiale@sjtu.edu.cn;
   shengbin@cs.sjtu.edu.cn; ehwu@um.edu.mo
RI Xu, Tianchen/T-9694-2019; Sheng, Bin/LMO-9532-2024
OI Sheng, Bin/0000-0001-8678-2784; Xu, Tianchen/0000-0002-5031-0715; Ren,
   Xiaohua/0000-0002-3196-9880; wu, en hua/0000-0002-2174-1428
FU National Natural Science Foundation of China [62332015, 62072449,
   62272298]; AMD Shanghai-Khronos3D
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62332015, 62072449, and 62272298, and
   in part by AMD Shanghai-Khronos3D.
CR [Anonymous], 2022, Vulkan 1.3.230-A Specification
   Bergner S, 2006, IEEE T VIS COMPUT GR, V12, P1353, DOI 10.1109/TVCG.2006.113
   Beyer J, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12605
   Bitterli B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392481
   Boissé G, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 TECHNICAL COMMUNICATIONS, DOI 10.1145/3478512.3488613
   Chandrasekhar S., 1960, Radiative Transfer
   Derin MO, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 TECHNICAL COMMUNICATIONS, DOI 10.1145/3478512.3488608
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Eisemann E., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Engel K., 2001, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Workshop on Graphics Hardware, P9, DOI [10.1145/383507.383515, DOI 10.1145/383507.383515]
   Fraboni B, 2022, COMPUT GRAPH FORUM, V41, P379, DOI 10.1111/cgf.14481
   Hillesland K. E., 2016, P 37 ANN C EUR ASS C, P73
   Iwasaki K., 2007, P 18 EUR C REND TECH, P35
   Jindal A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480514
   Karis B., 2014, P ACM SIGGRAPH COURS, P1
   Kettunen M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459937
   Kim TY, 2001, SPRING EUROGRAP, P177
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Kulla C, 2012, COMPUT GRAPH FORUM, V31, P1519, DOI 10.1111/j.1467-8659.2012.03148.x
   Lin DQ, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530158
   Lin DQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480499
   Marrs A, 2018, HIGH-PERFORMANCE GRAPHICS 2018, DOI 10.1145/3231578.3231579
   Morrical N, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P256, DOI 10.1109/visual.2019.8933539
   Muñoz A, 2014, COMPUT GRAPH FORUM, V33, P167, DOI 10.1111/cgf.12424
   Novák J, 2018, COMPUT GRAPH FORUM, V37, P551, DOI 10.1111/cgf.13383
   Ouyang Y, 2021, COMPUT GRAPH FORUM, V40, P17, DOI 10.1111/cgf.14378
   Parker S., 2005, P ACM SIGGRAPH COURS, P15, DOI [10.1145/1198555.1198754, DOI 10.1145/1198555.1198754]
   Raab M, 2008, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2006, P591, DOI 10.1007/978-3-540-74496-2_35
   Risser E., 2007, GPU Gems, V3, P481
   Schwarz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866201
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Smelyanskiy M, 2009, IEEE T VIS COMPUT GR, V15, P1563, DOI 10.1109/TVCG.2009.164
   Talbot JustinF., 2005, RENDERING TECHNIQUES, P139, DOI DOI 10.2312/EGWR/EGSR05/139-146
   Tokuyoshi Y., 2021, Tech. Rep. 21-11-ecdc
   Vasilakis A. A., 2014, P 18 M ACM SIGGRAPH, P143, DOI [10.1145/2556700.2556702, DOI 10.1145/2556700.2556702]
   Wang R., 2007, P 18 EUR C REND TECH, P13
   Xie F., 2007, P 18 EUROGRAPHICS C, P265, DOI [10.2312/EGWR/EGSR07/265-276, DOI 10.2312/EGWR/EGSR07/265-276]
   Xu TC, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 TECHNICAL COMMUNICATIONS, DOI 10.1145/3478512.3488598
   Yang L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618481
   Yuksel C, 2008, COMPUT GRAPH FORUM, V27, P675, DOI 10.1111/j.1467-8659.2008.01165.x
NR 41
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6625
EP 6638
DI 10.1109/TVCG.2023.3322416
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600005
PM 37801374
DA 2024-11-06
ER

PT J
AU Preim, B
   Meuschke, M
   Weiss, V
AF Preim, Bernhard
   Meuschke, Monique
   Weiss, Veronika
TI A Survey of Medical Visualization Through the Lens of Metaphors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human-Computer interaction; interaction design; medical visualization;
   metaphors; Human-Computer interaction; interaction design; medical
   visualization; metaphors
ID VIRTUAL LIVER RESECTION; REALITY; GENERATION; RECONSTRUCTION;
   INFORMATION; EXPLORATION; SIMULATION; ALGORITHM; SYSTEM; VIDEOS
AB We provide an overview of metaphors that were used in medical visualization and related user interfaces. Metaphors are employed to translate concepts from a source domain to a target domain. The survey is grounded in a discussion of metaphor-based design involving the identification and reflection of candidate metaphors. We consider metaphors that have a source domain in one branch of medicine, e.g., the virtual mirror that solves problems in orthopedics and laparoscopy with a mirror that resembles the dentist's mirror. Other metaphors employ the physical world as the source domain, such as crepuscular rays that inspire a solution for access planning in tumor therapy. Aviation is another source of inspiration, leading to metaphors, such as surgical cockpits, surgical control towers, and surgery navigation according to an instrument flight. This paper should raise awareness for metaphors and their potential to focus the design of computer-assisted systems on useful features and a positive user experience. Limitations and potential drawbacks of a metaphor-based user interface design for medical applications are also considered.
C1 [Preim, Bernhard; Meuschke, Monique] Univ Magdeburg, D-39106 Magdeburg, Germany.
   [Weiss, Veronika] Hsch RheinMain, D-65197 Wiesbaden, Germany.
C3 Otto von Guericke University
RP Preim, B (corresponding author), Univ Magdeburg, D-39106 Magdeburg, Germany.
EM bernhard.preim@ovgu.de; meuschke@isg.cs.uni-magdeburg.de;
   veronika.weiss@hs-rm.de
RI Preim, Bernhard/E-8037-2015
OI Preim, Bernhard/0000-0001-9826-9478; Weiss, Veronika/0000-0003-4313-9259
CR Aldrich Virgil., 1968, J AESTHET EDUC, V2, P73
   Allgaier M, 2023, INT J COMPUT ASS RAD, V18, P2013, DOI 10.1007/s11548-023-02851-z
   Altrogge I, 2006, LECT NOTES COMPUT SC, V4190, P486
   Amini F, 2017, IEEE T VIS COMPUT GR, V23, P501, DOI 10.1109/TVCG.2016.2598647
   Amini F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1459, DOI 10.1145/2702123.2702431
   Averbukh VL, 2001, PROGRAM COMPUT SOFT+, V27, P227, DOI 10.1023/A:1012333025189
   Bach B, 2017, IEEE COMPUT GRAPH, V37, P6, DOI 10.1109/MCG.2017.33
   Bade R., 2004, P ACM SIGCHI C HUM F, P112
   Baegert C, 2007, PROC SPIE, V6509, DOI 10.1117/12.708757
   Barr P., 2002, Proceedings of the SIGCHI-NZ Symposium on Computer-Human Interaction, Hamilton, New Zealand, P25
   Bartroli AV, 2001, IEEE VISUAL, P411, DOI 10.1109/VISUAL.2001.964540
   Bartrolí AV, 2001, SPRING EUROGRAP, P127
   Behrendt B., 2020, P EUR WORKSH VIS COM, P37
   Bhalla US, 2003, PROG BIOPHYS MOL BIO, V81, P45, DOI 10.1016/S0079-6107(02)00046-9
   Bichlmeier Christoph, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P91, DOI 10.1109/ISMAR.2010.5643555
   Bichlmeier C., 2006, AMIARCS THE TANGIBLE
   Bichlmeier C, 2009, IEEE T MED IMAGING, V28, P1498, DOI 10.1109/TMI.2009.2018622
   Bier E. A., 1993, Computer Graphics Proceedings, P73, DOI 10.1145/166117.166126
   Blackwell A. R., 2006, ACM Transactions on Computer-Human Interaction, V13, P490, DOI 10.1145/1188816.1188820
   Blum T, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P115, DOI 10.1109/VR.2012.6180909
   Bork F, 2019, ANAT SCI EDUC, V12, P585, DOI 10.1002/ase.1864
   Bourquain H, 2002, CARS 2002: COMPUTER ASSISTED RADIOLOGY AND SURGERY, PROCEEDINGS, P341
   Brooks CC, 2020, CLIN OPHTHALMOL, V14, P557, DOI 10.2147/OPTH.S239339
   Buccioli Matteo, 2016, Current Directions in Biomedical Engineering, V2, P345, DOI 10.1515/cdbme-2016-0077
   Carnahan J, 2001, IEEE POTENTIALS, V20, P21, DOI 10.1109/45.954644
   Carroll J. M., 1988, Handbook of Human-Computer Interaction, P67, DOI DOI 10.1016/B978-0-444-70536-5.50008-7
   Carroll Noel., 1994, ASPECTS OF METAPHOR, V238, P189, DOI [10.1007/978-94-015-8315-26, DOI 10.1007/978-94-015-8315-26]
   Casarett D, 2010, J PALLIAT MED, V13, P255, DOI 10.1089/jpm.2009.0221
   Chheang V, 2021, COMPUT GRAPH-UK, V99, P234, DOI 10.1016/j.cag.2021.07.009
   CIGNONI P, 1994, COMPUT GRAPH FORUM, V13, pC317, DOI 10.1111/1467-8659.1330317
   Coulehan Jack, 2003, Yale J Biol Med, V76, P87
   Darrell T, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P616, DOI 10.1109/AFGR.1998.671016
   Dee FR, 2009, HUM PATHOL, V40, P1112, DOI 10.1016/j.humpath.2009.04.010
   Demjen Z., 2016, The Routledge Handbook of Metaphor and Language, P417
   Eisert R, 2008, PROC CVPR IEEE, P1744
   Englund R., 2016, P EUR WORKSH VIS COM, P79
   Erickson T.D., 1995, Readings in Hitman-Computer Interaction, P147
   Eulzer P, 2022, COMPUT GRAPH FORUM, V41, P645, DOI 10.1111/cgf.14576
   Fischer F., 2012, P 14 EUR C VIS
   Franke S., 2012, Int. J. Comput. Assist. Radiol. Surg., V7, P507
   Furnas G. W., 1986, SIGCHI Bull, V17, P16, DOI DOI 10.1145/22339.22342
   Gambadauro P, 2012, SURG INNOV, V19, P76, DOI 10.1177/1553350611415424
   GARCIA EV, 1987, CARDIOVASC INTER RAD, V10, P374, DOI 10.1007/BF02577348
   Gasteiger R, 2011, IEEE T VIS COMPUT GR, V17, P2183, DOI 10.1109/TVCG.2011.243
   Gehrmann S, 2006, CLIN ANAT, V19, P258, DOI 10.1002/ca.20266
   Glasser S, 2017, COMPUT GRAPH FORUM, V36, P57, DOI 10.1111/cgf.12994
   Gombrich E. H., 1954, Visual Metaphors of Value in Art
   Grosjean J., 1999, P SPRING C COMP GRAP, P125
   Hamilton A., 2000, ACM J COMPUTER DOCUM, V24, P237, DOI [10.1145/353927.353935, DOI 10.1145/353927.353935]
   Hansen C, 2013, INT J COMPUT ASS RAD, V8, P419, DOI 10.1007/s11548-012-0790-6
   Harrington KJ, 2012, CLIN J ONCOL NURS, V16, P408, DOI 10.1188/12.CJON.408-412
   Havre S, 2000, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2000, P115, DOI 10.1109/INFVIS.2000.885098
   He TS, 1996, IEEE VISUAL, P227, DOI 10.1109/VISUAL.1996.568113
   Heinrich F, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364245
   Heinrich F, 2019, IEEE T VIS COMPUT GR, V25, P2157, DOI 10.1109/TVCG.2019.2903942
   Hemminger BM, 2003, J DIGIT IMAGING, V16, P292, DOI [10.1007/s10278-003-1659-6, 10.1007/s10278-003-1659-0]
   HENDERSON DA, 1986, ACM T GRAPHIC, V5, P211, DOI 10.1145/24054.24056
   Hohne K. H., 1992, Proceedings. Visualization '92 (Cat. No.92CH3201-1), P115, DOI 10.1109/VISUAL.1992.235218
   HOHNE KH, 1992, IEEE COMPUT GRAPH, V12, P72, DOI 10.1109/38.144829
   Hori K, 2001, INT CONGR SER, V1230, P1160
   Hutchins E., 1987, Tech. Rep.
   Iannessi A, 2018, INSIGHTS IMAGING, V9, P599, DOI 10.1007/s13244-018-0620-7
   Ikuta K, 2007, IEEE ASME INT C ADV, P1131
   Khlebnikov R, 2011, IEEE T VIS COMPUT GR, V17, P2163, DOI 10.1109/TVCG.2011.184
   Khoury G. R., 2004, P AS PAC C CONC MOD, P65
   Kitamura G, 2021, SKELETAL RADIOL, V50, P1809, DOI 10.1007/s00256-021-03733-8
   Kleinau A., 2022, P EUR WORKSH VIS COM, P21
   Knudsen S, 2005, PUBLIC UNDERST SCI, V14, P373, DOI 10.1177/0963662505056613
   Köhler B, 2015, BILDVERARBEITUNG FUR DIE MEDIZIN 2015: ALGORITHMEN - SYSTEME - ANWENDUNGEN, P257, DOI 10.1007/978-3-662-46224-9_45
   Konrad-Verse O., 2004, P SIM VIS, P203
   Koyama Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392444
   Kraak M-J., 1997, Proc. 18th Int. Cartogr. Conf, P253
   Kreiser J, 2018, COMPUT GRAPH FORUM, V37, P597, DOI 10.1111/cgf.13445
   Krüger A, 2008, IEEE T VIS COMPUT GR, V14, P1491, DOI 10.1109/TVCG.2008.161
   Kugelmann D, 2018, ANN ANAT, V215, P71, DOI 10.1016/j.aanat.2017.09.011
   Kuhn W., 1993, Spatial Information Theory. A Theoretical Basis for GIS. European Conference, COSIT '93 Proceedings, P366
   LAKOFF G, 1980, COGNITIVE SCI, V4, P195, DOI 10.1016/S0364-0213(80)80017-6
   Lamata P, 2010, SURG ENDOSC, V24, P2327, DOI 10.1007/s00464-010-0915-3
   Lang HK, 2005, ARCH SURG-CHICAGO, V140, P629, DOI 10.1001/archsurg.140.7.629
   Lawonn K, 2018, COMPUT GRAPH FORUM, V37, P205, DOI 10.1111/cgf.13322
   Lee B, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2257
   Lewis D, 1999, J AM MED INFORM ASSN, V6, P272, DOI 10.1136/jamia.1999.0060272
   Li FCY, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P125
   Lichan Hong, 1997, Computer Graphics Proceedings, SIGGRAPH 97, P27, DOI 10.1145/258734.258750
   LORENSEN WE, 1995, ST HEAL T, V18, P221
   Lundström C, 2011, IEEE T VIS COMPUT GR, V17, P1775, DOI 10.1109/TVCG.2011.224
   Luo H, 2006, IEEE T INF TECHNOL B, V10, P302, DOI 10.1109/TITB.2005.859872
   MacEachren Alan M., 2004, How Maps Work: Representation, Visualization, and Design
   MADSEN KH, 1994, COMMUN ACM, V37, P57, DOI 10.1145/198366.198381
   Mansmann F, 2013, P SIGCHI C HUM FACT, DOI DOI 10.1145/2470654.2466443
   Marcus A., 1998, Journal of Computer Documentation, V22, P43, DOI 10.1145/291391.291397
   Marcus A, 2005, HUM FAC ER, P51
   Marescaux J, 1998, ANN SURG, V228, P627, DOI 10.1097/00000658-199811000-00001
   Marks J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P389, DOI 10.1145/258734.258887
   Mascagni P, 2021, J VISC SURG, V158, pS18, DOI 10.1016/j.jviscsurg.2021.01.004
   Mattos LS, 2011, IEEE INT C INT ROBOT, P1359, DOI 10.1109/IROS.2011.6048239
   McGreevy JM, 2005, J AM COLL SURGEONS, V201, P110, DOI 10.1016/j.jamcollsurg.2005.02.024
   Meuschke M, 2016, COMPUT GRAPH FORUM, V35, P351, DOI 10.1111/cgf.12911
   Meuschke M, 2021, COMPUT GRAPH-UK, V99, P22, DOI 10.1016/j.cag.2021.05.005
   Meuschke M, 2019, IEEE T VIS COMPUT GR, V25, P997, DOI 10.1109/TVCG.2018.2864509
   Miao HC, 2017, IEEE T VIS COMPUT GR, V23, P1612, DOI 10.1109/TVCG.2017.2674938
   Mirhosseini S, 2019, IEEE T VIS COMPUT GR, V25, P2011, DOI 10.1109/TVCG.2019.2898763
   Mise Y, 2011, BRIT J SURG, V98, P1742, DOI 10.1002/bjs.7670
   Moise A, 2002, PROC SPIE, V4685, P189, DOI 10.1117/12.467006
   Navab N, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P43
   Neale Dennis C., 1997, Handbook of HumanComputer Interaction, P441, DOI DOI 10.1016/B978-044481862-1.50086-8
   Nie JB, 2016, AM J BIOETHICS, V16, P3, DOI 10.1080/15265161.2016.1214305
   Numminen K, 2005, EUR J RADIOL, V56, P179, DOI 10.1016/j.ejrad.2005.03.021
   Oeltze S., 2006, P 8 JOINT EUR IEEE V, P131
   Onken M., 2010, Biomed. Image Process., P454
   Owen F. J., 2010, US Patent App, Patent No. [12/201,652, 12201652]
   Pang A., 1995, P VIS SCI COMP, P1
   Perkins R., 1997, ACM Interact., V4, P40
   Pfister H, 2001, IEEE COMPUT GRAPH, V21, P16, DOI 10.1109/38.920623
   Pflesser B, 1998, LECT NOTES COMPUT SC, V1496, P853, DOI 10.1007/BFb0056273
   Pohlandt D., 2019, P MENSCH COMP, P102
   Preim B, 2002, CARS 2002: COMPUTER ASSISTED RADIOLOGY AND SURGERY, PROCEEDINGS, P353
   Preim B, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P21, DOI 10.1109/VISUAL.2002.1183752
   Preim B, 2013, Visual computing for medicine: theory, algorithms, and applications
   Puchalski S. M., 2008, Vet. Radiol. Ultrasound, V49, pS13
   Reitinger B, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P37, DOI 10.1109/TRIDUI.2006.1618268
   Rieder C, 2010, COMPUT GRAPH FORUM, V29, P1093, DOI 10.1111/j.1467-8659.2009.01665.x
   Ringl H, 2015, EUR RADIOL, V25, P1865, DOI 10.1007/s00330-015-3598-2
   Ritter F, 2000, PROC GRAPH INTERF, P171
   Ritter F.B. Berendt., 2002, Proceedings of Mensch Computer, P363
   Rocha A, 2019, IEEE T VIS COMPUT GR, V25, P2568, DOI 10.1109/TVCG.2018.2850781
   Ruthenbeck GS, 2008, STUD HEALTH TECHNOL, V132, P436
   RUTHERFORD HG, 1983, P SOC PHOTO-OPT INST, V418, P54
   Sanchez A., 2000, Educ. Inf. Technol., V5, P362
   Satava R M, 1994, Artif Intell Med, V6, P281, DOI 10.1016/0933-3657(94)90033-7
   Satava RM, 2001, SURG ENDOSC, V15, P232, DOI 10.1007/s004640000369
   Schon D. A., 1979, Metaphor Thought, V2, P163
   Schott D, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P296, DOI 10.1109/VR50410.2021.00052
   SKINNER CS, 1993, PATIENT EDUC COUNS, V22, P27, DOI 10.1016/0738-3991(93)90086-C
   Smith R. B., 1986, ACM SIGCHI Bull., V17, P67
   Strauss G, 2013, LARYNGO RHINO OTOL, V92, P102, DOI 10.1055/s-0032-1321849
   Stubblefield W. A., 1998, CHI 98. Human Factors in Computing Systems. CHI 98 Conference Proceedings, P73, DOI 10.1145/274644.274656
   Tao J, 2016, COMPUT GRAPH-UK, V59, P79, DOI 10.1016/j.cag.2016.05.024
   TIEDE U, 1993, AM J NEURORADIOL, V14, P551
   Tietjen C., 2006, P 8 JOINT EUR, P123
   Toga AW, 2003, NAT REV NEUROSCI, V4, P37, DOI 10.1038/nrn1009
   Urbaneja A, 2019, EUR J RADIOL, V110, P121, DOI 10.1016/j.ejrad.2018.11.011
   Vaananen K., 1994, P ACM SIGCHI C HUM F, P264
   van der Vorst JR, 2010, WORLD J SURG, V34, P2426, DOI 10.1007/s00268-010-0663-5
   van Geel K, 2019, EUR J RADIOL, V117, P62, DOI 10.1016/j.ejrad.2019.05.013
   Wang LJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P367
   Weber S, 2017, SCI ROBOT, V2, DOI 10.1126/scirobotics.aal4916
   Weiβ V., 2022, P NORD HUM COMP INT, P11
   Wigmore SJ, 2001, ANN SURG, V233, P221, DOI 10.1097/00000658-200102000-00011
   Camus JTW, 2009, DISCOURSE STUD, V11, P465, DOI 10.1177/1461445609105220
   Wolfsberger S, 2006, NEUROSURGERY, V59, P1001, DOI 10.1227/01.NEU.0000245594.61828.41
   Wu Jun, 2014, Stud Health Technol Inform, V196, P469
   Xia J, 2000, Int J Adult Orthodon Orthognath Surg, V15, P265
   Yang LN, 2022, IEEE T VIS COMPUT GR, V28, P922, DOI 10.1109/TVCG.2021.3114774
   Yu-jin F., 2005, P IEEE C INF ACQ, P9
   Zeineh MM, 2001, ANAT RECORD, V265, P111, DOI 10.1002/ar.1061
   Zhao LX, 2006, IEEE T VIS COMPUT GR, V12, P885, DOI 10.1109/TVCG.2006.158
   Ziemkiewicz C, 2008, IEEE T VIS COMPUT GR, V14, P1269, DOI 10.1109/TVCG.2008.171
NR 158
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6639
EP 6664
DI 10.1109/TVCG.2023.3330546
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600002
PM 37934633
OA hybrid
DA 2024-11-06
ER

PT J
AU Coscia, A
   Suh, A
   Chang, RM
   Endert, A
AF Coscia, Adam
   Suh, Ashley
   Chang, Remco
   Endert, Alex
TI Preliminary Guidelines for Combining Data Integration and Visual Data
   Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical behaviors; data integration; integration strategies; user
   interface design; visual analytics; Analytical behaviors; data
   integration; integration strategies; user interface design; visual
   analytics
AB Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis. However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research. We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process. We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations. Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files. Analyzing participants' interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias. Participants' time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration. Participants' integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights. With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process.
C1 [Coscia, Adam; Endert, Alex] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Suh, Ashley; Chang, Remco] Tufts Univ, Medford, MA 02155 USA.
C3 University System of Georgia; Georgia Institute of Technology; Tufts
   University
RP Coscia, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM acoscia6@gatech.edu; ashleysuh1@gmail.com; remco@cs.tufts.edu;
   endert@gatech.edu
RI Coscia, Adam/GUO-9221-2022
OI Coscia, Adam/0000-0002-0429-9295; Suh, Ashley/0000-0001-6513-8447;
   Chang, Remco/0000-0002-6484-6430
FU National Science Foundation [IIS-1813281, DRL-2247790]
FX This work was supported by National Science Foundation under Grants
   IIS-1813281 and DRL-2247790.
CR Thomas JJ, 2006, IEEE COMPUT GRAPH, V26, P10, DOI 10.1109/MCG.2006.5
NR 1
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6678
EP 6690
DI 10.1109/TVCG.2023.3334513
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600019
PM 37983146
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Zhang, YJ
   Yang, Q
   Zhou, YF
   Xu, XZ
   Yang, L
   Xu, YL
AF Zhang, Yujie
   Yang, Qi
   Zhou, Yifei
   Xu, Xiaozhong
   Yang, Le
   Xu, Yiling
TI TCDM: Transformational Complexity Based Distortion Metric for Perceptual
   Point Cloud Quality Assessment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image quality assessment; point cloud quality assessment (PCQA);
   predictive coding; vector autoregressive model; Image quality
   assessment; point cloud quality assessment (PCQA); predictive coding;
   vector autoregressive model
ID COLOR; MODEL
AB The goal of objective point cloud quality assessment (PCQA) research is to develop quantitative metrics that measure point cloud quality in a perceptually consistent manner. Merging the research of cognitive science and intuition of the human visual system (HVS), in this article, we evaluate the point cloud quality by measuring the complexity of transforming the distorted point cloud back to its reference, which in practice can be approximated by the code length of one point cloud when the other is given. For this purpose, we first make space segmentation for the reference and distorted point clouds based on a 3D Voronoi diagram to obtain a series of local patch pairs. Next, inspired by the predictive coding theory, we utilize a space-aware vector autoregressive (SA-VAR) model to encode the geometry and color channels of each reference patch with and without the distorted patch, respectively. Assuming that the residual errors follow the multi-variate Gaussian distributions, the self-complexity of the reference and transformational complexity between the reference and distorted samples are computed using covariance matrices. Additionally, the prediction terms generated by SA-VAR are introduced as one auxiliary feature to promote the final quality prediction. The effectiveness of the proposed transformational complexity based distortion metric (TCDM) is evaluated through extensive experiments conducted on five public point cloud quality assessment databases. The results demonstrate that TCDM achieves state-of-the-art (SOTA) performance, and further analysis confirms its robustness in various scenarios.
C1 [Zhang, Yujie; Xu, Yiling] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Yang, Qi] Tencent MediaLab, Shanghai 200433, Peoples R China.
   [Zhou, Yifei] Shanghai Maritime Univ, Shanghai 201306, Peoples R China.
   [Xu, Xiaozhong] Tencent MediaLab, Palo Alto, CA 94306 USA.
   [Yang, Le] Univ Canterbury, Dept Elect & Comp Engn, Christchurch 4800, New Zealand.
C3 Shanghai Jiao Tong University; Shanghai Maritime University; University
   of Canterbury
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM yujie19981026@sjtu.edu.cn; chinoyang@tencent.com;
   202110310163@stu.shmtu.edu.cn; xiaozhongxu@tencent.com;
   le.yang@canterbury.ac.nz; yl.xu@sjtu.edu.cn
OI Yang, Qi/0000-0002-4274-3457; Yang, Le/0000-0001-7945-6323; Zhang,
   Yujie/0000-0002-5534-0198
FU National Natural Science Foundation of China [61971282, U20A20185]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61971282, U20A20185.
CR Alexiou E, 2020, IEEE INT CONF MULTI
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Geusebroek JM, 2000, LECT NOTES COMPUT SC, V1842, P331
   Liu Q, 2023, IEEE T MULTIMEDIA, V25, P4533, DOI 10.1109/TMM.2022.3177926
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Shan Z., 2023, IEEE Trans. Vis. Comput.Graph., DOI [10.1109/TVCG.2023.3282802.2Q, DOI 10.1109/TVCG.2023.3282802.2Q]
   Su HL, 2023, IEEE T IMAGE PROCESS, V32, P1815, DOI 10.1109/TIP.2023.3253252
   Wu T., 2021, P 35 INT C NEURAL IN, V34, P29088
   Wu XJ, 2021, IEEE T CIRC SYST VID, V31, P4630, DOI 10.1109/TCSVT.2021.3101484
   Yang Q, 2022, IEEE T PATTERN ANAL, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yang Q, 2023, IEEE T PATTERN ANAL, V45, P6037, DOI 10.1109/TPAMI.2022.3213831
NR 12
TC 1
Z9 1
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6707
EP 6724
DI 10.1109/TVCG.2023.3338359
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600003
PM 38039169
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Liu, GX
   Xue, RX
   Li, JX
   Ding, DD
   Ma, Z
AF Liu, Gexin
   Xue, Ruixiang
   Li, Jiaxin
   Ding, Dandan
   Ma, Zhan
TI <i>GRNet:</i>Geometry Restoration for G-PCC Compressed Point Clouds
   Using Auxiliary Density Signaling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Auxiliary density information; coordinate expansion; coordinate
   refinement; geometry restoration; point cloud compression; Auxiliary
   density information; coordinate expansion; coordinate refinement;
   geometry restoration; point cloud compression
ID MODEL
AB The lossy Geometry-based Point Cloud Compression (G-PCC) inevitably impairs the geometry information of point clouds, which deteriorates the quality of experience (QoE) in reconstruction and/or misleads decisions in tasks such as classification. To tackle it, this work proposes GRNet for the geometry restoration of G-PCC compressed large-scale point clouds. By analyzing the content characteristics of original and G-PCC compressed point clouds, we attribute the G-PCC distortion to two key factors: point vanishing and point displacement. Visible impairments on a point cloud are usually dominated by an individual factor or superimposed by both factors, which are determined by the density of the original point cloud. To this end, we employ two different models for coordinate reconstruction, termed Coordinate Expansion and Coordinate Refinement, to attack the point vanishing and displacement, respectively. In addition, 4-byte auxiliary density information is signaled in the bitstream to assist the selection of Coordinate Expansion, Coordinate Refinement, or their combination. Before being fed into the coordinate reconstruction module, the G-PCC compressed point cloud is first processed by a Feature Analysis Module for multiscale information fusion, in which $k$kNN-based Transformer is leveraged at each scale to adaptively characterize neighborhood geometric dynamics for effective restoration. Following the common test conditions recommended in the MPEG standardization committee, GRNet significantly improves the G-PCC anchor and remarkably outperforms state-of-the-art methods on a great variety of point clouds (e.g., solid, dense, and sparse samples) both quantitatively and qualitatively. Meanwhile, GRNet runs fairly fast and uses a smaller-size model when compared with existing learning-based approaches, making it attractive to industry practitioners.
C1 [Liu, Gexin; Ding, Dandan] Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 311121, Zhejiang, Peoples R China.
   [Xue, Ruixiang; Li, Jiaxin; Ma, Zhan] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210093, Jiangsu, Peoples R China.
C3 Hangzhou Normal University; Nanjing University
RP Ding, DD (corresponding author), Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 311121, Zhejiang, Peoples R China.
EM liugexin@stu.hznu.edu.cn; xrxee@smail.nju.edu.cn;
   652022230029@smail.nju.edu.cn; dandanding@hznu.edu.cn; mazhan@nju.edu.cn
FU National Natural Science Foundation of China [62171174]
FX This research was supported by the National Natural Science Foundation
   of China under Grant 62171174.
CR 3DG, 2022, 1SC 3DG ISOIEC JTC
   Akhtar A, 2022, IEEE T IMAGE PROCESS, V31, P4133, DOI 10.1109/TIP.2022.3180904
   Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Alexandre Z., 2022, 1SC ISOIEC JTC
   Borges TM, 2022, IEEE T IMAGE PROCESS, V31, P1380, DOI 10.1109/TIP.2022.3141611
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Cao C, 2021, P IEEE, V109, P1537, DOI 10.1109/JPROC.2021.3085957
   Cao C, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338130
   Chang A.X., 2015, Technical Report
   Chen T, 2020, INT CONF ACOUST SPEE, P2163, DOI [10.1109/icassp40776.2020.9053885, 10.1109/ICASSP40776.2020.9053885]
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Nguyen DT, 2022, IEEE IMAGE PROC, P2341, DOI 10.1109/ICIP46576.2022.9897827
   Nguyen DT, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4220, DOI 10.1109/ICASSP39728.2021.9414763
   Davisson L., 1972, IEEE Trans. Commun., V20, P1202, DOI DOI 10.1109/TCOM.1972.1091311
   Fan X., 2022, arXiv
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Fu CY, 2022, AAAI CONF ARTIF INTE, P625
   Gao LY, 2021, IEEE IMAGE PROC, P3373, DOI 10.1109/ICIP42928.2021.9506631
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Guarda AFR, 2019, PICT COD SYMP
   Gwak JunYoung, 2020, P EUR C COMP VIS, P297
   He Y, 2023, PROC CVPR IEEE, P5354, DOI 10.1109/CVPR52729.2023.00518
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   Karczewicz M, 2021, IEEE T CIRC SYST VID, V31, P3907, DOI 10.1109/TCSVT.2021.3072297
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Liu H, 2022, IEEE T IMAGE PROCESS, V31, P7389, DOI 10.1109/TIP.2022.3222918
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Mao AH, 2023, IEEE T VIS COMPUT GR, V29, P4964, DOI 10.1109/TVCG.2022.3196334
   Nguyen DT, 2023, IEEE T CIRC SYST VID, V33, P4337, DOI 10.1109/TCSVT.2023.3239321
   Nguyen DT, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455990
   Pang Jiahao, 2022, APCCPA '22: Proceedings of the 1st International Workshop on Advances in Point Cloud Compression, Processing and Analysis, P11, DOI 10.1145/3552457.3555727
   Qian GC, 2021, PROC CVPR IEEE, P11678, DOI 10.1109/CVPR46437.2021.01151
   Quach M, 2019, IEEE IMAGE PROC, P4320, DOI [10.1109/ICIP.2019.8803413, 10.1109/icip.2019.8803413]
   Que ZZ, 2021, PROC CVPR IEEE, P6038, DOI 10.1109/CVPR46437.2021.00598
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang JQ, 2023, IEEE T PATTERN ANAL, V45, P9055, DOI 10.1109/TPAMI.2022.3225816
   Wang JQ, 2021, IEEE T CIRC SYST VID, V31, P4909, DOI 10.1109/TCSVT.2021.3051377
   Wang JQ, 2021, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC50243.2021.00015
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wu XZ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102866
   WU ZR, 2015, PROC CVPR IEEE, P1912, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Xue R., 2022, ARXIV
   Yan W., 2019, arXiv
   Yang Q, 2023, IEEE T PATTERN ANAL, V45, P6037, DOI 10.1109/TPAMI.2022.3213831
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Zhang Junteng, 2022, APCCPA '22: Proceedings of the 1st International Workshop on Advances in Point Cloud Compression, Processing and Analysis, P33, DOI 10.1145/3552457.3555731
   Zhang JT, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P9070, DOI 10.1145/3581783.3613847
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 54
TC 1
Z9 1
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6740
EP 6753
DI 10.1109/TVCG.2023.3336936
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600011
PM 38010929
DA 2024-11-06
ER

PT J
AU Zhou, KL
   Shum, HPH
   Li, FWB
   Liang, XH
AF Zhou, Kanglei
   Shum, Hubert P. H.
   Li, Frederick W. B.
   Liang, Xiaohui
TI Multi-Task Spatial-Temporal Graph Auto-Encoder for Hand Motion Denoising
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph convolutional network; hand motion denoising; hand motion
   prediction; multi-task learning; Graph convolutional network; hand
   motion denoising; hand motion prediction; multi-task learning
ID GENERATIVE ADVERSARIAL NETWORK
AB In many human-computer interaction applications, fast and accurate hand tracking is necessary for an immersive experience. However, raw hand motion data can be flawed due to issues such as joint occlusions and high-frequency noise, hindering the interaction. Using only current motion for interaction can lead to lag, so predicting future movement is crucial for a faster response. Our solution is the Multi-task Spatial-Temporal Graph Auto-Encoder (Multi-STGAE), a model that accurately denoises and predicts hand motion by exploiting the inter-dependency of both tasks. The model ensures a stable and accurate prediction through denoising while maintaining motion dynamics to avoid over-smoothed motion and alleviate time delays through prediction. A gate mechanism is integrated to prevent negative transfer between tasks and further boost multi-task performance. Multi-STGAE also includes a spatial-temporal graph autoencoder block, which models hand structures and motion coherence through graph convolutional networks, reducing noise while preserving hand physiology. Additionally, we design a novel hand partition strategy and hand bone loss to improve natural hand motion generation. We validate the effectiveness of our proposed method by contributing two large-scale datasets with a data corruption algorithm based on two benchmark datasets. To evaluate the natural characteristics of the denoised and predicted hand motion, we propose two structural metrics. Experimental results show that our method outperforms the state-of-the-art, showcasing how the multi-task framework enables mutual benefits between denoising and prediction.
C1 [Zhou, Kanglei; Liang, Xiaohui] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Shum, Hubert P. H.; Li, Frederick W. B.] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
   [Liang, Xiaohui] Zhongguancun Lab, Beijing 100081, Peoples R China.
C3 Beihang University; Durham University; Zhongguancun Laboratory
RP Liang, XH (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zhoukanglei@buaa.edu.cn; hubert.shum@durham.ac.uk;
   frederick.li@durham.ac.uk; liang_xiaohui@buaa.edu.cn
RI Li, Frederick/AAM-6662-2021; Shum, Hubert P. H./E-8060-2015
OI liang, xiaohui/0000-0001-6351-2538; Zhou, Kanglei/0000-0002-4660-581X;
   Shum, Hubert P. H./0000-0001-5651-6039; Li, Frederick W.
   B./0000-0002-4283-4228
FU National Natural Science Foundation of China [62272019]; EPSRC
   NortHFutures project [EP/X031012/1]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272019, and in part by EPSRC
   NortHFutures project under Grant EP/X031012/1.
CR Barquero G., 2022, P UND SOC BEH DYAD S, P107
   Burke M, 2016, J BIOMECH, V49, P1854, DOI 10.1016/j.jbiomech.2016.04.016
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Cai ZP, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3459992
   Centin M, 2018, IEEE T VIS COMPUT GR, V24, P2380, DOI 10.1109/TVCG.2017.2731771
   Chan A, 2008, IEEE T VIS COMPUT GR, V14, P146, DOI 10.1109/TVCG.2007.1056
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Chen K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459932
   Cui QJ, 2021, PROC CVPR IEEE, P4799, DOI 10.1109/CVPR46437.2021.00477
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   De Smedt Q., 2017, P 10 EUR WORKSH 3D O, P1
   Doshi K, 2022, IEEE COMPUT SOC CONF, P3888, DOI 10.1109/CVPRW56347.2022.00434
   Feng YF, 2014, INFORM SCIENCES, V277, P777, DOI 10.1016/j.ins.2014.03.013
   Ferles C, 2018, NEURAL NETWORKS, V105, P112, DOI 10.1016/j.neunet.2018.04.016
   Guo X, 2019, AAAI CONF ARTIF INTE, P2580
   Holden D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201302
   Holden Daniel, 2015, SIGGRAPH Asia 2015 Technical Briefs
   Jung H, 2021, IEEE T IMAGE PROCESS, V30, P8170, DOI 10.1109/TIP.2021.3113185
   Kim S. U., 2019, P SIGGRAPH AS POST, P1
   Kundu JN, 2019, AAAI CONF ARTIF INTE, P8553
   Leng ZY, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P226, DOI 10.1109/VR50410.2021.00044
   Li B, 2021, IEEE T IMAGE PROCESS, V30, P2562, DOI 10.1109/TIP.2020.3038362
   Li Lei, 2010, P 2010 ACM SIGGRAPH, P179, DOI [DOI 10.2312/SCA/SCA10/179-188, 10.2312/SCA/SCA10/179-188]
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li SL, 2020, IEEE INT CONF ROBOT, P993, DOI [10.1109/ICRA40945.2020.9197299, 10.1109/icra40945.2020.9197299]
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu SC, 2019, AAAI CONF ARTIF INTE, P9977
   Liu X, 2014, SIGNAL PROCESS, V105, P350, DOI 10.1016/j.sigpro.2014.06.009
   Liu ZG, 2021, AAAI CONF ARTIF INTE, V35, P2225
   Lou H, 2010, IEEE T VIS COMPUT GR, V16, P870, DOI 10.1109/TVCG.2010.23
   Luo RC, 2019, IEEE INT C INT ROBOT, P5958, DOI [10.1109/IROS40897.2019.8968192, 10.1109/iros40897.2019.8968192]
   Lyu K, 2022, NEUROCOMPUTING, V489, P345, DOI 10.1016/j.neucom.2022.02.045
   Majumdar A, 2019, IEEE T NEUR NET LEAR, V30, P312, DOI 10.1109/TNNLS.2018.2838679
   Mall U, 2017, Arxiv, DOI arXiv:1712.03380
   Men Q, 2021, IEEE T CIRC SYST VID, V31, P3417, DOI 10.1109/TCSVT.2020.3038145
   Kipf TN, 2017, Arxiv, DOI [arXiv:1609.02907, 10.48550/arXiv.1609.02907]
   Ng Evonne, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P11860, DOI 10.1109/CVPR46437.2021.01169
   Palmero C., 2022, P UND SOC BEH DYAD S, P4
   Park G, 2020, IEEE T VIS COMPUT GR, V26, P1891, DOI 10.1109/TVCG.2020.2973057
   Rehg J. M., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P35, DOI 10.1007/BFb0028333
   Ruiz AH, 2019, IEEE I CONF COMP VIS, P7133, DOI 10.1109/ICCV.2019.00723
   Shen JX, 2022, IEEE T VIS COMPUT GR, V28, P3618, DOI 10.1109/TVCG.2022.3203004
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shum Hubert P. H., 2011, International Journal of Virtual Reality, V10, P17
   Tang HY, 2020, RECSYS 2020: 14TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P269, DOI 10.1145/3383313.3412236
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Vaswani A., 2017, Proceedings of NeurIPS, Longbeach, P5998
   Wang H, 2021, IEEE T VIS COMPUT GR, V27, P216, DOI 10.1109/TVCG.2019.2936810
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Ye Q, 2018, LECT NOTES COMPUT SC, V11214, P817, DOI 10.1007/978-3-030-01249-6_49
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhong CY, 2022, PROC CVPR IEEE, P6437, DOI 10.1109/CVPR52688.2022.00634
   Zhou KL, 2023, INT SYM MIX AUGMENT, P167, DOI 10.1109/ISMAR59233.2023.00031
   Zhou KL, 2023, IEEE T VIS COMPUT GR, V29, P2456, DOI 10.1109/TVCG.2023.3247092
   Zhou KL, 2021, INT SYM MIX AUGMENT, P41, DOI 10.1109/ISMAR52148.2021.00018
   Zhou KL, 2020, OPT LASER TECHNOL, V121, DOI 10.1016/j.optlastec.2019.105769
NR 60
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6754
EP 6769
DI 10.1109/TVCG.2023.3337868
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600004
PM 38032781
OA Green Accepted
DA 2024-11-06
ER

PT J
AU Li, G
   Wang, JP
   Wang, Y
   Shan, GH
   Zhao, Y
AF Li, Guan
   Wang, Junpeng
   Wang, Yang
   Shan, Guihua
   Zhao, Ying
TI An In-Situ Visual Analytics Framework for Deep Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep learning model; gaussian mixture model; in-situ; visual analytics;
   Deep learning model; gaussian mixture model; in-situ; visual analytics
ID VISUALIZATION; INFORMATION; LIKELIHOOD
AB The past decade has witnessed the superior power of deep neural networks (DNNs) in applications across various domains. However, training a high-quality DNN remains a non-trivial task due to its massive number of parameters. Visualization has shown great potential in addressing this situation, as evidenced by numerous recent visualization works that aid in DNN training and interpretation. These works commonly employ a strategy of logging training-related data and conducting post-hoc analysis. Based on the results of offline analysis, the model can be further trained or fine-tuned. This strategy, however, does not cope with the increasing complexity of DNNs, because (1) the time-series data collected over the training are usually too large to be stored entirely; (2) the huge I/O overhead significantly impacts the training efficiency; (3) post-hoc analysis does not allow rapid human-interventions (e.g., stop training with improper hyper-parameter settings to save computational resources). To address these challenges, we propose an in-situ visualization and analysis framework for the training of DNNs. Specifically, we employ feature extraction algorithms to reduce the size of training-related data in-situ and use the reduced data for real-time visual analytics. The states of model training are disclosed to model designers in real-time, enabling human interventions on demand to steer the training. Through concrete case studies, we demonstrate how our in-situ framework helps deep learning experts optimize DNNs and improve their analysis efficiency.
C1 [Li, Guan; Wang, Yang; Shan, Guihua] Chinese Acad Sci, Comp Network Informat Ctr, Beijing 100045, Peoples R China.
   [Wang, Junpeng] Visa Res, Palo Alto, CA 94306 USA.
   [Wang, Yang; Shan, Guihua] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Zhao, Ying] Cent South Univ, Changsha 410017, Hunan, Peoples R China.
C3 Chinese Academy of Sciences; Computer Network Information Center, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Central South University
RP Shan, GH (corresponding author), Chinese Acad Sci, Comp Network Informat Ctr, Beijing 100045, Peoples R China.
EM liguan@sccas.cn; junpeng.wang.nk@gmail.com; wangyang@sccas.cn;
   sgh@sccas.cn; zhaoying@csu.edu.cn
RI Zhao, Liangyu/IAO-7294-2023
OI Wang, Junpeng/0000-0002-1130-9914; Li, Guan/0000-0001-6436-3650; Shan,
   Guihua/0000-0002-8283-2278
FU Strategic Priority Research Program of Chinese Academy of Sciences
   [XDB0500103]; National Natural Science Foundation of China [62202446]
FX This work was supported in part by the Strategic Priority Research
   Program of Chinese Academy of Sciences under Grant XDB0500103, and in
   part by the National Natural Science Foundation of China under Grant
   62202446.
CR Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440
   Achille Alessandro, 2017, arXiv
   Ahrens J., 2005, Vis. Handb., P717, DOI 10.1016/B978-012387582-2/50038-1
   Alsallakh B, 2018, IEEE T VIS COMPUT GR, V24, P152, DOI 10.1109/TVCG.2017.2744683
   [Anonymous], 2014, P INT C HIGH PERF CO
   Binotto APD, 2003, PVG 2003 PROCEEDINGS, P69, DOI 10.1109/PVGS.2003.1249044
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Dutta S, 2018, IEEE PAC VIS SYMP, P66, DOI 10.1109/PacificVis.2018.00017
   Dutta S, 2017, IEEE T VIS COMPUT GR, V23, P811, DOI 10.1109/TVCG.2016.2598604
   Fabian N., 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P105, DOI 10.1109/LDAV.2012.6378983
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo HQ, 2017, IEEE PAC VIS SYMP, P71, DOI 10.1109/PACIFICVIS.2017.8031581
   Hainaut A, 2020, IVAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 3: IVAPP, P231, DOI 10.5220/0008989702310239
   Hazarika S, 2019, IEEE T VIS COMPUT GR, V25, P1214, DOI 10.1109/TVCG.2018.2864801
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He WB, 2020, IEEE PAC VIS SYMP, P36, DOI 10.1109/PacificVis48177.2020.7127
   He WB, 2020, IEEE T VIS COMPUT GR, V26, P23, DOI 10.1109/TVCG.2019.2934312
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Hohman F, 2020, IEEE T VIS COMPUT GR, V26, P1096, DOI 10.1109/TVCG.2019.2934659
   Hong S, 2021, IEEE T VIS COMPUT GR, V27, P3670, DOI 10.1109/TVCG.2020.2990894
   Kahng M, 2019, IEEE T VIS COMPUT GR, V25, P310, DOI 10.1109/TVCG.2018.2864500
   Kahng M, 2018, IEEE T VIS COMPUT GR, V24, P88, DOI 10.1109/TVCG.2017.2744718
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   La Rosa B, 2023, COMPUT GRAPH FORUM, V42, P319, DOI 10.1111/cgf.14733
   Li G, 2021, IEEE T VIS COMPUT GR, V27, P1364, DOI 10.1109/TVCG.2020.3030461
   Li G, 2020, IEEE PAC VIS SYMP, P171, DOI 10.1109/PacificVis48177.2020.1186
   Li M., 2020, Distill
   Liu DY, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3200489
   Liu MC, 2018, IEEE T VIS COMPUT GR, V24, P77, DOI 10.1109/TVCG.2017.2744938
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma KL, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.120
   Pezzotti N, 2018, IEEE T VIS COMPUT GR, V24, P98, DOI 10.1109/TVCG.2017.2744358
   Posada D, 2004, SYST BIOL, V53, P793, DOI 10.1080/10635150490522304
   Pühringer M, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P291, DOI 10.1109/VIS47514.2020.00065
   Rauber PE, 2017, IEEE T VIS COMPUT GR, V23, P101, DOI 10.1109/TVCG.2016.2598838
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shusen Liu, 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P73, DOI 10.1109/LDAV.2012.6378978
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wang JP, 2023, Arxiv, DOI arXiv:2307.07712
   Wang JP, 2020, IEEE PAC VIS SYMP, P51, DOI 10.1109/PacificVis48177.2020.3542
   Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P288, DOI 10.1109/TVCG.2018.2864504
   Wang JP, 2018, IEEE T VIS COMPUT GR, V24, P1905, DOI 10.1109/TVCG.2018.2816223
   Wang KC, 2019, IEEE PAC VIS SYMP, P303, DOI 10.1109/PacificVis.2019.00043
   Wang KC, 2018, IEEE PAC VIS SYMP, P26, DOI 10.1109/PacificVis.2018.00013
   Wang KC, 2017, IEEE PAC VIS SYMP, P161, DOI 10.1109/PACIFICVIS.2017.8031590
   Wang QW, 2020, IEEE T VIS COMPUT GR, V26, P3340, DOI 10.1109/TVCG.2019.2921323
   Wang ZJ, 2021, IEEE T VIS COMPUT GR, V27, P1396, DOI 10.1109/TVCG.2020.3030418
   Wongsuphasawat K, 2018, IEEE T VIS COMPUT GR, V24, P1, DOI 10.1109/TVCG.2017.2744878
   Woodring J, 2011, COMPUT GRAPH FORUM, V30, P1151, DOI 10.1111/j.1467-8659.2011.01964.x
   Woodring J, 2016, IEEE T VIS COMPUT GR, V22, P857, DOI 10.1109/TVCG.2015.2467411
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Zeyen M., 2017, Proceedings of the, P12
   Zoph B, 2017, Arxiv, DOI arXiv:1611.01578
NR 56
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6770
EP 6786
DI 10.1109/TVCG.2023.3339585
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600021
PM 38051629
DA 2024-11-06
ER

PT J
AU Stokes, C
   Bearfield, CX
   Hearst, MA
AF Stokes, Chase
   Bearfield, Cindy Xiong
   Hearst, Marti A.
TI The Role of Text in Visualizations: How Annotations Shape Perceptions of
   Bias and Influence Predictions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Annotation; judgment; perceived bias; prediction; text; visualization;
   Annotation; judgment; perceived bias; prediction; text; visualization
AB This paper investigates the role of text in visualizations, specifically the impact of text position, semantic content, and biased wording. Two empirical studies were conducted based on two tasks (predicting data trends and appraising bias) using two visualization types (bar and line charts). While the addition of text had a minimal effect on how people perceive data trends, there was a significant impact on how biased they perceive the authors to be. This finding revealed a relationship between the degree of bias in textual information and the perception of the authors' bias. Exploratory analyses support an interaction between a person's prediction and the degree of bias they perceived. This paper also develops a crowdsourced method for creating chart annotations that range from neutral to highly biased. This research highlights the need for designers to mitigate potential polarization of readers' opinions based on how authors' ideas are expressed.
C1 [Stokes, Chase; Hearst, Marti A.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Bearfield, Cindy Xiong] Georgia Inst Technol, Atlanta, GA 30332 USA.
C3 University of California System; University of California Berkeley;
   University System of Georgia; Georgia Institute of Technology
RP Stokes, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM cstokes@ischool.berkeley.edu; cxiong@u.northwestern.edu;
   hearst@berkeley.edu
OI Hearst, Marti/0000-0002-4346-1603; Xiong Bearfield,
   Cindy/0000-0002-1451-4083; Stokes, Chase/0000-0001-7644-9021
FU Allen Institute for AI; EPIC Lab; NSF [IIS-2237585, IIS-2311575];
   National Science Foundation [DGE 2146752]
FX This work was supported in part by a gift from the Allen Institute for
   AI, in part by EPIC Lab, in part by NSF under Grants IIS-2237585 and
   IIS-2311575, and in part by National Science Foundation Graduate
   Research Fellowship Program under Grant DGE 2146752.
CR Ajani K, 2022, IEEE T VIS COMPUT GR, V28, P3351, DOI 10.1109/TVCG.2021.3068337
   Allen J, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502040
   AMABILE TM, 1982, J PERS SOC PSYCHOL, V43, P997, DOI 10.1037/0022-3514.43.5.997
   Baer J, 2004, CREATIVITY RES J, V16, P113, DOI 10.1207/s15326934crj1601_11
   Borkin MA, 2016, IEEE T VIS COMPUT GR, V22, P519, DOI 10.1109/TVCG.2015.2467732
   Boyd D, 2012, INFORM COMMUN SOC, V15, P662, DOI 10.1080/1369118X.2012.678878
   Brath Richard, 2020, Visualizing with Text
   Burns A, 2024, IEEE T VIS COMPUT GR, V30, P3427, DOI 10.1109/TVCG.2022.3231716
   Burns A, 2020, 2020 IEEE WORKSHOP ON EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES TO VISUALIZATION (BELIV 2020), P19, DOI 10.1109/BELIV51497.2020.00010
   Calero Valdez A., 2018, Cognitive Biases inVisualizations, P27
   Cohen J., 1998, Statistical power analysis for the behavioral sciences
   Correll M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300418
   Eberhard K, 2023, MANAG REV Q, V73, P167, DOI 10.1007/s11301-021-00235-8
   Ericson JD, 2022, BIG DATA SOC, V9, DOI 10.1177/20539517221080678
   Fan A., 2022, P CHI C HUM FACT COM, P12
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Gutwin C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P208, DOI 10.1145/3025453.3025984
   Healey CG, 2012, IEEE T VIS COMPUT GR, V18, P1170, DOI 10.1109/TVCG.2011.127
   Holder E., IEEE Trans. Vis. Comput. Graph., DOI [10.1109/TVCG.2023.3326512, DOI 10.1109/TVCG.2023.3326512]
   Kim D. H., 2021, P CHI C HUM FACT COM, P11
   Kim YS, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1375, DOI 10.1145/3025453.3025592
   Knaflic C. N., 2017, So what? storytelling with data blog
   Knaflic CN, 2015, STORYTELLING WITH DATA: A DATA VISUALIZATION GUIDE FOR BUSINESS PROFESSIONALS, P1, DOI 10.1002/9781119055259
   Kong HK, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300576
   Kong HK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174012
   Lauer C, 2023, IEEE T PROF COMMUN, V66, P220, DOI 10.1109/TPC.2023.3290948
   Lauer C, 2020, IEEE T PROF COMMUN, V63, P327, DOI 10.1109/TPC.2020.3032053
   Lundgard A, 2022, IEEE T VIS COMPUT GR, V28, P1073, DOI 10.1109/TVCG.2021.3114770
   Luo Y, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.01541
   Ottley A., 2019, P 2019 EUR IEEE VGTC, DOI DOI 10.2312/EVS.20191181
   Padilla LM, 2018, 2018 IEEE EVALUATION AND BEYOND - METHODOLOGICAL APPROACHES FOR VISUALIZATION (BELIV), P69, DOI 10.1109/BELIV.2018.8634267
   Palan S, 2018, J BEHAV EXP FINANC, V17, P22, DOI 10.1016/j.jbef.2017.12.004
   Pandey AV, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1469, DOI 10.1145/2702123.2702608
   Pinker S., 1990, Artificial Intelligence andthe Future of Testing, P126
   Pronin E, 2004, PSYCHOL REV, V111, P781, DOI 10.1037/0033-295X.111.3.781
   Setlur V., 2022, FUNCTIONAL AESTHETIC
   Srinivasan A, 2019, IEEE T VIS COMPUT GR, V25, P672, DOI 10.1109/TVCG.2018.2865145
   Stokes C., 2022, P WORKSH EXPL OPP CH, P7
   Stokes C., 2023, IEEE Trans. Vis. Comput. Graphics, V29
   Taber CS, 2006, AM J POLIT SCI, V50, P755, DOI 10.1111/j.1540-5907.2006.00214.x
   TAJFEL H, 1971, EUR J SOC PSYCHOL, V1, P149, DOI 10.1002/ejsp.2420010202
   Wall E, 2022, IEEE T VIS COMPUT GR, V28, P966, DOI 10.1109/TVCG.2021.3114862
   Wall E, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P111, DOI [10.1109/visual.2019.8933611, 10.1109/VISUAL.2019.8933611]
   WOOD D, 1993, SCI AM, V268, P88, DOI 10.1038/scientificamerican0593-88
   Xiong C., 2018, J. Vis, V18, P1327
   Xiong Cindy, 2023, IEEE Trans Vis Comput Graph, V29, P493, DOI 10.1109/TVCG.2022.3209405
   Xiong C, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501874
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P3051, DOI 10.1109/TVCG.2019.2917689
   Yang LN, 2023, IEEE T VIS COMPUT GR, V29, P1638, DOI 10.1109/TVCG.2021.3128157
   Zhu H. H., 2022, P WORKSH VIS COMM, P1
NR 50
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6787
EP 6800
DI 10.1109/TVCG.2023.3338451
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600013
PM 38039168
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Boorboor, S
   Castellana, MS
   Kim, Y
   Zhu-tian, C
   Beyer, J
   Pfister, H
   Kaufman, AE
AF Boorboor, Saeed
   Castellana, Matthew S.
   Kim, Yoonsang
   Zhu-tian, Chen
   Beyer, Johanna
   Pfister, Hanspeter
   Kaufman, Arie E.
TI VoxAR: Adaptive Visualization of Volume Rendered Objects in Optical
   See-Through Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptive visualization; augmented reality; situated visualization;
   volume rendering; Adaptive visualization; augmented reality; situated
   visualization; volume rendering
ID COLOR; COMPENSATION
AB We present VoxAR, a method to facilitate an effective visualization of volume-rendered objects in optical see-through head-mounted displays (OST-HMDs). The potential of augmented reality (AR) to integrate digital information into the physical world provides new opportunities for visualizing and interpreting scientific data. However, a limitation of OST-HMD technology is that rendered pixels of a virtual object can interfere with the colors of the real-world, making it challenging to perceive the augmented virtual information accurately. We address this challenge in a two-step approach. First, VoxAR determines an appropriate placement of the volume-rendered object in the real-world scene by evaluating a set of spatial and environmental objectives, managed as user-selected preferences and pre-defined constraints. We achieve a real-time solution by implementing the objectives using a GPU shader language. Next, VoxAR adjusts the colors of the input transfer function (TF) based on the real-world placement region. Specifically, we introduce a novel optimization method that adjusts the TF colors such that the resulting volume-rendered pixels are discernible against the background and the TF maintains the perceptual mapping between the colors and data intensity values. Finally, we present an assessment of our approach through objective evaluations and subjective user studies.
C1 [Boorboor, Saeed; Castellana, Matthew S.; Kim, Yoonsang; Kaufman, Arie E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Zhu-tian, Chen; Beyer, Johanna; Pfister, Hanspeter] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   Harvard University
RP Boorboor, S (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM sboorboor@cs.stonybrook.edu; matcastellan@cs.stonybrook.edu;
   yoonsakim@cs.stonybrook.edu; ztchen@umn.edu; jbeyer@seas.harvard.edu;
   pfister@g.harvard.edu; ari@cs.stonybrook.edu
RI Boorboor, Saeed/ABI-7739-2020
OI Boorboor, Saeed/0000-0001-6644-5983; Pfister,
   Hanspeter/0000-0002-3620-2582; Kim, Yoonsang/0009-0006-2341-3862; Beyer,
   Johanna/0000-0002-3505-9171
FU NSF [IIS2107224, IIS2107328, IIS1901030]; NCS [FO2124179, OAC1919752,
   ICER1940302]; IBM-SUNY [2106]
FX This work was supported in part by NSF underGrants IIS2107224,
   IIS2107328, IIS1901030, NCS-FO2124179, OAC1919752,and ICER1940302, and
   in part by IBM-SUNY under grant 2106.
CR [Anonymous], 2019, ISO/CIE 11664-4:2019
   [Anonymous], 2023, UNITY TECHNOLOGIES
   Belo J, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545651
   Cakmakci O, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P16, DOI 10.1109/ISMAR.2004.2
   Cheng Y., 2021, P ACM S US INT SOFTW, P297
   Chu YK, 2018, BIOMED OPT EXPRESS, V9, P5205, DOI 10.1364/BOE.9.005205
   Erick AO, 2020, 2020 INTERNATIONAL SAUPEC/ROBMECH/PRASA CONFERENCE, P190, DOI 10.1109/saupec/robmech/prasa48453.2020.9041002
   Fairchild M. D., 2013, COLOR APPEARANCE MOD, V3rd
   Fleck P, 2023, IEEE T VIS COMPUT GR, V29, P3281, DOI 10.1109/TVCG.2022.3157058
   Fukiage T, 2014, INT SYM MIX AUGMENT, P63, DOI 10.1109/ISMAR.2014.6948410
   Gabbard JL, 2010, P IEEE VIRT REAL ANN, P79, DOI 10.1109/VR.2010.5444808
   Gal R, 2014, INT SYM MIX AUGMENT, P207, DOI 10.1109/ISMAR.2014.6948429
   Gramazio CC, 2017, IEEE T VIS COMPUT GR, V23, P521, DOI 10.1109/TVCG.2016.2598918
   Hansen N, 2006, STUD FUZZ SOFT COMP, V192, P75
   Hincapié-Ramos JD, 2015, IEEE T VIS COMPUT GR, V21, P1336, DOI 10.1109/TVCG.2015.2450745
   Itoh Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3453157
   Itoh Y, 2015, IEEE T VIS COMPUT GR, V21, P1269, DOI 10.1109/TVCG.2015.2459892
   Kitware, HOLOGRAPHIC REMOTING
   Kiyokawa K, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P60, DOI 10.1109/ISAR.2000.880924
   Kutter O, 2008, AUGMENTED ENV MEDICA
   Langlotz T, 2016, IEEE T VIS COMPUT GR, V22, P2385, DOI 10.1109/TVCG.2016.2593781
   learn.microsoft, APP QUALITY CRITERIA
   Lee B, 2021, IEEE T VIS COMPUT GR, V27, P1171, DOI 10.1109/TVCG.2020.3030450
   Lee KH, 2018, OPT ENG, V57, DOI 10.1117/1.OE.57.12.123104
   Lindlbauer D, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P147, DOI 10.1145/3332165.3347945
   Ljung P, 2016, COMPUT GRAPH FORUM, V35, P669, DOI 10.1111/cgf.12934
   Luo MR, 2001, COLOR RES APPL, V26, P340, DOI 10.1002/col.1049
   Luo WZ, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501946
   Magic Leap Inc, 2023, MAGIC LEAP 2
   MAHNY M, 1994, COLOR RES APPL, V19, P105
   Marino E, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11093915
   Marriott K., 2018, IMMERSIVE ANAL, V11190, DOI 10/kt9x
   Mathur M, 2023, FINITE ELEM ANAL DES, V213, DOI 10.1016/j.finel.2022.103851
   Microsoft, HOLOGRAPHIC REMOTING
   Microsoft, 2022, MICROSOFTMIXED REALI
   Queisner M, 2022, SURG INNOV, V29, P406, DOI 10.1177/15533506211054240
   Ruofei Du, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P829, DOI 10.1145/3379337.3415881
   Snider J.G., 1969, SEMANTIC DIFFERENTIA
   Stone M., 2014, P COL IM C SOC, P253
   Szafir D.A., 2014, Color Imaging Conf., V22, P228, DOI [10.2352/CIC.2014.22.1.art00040, DOI 10.2352/CIC.2014.22.1.ART00040]
   Unity Technologies, 2023, ADV WORKFLOWS AR DEV
   Weiland C, 2009, LECT NOTES COMPUT SC, V5615, P603, DOI 10.1007/978-3-642-02710-9_67
   Zhang YJ, 2022, IEEE T VIS COMPUT GR, V28, P4490, DOI 10.1109/TVCG.2021.3091686
NR 43
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6801
EP 6812
DI 10.1109/TVCG.2023.3340770
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600015
PM 38096098
DA 2024-11-06
ER

PT J
AU Han, J
   Zheng, H
   Bi, CK
AF Han, Jun
   Zheng, Hao
   Bi, Chongke
TI KD-INR: Time-Varying Volumetric Data Compression via Knowledge
   Distillation-Based Implicit Neural Representation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time-varying data compression; implicit neural representation; knowledge
   distillation; volume visualization; Time-varying data compression;
   implicit neural representation; knowledge distillation; volume
   visualization
ID MULTILEVEL TECHNIQUES; SUPERRESOLUTION; REDUCTION
AB Traditional deep learning algorithms assume that all data is available during training, which presents challenges when handling large-scale time-varying data. To address this issue, we propose a data reduction pipeline called knowledge distillation-based implicit neural representation (KD-INR) for compressing large-scale time-varying data. The approach consists of two stages: spatial compression and model aggregation. In the first stage, each time step is compressed using an implicit neural representation with bottleneck layers and features of interest preservation-based sampling. In the second stage, we utilize an offline knowledge distillation algorithm to extract knowledge from the trained models and aggregate it into a single model. We evaluated our approach on a variety of time-varying volumetric data sets. Both quantitative and qualitative results, such as PSNR, LPIPS, and rendered images, demonstrate that KD-INR surpasses the state-of-the-art approaches, including learning-based (i.e., CoordNet, NeurComp, and SIREN) and lossy compression (i.e., SZ3, ZFP, and TTHRESH) methods, at various compression ratios ranging from hundreds to ten thousand.
C1 [Han, Jun] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China.
   [Han, Jun] Hong Kong Univ Sci & Technol, Hong Kong 999077, Peoples R China.
   [Zheng, Hao] Univ Notre Dame, Notre Dame, IN 46556 USA.
   [Zheng, Hao] Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
   [Bi, Chongke] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Hong Kong University of
   Science & Technology; University of Notre Dame; University of Louisiana
   Lafayette; Tianjin University
RP Han, J (corresponding author), Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China.; Han, J (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong 999077, Peoples R China.
EM hanjun@cuhk.edu.cn; haozheng9428@gmail.com; bichongke@tju.edu.cn
RI zheng, hao/HHM-6949-2022
OI Han, Jun/0000-0002-7286-062X; Zheng, Hao/0000-0002-9790-7607
FU Chinese University of Hong Kong, Shenzhen [UDF01002679]; Shenzhen
   Science and Technology Program [ZDSYS20211021111415025]; National
   Natural Science Foundation of China [62302422, 62172294]
FX This research was supported in part by the start-up fund UDF01002679 of
   the Chinese University of Hong Kong, Shenzhen, Shenzhen Science and
   Technology Program under Grant ZDSYS20211021111415025,and in part by the
   National Natural Science Foundation of China under Grant 62302422 and
   under Grant 62172294. Recommended for acceptance by C.Garth
CR Ainsworth M, 2019, SIAM J SCI COMPUT, V41, pA1278, DOI 10.1137/18M1166651
   Ainsworth M, 2018, COMPUT VIS SCI, V19, P65, DOI 10.1007/s00791-018-00303-9
   Ballester-Ripoll R, 2020, IEEE T VIS COMPUT GR, V26, P2891, DOI 10.1109/TVCG.2019.2904063
   Barrow H.G., 1977, P 5 INT JOINT C ART, P659
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   Gobbetti E, 2012, COMPUT GRAPH FORUM, V31, P1315, DOI 10.1111/j.1467-8659.2012.03124.x
   Han J, 2023, IEEE T VIS COMPUT GR, V29, P4951, DOI 10.1109/TVCG.2022.3197203
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P2445, DOI 10.1109/TVCG.2020.3032123
   Han J, 2022, IEEE T VIS COMPUT GR, V28, P270, DOI 10.1109/TVCG.2021.3114815
   Han J, 2021, IEEE T VIS COMPUT GR, V27, P1290, DOI 10.1109/TVCG.2020.3030346
   Han J, 2020, IEEE T VIS COMPUT GR, V26, P205, DOI 10.1109/TVCG.2019.2934255
   Han Song, 2016, ICLR
   He J., 2020, P IEEE C COMP VIS PA, P13935
   He JP, 2021, IEEE INT CONF COMP V, P2337, DOI 10.1109/ICCVW54120.2021.00265
   Hinton G., 2015, ARXIV
   Hoang D, 2021, IEEE T VIS COMPUT GR, V27, P603, DOI 10.1109/TVCG.2020.3030381
   Jiao CY, 2023, J VISUAL-JAPAN, V26, P649, DOI 10.1007/s12650-022-00891-2
   Kingma D., 2015, P INT C LEARN RREP
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai F, 2023, PROCEEDINGS OF THE 20TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, NSDI 2023, P769
   Li MY, 2020, PROC CVPR IEEE, P5283, DOI 10.1109/CVPR42600.2020.00533
   Liang X, 2023, IEEE T BIG DATA, V9, P485, DOI 10.1109/TBDATA.2022.3201176
   Liang X, 2018, IEEE INT CONF BIG DA, P438, DOI 10.1109/BigData.2018.8622520
   Lindstrom P, 2014, IEEE T VIS COMPUT GR, V20, P2674, DOI 10.1109/TVCG.2014.2346458
   Lu Y, 2021, COMPUT GRAPH FORUM, V40, P135, DOI 10.1111/cgf.14295
   Luo P, 2016, AAAI CONF ARTIF INTE, P3560
   Martel JNP, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459785
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Paszke A, 2019, ADV NEUR IN, V32
   Peebles W., 2022, arXiv
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Sahoo S, 2022, COMPUT GRAPH FORUM, V41, P391, DOI 10.1111/cgf.14549
   Saragadam V, 2022, LECT NOTES COMPUT SC, V13683, P318, DOI 10.1007/978-3-031-20050-2_19
   Shi N, 2022, IEEE T VIS COMPUT GR, V28, P2301, DOI 10.1109/TVCG.2022.3165345
   Sitzmann V., 2020, Advances in Neural Information Processing Systems, V33, P7462
   Soler M, 2018, IEEE PAC VIS SYMP, P46, DOI 10.1109/PacificVis.2018.00015
   Tao J, 2019, IEEE T VIS COMPUT GR, V25, P1236, DOI 10.1109/TVCG.2018.2864808
   Wang CL, 2008, IEEE T VIS COMPUT GR, V14, P1547, DOI 10.1109/TVCG.2008.140
   Whalen D, 2008, ASTROPHYS J, V673, P664, DOI 10.1086/524400
   Woodring J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P417, DOI 10.1109/VISUAL.2003.1250402
   Wurster SW, 2023, IEEE T VIS COMPUT GR, V29, P5483, DOI 10.1109/TVCG.2022.3214420
   Yang CG, 2022, PROC CVPR IEEE, P12309, DOI 10.1109/CVPR52688.2022.01200
   Yao Q., 2018, arXiv
   Zhang L., 2021, P INT C LEARN REPR
   Zhang LF, 2022, PROC CVPR IEEE, P12454, DOI 10.1109/CVPR52688.2022.01214
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao K, 2021, PROC INT CONF DATA, P1643, DOI 10.1109/ICDE51399.2021.00145
NR 48
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6826
EP 6838
DI 10.1109/TVCG.2945
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600010
PM 38127599
DA 2024-11-06
ER

PT J
AU Tesema, KW
   Hill, L
   Jones, MW
   Ahmad, MI
   Tam, GKL
AF Tesema, Keneni W.
   Hill, Lyndon
   Jones, Mark W.
   Ahmad, Muneeb I.
   Tam, Gary K. L.
TI Point Cloud Completion: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud completion; deep learning; computer vision; Point cloud
   completion; deep learning; computer vision
ID 3D SHAPE; SCENE COMPLETION; RECONSTRUCTION; SEGMENTATION; NETWORKS
AB Point cloud completion is the task of producing a complete 3D shape given an input of a partial point cloud. It has become a vital process in 3D computer graphics, vision and applications such as autonomous driving, robotics, and augmented reality. These applications often rely on the presence of a complete 3D representation of the environment. Over the past few years, many completion algorithms have been proposed and a substantial amount of research has been carried out. However, there are not many in-depth surveys that summarise the research progress in such a way that allows users to make an informed choice of what algorithms to employ given the type of data they have, the end result they want, the challenges they may face and the possible strategies they could use. In this study, we present a comprehensive survey and classification of articles on point cloud completion untill August 2023 based on the strategies, techniques, inputs, outputs, and network architectures. We will also cover datasets, evaluation methods, and application areas in point cloud completion. Finally, we discuss challenges faced by the research community and future research directions.
C1 [Tesema, Keneni W.; Jones, Mark W.; Ahmad, Muneeb I.; Tam, Gary K. L.] Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, Wales.
   [Hill, Lyndon] Vaarst Swansea, Bristol BS1 6BX, England.
C3 Swansea University
RP Tesema, KW (corresponding author), Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, Wales.
EM 2127952@swansea.ac.uk; lyndon.hill@vaarst.com; M.W.Jones@swansea.ac.uk;
   m.i.ahmad@swansea.ac.uk; k.l.tam@swansea.ac.uk
RI Jones, Mark W./F-1114-2015; Tam, Gary KL/E-5098-2011
OI Jones, Mark W./0000-0001-8991-1190; Tesema, Keneni
   Worku/0009-0003-1247-2435; Tam, Gary KL/0000-0001-7387-5180
FU EPSRC [EP/S021892/1]; Vaarst; Royal Society fund [IEC/NSFC/211159]
FX This work was supported by EPSRC under Grant EP/S021892/1 and Vaarst
   (vaarst.com). The work of Gary K.L. Tam was supported by a Royal Society
   fund under Grant IEC/NSFC/211159.
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Agrawal S, 2019, IEEE WINT CONF APPL, P1099, DOI 10.1109/WACV.2019.00122
   Aiello E, 2022, Arxiv, DOI arXiv:2209.09552
   Alexiou E, 2017, IEEE INT WORKSH MULT
   Alliegro A, 2021, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR46437.2021.00460
   Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   Phan AV, 2018, NEURAL NETWORKS, V108, P533, DOI 10.1016/j.neunet.2018.09.001
   Arora H., 2021, arXiv
   Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272
   Bellekens B., 2015, Int.J. Adv. Intell. Syst., V8, P118
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Brilakis I, 2011, AUTOMAT CONSTR, V20, P884, DOI 10.1016/j.autcon.2011.03.005
   Burguera AB, 2019, J MAR SCI ENG, V7, DOI 10.3390/jmse7080278
   Cai YJ, 2022, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR52688.2022.00546
   Cai ZP, 2015, PROCEEDINGS 2015 SECOND IEEE INTERNATIONAL CONFERENCE ON SPATIAL DATA MINING AND GEOGRAPHICAL KNOWLEDGE SERVICES (ICSDM 2015), P157, DOI 10.1109/ICSDM.2015.7298044
   Carrilho A., 2018, ISPRS J. Photogrammetry Remote Sens., V42, P92
   Caruso C, 1998, COMPUT MATH APPL, V35, P109, DOI 10.1016/S0898-1221(98)00101-1
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Chang YK, 2021, NEUROCOMPUTING, V460, P266, DOI 10.1016/j.neucom.2021.06.080
   Chauve AL, 2010, PROC CVPR IEEE, P1261, DOI 10.1109/CVPR.2010.5539824
   Chen X., 2019, arXiv
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen ZK, 2023, PROC CVPR IEEE, P13581, DOI 10.1109/CVPR52729.2023.01305
   Cheng R., 2021, P C ROB LEARN, P2161
   Cheng XH, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P618
   Cheng YC, 2023, PROC CVPR IEEE, P4456, DOI 10.1109/CVPR52729.2023.00433
   Choe J., 2021, arXiv
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Cui R., arXiv
   Cui RK, 2023, Arxiv, DOI arXiv:2307.14726
   Cui YD, 2022, IEEE T INTELL TRANSP, V23, P722, DOI 10.1109/TITS.2020.3023541
   Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Demir I, 2015, IEEE I CONF COMP VIS, P2147, DOI 10.1109/ICCV.2015.248
   Dinesh C, 2018, Arxiv, DOI arXiv:1804.10831
   Doria D., 2012, P IEEE COMP SOC C CO, P65
   Fan ZX, 2022, Arxiv, DOI arXiv:2204.09186
   Fei B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109051
   Fei B, 2022, IEEE T INTELL TRANSP, V23, P22862, DOI 10.1109/TITS.2022.3195555
   Flach PA, 2015, ADV NEUR IN, V28
   Friedman S., 2012, P IEEE COMP SOC C CO, P1
   Gama F, 2020, IEEE SIGNAL PROC MAG, V37, P128, DOI 10.1109/MSP.2020.3016143
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gong BC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12468, DOI 10.1109/ICCV48922.2021.01226
   Gong JY, 2022, LECT NOTES COMPUT SC, V13662, P517, DOI 10.1007/978-3-031-20086-1_30
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guo Q., 2021, arXiv, DOI DOI 10.3389/FNINS.2013.12345
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Hassani K, 2019, IEEE I CONF COMP VIS, P8159, DOI 10.1109/ICCV.2019.00825
   Hernández C, 2010, STUD COMPUT INTELL, V285, P281
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Hong SM, 2023, PROC CVPR IEEE, P9435, DOI 10.1109/CVPR52729.2023.00910
   Hosseinimanesh G., 2023, P MED IM 2023 PHYS M, VVolume 12463, P802
   Hou J, 2020, PROC CVPR IEEE, P2095, DOI 10.1109/CVPR42600.2020.00217
   Hu F, 2022, COMPUT GRAPH FORUM, V41, P153, DOI 10.1111/cgf.14665
   Hu T, 2020, AAAI CONF ARTIF INTE, V34, P10997
   Huang HH, 2019, CHIN CONT DECIS CONF, P4604, DOI [10.1109/ccdc.2019.8832956, 10.1109/CCDC.2019.8832956]
   Huang Z., 2020, P IEEE CVF C COMP VI, P7670
   Ibrahim Y, 2023, IMAGE VISION COMPUT, V134, DOI 10.1016/j.imavis.2023.104675
   Ibrahim Y, 2022, INT C PATT RECOG, P2121, DOI 10.1109/ICPR56361.2022.9956459
   Ichimaru K, 2019, IEEE WINT CONF APPL, P1543, DOI 10.1109/WACV.2019.00169
   Jaklic A, 2015, J ARCHAEOL SCI, V62, P143, DOI 10.1016/j.jas.2015.08.007
   Jiang JC, 2023, Arxiv, DOI arXiv:2207.01545
   Jiayuan Gu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P283, DOI 10.1007/978-3-030-58558-7_17
   Jones MW, 1996, COMPUT GRAPH FORUM, V15, P311, DOI 10.1111/1467-8659.1550311
   Kasten Y, 2023, Arxiv, DOI [arXiv:2306.10533, DOI 10.48550/ARXIV.2306.10533]
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan Michael., 2004, COMPUT GRAPH FORUM, P115
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Kimia B. B., 2003, Int. J. Comput. Vis., V54, P182
   Kroemer O, 2012, IEEE-RAS INT C HUMAN, P680, DOI 10.1109/HUMANOIDS.2012.6651593
   Lang I, 2020, PROC CVPR IEEE, P7575, DOI 10.1109/CVPR42600.2020.00760
   Li JX, 2023, ENTROPY-SWITZ, V25, DOI 10.3390/e25071018
   Li J, 2022, Arxiv, DOI arXiv:2205.14999
   Li R.-W., 2021, arXiv
   Li SS, 2023, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR52729.2023.00913
   Li X, 2023, IEEE T VIS COMPUT GR, V29, P3251, DOI 10.1109/TVCG.2022.3157061
   Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573
   Li YH, 2023, Arxiv, DOI arXiv:2303.10406
   Li ZY, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108684
   Lin T., 2022, AI Open, V3, P132
   Litany O, 2017, Arxiv, DOI arXiv:1612.04956
   Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596
   Liu Q, 2023, IEEE T VIS COMPUT GR, V29, P3642, DOI 10.1109/TVCG.2022.3167151
   Long J., 2023, P IEEE INT C AC SPEE, P1
   Lou MZ, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.947690
   Lyu Zhaoyang, 2021, arXiv
   Ma C., 2022, P ADV NEUR INF PROC, P3568
   Ma CF, 2023, PROC CVPR IEEE, P13560, DOI 10.1109/CVPR52729.2023.01303
   Ma CF, 2022, Arxiv, DOI arXiv:2203.09772
   Ma XH, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22197346
   Mao AH, 2023, IEEE T VIS COMPUT GR, V29, P4964, DOI 10.1109/TVCG.2022.3196334
   Mendoza A, 2020, Arxiv, DOI arXiv:2010.04278
   Minár J, 2020, EARTH-SCI REV, V211, DOI 10.1016/j.earscirev.2020.103414
   Mirbauer M, 2022, IEEE T PATTERN ANAL, V44, P8635, DOI 10.1109/TPAMI.2021.3102676
   Mittal H., 2021, arXiv
   Mohammadi SS, 2023, IEEE INT CONF ROBOT, P3815, DOI 10.1109/ICRA48891.2023.10160350
   Nguyen T., 2023, P IEEE INT C AC SPEE, P1
   Nguyen T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10458, DOI 10.1109/ICCV48922.2021.01031
   Nie Y., 2020, P 34 INT C NEUR INF, P16130
   Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842
   Pauly M., 2005, P EUR S GEOM PROC, P32
   Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642
   Peng YJ, 2020, IEEE ACCESS, V8, P30969, DOI 10.1109/ACCESS.2020.2973003
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qian GC, 2021, PROC CVPR IEEE, P11678, DOI 10.1109/CVPR46437.2021.01151
   Quispe AH, 2015, IEEE INT CONF ROBOT, P3702, DOI 10.1109/ICRA.2015.7139713
   Ren Y., 2022, P IEEE INT C MULT EX, P6
   Roldao L, 2022, INT J COMPUT VISION, V130, P1978, DOI 10.1007/s11263-021-01504-5
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Rumezhak T, 2021, IEEE INT CONF COMP V, P2542, DOI 10.1109/ICCVW54120.2021.00287
   Rundi Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P281, DOI 10.1007/978-3-030-58548-8_17
   Sarkar K, 2017, INT CONF 3D VISION, P383, DOI 10.1109/3DV.2017.00051
   Saroha A, 2022, Arxiv, DOI arXiv:2204.10060
   Savva Manolis, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P24, DOI 10.1109/CVPRW.2015.7301289
   Schnabel R, 2009, COMPUT GRAPH FORUM, V28, P503, DOI 10.1111/j.1467-8659.2009.01389.x
   Selvaraju P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10377, DOI 10.1109/ICCV48922.2021.01023
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Shen CH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366199
   Shi JQ, 2023, Arxiv, DOI arXiv:2303.01804
   Shi JQ, 2022, IEEE ROBOT AUTOM LET, V7, P4165, DOI 10.1109/LRA.2022.3146585
   Shi JQ, 2021, IEEE ROBOT AUTOM LET, V6, P7081, DOI 10.1109/LRA.2021.3097081
   Shi PC, 2023, Arxiv, DOI arXiv:2305.09132
   Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396
   Singh P, 2022, Arxiv, DOI arXiv:2110.03170
   Siris Avishek, 2021, P IEEE CVF INT C COM, P4156
   Soans N, 2020, IEEE INT CONF ROBOT, P2153, DOI [10.1109/icra40945.2020.9197393, 10.1109/ICRA40945.2020.9197393]
   Sokolova M, 2006, LECT NOTES COMPUT SC, V4304, P1015
   Son H., 2020, P AS C COMP VIS, P174
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Spurek P, 2022, IEEE INT C INT ROBOT, P6848, DOI 10.1109/IROS47612.2022.9981829
   Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tang JS, 2022, Arxiv, DOI arXiv:2209.01733
   Tang JS, 2022, PROC CVPR IEEE, P1716, DOI 10.1109/CVPR52688.2022.00177
   Tang Yujin, 2021, Advances in Neural Information Processing Systems
   Tay Y, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3530811
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Theis L, 2016, Arxiv, DOI arXiv:1511.01844
   Toscano JD, 2023, J COMPUT INF SCI ENG, V23, DOI 10.1115/1.4056566
   Tretschk Edgar, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P293, DOI 10.1007/978-3-030-58517-4_18
   Tsai D, 2023, IEEE INT CONF ROBOT, P9346, DOI 10.1109/ICRA48891.2023.10160707
   Varley J, 2017, IEEE INT C INT ROBOT, P2442, DOI 10.1109/IROS.2017.8206060
   Wang HC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9762, DOI 10.1109/ICCV48922.2021.00964
   Wang J., 2022, arXiv
   Wang P, 2023, IEEE ROBOT AUTOM LET, V8, P1935, DOI 10.1109/LRA.2023.3244412
   Wang PS, 2020, IEEE COMPUT SOC CONF, P1074, DOI 10.1109/CVPRW50498.2020.00141
   Wang Q, 2020, ARCH COMPUT METHOD E, V27, P479, DOI 10.1007/s11831-019-09320-4
   Wang XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13169, DOI 10.1109/ICCV48922.2021.01294
   Wang XG, 2022, IEEE T PATTERN ANAL, V44, P8139, DOI 10.1109/TPAMI.2021.3108410
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wang YD, 2022, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR52688.2022.00162
   Wang YD, 2022, INT J COMPUT VISION, V130, P1145, DOI 10.1007/s11263-022-01588-7
   Wei J., 2022, AS C COMP VIS, P4310
   Wen CL, 2019, ISPRS J PHOTOGRAMM, V147, P178, DOI 10.1016/j.isprsjprs.2018.10.007
   Wen X, 2023, IEEE T PATTERN ANAL, V45, P852, DOI 10.1109/TPAMI.2022.3159003
   Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736
   Wen X, 2021, PROC CVPR IEEE, P13075, DOI 10.1109/CVPR46437.2021.01288
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Williams SB, 2016, SPRINGER TRAC ADV RO, V113, P45, DOI 10.1007/978-3-319-27702-8_4
   Wu LM, 2023, PROC CVPR IEEE, P9445, DOI 10.1109/CVPR52729.2023.00911
   Wu LT, 2023, Arxiv, DOI arXiv:2212.00564
   Wu T, 2021, ADV NEUR IN, V34
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xi L, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109476
   Xia Y, 2021, ISPRS J PHOTOGRAMM, V174, P166, DOI 10.1016/j.isprsjprs.2021.01.027
   Xia YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1938, DOI 10.1145/3474085.3475348
   Xia ZY, 2023, PROC CVPR IEEE, P17642, DOI 10.1109/CVPR52729.2023.01692
   Xiang M, 2023, APPL INTELL, V53, P14971, DOI 10.1007/s10489-022-04219-3
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Xiao HH, 2023, IEEE T CIRC SYST VID, V33, P5160, DOI 10.1109/TCSVT.2023.3250970
   Xie H., 2020, P EUR C COMP VIS, P381
   Xu DL, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12061296
   Xu MY, 2023, IEEE T PATTERN ANAL, V45, P9583, DOI 10.1109/TPAMI.2023.3257026
   Yan X, 2021, AAAI CONF ARTIF INTE, V35, P3101
   Yan XJ, 2022, LECT NOTES COMPUT SC, V13662, P676, DOI 10.1007/978-3-031-20086-1_39
   Yang XM, 2021, IEEE INT C INT ROBOT, P3555, DOI 10.1109/IROS51168.2021.9636662
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yida Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P70, DOI 10.1007/978-3-030-58580-8_5
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang BW, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555389
   Zhang JM, 2023, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR52729.2023.00515
   Zhang JM, 2021, IEEE ROBOT AUTOM LET, V6, P596, DOI 10.1109/LRA.2020.3048658
   Zhang JZ, 2021, PROC CVPR IEEE, P1768, DOI 10.1109/CVPR46437.2021.00181
   Zhang KY, 2022, AAAI CONF ARTIF INTE, P3291
   Zhang KY, 2023, Arxiv, DOI arXiv:2202.02669
   Zhang SP, 2023, INT J COMPUT VISION, V131, P2425, DOI 10.1007/s11263-023-01820-y
   Zhang SL, 2021, AAAI CONF ARTIF INTE, V35, P3385
   Zhang W., 2020, P EUR C COMP VIS, P528
   Zhang WX, 2023, IEEE T VIS COMPUT GR, V29, P4229, DOI 10.1109/TVCG.2022.3185247
   Zhang WX, 2020, COMPUT AIDED GEOM D, V82, DOI 10.1016/j.cagd.2020.101925
   Zhang XC, 2021, PROC CVPR IEEE, P15885, DOI 10.1109/CVPR46437.2021.01563
   Zhang Y, 2022, ALGORITHMS, V15, DOI 10.3390/a15040124
   Zhang Y, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071514
   Zhao X, 2022, IEEE T VIS COMPUT GR, V28, P4940, DOI 10.1109/TVCG.2021.3109392
   Zheng YC, 2022, IEEE T INTELL TRANSP, V23, P22312, DOI 10.1109/TITS.2022.3153133
   Zhong G., 2016, J FINANCE DATA SCI, V2, P265, DOI [10.1016/j.jfds.2017.05.001, DOI 10.1016/J.JFDS.2017.05.001]
   Zhou HR, 2022, LECT NOTES COMPUT SC, V13663, P416, DOI 10.1007/978-3-031-20062-5_24
   Zhou J, 2023, IEEE IJCNN, DOI 10.1109/IJCNN54540.2023.10191240
   Zhou LQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5806, DOI 10.1109/ICCV48922.2021.00577
   Zhu LP, 2021, NEUROCOMPUTING, V461, P1, DOI 10.1016/j.neucom.2021.07.035
   Zhu Z, 2023, IEEE I CONF COMP VIS, P14462, DOI 10.1109/ICCV51070.2023.01334
   Zhu Z, 2024, IEEE T VIS COMPUT GR, V30, P3545, DOI 10.1109/TVCG.2023.3236061
   Zong DM, 2021, AAAI CONF ARTIF INTE, V35, P3625
NR 212
TC 1
Z9 1
U1 14
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6880
EP 6899
DI 10.1109/TVCG.2023.3344935
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600008
PM 38117617
OA hybrid, Green Accepted
DA 2024-11-06
ER

PT J
AU Li, TX
   Shi, R
   Zhu, Q
   Kanai, T
AF Li, Tianxing
   Shi, Rui
   Zhu, Qing
   Kanai, Takashi
TI SwinGar: Spectrum-Inspired Neural Dynamic Deformation for Free-Swinging
   Garments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clothing deformation; dynamics; graph learning; spectral analysis;
   Clothing deformation; dynamics; graph learning; spectral analysis
AB Our work presents a novel spectrum-inspired learning-based approach for generating clothing deformations with dynamic effects and personalized details. Existing methods in the field of clothing animation are limited to either static behavior or specific network models for individual garments, which hinders their applicability in real-world scenarios where diverse animated garments are required. Our proposed method overcomes these limitations by providing a unified framework that predicts dynamic behavior for different garments with arbitrary topology and looseness, resulting in versatile and realistic deformations. First, we observe that the problem of bias towards low frequency always hampers supervised learning and leads to overly smooth deformations. To address this issue, we introduce a frequency-control strategy from a spectral perspective that enhances the generation of high-frequency details of the deformation. In addition, to make the network highly generalizable and able to learn various clothing deformations effectively, we propose a spectral descriptor to achieve a generalized description of the global shape information. Building on the above strategies, we develop a dynamic clothing deformation estimator that integrates graph attention mechanisms with long short-term memory. The estimator takes as input expressive features from garments and human bodies, allowing it to automatically output continuous deformations for diverse clothing types, independent of mesh topology or vertex count. Finally, we present a neural collision handling method to further enhance the realism of garments. Our experimental results demonstrate the effectiveness of our approach on a variety of free-swinging garments and its superiority over state-of-the-art methods.
C1 [Li, Tianxing; Shi, Rui; Zhu, Qing] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Kanai, Takashi] Univ Tokyo, Dept Gen Syst Studies, Tokyo 1538902, Japan.
C3 Beijing University of Technology; University of Tokyo
RP Shi, R (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM litianxing1992@gmail.com; ruishi@bjut.edu.cn; ccgszq@bjut.edu.cn;
   kanait@acm.org
RI Shi, Rui/KUD-2835-2024; Li, Tianxing/AAL-7163-2020
OI Kanai, Takashi/0000-0002-1635-3818; Li, Tianxing/0000-0002-2489-4884
FU Beijing Natural Science Foundation [4232017]; JSPS KAKENHI [22K12331]
FX This work was supported in part by Beijing Natural Science Foundation
   under Grant 4232017 and in part by JSPS KAKENHI under Grant 22K12331.
CR Atzmon M., 2019, Controlling Neural Level Sets
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bertiche Hugo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P344, DOI 10.1007/978-3-030-58565-5_21
   Bertiche H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555491
   Bertiche H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5451, DOI 10.1109/ICCV48922.2021.00542
   Bertiche H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480479
   Carnegie-Mellon, 2010, "CMU graphics lab motion capture database
   Casado-Elvira A, 2022, COMPUT GRAPH FORUM, V41, P293, DOI 10.1111/cgf.14644
   Chen Z, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3462758
   Dasoulas G, 2021, PR MACH LEARN RES, V139
   De Luigi L, 2023, PROC CVPR IEEE, P1451, DOI 10.1109/CVPR52729.2023.00146
   Deng B., 2020, P COMP VIS ECCV 2020, P612, DOI [10.1007/978-3-030-58571-636, DOI 10.1007/978-3-030-58571-636]
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Grigorev A, 2023, PROC CVPR IEEE, P16965, DOI 10.1109/CVPR52729.2023.01627
   Gropp A., 2020, P MACH LEARN SYST 20, P3569
   Gundogdu E, 2022, IEEE T PATTERN ANAL, V44, P181, DOI 10.1109/TPAMI.2020.3010886
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Jiang CFF, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073623
   Katznelson Y., 2004, An Introduction to Harmonic Analysis
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Li C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417763
   Li J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201308
   Li MC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459767
   Li PZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459852
   Li T, 2023, COMPUT GRAPH FORUM, V42, P231, DOI 10.1111/cgf.14651
   Li TX, 2021, COMPUT GRAPH FORUM, V40, P537, DOI 10.1111/cgf.142653
   Li TX, 2020, I3D 2020: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, DOI 10.1145/3384382.3384525
   Li YD, 2022, COMPUT GRAPH FORUM, V41, P547, DOI 10.1111/cgf.14493
   Liang JB, 2019, ADV NEUR IN, V32
   Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322969
   Liu TT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508406
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ly M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392396
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Marvelous designer, About us
   Mihajlovic M, 2021, PROC CVPR IEEE, P10456, DOI 10.1109/CVPR46437.2021.01032
   MMC16 MACKLIN MILES, 2016, P 9 INT C MOT GAM MI, P49, DOI [DOI 10.1145/2994258.29942722,3, 10.1145/2994258.2994272, DOI 10.1145/2994258.2994272]
   Mu JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12981, DOI 10.1109/ICCV48922.2021.01276
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Pan Xiaoyu, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530709
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Patel C, 2020, PROC CVPR IEEE, P7363, DOI 10.1109/CVPR42600.2020.00739
   Rodriguez-Pardo C, 2023, COMPUT GRAPH FORUM, V42, P149, DOI 10.1111/cgf.14750
   Santesteban I, 2022, ADV NEUR IN
   Santesteban I, 2022, PROC CVPR IEEE, P8130, DOI 10.1109/CVPR52688.2022.00797
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Santesteban I, 2019, COMPUT GRAPH FORUM, V38, P355, DOI 10.1111/cgf.13643
   Shi ZL, 2022, INT J COMPUT VISION, V130, P885, DOI 10.1007/s11263-021-01572-7
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Tan QY, 2022, LECT NOTES COMPUT SC, V13663, P451, DOI 10.1007/978-3-031-20062-5_26
   Tang M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275005
   Tiwari G., 2020, P EUR C COMP VIS, P18, DOI [10.1007/978--3-030-58580-8_1, DOI 10.1007/978--3-030-58580-8_1]
   Velickovic P., 2017, ARXIV
   Vidaurre R, 2020, COMPUT GRAPH FORUM, V39, P145, DOI 10.1111/cgf.14109
   Wang HM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459787
   Wang TY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356512
   Weil J., 1986, Computer Graphics, V20, P49, DOI 10.1145/15886.15891
   Xie YH, 2022, COMPUT GRAPH FORUM, V41, P641, DOI 10.1111/cgf.14505
   Xu Z, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392379
   Zhang M., 2022, ACM Trans. Graph., V41, P1, DOI DOI 10.1145/3550454.3555485
   Zhang M, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480497
   Zhang M, 2021, COMPUT GRAPH FORUM, V40, P399, DOI 10.1111/cgf.142642
NR 63
TC 1
Z9 1
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6913
EP 6927
DI 10.1109/TVCG.2023.3346055
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600009
PM 38133986
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Oral, B
   Dragicevic, P
   Telea, A
   Dimara, E
AF Oral, Basak
   Dragicevic, Pierre
   Telea, Alexandru
   Dimara, Evanthia
TI Decoupling Judgment and Decision Making: A Tale of Two Tails
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cognition; decision making; judgment; psychology; visualization;
   Cognition; decision making; judgment; psychology; visualization
ID CONFIDENCE-INTERVALS; GRAPHICAL DISPLAYS; VISUALIZATION; INFORMATION;
   COMMUNICATION; DESIGN
AB Is it true that if citizens understand hurricane probabilities, they will make more rational decisions for evacuation? Finding answers to such questions is not straightforward in the literature because the terms "judgment" and "decision making" are often used interchangeably. This terminology conflation leads to a lack of clarity on whether people make suboptimal decisions because of inaccurate judgments of information conveyed in visualizations or because they use alternative yet currently unknown heuristics. To decouple judgment from decision making, we review relevant concepts from the literature and present two preregistered experiments (N = 601) to investigate if the task (judgment versus decision making), the scenario (sports versus humanitarian), and the visualization (quantile dotplots, density plots, probability bars) affect accuracy. While experiment 1 was inconclusive, we found evidence for a difference in experiment 2. Contrary to our expectations and previous research, which found decisions less accurate than their direct-equivalent judgments, our results pointed in the opposite direction. Our findings further revealed that decisions were less vulnerable to status-quo bias, suggesting decision makers may disfavor responses associated with inaction. We also found that both scenario and visualization types can influence people's judgments and decisions. Although effect sizes are not large and results should be interpreted carefully, we conclude that judgments cannot be safely used as proxy tasks for decision making, and discuss implications for visualization research and beyond. Materials and preregistrations are available at https://osf.io/ufzp5/?view_only=adc0f78a23804c31bf7fdd9385cb264f.
C1 [Oral, Basak; Telea, Alexandru; Dimara, Evanthia] Univ Utrecht, NL-3584 CS Utrecht, Netherlands.
   [Dragicevic, Pierre] Inria Bordeaux, F-33405 Talence, France.
C3 Utrecht University
RP Oral, B (corresponding author), Univ Utrecht, NL-3584 CS Utrecht, Netherlands.
EM e.oral@uu.nl; pierre.dragice@gmail.com; a.c.telea@uu.nl;
   evanthia.dimara@gmail.com
RI Dragicevic, Pierre/HKV-4981-2023
OI Telea, Alexandru Cristian/0000-0003-0750-0502; Dimara,
   Evanthia/0000-0001-5212-7888
FU Department of Information and Computing Sciences at Utrecht University
FX This work was supported by the Department of Information and Computing
   Sciences at Utrecht University.
CR A. P. Association, 2015, APA dictionary of psychology, V2nd
   Berger JO., 2013, STAT DECISION THEORY
   Bertin J., 1983, Semiology of Graph.: Diagrams, Networks, Maps
   Besançon L, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3310432
   Borkin MA, 2011, IEEE T VIS COMPUT GR, V17, P2479, DOI 10.1109/TVCG.2011.192
   Castro SC, 2022, IEEE T VIS COMPUT GR, V28, P411, DOI 10.1109/TVCG.2021.3114803
   Chatzimparmpas A, 2020, COMPUT GRAPH FORUM, V39, P713, DOI 10.1111/cgf.14034
   Cheng HF, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300789
   Cheong L, 2016, INT J GEOGR INF SCI, V30, P1377, DOI 10.1080/13658816.2015.1131829
   Chua HF, 2006, MEM COGNITION, V34, P399, DOI 10.3758/BF03193417
   Cibulski L, 2022, IEEE COMPUT GRAPH, V42, P21, DOI 10.1109/MCG.2022.3156846
   Cokely ET, 2012, JUDGM DECIS MAK, V7, P25
   Cumming G, 2005, AM PSYCHOL, V60, P170, DOI 10.1037/0003-066X.60.2.170
   Cumming G, 2014, PSYCHOL SCI, V25, P7, DOI 10.1177/0956797613504966
   Decock L, 2014, NOUS, V48, P653, DOI 10.1111/nous.12003
   Diederich A., 2012, Judgment and Decision Making, P683
   Dimara E, 2022, IEEE T VIS COMPUT GR, V28, P1128, DOI 10.1109/TVCG.2021.3114813
   Dimara E, 2022, IEEE T VIS COMPUT GR, V28, P4101, DOI 10.1109/TVCG.2021.3074023
   Dimara E, 2020, IEEE T VIS COMPUT GR, V26, P1413, DOI 10.1109/TVCG.2018.2872577
   Dimara E, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5475, DOI 10.1145/3025453.3025870
   Dimara E, 2018, IEEE T VIS COMPUT GR, V24, P749, DOI 10.1109/TVCG.2017.2745138
   Dragicevic P, 2016, HUM-COMPUT INT-SPRIN, P291, DOI 10.1007/978-3-319-26633-6_13
   Dy B, 2022, IEEE T VIS COMPUT GR, V28, P3405, DOI 10.1109/TVCG.2021.3065126
   Eberhard K, 2023, MANAG REV Q, V73, P167, DOI 10.1007/s11301-021-00235-8
   Elliott MA, 2021, IEEE T VIS COMPUT GR, V27, P1117, DOI 10.1109/TVCG.2020.3029413
   Fernandes M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173718
   Ferstl F, 2017, IEEE T VIS COMPUT GR, V23, P831, DOI 10.1109/TVCG.2016.2598868
   Fischhoff B, 2020, ANNU REV PSYCHOL, V71, P331, DOI 10.1146/annurev-psych-010419-050747
   Fleming SM, 2010, P NATL ACAD SCI USA, V107, P6005, DOI 10.1073/pnas.0910380107
   Galesic M, 2009, HEALTH PSYCHOL, V28, P210, DOI 10.1037/a0014474
   Gelman A, 2006, AM STAT, V60, P328, DOI 10.1198/000313006X152649
   Gescheider G. A., 2013, Psychophysics: The fundamentals
   Gigerenzer G, 2011, ANNU REV PSYCHOL, V62, P451, DOI 10.1146/annurev-psych-120709-145346
   Gold N, 2015, J ECON PSYCHOL, V47, P50, DOI 10.1016/j.joep.2015.01.001
   Gramazio CC, 2017, IEEE T VIS COMPUT GR, V23, P521, DOI 10.1109/TVCG.2016.2598918
   Hampton JA, 2006, PSYCHOL LEARN MOTIV, V46, P79, DOI 10.1016/S0079-7421(06)46003-5
   Haroz S, 2012, IEEE T VIS COMPUT GR, V18, P2402, DOI 10.1109/TVCG.2012.233
   Harrison L, 2014, IEEE T VIS COMPUT GR, V20, P1943, DOI 10.1109/TVCG.2014.2346979
   Hindalong Emily, 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3512896
   Holten D, 2006, IEEE T VIS COMPUT GR, V12, P741, DOI 10.1109/TVCG.2006.147
   IEEE Visualization, 2022, Analytics and decisions area model
   Jardine N, 2020, IEEE T VIS COMPUT GR, V26, P1012, DOI 10.1109/TVCG.2019.2934786
   Kahneman D, 2003, AM PSYCHOL, V58, P697, DOI 10.1037/0003-066X.58.9.697
   KAHNEMAN D, 1991, PSYCHOL SCI, V2, P142, DOI 10.1111/j.1467-9280.1991.tb00121.x
   Kale A, 2021, IEEE T VIS COMPUT GR, V27, P272, DOI 10.1109/TVCG.2020.3030335
   Kale A, 2019, IEEE T VIS COMPUT GR, V25, P892, DOI 10.1109/TVCG.2018.2864909
   KARLSSON G, 1988, ACTA PSYCHOL, V68, P7, DOI 10.1016/0001-6918(88)90042-X
   Kayongo P, 2022, IEEE T VIS COMPUT GR, V28, P465, DOI 10.1109/TVCG.2021.3114842
   Kirby KN, 2013, BEHAV RES METHODS, V45, P905, DOI 10.3758/s13428-013-0330-5
   Krzywinski M., 2013, Nature Methods, V10, P923
   Liston DB, 2013, J VISION, V13, DOI 10.1167/13.8.1
   Lufityanto G, 2016, PSYCHOL SCI, V27, P622, DOI 10.1177/0956797616629403
   Lynn SK, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00952
   Mittelstädt S, 2014, COMPUT GRAPH FORUM, V33, P231, DOI 10.1111/cgf.12379
   Morais Luiz, 2021, P 2021 CHI C HUM FAC, DOI DOI 10.1145/3411764.3445637
   Munzner T., 2014, Visualization analysis and design, DOI 10.1201/b17511
   Narechania A, 2022, IEEE T VIS COMPUT GR, V28, P1009, DOI 10.1109/TVCG.2021.3114827
   Nieuwenhuis S, 2011, NAT NEUROSCI, V14, P1105, DOI 10.1038/nn.2886
   Oral E, 2024, IEEE T VIS COMPUT GR, V30, P359, DOI 10.1109/TVCG.2023.3326593
   Padilla LM, 2018, COGN RES, V3, DOI 10.1186/s41235-018-0120-9
   Patil Ameya, 2023, IEEE Trans Vis Comput Graph, V29, P407, DOI 10.1109/TVCG.2022.3209426
   Pirolli P, 2005, SENSEMAKING PROCESS
   Pletti C, 2017, BRIT J PSYCHOL, V108, P351, DOI 10.1111/bjop.12205
   Rauber PE, 2018, INFORM VISUAL, V17, P282, DOI 10.1177/1473871617713337
   Savani K., 2015, Culture and Judgment and Decision Making, V2, P456
   Savikhin A, 2008, IEEE CONF VIS ANAL, P107, DOI 10.1109/VAST.2008.4677363
   Schwarz J, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1245
   Sedig K., 2012, JMPT, V3, P12
   Simon H. A., 1960, The New Science of Management Decision
   Song H, 2019, IEEE T VIS COMPUT GR, V25, P914, DOI 10.1109/TVCG.2018.2864914
   Sperrle F, 2021, COMPUT GRAPH FORUM, V40, P543, DOI 10.1111/cgf.14329
   Stone ER, 2003, ORGAN BEHAV HUM DEC, V90, P19, DOI 10.1016/S0749-5978(03)00003-7
   Stone ER, 1997, J EXP PSYCHOL-APPL, V3, P243, DOI 10.1037/1076-898X.3.4.243
   Talbot J, 2012, IEEE T VIS COMPUT GR, V18, P2613, DOI 10.1109/TVCG.2012.196
   Telea AlexandruC., 2014, DATA VISUALIZATION P
   Van Norman ER, 2013, J BEHAV EDUC, V22, P283, DOI 10.1007/s10864-013-9176-2
   Veras R, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300771
   Visschers VHM, 2009, RISK ANAL, V29, P267, DOI 10.1111/j.1539-6924.2008.01137.x
   Ward M.O., 2015, INTERACTIVE DATA VIS
   Willems N, 2009, COMPUT GRAPH FORUM, V28, P959, DOI 10.1111/j.1467-8659.2009.01440.x
   Wispinski NJ, 2020, ANN NY ACAD SCI, V1464, P30, DOI 10.1111/nyas.13973
   Wu CM, 2017, J EXP PSYCHOL LEARN, V43, P1274, DOI 10.1037/xlm0000374
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P853, DOI 10.1109/TVCG.2019.2934399
   Xiong C, 2020, IEEE T VIS COMPUT GR, V26, P301, DOI 10.1109/TVCG.2019.2934400
   Zhang H, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00001
   Zhang YF, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2211, DOI 10.1145/2702123.2702239
   Zhao J, 2018, IEEE T VIS COMPUT GR, V24, P340, DOI 10.1109/TVCG.2017.2745279
NR 87
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6928
EP 6940
DI 10.1109/TVCG.2023.3346640
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600018
PM 38145516
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Ferrarotti, A
   Baldoni, S
   Carli, M
   Battisti, F
AF Ferrarotti, Anna
   Baldoni, Sara
   Carli, Marco
   Battisti, Federica
TI Stress Assessment for Augmented Reality Applications Based on Head
   Movement Features
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; stress detection; machine learning classifier;
   Augmented reality; stress detection; machine learning classifier
ID INTERFERENCE
AB Augmented reality is one of the enabling technologies of the upcoming future. Its usage in working and learning scenarios may lead to a better quality of work and training by helping the operators during the most crucial stages of processes. Therefore, the automatic detection of stress during augmented reality experiences can be a valuable support to prevent consequences on people's health and foster the spreading of this technology. In this work, we present the design of a non-invasive stress assessment approach. The proposed system is based on the analysis of the head movements of people wearing a Head Mounted Display while performing stress-inducing tasks. First, we designed a subjective experiment consisting of two stress-related tests for data acquisition. Then, a statistical analysis of head movements has been performed to determine which features are representative of the presence of stress. Finally, a stress classifier based on a combination of Support Vector Machines has been designed and trained. The proposed approach achieved promising performances thus paving the way for further studies in this research direction.
C1 [Ferrarotti, Anna; Carli, Marco] Roma Tre Univ, Dept Ind Elect & Mech Engn, I-00146 Rome, Italy.
   [Baldoni, Sara; Battisti, Federica] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
C3 Roma Tre University; University of Padua
RP Baldoni, S (corresponding author), Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
EM anna.ferrarotti@uniroma3.it; sara.baldoni@unipd.it;
   marco.carli@uniroma3.it; federica.battisti@unipd.it
RI Baldoni, Sara/AAL-4299-2020; Ferrarotti, Anna/KUD-2869-2024; Carli,
   Marco/B-7111-2013
OI Battisti, Federica/0000-0002-0846-5879; Baldoni,
   Sara/0000-0001-5642-3430; Carli, Marco/0000-0002-7489-3767; Ferrarotti,
   Anna/0009-0003-5021-3266
FU European Union under the Italian National Recovery and Resilience Plan
   (NRRP) of NextGenerationEU [PE0000001]
FX This work was supported in part by the European Union under the Italian
   National Recovery and Resilience Plan (NRRP) of NextGenerationEU,
   partnership on "Telecommunications of the Future" (PE0000001 - program
   "RESTART")
CR Akmandor AO, 2017, IEEE T MULTI-SCALE C, V3, P269, DOI 10.1109/TMSCS.2017.2703613
   Allen AP, 2017, NEUROBIOL STRESS, V6, P113, DOI 10.1016/j.ynstr.2016.11.001
   Andrade Tiago, 2019, 2019 5th Experiment@ International Conference (exp.at'19). Proceedings, P107, DOI 10.1109/EXPAT.2019.8876559
   Arushi, 2021, IEEE CONF COMPU INTE, P994, DOI 10.1109/COG52621.2021.9618989
   Bibbo D, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9040601
   Bibbo D, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030455
   Blackledge J.M., 2006, DIGIT SIGNAL PROCESS
   Bu N, 2021, 2021 IEEE 3RD GLOBAL CONFERENCE ON LIFE SCIENCES AND TECHNOLOGIES (IEEE LIFETECH 2021), P408, DOI [10.1109/LifeTech52111.2021.9391853, 10.1109/LIFETECH52111.2021.9391853]
   Castaldo R, 2019, BMC MED INFORM DECIS, V19, DOI 10.1186/s12911-019-0742-y
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Ehrhart M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22165969
   Games P. A., 1976, J EDUCATIONAL STATIS, V1, P113, DOI [DOI 10.3102/10769986001002113, DOI 10.2307/1164979]
   Gedam S, 2021, IEEE ACCESS, V9, P84045, DOI 10.1109/ACCESS.2021.3085502
   Giannakakis G, 2022, IEEE T AFFECT COMPUT, V13, P440, DOI 10.1109/TAFFC.2019.2927337
   Giannakakis G, 2020, IEEE INT CONF AUTOMA, P728, DOI 10.1109/FG47880.2020.00129
   Giannakakis G, 2018, IEEE INT CONF AUTOMA, P710, DOI 10.1109/FG.2018.00112
   Grier R. A., 2015, P HUM FACT ERG SOC A, P1731
   HART S G, 1988, P139
   He DB, 2019, IEEE T HUM-MACH SYST, V49, P362, DOI 10.1109/THMS.2019.2917194
   He JY, 2019, IEEE ACCESS, V7, P42710, DOI 10.1109/ACCESS.2019.2907076
   Heard J, 2018, IEEE T HUM-MACH SYST, V48, P434, DOI 10.1109/THMS.2017.2782483
   Hung SW, 2021, TECHNOL SOC, V67, DOI 10.1016/j.techsoc.2021.101757
   Hussain MS, 2014, INTERACT COMPUT, V26, P256, DOI 10.1093/iwc/iwt032
   Ishihara S., 1918, American Journal of Ophthalmology, V1, P376, DOI [DOI 10.5555/URI:PII:S0002939418901303, 10.1016/s0002-9394, DOI 10.1016/S0002-9394, 10.1016/S0002-9394(18)90663-X, DOI 10.1016/S0002-9394(18)90663-X]
   ITU-R, 2019, BT.50014: Methodologies for the subjective assessment of the quality of television images
   Jiao Y, 2023, BIOMED SIGNAL PROCES, V79, DOI 10.1016/j.bspc.2022.104145
   Jun G, 2016, IEEE SYS MAN CYBERN, P3270, DOI 10.1109/SMC.2016.7844738
   Kaakinen JK, 2018, J EXP PSYCHOL LEARN, V44, P1671, DOI 10.1037/xlm0000539
   Kaplan AD, 2021, HUM FACTORS, V63, P706, DOI 10.1177/0018720820904229
   Karthikeyan P, 2014, ARAB J SCI ENG, V39, P1835, DOI 10.1007/s13369-013-0786-8
   Karthikeyan P., 2011, Proceedings of the 2011 IEEE International Conference on Control System, Computing and Engineering (ICCSCE), P258, DOI 10.1109/ICCSCE.2011.6190533
   Katmah R, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21155043
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kern Florian, 2023, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2023.3247098
   Kolmogorov A., 1992, On the Empirical Determination of a Distribution Function, P106, DOI [DOI 10.1007/978-1-4612-4380-910, DOI 10.1007/978-1-4612-4380-9]
   Kumar G. S., 2022, P INT C ADV SMART SE, P6
   Kusano H, 2020, PR INT CONF DATA SC, P488, DOI 10.1109/DSAA49011.2020.00063
   Lamotte G, 2021, CLIN AUTON RES, V31, P153, DOI 10.1007/s10286-021-00796-4
   Lang PJ., 1997, NIMH Center for the Study of Emotion and Attention, V1, P3
   Lee DS, 2017, IEEE SENS J, V17, P194, DOI 10.1109/JSEN.2016.2625323
   Lee J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21072381
   Lee MH, 2004, P ANN INT IEEE EMBS, V26, P2364
   Marques B, 2022, INT J INTERACT DES M, V16, P419, DOI 10.1007/s12008-021-00798-6
   Miles S, 2021, BEHAV RES METHODS, V53, P2083, DOI 10.3758/s13428-021-01551-3
   Mohammadi A, 2022, IEEE SENS J, V22, P8216, DOI 10.1109/JSEN.2022.3157795
   Moser MK, 2023, IEEE SENS J, V23, P22845, DOI 10.1109/JSEN.2023.3304422
   Nolan S.A., 2011, Statistics for the Behavioral Sciences
   O'Connor DB, 2021, ANNU REV PSYCHOL, V72, P663, DOI 10.1146/annurev-psych-062520-122331
   Panagi L, 2019, ANN BEHAV MED, V53, P309, DOI 10.1093/abm/kay039
   Pepa L, 2021, IEEE T CONSUM ELECTR, V67, P12, DOI 10.1109/TCE.2020.3045228
   Pillai P, 2022, IEEE-ASME T MECH, V27, P2134, DOI 10.1109/TMECH.2022.3175774
   Radu I, 2023, IEEE T VIS COMPUT GR, V29, P3734, DOI 10.1109/TVCG.2022.3169980
   Reipschlager P, 2021, IEEE T VIS COMPUT GR, V27, P1182, DOI 10.1109/TVCG.2020.3030460
   Renaud P, 1997, INT J PSYCHOPHYSIOL, V27, P87, DOI 10.1016/S0167-8760(97)00049-4
   Scarpina F, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00557
   Sharma LD, 2022, EXPERT SYST APPL, V197, DOI 10.1016/j.eswa.2022.116634
   Shilton AL, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00567
   Shon D, 2018, INT J ENV RES PUB HE, V15, DOI 10.3390/ijerph15112461
   Snellen H., 1873, Probebuchstaben zur Bestimmung der Sehscharfe: 1
   Sriramprakash S., 2017, Procedia Comput. Sci., V115, P366
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Sun F. T., 2012, MOBILE COMPUTING APP, P282, DOI [DOI 10.1007/978-3-642-29336-8_12, https://doi.org/10.1007/978-3-642-29336-8_12, DOI 10.1007/978-3-642-29336-8]
   Tombaugh TN, 2006, ARCH CLIN NEUROPSYCH, V21, P53, DOI 10.1016/j.acn.2005.07.006
   Velozo JD, 2021, PSYCHONEUROENDOCRINO, V128, DOI 10.1016/j.psyneuen.2021.105217
   WELCH BL, 1951, BIOMETRIKA, V38, P330, DOI 10.2307/2332579
   Yousefi MS, 2022, IEEE ACCESS, V10, P118941, DOI 10.1109/ACCESS.2022.3221179
   Zubair M, 2020, BIOMED SIGNAL PROCES, V57, DOI 10.1016/j.bspc.2019.101736
NR 67
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6970
EP 6983
DI 10.1109/TVCG.2024.3385637
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600020
PM 38578850
OA hybrid
DA 2024-11-06
ER

PT J
AU Zhang, F
   Wang, ZH
   Lyu, X
   Zhao, SY
   Li, MJ
   Geng, WD
   Ji, NY
   Du, H
   Gao, FX
   Wu, H
   Li, SM
AF Zhang, Fan
   Wang, Zhaohan
   Lyu, Xin
   Zhao, Siyuan
   Li, Mengjian
   Geng, Weidong
   Ji, Naye
   Du, Hui
   Gao, Fuxing
   Wu, Hao
   Li, Shunman
TI Speech-Driven Personalized Gesture Synthetics: Harnessing Automatic
   Fuzzy Feature Inference
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Speech-driven; gesture synthesis; fuzzy inference; AdaLN; diffusion;
   transformer; DiTs; Speech-driven; gesture synthesis; fuzzy inference;
   AdaLN; diffusion; transformer; DiTs
AB Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture (DiTs-based). The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN transformer. The AdaLN transformer introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-speech synchronization while preserving naturalness. Finally, we employ the diffusion model to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system's usability and generalization capabilities, setting a new benchmark in speech-driven gesture synthesis and broadening the horizon for virtual human technology.
C1 [Zhang, Fan; Zhao, Siyuan] Macau Univ Sci & Technol, Fac Humanities & Arts, Macau 999078, Peoples R China.
   [Zhang, Fan; Ji, Naye; Du, Hui; Gao, Fuxing; Wu, Hao] Commun Univ Zhejiang, Coll Media Engn, Hangzhou 310019, Peoples R China.
   [Zhang, Fan; Li, Mengjian; Geng, Weidong] Zhejiang Lab, Res Ctr Artificial Intelligence & Fine Arts, Hangzhou 311121, Zhejiang, Peoples R China.
   [Wang, Zhaohan; Lyu, Xin] Commun Univ China, Sch Animat & Digital Arts, Beijing 100024, Peoples R China.
   [Geng, Weidong] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Li, Shunman] Zhejiang Inst Econ & Trade, Hangzhou 310006, Zhejiang, Peoples R China.
C3 Macau University of Science & Technology; Communication University of
   Zhejiang; Zhejiang Laboratory; Communication University of China;
   Zhejiang University; Zhejiang Institute of Economics & Trade
RP Ji, NY (corresponding author), Commun Univ Zhejiang, Coll Media Engn, Hangzhou 310019, Peoples R China.
EM fanzhang@cuz.edu.cn; 2022201305j6018@cuc.edu.cn; lvxinlx@cuc.edu.cn;
   2109853jai30001@student.must.edu.mo; limengjian@zhejianglab.com;
   gengwd@zju.edu.cn; jinaye@cuz.edu.cn; duhui@cuz.edu.cn;
   fuxing@cuz.edu.cn; 210207140@stu.cuz.edu.cn; 2017000018@zjiet.edu.cn
RI Zhao, Siyuan/HPG-0299-2023; Ji, Naye/GRF-3137-2022
OI Ji, Naye/0000-0002-6986-3766; Wang, Zhaohan/0009-0005-4783-6213; Fan,
   Zhang/0000-0002-9534-1777; Li, Mengjian/0009-0000-9698-8508
FU The "Pioneer" and "Leading Goose" R&D Program of Zhejiang [2023 C01212];
   National Key Research and Development Program of China [2022YFF0902305];
   Public Welfare Technology Application Research Project of Zhejiang
   [LGF21F020002, LGF22F020008]; Key Program and development projects of
   Zhejiang Province of China [2021C03137]; Key Lab of Film and TV Media
   Technology of Zhejiang Province [2020E10015]
FX This work was supported in part by the "Pioneer" and "Leading Goose" R&D
   Program of Zhejiang under Grant 2023 C01212, in part by the National Key
   Research and Development Program of China under Grant 2022YFF0902305, in
   part by the Public Welfare Technology Application Research Project of
   Zhejiang under Grant LGF21F020002 and Grant LGF22F020008, in part by the
   Key Program and development projects of Zhejiang Province of China under
   Grant 2021C03137, and in parg by the Key Lab of Film and TV Media
   Technology of Zhejiang Province under Grant 2020E10015.
CR Alexanderson Simon, 2023, ACM Transactions on Graphics, DOI 10.1145/3592458
   Alexanderson S, 2020, COMPUT GRAPH FORUM, V39, P487, DOI 10.1111/cgf.13946
   Ao TL, 2023, Arxiv, DOI arXiv:2303.14613
   Bhattacharya U, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2027, DOI 10.1145/3474085.3475223
   Bhattacharya U, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P160, DOI 10.1109/VR50410.2021.00037
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Calvo RA., 2015, OXFORD HDB AFFECTIVE
   Cambria Erik, 2012, Cognitive Behavioural Systems (COST 2012). International Training School. Revised Selected Papers, P144, DOI 10.1007/978-3-642-34584-5_11
   Campbell-Kibler K, 2011, AM SPEECH, V86, P52, DOI 10.1215/00031283-1277510
   Chen SY, 2022, IEEE J-STSP, V16, P1505, DOI 10.1109/JSTSP.2022.3188113
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ferstl Y, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P93, DOI 10.1145/3267851.3267898
   Ghorbani S, 2023, COMPUT GRAPH FORUM, V42, P206, DOI 10.1111/cgf.14734
   Goudbeek M, 2010, J ACOUST SOC AM, V128, P1322, DOI 10.1121/1.3466853
   Grassia F. S., 1998, Journal of Graphics Tools, V3, P29
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Hensel M, 2017, ADV NEUR IN, V30
   Hirschberg J, 2015, SCIENCE, V349, P261, DOI 10.1126/science.aaa8685
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Kaiser L, 2017, Arxiv, DOI arXiv:1706.03059
   Kucherenko Taras, 2020, ICMI '20: Proceedings of the 2020 International Conference on Multimodal Interaction, P242, DOI 10.1145/3382507.3418815
   Kucherenko T, 2024, Arxiv, DOI arXiv:2303.08737
   Langevin P, 1908, CR HEBD ACAD SCI, V146, P530
   Li J., IEEE Trans. Vis. Comput. Graph., DOI [10.1109/TVCG.2023.3276973, DOI 10.1109/TVCG.2023.3276973]
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Liu HY, 2022, LECT NOTES COMPUT SC, V13667, P612, DOI 10.1007/978-3-031-20071-7_36
   Mikolov T., 2013, P 26 C NEUR INF PROC, P3111
   Patil NM, 2019, ADV INTELL SYST, V924, P263, DOI 10.1007/978-981-13-6861-5_23
   Peebles W, 2023, Arxiv, DOI [arXiv:2212.09748, DOI 10.48550/ARXIV.2212.09748]
   Pennington J, 2014, PROCEEDING 2014 C EM, P1532, DOI DOI 10.3115/V1/D14-1162
   Rasul K, 2021, PR MACH LEARN RES, V139
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Taylor S., 2021, P EUR C VIS MED PROD, P1
   Vashishtha S, 2020, 2020 IEEE SIXTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2020), P394, DOI 10.1109/BigMM50055.2020.00067
   Wennberg U, 2021, ACL-IJCNLP 2021: THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 2, P130
   Windle J, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2022, P771, DOI 10.1145/3536221.3558065
   Wolfert Pieter, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P494, DOI 10.1145/3462244.3479889
   Wolfert P, 2022, IEEE T HUM-MACH SYST, V52, P379, DOI 10.1109/THMS.2022.3149173
   Yang SC, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P1033, DOI 10.1145/3581783.3612503
   Yang SC, 2023, Arxiv, DOI arXiv:2308.13879
   Yang SC, 2023, PROC CVPR IEEE, P2321, DOI 10.1109/CVPR52729.2023.00230
   Yang SC, 2023, Arxiv, DOI arXiv:2305.04919
   Yang SC, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2022, P758, DOI 10.1145/3536221.3558066
   Yoon Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417838
   Yoon Y, 2019, IEEE INT CONF ROBOT, P4303, DOI [10.1109/icra.2019.8793720, 10.1109/ICRA.2019.8793720]
   Yu LY, 2022, IEEE T MULTIMEDIA, V24, P2950, DOI 10.1109/TMM.2021.3091863
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang F, 2023, LECT NOTES COMPUT SC, V13833, P231, DOI 10.1007/978-3-031-27077-2_18
   Zhao MH, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3318013
   Zhi Y., 2023, P IEEE CVF INT C COM, p20 807
   Zhu LT, 2023, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR52729.2023.01016
NR 51
TC 0
Z9 0
U1 12
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6984
EP 6996
DI 10.1109/TVCG.2024.3393236
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600001
PM 38656863
OA hybrid, Green Submitted
DA 2024-11-06
ER

PT J
AU Eck, U
   Sugimoto, M
   Sra, M
   Tatzgern, M
   Stefanucci, J
   Williams, I
AF Eck, Ulrich
   Sugimoto, Maki
   Sra, Misha
   Tatzgern, Markus
   Stefanucci, Jeanine
   Williams, Ian
TI Message from the ISMAR 2024 Science and Technology Program Chairs and
   TVCG Guest Editors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Editorial Material
DE Special issues and sections; Meetings; Augmented reality; Virtual
   reality; Mixed reality
AB In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the journal papers from the 23rd IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2024), which will be held as a hybrid conference between October 21 and 25, 2024 in the Greater Seattle Area, USA. ISMAR continues the over twenty-year long tradition of IWAR, ISMR, and ISAR, and is the premier conference for Mixed and Augmented Reality in the world.
C1 [Eck, Ulrich] Tech Univ Munich, Munich, Germany.
   [Sugimoto, Maki] Keio Univ, Yokohama, Japan.
   [Sra, Misha] UC Santa Barbara, Santa Barbara, CA USA.
   [Tatzgern, Markus] Salzburg Univ Appl Sci, Salzburg, Austria.
   [Stefanucci, Jeanine] Univ Utah, Salt Lake City, UT USA.
   [Williams, Ian] Birmingham City Univ, Birmingham, England.
C3 Technical University of Munich; Keio University; University of
   California System; University of California Santa Barbara; Utah System
   of Higher Education; University of Utah; Birmingham City University
RP Eck, U (corresponding author), Tech Univ Munich, Munich, Germany.
EM ulrich.eck@tum.de
RI Tatzgern, Markus/JMQ-2141-2023
NR 0
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 0vii
EP 0vii
DI 10.1109/TVCG.2024.3456175
PG 1
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300001
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Chiossi, F
   Trautmannsheimer, I
   Ou, C
   Gruenefeld, U
   Mayer, S
AF Chiossi, Francesco
   Trautmannsheimer, Ines
   Ou, Changkun
   Gruenefeld, Uwe
   Mayer, Sven
TI Searching Across Realities: Investigating ERPs and Eye-Tracking
   Correlates of Visual Search in Mixed Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Search problems; Virtual reality; Gaze tracking;
   Measurement; Cognitive load; Resource management; Mixed Reality;
   Augmented Reality; Augmented Virtuality; Visual Search; EEG; Eye
   Tracking; Event-Related Potentials
ID PERFORMANCE; TARGET
AB Mixed Reality allows us to integrate virtual and physical content into users' environments seamlessly. Yet, how this fusion affects perceptual and cognitive resources and our ability to find virtual or physical objects remains uncertain. Displaying virtual and physical information simultaneously might lead to divided attention and increased visual complexity, impacting users' visual processing, performance, and workload. In a visual search task, we asked participants to locate virtual and physical objects in Augmented Reality and Augmented Virtuality to understand the effects on performance. We evaluated search efficiency and attention allocation for virtual and physical objects using event-related potentials, fixation and saccade metrics, and behavioral measures. We found that users were more efficient in identifying objects in Augmented Virtuality, while virtual objects gained saliency in Augmented Virtuality. This suggests that visual fidelity might increase the perceptual load of the scene. Reduced amplitude in distractor positivity ERP, and fixation patterns supported improved distractor suppression and search efficiency in Augmented Virtuality. We discuss design implications for mixed reality adaptive systems based on physiological inputs for interaction.
C1 [Chiossi, Francesco; Trautmannsheimer, Ines; Ou, Changkun; Mayer, Sven] Ludwig Maximilians Univ Munchen, Munich, Germany.
   [Gruenefeld, Uwe] Univ Duisburg Essen, Essen, Germany.
C3 University of Munich; University of Duisburg Essen
RP Chiossi, F (corresponding author), Ludwig Maximilians Univ Munchen, Munich, Germany.
EM francesco.chiossi@um.ifi.lmu.de; i.trautmannsheimer@campus.lmu.de;
   research@changkun.de; uwe.gruenefeld@uni-due.de; info@sven-mayer.com
RI Gruenefeld, Uwe/ABB-7978-2021; Mayer, Sven/A-5174-2019
OI Mayer, Sven/0000-0001-5462-8782
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [251654672-TRR 161 (C06)]
FX Francesco Chiossi was supported by the Deutsche Forschungsgemeinschaft
   (DFG, German Research Foundation), Project-ID 251654672-TRR 161 (C06).
CR Auda J, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3616536
   Bafna T., 2020, S EYE TRACK RES APPL
   Beintema JA, 2005, J VISION, V5, P150, DOI 10.1167/5.3.1
   Bigdely-Shamlo N, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00016
   Brouwer AM, 2013, J VISION, V13, DOI 10.1167/13.3.17
   Carter BT, 2020, INT J PSYCHOPHYSIOL, V155, P49, DOI 10.1016/j.ijpsycho.2020.05.010
   Charles RL, 2019, APPL ERGON, V74, P221, DOI 10.1016/j.apergo.2018.08.028
   Cheng Y., 2021, 34 ANN ACM S US INT
   Cheng YF, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517452
   Chiossi F., 2024, Proc. ACM Hum.-Comput. Interact., (MHCI)
   Chiossi F, 2024, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, AVI 2024, DOI 10.1145/3656650.3656657
   Chiossi F, 2023, Arxiv, DOI arXiv:2311.10447
   Chiossi F, 2022, IT-INF TECHNOL, V64, P133, DOI 10.1515/itit-2022-0035
   David-John B., 2021, S EYE TRACK RES APPL
   Dilworth J, 2010, PHILOS PSYCHOL, V23, P23, DOI 10.1080/09515080903533942
   Dixon BJ, 2013, SURG ENDOSC, V27, P454, DOI 10.1007/s00464-012-2457-3
   Drey T, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581004
   Duchowski AT, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173856
   Durugbo CM, 2021, ERGONOMICS, V64, P225, DOI 10.1080/00140139.2020.1822547
   Souza RHCE, 2021, FRONT PHYSIOL, V12, DOI 10.3389/fphys.2021.727840
   Eckstein MP, 2011, J VISION, V11, DOI 10.1167/11.5.14
   Engbert R, 2003, VISION RES, V43, P1035, DOI 10.1016/S0042-6989(03)00084-1
   Fairclough SH, 2009, INTERACT COMPUT, V21, P133, DOI 10.1016/j.intcom.2008.10.011
   Feick M., 2020, S US INT SOFTW TECHN
   Forschack N, 2022, NEUROIMAGE, V264, DOI 10.1016/j.neuroimage.2022.119759
   Furmanski C, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P215, DOI 10.1109/ISMAR.2002.1115091
   Gardony AL, 2020, PROC SPIE, V11310, DOI 10.1117/12.2542699
   Gaspelin N, 2023, J COGNITIVE NEUROSCI, V35, P1693, DOI 10.1162/jocn_a_02051
   Gerup J, 2020, INT J MED EDUC, V11, DOI 10.5116/ijme.5e01.eb1a
   Goldberg J. H., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P51, DOI 10.1145/507072.507082
   Gonçalves G, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3533377
   Gorjan D, 2022, J NEURAL ENG, V19, DOI 10.1088/1741-2552/ac542c
   Gramfort A, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00267
   Gruenefeld U, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501821
   Hadnett-Hunter J., 2022, Journal of Vision, V22
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   HASLWANTER T, 1995, VISION RES, V35, P1727, DOI 10.1016/0042-6989(94)00257-M
   Heuer A, 2020, PSYCHOL RES-PSYCH FO, V84, P2111, DOI 10.1007/s00426-019-01211-4
   Hickey C, 2009, J COGNITIVE NEUROSCI, V21, P760, DOI 10.1162/jocn.2009.21039
   Hu ZM, 2021, IEEE T VIS COMPUT GR, V27, P2681, DOI 10.1109/TVCG.2021.3067779
   Hughes CE, 2005, IEEE COMPUT GRAPH, V25, P24, DOI 10.1109/MCG.2005.139
   Hussain M, 2024, INT J HUM-COMPUT INT, V40, P4265, DOI 10.1080/10447318.2023.2212218
   Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661
   Jackson KM, 2023, ERGONOMICS, V66, P125, DOI 10.1080/00140139.2022.2061053
   Janssens C, 2018, PSYCHOPHYSIOLOGY, V55, DOI 10.1111/psyp.13011
   Jayawardena G, 2020, ACM ETRA
   Ke FF, 2016, COMPUT HUM BEHAV, V62, P212, DOI 10.1016/j.chb.2016.03.094
   Kenyon R. V., 2014, Virtual reality for physical and motor rehabilitation
   Kim YJ, 2022, IEEE T VIS COMPUT GR, V28, P3788, DOI 10.1109/TVCG.2022.3203093
   Kockord R, 2021, MENSCH AND COMPUTER 2021 (MUC 21), P260, DOI 10.1145/3473856.3474003
   Kovacs BI, 2023, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR59233.2023.00015
   Krakowczyk D. G., 2023, P S EYE TRACK RES AP
   Lamy D, 2003, J EXP PSYCHOL HUMAN, V29, P1003, DOI 10.1037/0096-1523.29.5.1003
   Lee C, 2013, IEEE T VIS COMPUT GR, V19, P547, DOI 10.1109/TVCG.2013.41
   Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719
   Li A., 2022, Journal of Open Source Software, V7
   Lindlbauer D, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P147, DOI 10.1145/3332165.3347945
   Long X., 2024, P MENSCH COMP 2024
   LUCK SJ, 1994, J EXP PSYCHOL HUMAN, V20, P1000, DOI 10.1037/0096-1523.20.5.1000
   MacInnes W. J., 2020, Vision, V4
   Mahanama B, 2022, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.733531
   Marini F, 2019, NEUROIMAGE, V195, P232, DOI 10.1016/j.neuroimage.2019.02.026
   Matsuda N, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555427
   McGill M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2143, DOI 10.1145/2702123.2702382
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Naderi M, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-34758-9
   Noonan MP, 2016, J NEUROSCI, V36, P1797, DOI 10.1523/JNEUROSCI.2133-15.2016
   Nwagu C., 2023, Proc. ACM Hum.-Comput. Interact., V7
   Onikura K, 2015, 2015 8TH BIOMEDICAL ENGINEERING INTERNATIONAL CONFERENCE (BMEICON)
   Palinko O., 2010, P ETRA 10
   PERRIN F, 1989, ELECTROEN CLIN NEURO, V72, P184, DOI 10.1016/0013-4694(89)90180-6
   Plopski A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3491207
   Pomplun M, 2013, J VISION, V13, DOI 10.1167/13.3.24
   Ragan ED, 2015, IEEE T VIS COMPUT GR, V21, P794, DOI 10.1109/TVCG.2015.2403312
   Rappa NA, 2022, INTERACT LEARN ENVIR, V30, P1338, DOI 10.1080/10494820.2019.1702560
   Rehman U., 2016, Transactions on Human-Machine Systems, V47
   Roche RAP, 2005, EXP BRAIN RES, V160, P60, DOI 10.1007/s00221-004-1985-z
   Roper ZJJ, 2013, J EXP PSYCHOL HUMAN, V39, P1340, DOI 10.1037/a0031616
   Rzepka AM, 2023, PHILOS T R SOC B, V378, DOI 10.1098/rstb.2021.0464
   Salvucci D.D, 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   Samini A, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P49, DOI 10.1109/CW52790.2021.00015
   Satkowski M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445330
   Sawaki R, 2012, J NEUROSCI, V32, P10725, DOI 10.1523/JNEUROSCI.1864-12.2012
   Speicher M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300767
   Sun L, 2013, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON COMPUTER-AIDED ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2013), P457
   Tang A, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P265, DOI 10.1109/ISMAR.2002.1115105
   van den Berg AV, 2005, VISION RES, V45, P1543, DOI 10.1016/j.visres.2004.12.018
   van den Oever F, 2022, IEEE INT SYMP M AU R, P411, DOI 10.1109/ISMAR-Adjunct57072.2022.00089
   Wang B.-S., 2009, Journal of statistical software, V29
   Wang CH, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545686
   Wang RI, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P184, DOI 10.1109/3DIMPVT.2012.37
   Wolfe JM, 2020, ANNU REV VIS SCI, V6, P539, DOI [10.1146/annurev-vision-091718-015048, 10.1146/annurev-vision-091718015048]
   Woodman GF, 1999, NATURE, V400, P867, DOI 10.1038/23698
   Zhang JJ, 2020, PROCEEDINGS OF THE 2020 ACM SYMPOSIUM ON SPATIAL USER INTERACTION, SUI 2020, DOI 10.1145/3385959.3422697
NR 94
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 6997
EP 7007
DI 10.1109/TVCG.2024.3453148
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300015
PM 39264778
DA 2024-11-06
ER

PT J
AU Martin, J
   Hoyet, L
   Pinsard, E
   Paillat, JL
   Pettré, J
AF Martin, Jordan
   Hoyet, Ludovic
   Pinsard, Etienne
   Paillat, Jean-Luc
   Pettre, Julien
TI Virtual Crowds Rheology: Evaluating the Effect of Character
   Representation on User Locomotion in Crowds
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Navigation; Human Interaction; Visual Representation; Crowd; Virtual
   Reality; Virtual Reality; Navigation; Visual Representation; Virtual
   Reality
ID AVOIDANCE-BEHAVIOR; MOTION; APPEARANCE; NAVIGATION; SIMULATION
AB Crowd data is a crucial element in the modeling of collective behaviors, and opens the way to simulation for their study or prediction. Given the difficulty of acquiring such data, virtual reality is useful for simplifying experimental processes and opening up new experimental opportunities. This comes at the cost of the need to assess the biases introduced by the use of this technology. Our paper is part of this effort, and investigates the effect of the graphical representation of a crowd on the behavior of a user immersed within. More specifically, we inspect the virtual navigation through virtual crowds, in terms of travel speeds and local navigation choices as a function of the visual representation of the virtual agents that make up the crowd (simple geometric model, anthropomorphic model or realistic model). Through an experiment in which we ask a user to navigate virtual crowds of varying densities, we show that the effect of the visual representation is limited, but that an anthropomorphic representation offers the best trade-off between computational complexity and ecological validity, even though a more realistic representation can be preferred when user behaviour is studied in more details. Our work leads to clear recommendations on the design of immersive simulations for the study of crowd behavior.
C1 [Martin, Jordan; Hoyet, Ludovic; Paillat, Jean-Luc] Univ Rennes, Inria, CNRS, IRISA, Rennes, France.
   [Martin, Jordan; Pinsard, Etienne; Paillat, Jean-Luc] LCPP, Paris, France.
C3 Inria; Universite de Rennes; Centre National de la Recherche
   Scientifique (CNRS)
RP Martin, J (corresponding author), Univ Rennes, Inria, CNRS, IRISA, Rennes, France.; Martin, J (corresponding author), LCPP, Paris, France.
EM jordan.martin@interieur.gouv.fr; ludovic.hoyet@interieur.gouv.fr;
   etienne.pinsard@interieur.gouv.fr; jean-luc.paillat@interieur.gouv.fr;
   julien.pettre@interieur.gouv.fr
RI Hoyet, Ludovic/IWU-9100-2023; Pettre, Julien/KZT-8249-2024
OI Hoyet, Ludovic/0000-0002-7373-6049; Pettre, Julien/0000-0003-1812-1436
FU French National Research Agency under the Investments for the Future
   program (PIA) grant [ANR-21- ESRE-0030 CONTINUUM]
FX This work was sponsored by the French National Research Agency under the
   Investments for the Future program (PIA) grant (ANR-21- ESRE-0030
   CONTINUUM).
CR Amirian J, 2019, PROCEEDINGS OF THE 32ND INTERNATIONAL CONFERENCE ON COMPUTER ANIMATION AND SOCIAL AGENTS (CASA 2019), P7, DOI 10.1145/3328756.3328769
   Appert-Rolland C., 2023, Properties of pedestrians walking in line without density constraint, P2
   Bain N, 2019, SCIENCE, V363, P46, DOI 10.1126/science.aat9891
   Berton F, 2022, IEEE T VIS COMPUT GR, V28, P2589, DOI 10.1109/TVCG.2020.3041341
   Berton F, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P322, DOI [10.1109/VR46266.2020.1581264804299, 10.1109/VR46266.2020.00-52]
   Berton F, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P717, DOI [10.1109/VR.2019.8798204, 10.1109/vr.2019.8798204]
   Biswas J, 2012, IEEE INT CONF ROBOT, P1697, DOI 10.1109/icra.2012.6224766
   Bönsch A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P199
   Bönsch A, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P145, DOI 10.1109/3DUI.2016.7460045
   Bohannon RW, 1997, AGE AGEING, V26, P15, DOI 10.1093/ageing/26.1.15
   Bruneau J, 2015, IEEE T VIS COMPUT GR, V21, P520, DOI 10.1109/TVCG.2015.2391862
   Bruno L., 2008, P FOOTBRIDGE
   Chahal M., 2013, International Journal of Science, Engineering and Computer Technology, V3, P103
   Charalambous P, 2014, COMPUT GRAPH FORUM, V33, P95, DOI 10.1111/cgf.12403
   Chattaraj U, 2009, ADV COMPLEX SYST, V12, P393, DOI 10.1142/S0219525909002209
   Corbetta A, 2023, ANNU REV CONDEN MA P, V14, P311, DOI 10.1146/annurev-conmatphys-031620-100450
   Duives DC, 2015, PHYSICA A, V427, P162, DOI 10.1016/j.physa.2014.11.054
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Fujita A, 2019, PHYS REV E, V99, DOI 10.1103/PhysRevE.99.062307
   Garcimartin A, 2018, NEW J PHYS, V20, DOI 10.1088/1367-2630/aaf4ca
   Ghinea M, 2018, LECT NOTES COMPUT SC, V10850, P148, DOI 10.1007/978-3-319-95270-3_10
   Goupil V., 2023, ICAT EGVE 2023, DOI [10.2312/egve, DOI 10.2312/EGVE]
   Haghani M, 2018, TRANSPORT RES B-METH, V107, P253, DOI 10.1016/j.trb.2017.06.017
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hoogendoorn SP, 2005, TRANSPORT SCI, V39, P147, DOI 10.1287/trsc.1040.0102
   Jelic A, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.036111
   Kihlstrom JF, 2021, PERSPECT PSYCHOL SCI, V16, P466, DOI 10.1177/1745691620966791
   Lemercier S, 2012, COMPUT GRAPH FORUM, V31, P489, DOI 10.1111/j.1467-8659.2012.03028.x
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Lopez A., 2019, MIG 2019, P1, DOI [10.1145/3359566.33600852, DOI 10.1145/3359566.33600852]
   Lynch SD, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P136, DOI 10.1109/VR.2018.8446180
   Lynch SD, 2018, IEEE T VIS COMPUT GR, V24, P2078, DOI 10.1109/TVCG.2017.2718514
   Matsushita K., 2016, Acetic acid bacteria. Ecology and physiology., DOI DOI 10.1007/978-4-431-55933-7
   Meerhoff LA, 2018, ACTA PSYCHOL, V190, P248, DOI 10.1016/j.actpsy.2018.07.009
   Mousas C, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P40, DOI 10.1109/VR50410.2021.00024
   Mousas C, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P726, DOI [10.1109/vr.2019.8798043, 10.1109/VR.2019.8798043]
   Mousas C, 2018, COMPUT HUM BEHAV, V86, P99, DOI 10.1016/j.chb.2018.04.036
   Mullick P, 2022, PLOS COMPUT BIOL, V18, DOI 10.1371/journal.pcbi.1010210
   Musse SR, 2021, VISUAL COMPUT, V37, P3077, DOI 10.1007/s00371-021-02252-w
   Nabiyouni Mahdi, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2015.7131717
   Nakatsuka T, 2018, IEEE SYS MAN CYBERN, P3738, DOI 10.1109/SMC.2018.00633
   Nelson M, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365709
   Nelson MG, 2022, IEEE INT SYMP M AU R, P594, DOI 10.1109/ISMAR-Adjunct57072.2022.00123
   Nicolas A., 2018, Mechanical response of dense pedestrian crowds to the crossing of intruders, P2
   Olivier Anne-Helene, 2018, IEEE Transactions on Visualization and Computer Graphics, V24, P2251, DOI 10.1109/TVCG.2017.2714665
   Olivier AH, 2014, TRANSP RES PROC, V2, P114, DOI 10.1016/j.trpro.2014.09.015
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Paetzke S, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13095450
   Paris S, 2007, COMPUT GRAPH FORUM, V26, P665, DOI 10.1111/j.1467-8659.2007.01090.x
   Patla AE, 1997, GAIT POSTURE, V5, P54, DOI 10.1016/S0966-6362(96)01109-5
   Pazhoohi F, 2019, PSYCHOL RES-PSYCH FO, V83, P1184, DOI 10.1007/s00426-017-0968-1
   Perrinet J., 2013, P ACM S APPL PERC, P59
   Pettre J., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '09, P189, DOI DOI 10.1145/1599470.1599495
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Rio K, 2014, TRANSP RES PROC, V2, P132, DOI 10.1016/j.trpro.2014.09.017
   Rio KW, 2014, J VISION, V14, DOI 10.1167/14.2.4
   Ríos A, 2020, VIRTUAL REAL-LONDON, V24, P683, DOI 10.1007/s10055-020-00428-8
   Roser M., 2021, Our World in Data
   Sanz FA, 2015, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2015.7223327
   Seyfried A, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/10/P10002
   Shiomi M, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0279717
   Shiwakoti N, 2019, SAFETY SCI, V113, P54, DOI 10.1016/j.ssci.2018.11.016
   Sreenu G, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0212-5
   Stuvel S. A., 2015, P 8 ACM SIGGRAPH C M, P85
   Stüvel SA, 2017, IEEE T VIS COMPUT GR, V23, P1823, DOI 10.1109/TVCG.2016.2545670
   Sundararaman R, 2021, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR46437.2021.00386
   Tajima Y, 2001, PHYSICA A, V292, P545, DOI 10.1016/S0378-4371(00)00630-0
   Tavana H., 2022, Collective Dynamics, V6, P1, DOI [10.17815/CD.2021.1282, DOI 10.17815/CD.2021.1282]
   Ullah M, 2016, IEEE IMAGE PROC, P1195, DOI 10.1109/ICIP.2016.7532547
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P139
   van Toll W, 2021, COMPUT GRAPH FORUM, V40, P731, DOI 10.1111/cgf.142664
   van Toll W., 2020, Generalized microscropic crowd simulation using costs in velocity space, P1, DOI [10.1145/3384382.33845322, DOI 10.1145/3384382.33845322]
   Wolinski D, 2014, COMPUT GRAPH FORUM, V33, P303, DOI 10.1111/cgf.12328
   Yang SW, 2020, GRAPH MODELS, V111, DOI 10.1016/j.gmod.2020.101081
   Yin TR, 2022, IEEE T VIS COMPUT GR, V28, P2245, DOI 10.1109/TVCG.2022.3150507
   Yun HR, 2024, Symposium Virtual Re, P472, DOI 10.1109/VR58804.2024.00068
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zhang J, 2020, LECT NOTES COMPUT SC, V12192, P255, DOI 10.1007/978-3-030-49788-0_19
   Zhong JH, 2022, ACM T MODEL COMPUT S, V32, DOI 10.1145/3481299
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 82
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7008
EP 7019
DI 10.1109/TVCG.2024.3453128
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300044
PM 39250415
DA 2024-11-06
ER

PT J
AU Dastan, M
   Fiorentino, M
   Uva, AE
AF Dastan, Mine
   Fiorentino, Michele
   Uva, Antonio E.
TI Precise Tool to Target Positioning Widgets (TOTTA) in Spatial
   Environments: A Systematic Review
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Resists; Visualization; Input devices;
   6-DOF; Surgery; Shape; Virtual environments; 3D user interface; tool to
   target manipulation; widgets; 3D positioning
ID OBJECT MANIPULATION; AUGMENTED REALITY; VIRTUAL-REALITY; DESIGN
AB TOTTA outlines the spatial position and rotation guidance of a real/virtual tool (TO) towards a real/virtual target (TA), which is a key task in Mixed reality applications. The task error can have critical consequences regarding safety, performance, and quality, such as surgical implantology or industrial maintenance scenarios. The TOTTA problem lacks a dedicated study and it is scattered in different domains with isolated designs. This work contributes to a systematic review of the TOTTA visual widgets, studying 70 unique designs from 24 papers. TOTTA is commonly guided by the visual overlap -an intuitive, pre-attentive "collimation" feedback- of simple shaped widgets: Box, 3D Axes, 3D Model, 2D Crosshair, Globe, Tetrahedron, Line, Plane. Our research discovers that TO and TA are often represented with the same shape. They are distinguished by topological elements (e.g. edges/vertices/faces), colors, transparency levels, and added. shapes, widget quantity, and size. Meanwhile some designs provide continuous "during manipulation feedback" relative to the distance between TO and TA by text, dynamic color, sonification, and amplified graphical visualization. Some approaches trigger discrete "TA reached feedback" such as color alteration, added sound, TA shape change, and added text. We found the lack of golden standards, including in testing procedures, as current ones are limited to partial sets with different and incomparable setups (different target configurations, avatar, background, etc.). We also found a bias in participants: right-handed, young male, non-color impaired.
C1 [Dastan, Mine; Fiorentino, Michele; Uva, Antonio E.] Polytech Univ Bari, Bari, Italy.
C3 Politecnico di Bari
RP Dastan, M (corresponding author), Polytech Univ Bari, Bari, Italy.
EM mine.dastan@poliba.it
RI Uva, Antonio/A-9673-2012; Fiorentino, Michele/M-6976-2015; Dastan,
   Mine/LFR-9751-2024
OI Fiorentino, Michele/0000-0003-2197-6574; Dastan,
   Mine/0000-0003-0555-155X
FU MICS (Made in Italy - Circular and Sustainable) - National Recovery and
   Resilience Plan (PNRR), Italian Ministry of University and Research,
   European Union -NextGenerationEU [341, PE00000004, CUP D93C22000920001];
   Emme Evolution S.r.l. FESR Obiettivo Convergenza; Polytechnic University
   of Bari [CUP B95H22000810007]
FX Co-founded by MICS (Made in Italy - Circular and Sustainable) - National
   Recovery and Resilience Plan (PNRR), 4-2-1.3 - No. 341, Italian Ministry
   of University and Research, European Union -NextGenerationEU PE00000004,
   CUP D93C22000920001. Co-funding by Emme Evolution S.r.l. FESR 2014 -
   2020 Obiettivo Convergenza. Project development organizations: with
   Polytechnic University of Bari, CUP B95H22000810007.
CR Andersen D, 2020, INT SYM MIX AUGMENT, P289, DOI 10.1109/ISMAR50242.2020.00055
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Bergström J, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445193
   Boritz J, 1998, P IEEE VIRT REAL ANN, P139, DOI 10.1109/VRAIS.1998.658476
   Bowman DA, 2001, PRESENCE-TELEOP VIRT, V10, P96, DOI 10.1162/105474601750182342
   Brewster S, 1998, INTERACT COMPUT, V11, P211, DOI 10.1016/S0953-5438(98)00028-9
   Buchner J, 2022, J COMPUT ASSIST LEAR, V38, P285, DOI 10.1111/jcal.12617
   Çöltekin A, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9070439
   Conner D.B., 1992, P S INTERACTIVE 3D G, P183, DOI DOI 10.1145/147156.147199
   Dastan M, 2022, INT SYM MIX AUGMENT, P187, DOI 10.1109/ISMAR55827.2022.00033
   de Belen R. A. J., 2019, AIMS ELECT ELECT ENG, V3, P181
   El Radaf IM, 2020, J MATER SCI-MATER EL, V31, P18151, DOI 10.1007/s10854-020-04364-w
   Erdenebat M.-U., 2018, State of the Art Virtual Reality and Augmented Reality Knowhow, DOI [10.5772/intechopen.75172, DOI 10.5772/INTECHOPEN.75172]
   Eschen H, 2018, PROCEDIA MANUF, V19, P156, DOI 10.1016/j.promfg.2018.01.022
   Feng J., 2015, Proceedings of the 3rd ACM Symposium on Spatial User Interaction - SUI'15, P2, DOI [DOI 10.1145/2788940, 10.1145/2788940.2788942, DOI 10.1145/2788940.2788942]
   Fiorentino M., 2003, 3d pointing in virtual reality: experimental study
   Frediani G, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-77591-0
   Fuvattanasilp V, 2021, COMPUT GRAPH-UK, V95, P23, DOI 10.1016/j.cag.2021.01.005
   Galais T, 2019, IHM19:ANNEXES DES ACTES DE LA 31E CONFERENCE FRANCOPHONE SUR I'INTERACTION HOMME-MACHINE, DOI 10.1145/3366551.3370342
   Ganias G, 2023, IEEE T VIS COMPUT GR, V29, P2369, DOI 10.1109/TVCG.2023.3247039
   Ha T, 2014, INT SYM MIX AUGMENT, P219, DOI 10.1109/ISMAR.2014.6948431
   Hand C, 1997, COMPUT GRAPH FORUM, V16, P269, DOI 10.1111/1467-8659.00194
   Heinrich F, 2019, IEEE T VIS COMPUT GR, V25, P2157, DOI 10.1109/TVCG.2019.2903942
   Kang HJ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P275, DOI [10.1109/VR46266.2020.00047, 10.1109/VR46266.2020.00-57]
   Kim M, 2019, INT J HUM-COMPUT INT, V35, P1147, DOI 10.1080/10447318.2018.1514163
   Kim S, 2019, APPL ERGON, V74, P186, DOI 10.1016/j.apergo.2018.08.026
   Korkut EH, 2023, VIRTUAL REAL-LONDON, V27, P1447, DOI 10.1007/s10055-023-00753-8
   Krichenbauer M, 2018, IEEE T VIS COMPUT GR, V24, P1038, DOI 10.1109/TVCG.2017.2658570
   Kuber PM, 2023, ANN BIOMED ENG, V51, P1910, DOI 10.1007/s10439-023-03292-0
   Laviola E, 2022, INT J ADV MANUF TECH, V119, P1769, DOI 10.1007/s00170-021-08449-6
   Lee CY, 2021, PROCEEDINGS OF THE 2021 ACM SYMPOSIUM ON SPATIAL USER INTERACTION, SUI 2021, DOI 10.1145/3485279.3485296
   Liu C, 2020, COMPUT GRAPH-UK, V89, P1, DOI 10.1016/j.cag.2020.04.005
   Martin-Gomez A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P735, DOI [10.1109/vr.2019.8798135, 10.1109/VR.2019.8798135]
   Mendes D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P261, DOI 10.1145/2993369.2993396
   Milani AS, 2024, INT J HUM-COMPUT INT, V40, P515, DOI 10.1080/10447318.2022.2116530
   Moher D, 2009, ANN INTERN MED, V151, P264, DOI [10.1136/bmj.i4086, 10.1186/2046-4053-4-1, 10.1136/bmj.b2535, 10.1136/bmj.b2700, 10.1016/j.ijsu.2010.02.007, 10.1016/j.ijsu.2010.07.299, 10.1371/journal.pmed.1000097]
   Müller J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1245, DOI 10.1145/2858036.2858043
   Muñoz A, 2019, J MANUF SYST, V53, P75, DOI 10.1016/j.jmsy.2019.08.004
   Nesenbergs K, 2021, EDUC SCI, V11, DOI 10.3390/educsci11010008
   Park Y, 2008, INT SYM MIX AUGMENT, P117, DOI 10.1109/ISMAR.2008.4637336
   Ragan E, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P287, DOI 10.1109/VR.2009.4811058
   Raj M., 2012, Proceedings of the ACM Symposium on Applied Perception, SAP '12, P99, DOI [10.1145/2338676.2338697, DOI 10.1145/2338676.2338697]
   Ramos G., 2004, Proceedings of the CHI '04 Conference on Human Factors in Computing Systems, P487
   Rehbein J, 2022, J NONDESTRUCT EVAL, V41, DOI 10.1007/s10921-022-00860-7
   Rethlefsen ML, 2021, SYST REV-LONDON, V10, DOI 10.1186/s13643-020-01542-z
   Ro H, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153078
   Rodriguez L, 2015, PROCEDIA COMPUT SCI, V75, P327, DOI 10.1016/j.procs.2015.12.254
   Rokhsaritalemi S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020636
   Schlunsen R., 2019, P MENSCH COMP 2019 S, P223, DOI [10.1145/3340764.3340791, DOI 10.1145/3340764.3340791]
   Schmidt S, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357591
   Sumdani H, 2022, WORLD NEUROSURG, V161, pE8, DOI 10.1016/j.wneu.2021.08.002
   Sun WX, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P358, DOI 10.1109/AIVR50618.2020.00073
   Veit M, 2011, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2011.5759440
   Verhey JT, 2020, INT J MED ROBOT COMP, V16, DOI 10.1002/rcs.2067
   Vuibert V., 2015, P 3 ACM S SPATIAL US, P44, DOI [10.1145/2788940.2788950, DOI 10.1145/2788940.2788950]
   Walker M, 2023, ACM T HUM-ROBOT INTE, V12, DOI 10.1145/3597623
   Wang J, 2015, P IEEE VIRT REAL ANN, P111, DOI 10.1109/VR.2015.7223332
   Weib M, 2021, IEEE T VIS COMPUT GR, V27, P1204, DOI 10.1109/TVCG.2020.3030400
   Xu XH, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.692103
   Yu DF, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445343
   Zeleznik R. C., 1993, Computer Graphics Proceedings, P81, DOI 10.1145/166117.166127
   Zhai S., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P308, DOI 10.1145/238386.238534
   Zhou M, 2012, SURG ENDOSC, V26, P1128, DOI 10.1007/s00464-011-2011-8
NR 63
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7020
EP 7030
DI 10.1109/TVCG.2024.3456172
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300016
PM 39255173
DA 2024-11-06
ER

PT J
AU Kusuyama, H
   Kageyama, Y
   Iwai, D
   Sato, K
AF Kusuyama, Hiroki
   Kageyama, Yuta
   Iwai, Daisuke
   Sato, Kosuke
TI A Multi-aperture Coaxial Projector Balancing Shadow Suppression and
   Deblurring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human-centered computing; Human computer interaction (HCI); Interaction
   devices; Displays and imagers; Computing methodologies; Computer
   graphics; Graphics systems and interfaces; Mixed / augmented reality
ID REMOVAL; SHAPE
AB This paper proposes a projection system that optically removes the cast shadow in projection mapping. Specifically, we realize the large-aperture (LA) projection using a large-format Fresnel lens to suppress cast shadows by condensing the projection light from a wide viewing angle. However, the resolution and contrast of the projected results are significantly degraded by defocus blur, veiling glare, and stray light caused by the aberration of an LA Fresnel lens. To solve the technical problems, we employ two different approaches: optical and digital image processing methods. First, we introduce a residual projector with a typical aperture lens on the same optical axis as the LA projector, projecting the residual (i.e., high-frequency) components attenuated in the LA projection. These projectors play different roles in shadow suppression and blur compensation, both achieved by projecting simultaneously. Secondly, we optimize the pair of projection images that can balance the shadow suppression and deblurring performance of our projection system. We implemented a proof-of-concept prototype and validated the above-mentioned techniques through projection experiments and a user study.
C1 [Kusuyama, Hiroki; Kageyama, Yuta; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Suita, Osaka, Japan.
C3 Osaka University
RP Kusuyama, H (corresponding author), Osaka Univ, Suita, Osaka, Japan.
EM hiroki.kusuyama@sens.sys.es.osaka-u.ac.jp
RI Iwai, Daisuke/R-8174-2019
OI Sato, Kosuke/0000-0003-1429-9990
FU JSPS KAKENHI [JP20H05958]; JST PRESTO [JPMJPR19J2]; Future Social Value
   Co-Creation Project, Osaka University
FX This work was supported by JSPS KAKENHI grant number JP20H05958, JST
   PRESTO Grant Number JPMJPR19J2, and the Future Social Value Co-Creation
   Project, Osaka University.
CR Audet S., 2007, 2007 IEEE C COMP VIS, P1
   Bandyopadhyay D, 2001, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDINGS, P207, DOI 10.1109/ISAR.2001.970539
   Bang K, 2021, IEEE T VIS COMPUT GR, V27, P2545, DOI 10.1109/TVCG.2021.3067758
   Bermano A. H., 2017, Computer Graphics Forum, V36, P1
   Bimber O, 2005, IEEE MULTIMEDIA, V12, P16, DOI 10.1109/MMUL.2005.9
   Bimber O., 2005, INT C COMPUTER GRAPH, P1
   Brown M. S., 2006, 2006 IEEE COMP SOC C, V2, P1956
   Cascini G, 2020, COMPUT IND, V123, DOI 10.1016/j.compind.2020.103308
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43
   Flagg M., 2006, P 19 ANN ACM S US IN, P244
   Fujii K, 2005, PROC CVPR IEEE, P814, DOI 10.1109/CVPR.2005.41
   Geng Y, 2018, PROC SPIE, V10676, DOI 10.1117/12.2307671
   Haitao Huang, 2022, SID Symposium Digest of Technical Papers, V53, P827, DOI 10.1002/sdtp.15620
   Hiratani K., 2023, IEEE Transactions on Visualization and Computer Graphics, V29, P2
   Hiratani K, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1309, DOI [10.1109/VR.2019.8798245, 10.1109/vr.2019.8798245]
   Iwai D., 2006, P ACM S VIRT REAL SO, P112, DOI DOI 10.1145/1180495.1180519
   Iwai D, 2018, IEEE ACCESS, V6, P6293, DOI 10.1109/ACCESS.2017.2781699
   Iwai D, 2015, IEEE T VIS COMPUT GR, V21, P462, DOI 10.1109/TVCG.2015.2391861
   Iwai D, 2014, VIRTUAL REAL-LONDON, V18, P245, DOI 10.1007/s10055-014-0250-4
   Iwai D, 2011, VIRTUAL REAL-LONDON, V15, P147, DOI 10.1007/s10055-010-0159-5
   Jaynes C, 2004, IEEE T VIS COMPUT GR, V10, P290, DOI 10.1109/TVCG.2004.1272728
   Jaynes C, 2001, IEEE VISUAL, P175, DOI 10.1109/VISUAL.2001.964509
   Jiang C., 2023, Optik, V288, P2
   Kageyama Y., 2024, IEEE Transactions on Visualization and Computer Graphics, P10
   Kageyama Y, 2022, IEEE T VIS COMPUT GR, V28, P2223, DOI 10.1109/TVCG.2022.3150465
   Kageyama Y, 2020, OPT EXPRESS, V28, P20391, DOI 10.1364/OE.396159
   Kim J, 2019, COMPUT GRAPH FORUM, V38, P443, DOI 10.1111/cgf.13541
   Kitajima Y, 2017, IEEE T VIS COMPUT GR, V23, P2419, DOI 10.1109/TVCG.2017.2734478
   Kiyokawa M., 2019, SIGGRAPH Asia 2019 Posters, SA '19, DOI [10.1145/3355056.33645512,9, DOI 10.1145/3355056.33645512,9]
   Kiyokawa M, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P597, DOI 10.1109/VRW52623.2021.00181
   Lamb D. J., 1999, SPIE, V3779, P2
   Levoy M, 2004, ACM T GRAPHIC, V23, P825, DOI 10.1145/1015706.1015806
   Li Y., 2023, IEEE Transactions on Visualization and Computer Graphics, P10
   Li YQ, 2023, Symposium Virtual Re, P449, DOI 10.1109/VR55154.2023.00060
   Marner MR, 2014, IEEE COMPUT GRAPH, V34, P74, DOI 10.1109/MCG.2014.117
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Matsushita K., 2011, P 2 AUGM HUM INT C
   Menk C, 2011, COMPUT GRAPH FORUM, V30, P2354, DOI 10.1111/j.1467-8659.2011.02066.x
   Nagase M, 2011, VIRTUAL REAL-LONDON, V15, P119, DOI 10.1007/s10055-010-0168-4
   Nishino H, 2018, ANN SURG, V267, P1134, DOI 10.1097/SLA.0000000000002172
   Nomoto Takashi, 2020, SA '20: SIGGRAPH Asia 2020 Emerging Technologies, DOI 10.1145/3415255.3422888
   Okuda S., 2021, SPIE, V11766, P334, DOI [10.1117/12.25910299, DOI 10.1117/12.25910299]
   Oyamada Y., 2007, 2007 IEEE C COMP VIS, P1
   Pejsa T, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P1716, DOI 10.1145/2818048.2819965
   Raskar R, 2004, ACM T GRAPHIC, V23, P406, DOI 10.1145/1015706.1015738
   Raskar R., 1998, P 25 ANN C COMP GRAP, P188
   Rivers A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366176
   Rodriguez L., 2015, Procedia computer science, V75, P8
   Sakai T, 2019, OPT REV, V26, P447, DOI 10.1007/s10043-019-00530-6
   Sand Oliver, 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P643, DOI 10.1007/978-3-319-39907-2_61
   Schmidt S, 2019, COMPUT GRAPH-UK, V83, P1, DOI 10.1016/j.cag.2019.06.002
   Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257
   Siegl C, 2017, IEEE T VIS COMPUT GR, V23, P2440, DOI 10.1109/TVCG.2017.2734428
   Sugaya Y., 2010, 2010 IEEE COMP SOC C, P96
   Sukthankar R, 2001, PROC CVPR IEEE, P151
   Summet Jay., 2005, CHI'05 Extended Abstracts, P1997
   Takezawa T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P251, DOI [10.1109/vr.2019.8797923, 10.1109/VR.2019.8797923]
   Tsukamoto J, 2017, COMPUT GRAPH FORUM, V36, P369, DOI 10.1111/cgf.13085
   Tsukamoto J, 2015, IEEE T VIS COMPUT GR, V21, P1221, DOI 10.1109/TVCG.2015.2459905
   Underkoffler J., 1999, P SIGCHI C HUM FACT, P393
   Wang L., 2023, Optics Express, V31, P9
   Wang Y., 2024, IEEE Transactions on Visualization and Com- puter Graphics
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wetzstein G., 2007, Technical report, P3
   Yamamoto K., 2022, IEEE Transactions on Visualization and Computer Graphics
   Yoshida T., 2003, P VSMM, V3, P1
   Zhang L, 2006, ACM T GRAPHIC, V25, P907, DOI 10.1145/1141911.1141974
NR 69
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7031
EP 7041
DI 10.1109/TVCG.2024.3456183
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300035
PM 39255117
OA Green Published
DA 2024-11-06
ER

PT J
AU Zhang, V
   Albers, A
   Saeedi-Givi, C
   Kristensson, PO
   Bohné, T
   Tadeja, S
AF Zhang, Vicky
   Albers, Alexander
   Saeedi-Givi, Christine
   Kristensson, Per Ola
   Bohne, Thomas
   Tadeja, Slawomir
TI Should I Evaluate My Augmented Reality System in an Industrial
   Environment? Investigating the Effects of Classroom and Shop Floor
   Settings on Guided Assembly
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Assembly; Manuals; Manufacturing; Laboratories; Resists; Noise;
   Industries; Augmented reality; manual assembly; user study; classroom
   setting; shop floor setting; augmented reality guidelines
ID ECOLOGICAL VALIDITY; PERFORMANCE; ASSISTANCE; CHOKING; ANXIETY; SAFETY
AB Numerous prior studies have investigated real-time assembly instructions using Augmented Reality (AR). However, most such experiments were conducted in laboratory settings with simplistic assembly tasks, failing to represent real-world industrial conditions. To ascertain to what extent results obtained in a laboratory environment may differ from studies in actual industrial environments, we carried out a user study with 32 manufacturing apprentices. We compared assembly task execution results in two settings, a classroom and an industrial workshop environment. To facilitate the experiments, we developed AR-guided manual assembly systems for simple and more complex assets. Our findings reveal a significantly improved task performance in the industrial workshop, reflected in faster task completion times, fewer errors, and subjectively perceived higher flow. This contradicted participants' subjective ratings, as they expected to perform better in the classroom environment. Our results suggest that the actual manufacturing environment is critical in evaluating AR systems for real-world industrial applications.
C1 [Zhang, Vicky; Albers, Alexander; Saeedi-Givi, Christine; Kristensson, Per Ola; Bohne, Thomas; Tadeja, Slawomir] Univ Cambridge, Cambridge, England.
   [Albers, Alexander] Karlsruhe Inst Technol, Karlsruhe, Germany.
   [Saeedi-Givi, Christine] Univ Konstanz, Constance, Germany.
C3 University of Cambridge; Helmholtz Association; Karlsruhe Institute of
   Technology; University of Konstanz
RP Tadeja, S (corresponding author), Univ Cambridge, Cambridge, England.
EM slawomir.tadeja@gmail.com
RI Zhang, Vicky/IYN-1630-2023; Dastan, Mine/LFR-9751-2024
OI Dastan, Mine/0000-0003-0555-155X; Kristensson, Per
   Ola/0000-0002-7139-871X; Bohne, Thomas/0000-0001-5986-8638
FU Engineering and Physical Sciences Research Council (EPSRC)
   [EP/V062123/1]
FX This work was supported by Engineering and Physical Sciences Research
   Council (EPSRC) grant no. EP/V062123/1.
CR Alem L, 2011, LECT NOTES COMPUT SC, V6949, P442, DOI 10.1007/978-3-642-23768-3_53
   Alves J, 2019, IEEE INT CONF AUTON, P168
   Alves JB, 2022, VIRTUAL REAL-LONDON, V26, P235, DOI 10.1007/s10055-021-00557-8
   Andrade C, 2018, INDIAN J PSYCHOL MED, V40, P498, DOI 10.4103/IJPSYM.IJPSYM_334_18
   Applebaum D, 2010, J NURS ADMIN, V40, P323, DOI 10.1097/NNA.0b013e3181e9393b
   Arige A. D., 2022, 27 WEB3D 2022, DOI [10.1145/3564533.3564568, DOI 10.1145/3564533.3564568]
   Atici-Ulusu H, 2021, INT J COMPUT INTEG M, V34, P487, DOI 10.1080/0951192X.2021.1901314
   Baudin M., 2023, Introduction to manufacturing: an industrial engineering and management perspective, V1, P2
   Beilock SL, 2008, CURR DIR PSYCHOL SCI, V17, P339, DOI 10.1111/j.1467-8721.2008.00602.x
   Bendzioch S, 2020, ADV INTELL SYST, V1026, P20, DOI 10.1007/978-3-030-27928-8_4
   BOMMER WH, 1995, PERS PSYCHOL, V48, P587, DOI 10.1111/j.1744-6570.1995.tb01772.x
   Braun V., 2012, APA Handbook of Research Methods in Psychology Volume 2: Research designs: Quantitative, qualitative, neuropsychological, and biological, V2, P57, DOI DOI 10.1037/13620-004
   Brooke J., 1995, USABILITY EVAL IND, P189
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Carvalho AF, 2021, AGING MENT HEALTH, V25, P468, DOI 10.1080/13607863.2019.1699021
   Costa GD, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072725
   DAVIS FD, 1989, MANAGE SCI, V35, P982, DOI 10.1287/mnsc.35.8.982
   DeCaro MS, 2011, J EXP PSYCHOL GEN, V140, P390, DOI 10.1037/a0023466
   Deshpande A, 2018, ADV ENG INFORM, V38, P760, DOI 10.1016/j.aei.2018.10.004
   Dey A, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00037
   Drouot M, 2021, DISPLAYS, V66, DOI 10.1016/j.displa.2021.101987
   Eder M., 2021, Proceedings of the CLF, DOI [10.2139/ssrn.38584562, DOI 10.2139/SSRN.38584562]
   Embrey D., 1986, Proceedings of the International Topical Meeting on Advances in Human Factors in Nuclear Power Systems. 1 January, P184
   Engeser S, 2012, ADVANCES IN FLOW RESEARCH, P1, DOI 10.1007/978-1-4614-2359-1
   Fang W, 2023, ROBOT CIM-INT MANUF, V83, DOI 10.1016/j.rcim.2023.102567
   Galy E, 2018, ERGONOMICS, V61, P517, DOI 10.1080/00140139.2017.1369583
   GLASS GV, 1972, REV EDUC RES, V42, P237, DOI 10.3102/00346543042003237
   HART S G, 1988, P139
   Horejsi P, 2020, IEEE ACCESS, V8, P94330, DOI 10.1109/ACCESS.2020.2994650
   Hou L, 2015, J COMPUT CIVIL ENG, V29, DOI 10.1061/(ASCE)CP.1943-5487.0000344
   Hou L, 2013, J COMPUT CIVIL ENG, V27, P439, DOI 10.1061/(ASCE)CP.1943-5487.0000184
   Illing Jannike, 2020, MUM 2020: 19th International Conference on Mobile and Ubiquitous Multimedia, P277, DOI 10.1145/3428361.3428398
   International-Noise-Awareness-Day, Common noise levels-how loud is too loud?
   Irrazabal N, 2016, APPL COGNITIVE PSYCH, V30, P1052, DOI 10.1002/acp.3299
   Joshi S. J., 2022, Integration of intelligent manufacturing in smart factories as part of industry, P1, DOI [10.1109/SPICON56577.2022.10180471, DOI 10.1109/SPICON56577.2022.10180471]
   Kaufeld M, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102283
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kilgour P., 2006, PhD Dissertation, P8
   Kim S, 2016, IISE T OCCUP ERG HUM, V4, P253, DOI 10.1080/21577323.2016.1214635
   Krupenia S., 2006, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P1638, DOI [DOI 10.1177/154193120605001626, 10.1177/154193120605001626]
   Lampen E, 2019, PROC CIRP, V81, P588, DOI 10.1016/j.procir.2019.03.160
   Lavric T., 2021, An industry-adapted ar training method for manual assembly operations, P282, DOI [10.1007/978-3-030-90963-5_22 2,9, DOI 10.1007/978-3-030-90963-5_222,9]
   Lewkowicz DJ, 2001, INFANCY, V2, P437, DOI 10.1207/S15327078IN0204_03
   Liu D, 2009, ANESTH ANALG, V109, P1135, DOI 10.1213/ANE.0b013e3181b5a200
   Lotsaris Konstantinos., 2021, Procedia CIRP, V96, P301, DOI DOI 10.1016/J.PROCIR.2021.01.091
   Lysakowski M., 2023 IEEE ISMAR ADJU, V3, P7
   Maio R., 2023, Real-time data monitoring of an industry 4.0 assembly line using pervasive augmented reality: First impressions, P414, DOI [10.1109/VRW58643.2023.000902,3,9, DOI 10.1109/VRW58643.2023.000902,3,9]
   Marino E, 2021, COMPUT IND, V127, DOI 10.1016/j.compind.2021.103412
   Marques Bernardo, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3427324
   Marques B, 2023, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2023, P234, DOI 10.1145/3577190.3614134
   Mattsson S, 2016, IFAC PAPERSONLINE, V49, P209, DOI 10.1016/j.ifacol.2016.07.598
   Merchant K. A., 2010, AFAANZ C, P1
   Merino L, 2020, INT SYM MIX AUGMENT, P438, DOI 10.1109/ISMAR50242.2020.00069
   Microsoft, 2022, What is Mixed Reality Toolkit 2, P3
   Microsoft-Corp, 2019, HoloLens 2 Technical Specifications, P3
   Tran TTM, 2023, IEEE T VIS COMPUT GR, V29, P4782, DOI 10.1109/TVCG.2023.3320231
   Moher D, 2009, ANN INTERN MED, V151, P264, DOI [10.1136/bmj.i4086, 10.1186/2046-4053-4-1, 10.1136/bmj.b2535, 10.1136/bmj.b2700, 10.1016/j.ijsu.2010.02.007, 10.1016/j.ijsu.2010.07.299, 10.1371/journal.pmed.1000097]
   Moser T., 2019, Mixed Reality Applications in Industry: Challenges and Research Areas, V338, P95, DOI [10.1007/978-3-030-05767-1_7 1, DOI 10.1007/978-3-030-05767-1_71]
   Neb A, 2018, PROC CIRP, V72, P1118, DOI 10.1016/j.procir.2018.03.210
   Nelson-Le GallS., 1981, Developmental Review, V1, P224, DOI [10.1016/0273-2297(81)90019-8, DOI 10.1016/0273-2297(81)90019-8]
   Norouzi N, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P17, DOI 10.1145/3267851.3267901
   Osborne M, 2019, PROC INT CONF EDU IN, P142, DOI 10.1109/EITT.2019.00035
   Palmarini R, 2018, ROBOT CIM-INT MANUF, V49, P215, DOI 10.1016/j.rcim.2017.06.002
   Peniche A., 2012, Combining virtual and augmented reality to improve the mechanical assembly training process in manufacturing, P2
   PTC, 2023, Vuforia -Model Targets Supported Objects
   Qin YM, 2023, J INF TECHNOL CONSTR, V28, DOI 10.36680/j.itcon.2023.004
   Ratcliffe J, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445170
   Rice M., 2016, P CHI C HUM FACT COM, P2279, DOI [10.1145/2851581.28923472, DOI 10.1145/2851581.28923472]
   Schuster F., 2020, A user study on AR-assisted industrial assembly, P135, DOI [10.1109/ISMAR-Adjunct51615.2020.000472, DOI 10.1109/ISMAR-ADJUNCT51615.2020.000472]
   Schuster F, 2021, J MANUF SYST, V61, P660, DOI 10.1016/j.jmsy.2020.12.012
   Seeliger Arne, 2022, Procedia CIRP, P570, DOI 10.1016/j.procir.2022.05.027
   Shen Y., 2023, Information, V14, DOI [10.3390/info140201002, DOI 10.3390/INFO140201002]
   Siegel J, 1997, FIRST INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P18, DOI 10.1109/ISWC.1997.629914
   Steed A, 2023, FRONT COMP SCI-SWITZ, V4, DOI 10.3389/fcomp.2022.966319
   Syberfeldt A, 2016, PROC CIRP, V44, P108, DOI 10.1016/j.procir.2016.02.017
   Tadeja SK, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P457, DOI 10.1109/VRW55335.2022.00103
   Tadeja SK, 2021, INT SYM MIX AUGMENT, P376, DOI 10.1109/ISMAR52148.2021.00054
   Tang A., 2003, COMP EFFECTIVENESS A, P73
   Tu PX, 2021, COMPUT BIOL MED, V133, DOI 10.1016/j.compbiomed.2021.104402
   Visometry, 2023, VisionLib-Augmented Reality Tracking for Industries
   Wang J, 2004, J SCI MED SPORT, V7, P174, DOI 10.1016/S1440-2440(04)80007-0
   Werrlich S, 2017, 10TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2017), P416, DOI 10.1145/3056540.3076190
   Wu SZ, 2023, AUTOMAT CONSTR, V149, DOI 10.1016/j.autcon.2023.104802
   Zigart T, 2022, IEEE INT SYMP M AU R, P60, DOI 10.1109/ISMAR-Adjunct57072.2022.00022
NR 84
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7042
EP 7052
DI 10.1109/TVCG.2024.3456206
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300032
PM 39250381
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Dastan, M
   Fiorentino, M
   Walter, ED
   Diegritz, C
   Uva, AE
   Eck, U
   Navab, N
AF Dastan, Mine
   Fiorentino, Michele
   Walter, Elias D.
   Diegritz, Christian
   Uva, Antonio E.
   Eck, Ulrich
   Navab, Nassir
TI Co-Designing Dynamic Mixed Reality Drill Positioning Widgets: A
   Collaborative Approach with Dentists in a Realistic Setup
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dentistry; Visualization; Three-dimensional displays; Phantoms;
   Implants; Surgery; Navigation; Dynamic widgets; precise tool
   positioning; usability testing; co-design; dentistry; mixed reality
ID GUIDED IMPLANT-SURGERY; AUGMENTED REALITY; INTERFACES; NAVIGATION;
   TEMPLATE; ACCURACY; SURVIVAL; DISTANCE; SYSTEM
AB Mixed Reality (MR) is proven in the literature to support precise spatial dental drill positioning by superimposing 3D widgets. Despite this, the related knowledge about widget's visual design and interactive user feedback is still limited. Therefore, this study is contributed to by co-designed MR drill tool positioning widgets with two expert dentists and three MR experts. The results of co-design are two static widgets (SWs): a simple entry point, a target axis, and two dynamic widgets (DWs), variants of dynamic error visualization with and without a target axis (DWTA and DWEP). We evaluated the co-designed widgets in a virtual reality simulation supported by a realistic setup with a tracked phantom patient, a virtual magnifying loupe, and a dentist's foot pedal. The user study involved 35 dentists with various backgrounds and years of experience. The findings demonstrated significant results; DWs outperform SWs in positional and rotational precision, especially with younger generations and subjects with gaming experiences. The user preference remains for DWs (19) instead of SWs (16). However, findings indicated that the precision positively correlates with the time trade-off. The post-experience questionnaire (NASA-TLX) showed that DWs increase mental and physical demand, effort, and frustration more than SWs. Comparisons between DWEP and DWTA show that the DW's complexity level influences time, physical and mental demands. The DWs are extensible to diverse medical and industrial scenarios that demand precision.
C1 [Dastan, Mine; Fiorentino, Michele; Uva, Antonio E.] Polytech Univ Bari, Bari, Italy.
   [Walter, Elias D.; Diegritz, Christian] Ludwig Maximilian Univ Munich, Munich, Germany.
   [Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Munich, Germany.
C3 Politecnico di Bari; University of Munich; Technical University of
   Munich
RP Dastan, M (corresponding author), Polytech Univ Bari, Bari, Italy.
EM mine.dastan@poliba.it
RI Uva, Antonio/A-9673-2012; Dastan, Mine/LFR-9751-2024; Fiorentino,
   Michele/M-6976-2015
OI Dastan, Mine/0000-0003-0555-155X; Eck, Ulrich/0000-0002-5322-4724;
   Fiorentino, Michele/0000-0003-2197-6574
FU Italian Ministry of Education, University and Research (MUR)
   [L.232/2016]
FX Support from the Italian Ministry of Education, University and Research
   (MUR) under the program "Departments of Excellence"(L.232/2016). The
   authors express their gratitude to the dentists at the Department of
   Conservative Dentistry and Periodontology, LMUK linikum Hospital,
   Munich, for their contribution of the user study.
CR Aldosari Mohammad A, 2021, J Contemp Dent Pract, V22, P310
   Bertollo N., 2011, Biomechanics in Applications, P53, DOI DOI 10.5772/209311
   Besançon L, 2021, COMPUT GRAPH FORUM, V40, P293, DOI 10.1111/cgf.14189
   Bird M, 2021, Res Involv Engagem, V7, P12, DOI 10.1186/s40900-021-00252-7
   Block MS, 2017, J ORAL MAXIL SURG, V75, P1377, DOI 10.1016/j.joms.2017.02.026
   Branson BG, 2018, WORK, V59, P131, DOI 10.3233/WOR-172681
   Burström G, 2021, ACTA NEUROCHIR, V163, P843, DOI 10.1007/s00701-021-04708-3
   Busciantella-Ricci D, 2024, DES SCI, V10, DOI 10.1017/dsj.2023.35
   Cassetta M, 2020, INT J ORAL MAX SURG, V49, P1335, DOI 10.1016/j.ijom.2020.03.007
   Cassetta M, 2017, INT J ORAL MAX SURG, V46, P922, DOI 10.1016/j.ijom.2017.03.010
   Chackartchi T, 2022, PERIODONTOL 2000, V88, P64, DOI 10.1111/prd.12411
   Conner D.B., 1992, P S INTERACTIVE 3D G, P183, DOI DOI 10.1145/147156.147199
   Dastan M, 2022, INT SYM MIX AUGMENT, P187, DOI 10.1109/ISMAR55827.2022.00033
   Duré M, 2021, INT J COMPUT DENT, V24, P9
   El Jamiy F, 2019, IET IMAGE PROCESS, V13, P707, DOI 10.1049/iet-ipr.2018.5920
   El-Jarn H., 2020, Journal of Work-Applied Management, V12, P191, DOI [10.1108/JWAM-04-2020-0022, DOI 10.1108/JWAM-04-2020-0022]
   Elani HW, 2018, J DENT RES, V97, P1424, DOI 10.1177/0022034518792567
   Eom S, 2022, INT SYM MIX AUGMENT, P355, DOI 10.1109/ISMAR55827.2022.00051
   Fahim S, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12083719
   Farronato M, 2023, J DENT, V132, DOI 10.1016/j.jdent.2023.104476
   Farronato M, 2019, BMC ORAL HEALTH, V19, DOI 10.1186/s12903-019-0808-3
   Freudenthal A, 2011, J BIOMED INFORM, V44, P198, DOI 10.1016/j.jbi.2010.11.006
   Galindo-Moreno P, 2017, CLIN ORAL IMPLAN RES, V28, P704, DOI 10.1111/clr.12867
   Greenstein G., 2015, Compend Contin Educ Dent, V36, P1
   Harris DJ, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00605
   Harrison R, 2022, J ROY SOC MED, V115, P48, DOI 10.1177/01410768211070206
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Henderson SJ, 2011, INT SYM MIX AUGMENT
   Joda T, 2019, COMPUT BIOL MED, V108, P93, DOI 10.1016/j.compbiomed.2019.03.012
   Jung RE, 2008, CLIN ORAL IMPLAN RES, V19, P119, DOI 10.1111/j.1600-0501.2007.01453.x
   Kaplan AD, 2021, HUM FACTORS, V63, P706, DOI 10.1177/0018720820904229
   Katic D, 2010, LECT NOTES COMPUT SC, V6326, P531
   Kivovics M, 2022, J DENT, V119, DOI 10.1016/j.jdent.2022.104070
   Lee C, 2010, P IEEE VIRT REAL ANN, P11, DOI 10.1109/VR.2010.5444820
   Lee C, 2009, INT SYM MIX AUGMENT, P203, DOI 10.1109/ISMAR.2009.5336464
   Lim Y., 2007, Proceedings of the Conference on Designing Pleasurable Products and interfaces (DPPI'07), P239, DOI DOI 10.1145/1314161.1314183
   Lin YK, 2015, CLIN IMPLANT DENT R, V17, P543, DOI 10.1111/cid.12119
   Ma LF, 2023, PHYS MED BIOL, V68, DOI 10.1088/1361-6560/acaf23
   Ma LF, 2019, MED BIOL ENG COMPUT, V57, P47, DOI 10.1007/s11517-018-1861-9
   Markovic J, 2024, J ESTHET RESTOR DENT, V36, P207, DOI 10.1111/jerd.13171
   Martin-Gonzalez A, 2009, INT SYM MIX AUGMENT, P207, DOI 10.1109/ISMAR.2009.5336459
   Mendes D, 2019, COMPUT GRAPH FORUM, V38, P21, DOI 10.1111/cgf.13390
   Mendes D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P261, DOI 10.1145/2993369.2993396
   Mine M. R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P19, DOI 10.1145/258734.258747
   Mistry A, 2021, DENT J-BASEL, V9, DOI 10.3390/dj9090099
   Mühler K, 2010, IEEE T VIS COMPUT GR, V16, P133, DOI 10.1109/TVCG.2009.58
   Nyumba TO, 2018, METHODS ECOL EVOL, V9, P20, DOI 10.1111/2041-210X.12860
   Ohlendorf D, 2017, BMC MUSCULOSKEL DIS, V18, DOI 10.1186/s12891-017-1650-x
   Park HS, 2015, J PHYS THER SCI, V27, P3651, DOI 10.1589/jpts.27.3651
   Pellegrino G, 2021, INT J ORAL MAX IMPL, V36, pE121, DOI 10.11607/jomi.8770
   Pietruski P, 2019, J CRANIO MAXILL SURG, V47, P854, DOI 10.1016/j.jcms.2019.03.004
   Preim B., 2007, Visualization in medicine: theory, algorithms, and applications, P2
   Qian L, 2022, IEEE T VIS COMPUT GR, V28, P2550, DOI 10.1109/TVCG.2020.3037284
   Raikar S, 2017, J INT SOC PREV COMMU, V7, P351, DOI 10.4103/jispcd.JISPCD_380_17
   Schneider D, 2009, CLIN ORAL IMPLAN RES, V20, P73, DOI 10.1111/j.1600-0501.2009.01788.x
   Schubert O, 2019, BRIT DENT J, V226, P101, DOI 10.1038/sj.bdj.2019.44
   Shaalan D, 2020, INT C ELECT COMPUT, DOI 10.1109/ecai50035.2020.9223236
   Slattery P, 2020, HEALTH RES POLICY SY, V18, DOI 10.1186/s12961-020-0528-9
   Song TY, 2018, HEALTHC TECHNOL LETT, V5, P201, DOI 10.1049/htl.2018.5062
   Spille J, 2022, DENT J-BASEL, V10, DOI 10.3390/dj10100187
   Takács A, 2023, J DENT, V139, DOI 10.1016/j.jdent.2023.104748
   Tao B., 2023, Journal of Dental Sciences, DOI [10.1016/j.jds.2023.05.0062,3,4,5, DOI 10.1016/J.JDS.2023.05.0062,3,4,5]
   Tao BX, 2024, J DENT SCI, V19, P196, DOI 10.1016/j.jds.2023.05.006
   Unity, Unity
   Volmer B, 2023, IEEE T VIS COMPUT GR, V29, P4449, DOI 10.1109/TVCG.2023.3320246
   Waelkens P., 2016, Radioguided Surgery, P57, DOI [DOI 10.1007/978-3-319-26051-8_4, 10.1007/978-3-319-26051-84, DOI 10.1007/978-3-319-26051-84]
   Wajngarten D, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0259768
   Wang JC, 2014, IEEE T BIO-MED ENG, V61, P1295, DOI 10.1109/TBME.2014.2301191
   Wang WY, 2023, EUR J DENT EDUC, V27, P438, DOI 10.1111/eje.12825
   Wu BZ, 2024, CLIN ORAL IMPLAN RES, V35, P386, DOI 10.1111/clr.14237
   Yari A, 2024, J STOMATOL ORAL MAXI, V125, DOI 10.1016/j.jormas.2023.101749
   Yeo AD, 2024, ELECTRONICS-SWITZ, V13, DOI 10.3390/electronics13030600
   ZHAI SM, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P155, DOI 10.1109/VRAIS.1993.380784
   Zhu M, 2017, SCI REP-UK, V7, DOI 10.1038/srep42365
NR 74
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7053
EP 7063
DI 10.1109/TVCG.2024.3456170
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300004
PM 39250405
DA 2024-11-06
ER

PT J
AU Enriquez, D
   Moon, H
   Bowman, DA
   Jeon, M
   Lee, SW
AF Enriquez, Daniel
   Moon, Hayoun
   Bowman, Doug A.
   Jeon, Myounghoon
   Lee, Sang Won
TI Investigating Object Translation in Room-scale, Handheld Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Resists; Performance evaluation; Usability;
   Virtual environments; Predictive models; Moon; Virtual Reality;
   Augmented Reality; Handheld VR; 3D manipulation
ID INFORMATION CAPACITY; AUGMENTED REALITY; FITTS LAW
AB Handheld devices have become an inclusive alternative to head-mounted displays in virtual reality (VR) environments, enhancing accessibility and allowing cross-device collaboration. Object manipulation techniques in 3D space with handheld devices, such as those in handheld augmented reality (AR), have been typically evaluated in table-top scale and we currently lack an understanding of how these techniques perform in larger scale environments. We conducted two studies, each with 30 participants, to investigate how different techniques impact usability and performance for room-scale handheld VR object translations. We compared three translation techniques that are similar to commonly studied techniques in handheld AR: 3DSlide, VirtualGrasp, and Joystick. We also examined the effects of target size, target distance, and user mobility conditions (stationary vs. moving). Results indicated that the Joystick technique, which allowed translation in relation to the user's perspective, was the fastest and most preferred, without difference in precision. Our findings provide insights for designing room-scale handheld VR systems, with potential implications for mixed reality systems involving handheld devices.
C1 [Enriquez, Daniel; Moon, Hayoun; Bowman, Doug A.; Jeon, Myounghoon; Lee, Sang Won] Virginia Tech, Ctr Human Comp Interact, Blacksburg, VA 24061 USA.
C3 Virginia Polytechnic Institute & State University
RP Enriquez, D (corresponding author), Virginia Tech, Ctr Human Comp Interact, Blacksburg, VA 24061 USA.
EM denriquez@vt.edu; moonhy@vt.edu; dbowman@vt.edu; myounghoonjeon@vt.edu;
   sangwonlee@vt.edu
OI Jeon, Myounghoon/0000-0003-2908-671X; Enriquez,
   Daniel/0000-0002-1903-6615; Lee, Sang Won/0000-0002-1026-315X; Bowman,
   Doug/0000-0003-0491-5067
FU National Science Foundation [NSF 2119011]
FX This work is based upon work supported by the National Science
   Foundation under Grant No. NSF 2119011. We thank our user study
   participants for their time, effort, and feedback.
CR Accot J., 2003, P SIGCHI C HUM FACT, P193, DOI DOI 10.1145/642611.642646
   Alienware, 2020, Alienware aurora r12 gaming desktop
   Bai H., 2013, SIGGRAPH ASIA 2013 S, P1
   Bambusek D, 2023, Symposium Virtual Re, P115, DOI 10.1109/VR55154.2023.00027
   Benko H, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P79
   Bowman D. A., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P35, DOI 10.1145/253284.253301
   Bowman D. A., 2012, INT IND TRAIN SIM ED, V4, P44
   Cha Y, 2013, INT J IND ERGONOM, V43, P350, DOI 10.1016/j.ergon.2013.05.005
   Clark LD, 2020, INT J HUM-COMPUT ST, V139, DOI 10.1016/j.ijhcs.2020.102413
   Crotte AM, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312777
   D. G. Bianca-Cerasela-Zelia Blaga, Dar: Implementation of a drone augmented reality video game, P2
   Drey T, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581004
   Drey T, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517641
   El Jamiy F, 2019, IET IMAGE PROCESS, V13, P707, DOI 10.1049/iet-ipr.2018.5920
   FITTS PM, 1964, J EXP PSYCHOL, V67, P103, DOI 10.1037/h0045689
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Freiwald JP, 2021, LECT NOTES COMPUT SC, V12932, P352, DOI 10.1007/978-3-030-85623-6_22
   Gillan D. J., 1990, SIGCHI Bulletin, P227
   Goh ES, 2019, IEEE ACCESS, V7, P40581, DOI 10.1109/ACCESS.2019.2906394
   Gugenheimer J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4021, DOI 10.1145/3025453.3025683
   Ha T, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P91, DOI 10.1109/3DUI.2010.5444713
   HART S G, 1988, P139
   Hassan SA, 2022, UNIVERSAL ACCESS INF, V21, P545, DOI 10.1007/s10209-020-00790-z
   Hellmuth Carl-Philipp, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P604, DOI 10.1145/3462244.3479945
   HOFFMANN ER, 1995, ERGONOMICS, V38, P828, DOI 10.1080/00140139508925153
   Hoffmann ER, 2011, ERGONOMICS, V54, P1175, DOI 10.1080/00140139.2011.614356
   HTC, 2011, Vivetm | discover virtual reality beyond imagination
   Isaza C. A. B., 2024, P INT ACM C COMP SUP, P8
   Jacob R.J. K., 1994, ACM Transactions Computer-Human Interaction, V1, P3, DOI DOI 10.1145/174630.174631
   Jerald J., 2015, Morgan & Claypool, V2, P9
   Kaimara P, 2022, VIRTUAL REAL-LONDON, V26, P697, DOI 10.1007/s10055-021-00563-w
   Kawakita J., 1991, The Original KJ Method, P5
   Kerdvibulvech C, 2019, LECT NOTES COMPUT SC, V11786, P233, DOI 10.1007/978-3-030-30033-3_18
   Kumaravel Balasaravanan Thoravi, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P182, DOI 10.1145/3379337.3415827
   LaViola Jr J. J., 2017, 3D user interfaces: theory and practice, V1, P2
   Lee G. A., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P245, DOI 10.1109/ISMAR.2011.6092398
   Lyu H., 2022, CHI C HUM FACT COMP, P1
   MacKenzie I. S., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P219, DOI 10.1145/142750.142794
   MACKENZIE IS, 1989, J MOTOR BEHAV, V21, P323
   Mboya A. M., 2020, The oculus go wasn't designed for black hair
   MILGRAM P, 1994, P SOC PHOTO-OPT INS, V2351, P282
   Mossel A., 2013, Proc. Virtual Real. Int. Conf. Laval Virtual - VRIC '13, P1, DOI DOI 10.1145/2466816.2466829
   Murata A, 2001, HUM MOVEMENT SCI, V20, P791, DOI 10.1016/S0167-9457(01)00058-6
   Nebeling M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376637
   Nor'a Muhammad Nur Affendy, 2020, 2020 IEEE 5th International Conference on Computing Communication and Automation (ICCCA), P161, DOI 10.1109/ICCCA49541.2020.9250913
   Owlchemy, Owlchemy lab report: Mobile spectator-ar spectator camera experiment
   Photon, Photon unity 3d networking framework sdks and game backend | photon engine
   Polvi J, 2016, COMPUT GRAPH-UK, V55, P33, DOI 10.1016/j.cag.2015.10.013
   Samsung, 2021, Galaxy tab a7 lite
   Santos M. E. C., 2014, P 20 ACM S VIRT REAL, P167, DOI DOI 10.1145/2671015.2671019
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Steam, 2016, Steamvr on steam
   Triantafyllidis E, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3443442
   Unity, Unity-unity
   Wither J, 2005, NINTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P92, DOI 10.1109/ISWC.2005.41
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Yusof CS, 2016, J TEKNOL, V78, P15
NR 57
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7064
EP 7074
DI 10.1109/TVCG.2024.3456208
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300008
PM 39250392
DA 2024-11-06
ER

PT J
AU Wan, TJ
   Zhang, LYT
   Xu, YX
   Guo, ZX
   Gao, BY
   Liang, HN
AF Wan, Tingjie
   Zhang, Liangyuting
   Xu, Yunxin
   Guo, Zixuan
   Gao, Boyu
   Liang, Hai-Ning
TI Analysis and Design of Efficient Authentication Techniques for Password
   Entry with the Qwerty Keyboard for VR Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Keyboards; Passwords; Layout; Authentication; Security; Biometrics; User
   experience; Virtual reality; text entry; password; keyboard layout; user
   study
ID INPUT
AB Authentication in digital security relies heavily on text-based passwords, even with other available methods like biometrics and graphical passwords. While virtual reality (VR) keyboards are typically invisible to onlookers, the presence of inconspicuous sensors, including accelerometers, gyroscopes, and barometers, poses a potential risk of unauthorized observation and recording. Traditional defense shoulder-surfing attack methods typically involve breaking apart the Qwerty layout, which destroys the user's inherent familiarity with the layout. This research addresses the need for secure password entry in VR environments while retaining the Qwerty layout. We explore three keyboard-related position alteration strategies to ensure security while mitigating the decline in user experience. These strategies involve moving the entire keyboard, cursor, and keys. Our theoretical study assesses the effectiveness of these strategies against shoulder-surfing attacks. Two user studies, employing ray-based and position-based text entry methods, respectively, evaluate the practical effectiveness of the three strategies in resisting shoulder-surfing attacks, as well as their impact on typing performance and user experience. Our findings demonstrate that the three strategies achieve shoulder-surfing attack resistance comparable to a random layout keyboard. Moreover, compared to a random layout, the two strategies involving the movement of the entire keyboard and the repositioning of keys support faster entry rates and enhanced user experience.
C1 [Wan, Tingjie; Zhang, Liangyuting; Xu, Yunxin; Guo, Zixuan] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou, Peoples R China.
   [Wan, Tingjie; Guo, Zixuan] Univ Liverpool, Liverpool, England.
   [Gao, Boyu] Jinan Univ, Coll Cyber Secur, Guangzhou, Peoples R China.
   [Gao, Boyu] Jinan Univ, Guangdong Inst Smart Educ, Guangzhou, Peoples R China.
   [Liang, Hai-Ning] Hong Kong Univ Sci & Technol Guangzhou, Computat Media & Arts Thrust, Guangzhou, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; University of Liverpool; Jinan
   University; Jinan University; Hong Kong University of Science &
   Technology (Guangzhou)
RP Liang, HN (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Computat Media & Arts Thrust, Guangzhou, Peoples R China.
EM hainingliang@hkust-gz.edu.cn
RI Gao, Boyu/JNE-3525-2023
OI Gao, BoYu/0000-0001-8523-2828; Liang, Hai-Ning/0000-0003-3600-8955; Xu,
   Yunxin/0009-0007-5671-2516
FU Suzhou Municipal Key Laboratory for Intelligent Virtual Engineering
   [SZS2022004]; Natural Science Foundation of China [62372212]; Guangdong
   Province Science Foundation [2024A1515011515]
FX This research was funded in part by the Suzhou Municipal Key Laboratory
   for Intelligent Virtual Engineering (#SZS2022004), the Natural Science
   Foundation of China (#62372212), and the Guangdong Province Science
   Foundation (#2024A1515011515).
CR Ahmad SMS, 2012, INT J INNOV COMPUT I, V8, P7983
   Ahuja Karan, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3214260
   Awad M, 2016, INT C ELECT DEVICE S
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Binbeshr F, 2021, COMPUT SECUR, V101, DOI 10.1016/j.cose.2020.102116
   Boletsis C, 2019, TECHNOLOGIES, V7, DOI 10.3390/technologies7020031
   Boletsis Costas., 2019, INT J VIRTUAL REALIT, V19, P2, DOI DOI 10.20870/IJVR.2019.19.3.2917
   Boneh D., 2017, P 3 S US PRIV SEC SO, P13, DOI DOI 10.1145/1280680.1280683
   Bosnjak L, 2020, COMPUT SECUR, V99, DOI 10.1016/j.cose.2020.102023
   Brickler D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P28, DOI [10.1109/VR.2019.8797744, 10.1109/vr.2019.8797744]
   Brooke J., 1995, USABILITY EVAL IND, P189
   Chen SB, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312762
   Clemens IAH, 2012, J VISION, V12, DOI 10.1167/12.12.8
   Derby Jessyca L., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P1476, DOI 10.1177/1071181319631279
   Dube TJ, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382882
   Dudley JJ, 2019, INT SYM MIX AUGMENT, P289, DOI 10.1109/ISMAR.2019.00027
   Dudley JJ, 2018, ACM T COMPUT-HUM INT, V25, DOI 10.1145/3232163
   Finn R. L., 2013, Seven Types of Privacy, P3, DOI [DOI 10.1007/978-94-007-5170-51, 10.1007/978-94-007, DOI 10.1007/978-94-007]
   Galati G, 2010, EXP BRAIN RES, V206, P109, DOI 10.1007/s00221-010-2168-8
   George C., 2017, Seamless and secure vr: Adapting and evaluating established authentication systems for virtual reality, DOI [10.14722/usec.2017.230282, DOI 10.14722/USEC.2017.230282]
   George C, 2020, LECT NOTES COMPUT SC, V12242, P61, DOI 10.1007/978-3-030-58465-8_5
   GUIARD Y, 1987, J MOTOR BEHAV, V19, P486
   Gupta A, 2020, INT SYM MIX AUGMENT, P350, DOI 10.1109/ISMAR50242.2020.00062
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   HIRSCH RS, 1970, J APPL PSYCHOL, V54, P484, DOI 10.1037/h0030143
   Hirzle T, 2020, ETRA'20 FULL PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3379155.3391313
   Hochreiter J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P379, DOI 10.1109/VR.2018.8446574
   kaggle, Common password list (rockyou.txt)
   Knapp B. G., 1990, Human performance concerns for the trackwolf system, P6
   Kreider C., 2018, SAIS 2018 Proceedings, V23
   Krishna V, 2019, ICMI'19: ADJUNCT OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, DOI 10.1145/3351529.3360655
   Kroger J., 2018, IFIP INT INT THINGS, P147
   Li SK, 2019, IEEE INT C COMPUT, P115, DOI 10.1109/CSE/EUC.2019.00031
   Li Y, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P327, DOI 10.1145/3052973.3053042
   Li Y, 2022, INT J HUM-COMPUT ST, V165, DOI 10.1016/j.ijhcs.2022.102835
   Liu XY, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1273, DOI 10.1145/2810103.2813668
   Liu X, 2010, MEM COGNITION, V38, P474, DOI 10.3758/MC.38.4.474
   Lu XS, 2020, INT SYM MIX AUGMENT, P344, DOI 10.1109/ISMAR50242.2020.00061
   Luo SQ, 2020, 27TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2020), DOI 10.14722/ndss.2020.24079
   Maiti Anindya, 2017, 2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), P630, DOI 10.1109/PERCOMW.2017.7917636
   Miller R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P311, DOI [10.1109/VRW50115.2020.00070, 10.1109/VRW50115.2020.0-206]
   Miluzzo E., 2012, P 10 INT C MOB SYST, P323, DOI [10.1145/2307636.2307666, DOI 10.1145/2307636.2307666]
   Owusu E., 2012, P 12 WORKSH MOB COMP, DOI [10.1145/2162081.21620953,8, DOI 10.1145/2162081.21620953,8]
   Pfeuffer K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300340
   Pham DM, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364265
   Ploetzner R, 2021, INSTR SCI, V49, P497, DOI 10.1007/s11251-021-09541-w
   Schneegass S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1379, DOI 10.1145/2858036.2858152
   Schneider D, 2019, IEEE T VIS COMPUT GR, V25, P3190, DOI 10.1109/TVCG.2019.2932239
   Shukla D, 2019, IEEE T INF FOREN SEC, V14, P3086, DOI 10.1109/TIFS.2019.2911171
   Sidenmark L, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3361218
   Song ZM, 2022, INT SYM MIX AUGMENT, P864, DOI 10.1109/ISMAR55827.2022.00105
   Soukoreff R. W., 2003, P SIGCHI C HUM FACT, P113, DOI DOI 10.1145/642611.642632
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Sulaiman Norrozila, 2022, International Conference on Innovative Computing and Communications: Proceedings of ICICC 2021. Advances in Intelligent Systems and Computing (1387), P421, DOI 10.1007/978-981-16-2594-7_35
   Viswanathan K., 2022, Security considerations for virtual reality systems, P2
   Wan T., 2024, 2024 IEEE INT S MIX
   Wan TJ, 2024, PROCEEDINGS OF THE 2024 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYTEMS (CHI 2024), DOI 10.1145/3613904.3642757
   Wan TJ, 2024, VIRTUAL REAL-LONDON, V28, DOI 10.1007/s10055-023-00902-z
   Wan TJ, 2024, IEEE T VIS COMPUT GR, V30, P6493, DOI 10.1109/TVCG.2024.3349428
   Wang H, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P155, DOI 10.1145/2789168.2790121
   Wobbrock J. O., 2011, The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only Anova Procedures, V5, P143
   Wu Y, 2023, P IEEE S SECUR PRIV, P3382, DOI 10.1109/SP46215.2023.10179301
   Xu WG, 2019, INT SYM MIX AUGMENT, P279, DOI 10.1109/ISMAR.2019.00026
   Xu Z., 2015, P 5 ACM C DAT APPL S, P61, DOI DOI 10.1145/2699026.2699114
   Xu Z., 2012, P 5 ACM C SEC PRIV W, P113, DOI [10.1145/2185448.21854653,8, DOI 10.1145/2185448.21854653,8]
   Yamada H., 1980, Journal of Information Processing, V5
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
   Zhang R., 2017, 2017 IEEE INT C COMM, P1, DOI [10.1109/ICC.2017.7997251, DOI 10.1109/ICC.2017.7997251]
   Zhu HD, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3432217
NR 69
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7075
EP 7085
DI 10.1109/TVCG.2024.3456149
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300007
PM 39255157
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Cheng, SW
   Liu, Y
   Gao, YF
   Dong, ZX
AF Cheng, Shiwei
   Liu, Yang
   Gao, Yuefan
   Dong, Zhanxun
TI "As if it were my own hand": inducing the rubber hand illusion through
   virtual reality for motor imagery enhancement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; Rubber hand illusion; Assistive technologies;
   Brain-computer interface; Virtual reality; Rubber hand illusion;
   Assistive technologies; Brain-computer interface
ID BRAIN-COMPUTER INTERFACE; DESYNCHRONIZATION; SYNCHRONIZATION;
   DISCRIMINATION; STIMULATION; PLASTICITY; SYSTEM
AB Brain-computer interfaces (BCI) are widely used in the field of disability assistance and rehabilitation, and virtual reality (VR) is increasingly used for visual guidance of BCI-MI (motor imagery). Therefore, how to improve the quality of electroencephalogram (EEG) signals for MI in VR has emerged as a critical issue. People can perform MI more easily when they visualize the hand used for visual guidance as their own, and the Rubber Hand Illusion (RHI) can increase people's ownership of the prosthetic hand. We proposed to induce RHI in VR to enhance participants' MI ability and designed five methods of inducing RHI, namely active movement, haptic stimulation, passive movement, active movement mixed with haptic stimulation, and passive movement mixed with haptic stimulation, respectively. We constructed a first-person training scenario to train participants' MI ability through the five induction methods. The experimental results showed that through the training, the participants' feeling of ownership of the virtual hand in VR was enhanced, and the MI ability was improved. Among them, the method of mixing active movement and tactile stimulation proved to have a good effect on enhancing MI. Finally, we developed a BCI system in VR utilizing the above training method, and the performance of the participants improved after the training. This also suggests that our proposed method is promising for future application in BCI rehabilitation systems.
C1 [Cheng, Shiwei; Liu, Yang] Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.
   [Cheng, Shiwei] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Gao, Yuefan] Cyborg Intelligence Technol Co, Changzhou, Jiangsu, Peoples R China.
   [Dong, Zhanxun] Shanghai Jiao Tong Univ, Sch Design, Shanghai, Peoples R China.
C3 Zhejiang University of Technology; Shanghai Jiao Tong University;
   Shanghai Jiao Tong University
RP Cheng, SW (corresponding author), Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.; Cheng, SW (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM 249401866@qq.com; liuyoung@zjut.edu.cn; 913201871@qq.com;
   dongzx@sjtu.edu.cn
RI Cheng, Shiwei/JNR-8400-2023; Dong, Zhanxun/JVM-8882-2024
OI Liu, Yang/0009-0005-0760-9382; Cheng, Shiwei/0000-0003-4716-4179
FU Zhejiang Provincial Natural Science Foundation of China [LR22F020003];
   National Natural Science Foundation of China [62172368]; Zhejiang
   Provincial Key Research and Development Program [2023C01045]
FX We thank all the volunteers who participated in the experiments. This
   research work was supported in part by the Zhejiang Provincial Natural
   Science Foundation of China under grant number LR22F020003, the National
   Natural Science Foundation of China under grant number 62172368, and the
   Zhejiang Provincial Key Research and Development Program under grant
   number 2023C01045.
CR Ahn M, 2015, J NEUROSCI METH, V243, P103, DOI 10.1016/j.jneumeth.2015.01.033
   Ahn M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080886
   Alchalabi B, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/2503431
   Azmandian M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1968, DOI 10.1145/2858036.2858226
   Blankertz B, 2006, IEEE T NEUR SYS REH, V14, P147, DOI 10.1109/TNSRE.2006.875557
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Braun N, 2016, SCI REP-UK, V6, DOI 10.1038/srep37696
   Caspar EA, 2015, BEHAV RES METHODS, V47, P744, DOI 10.3758/s13428-014-0498-3
   Cheng SW, 2023, Symposium Virtual Re, P592, DOI 10.1109/VR55154.2023.00074
   Crapse TB, 2008, NAT REV NEUROSCI, V9, P587, DOI 10.1038/nrn2457
   Daly JJ, 2008, LANCET NEUROL, V7, P1032, DOI 10.1016/S1474-4422(08)70223-0
   Daly JJ, 2007, THESCIENTIFICWORLDJO, V7, P2031, DOI 10.1100/tsw.2007.299
   David N, 2008, CONSCIOUS COGN, V17, P523, DOI 10.1016/j.concog.2008.03.004
   Evans N, 2013, NEUROIMAGE, V64, P216, DOI 10.1016/j.neuroimage.2012.09.027
   Faller J, 2012, IEEE T NEUR SYS REH, V20, P313, DOI 10.1109/TNSRE.2012.2189584
   Finotti G, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-33747-2
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Garrison KA, 2010, NEUROREHAB NEURAL RE, V24, P404, DOI 10.1177/1545968309354536
   Guger C, 2003, IEEE T NEUR SYS REH, V11, P145, DOI 10.1109/TNSRE.2003.814481
   Herwig U, 2003, BRAIN TOPOGR, V16, P95, DOI 10.1023/B:BRAT.0000006333.93597.9d
   Jackson PL, 2001, ARCH PHYS MED REHAB, V82, P1133, DOI 10.1053/apmr.2001.24286
   Jones TA, 1999, J NEUROSCI, V19, P10153
   KALCHER J, 1995, ELECTROEN CLIN NEURO, V94, P381, DOI 10.1016/0013-4694(95)00040-6
   Kalckert A, 2014, CONSCIOUS COGN, V26, P117, DOI 10.1016/j.concog.2014.02.003
   Kalckert A, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00040
   Kammers MPM, 2009, PERCEPTION, V38, P1804, DOI 10.1068/p6389
   Kocur M.., 2022, P 28 ACM S VIRT REAL, P1
   Leeb R, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/2/025011
   Liang S, 2016, COMPUT METH PROG BIO, V132, P63, DOI 10.1016/j.cmpb.2016.04.023
   Lotte F, 2007, J NEURAL ENG, V4, pR1, DOI 10.1088/1741-2560/4/2/R01
   Lotte F, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aac577
   McCrimmon CM, 2015, J NEUROENG REHABIL, V12, DOI 10.1186/s12984-015-0050-4
   Michielsen ME, 2011, NEUROREHAB NEURAL RE, V25, P223, DOI 10.1177/1545968310385127
   Nojima I, 2022, NEUROREHAB NEURAL RE, V36, P83, DOI 10.1177/15459683211062895
   PFURTSCHELLER G, 1977, ELECTROEN CLIN NEURO, V42, P817, DOI 10.1016/0013-4694(77)90235-8
   PFURTSCHELLER G, 1992, ELECTROEN CLIN NEURO, V83, P62, DOI 10.1016/0013-4694(92)90133-3
   Pfurtscheller G, 1999, CLIN NEUROPHYSIOL, V110, P1842, DOI 10.1016/S1388-2457(99)00141-8
   Riemer M, 2013, EXP BRAIN RES, V229, P383, DOI 10.1007/s00221-012-3374-3
   Schomer D. L.., 2012, Niedermeyer's electroencephalography: basic principles, clinical applications, and related fields, P4
   Sharma R, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103101
   Skola F, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300477
   Small SL, 2012, DEV PSYCHOBIOL, V54, P293, DOI 10.1002/dev.20504
   Soekadar SR, 2011, IEEE T NEUR SYS REH, V19, P542, DOI 10.1109/TNSRE.2011.2166809
   Sollfrank T, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00463
   Song M, 2019, IEEE T NEUR SYS REH, V27, P477, DOI 10.1109/TNSRE.2019.2895029
   Sun Y, 2016, TOP STROKE REHABIL, V23, P245, DOI 10.1080/10749357.2016.1141472
   Vatavu RD, 2022, INT SYM MIX AUGMENT, P685, DOI 10.1109/ISMAR55827.2022.00086
   Veale JF, 2014, LATERALITY, V19, P164, DOI 10.1080/1357650X.2013.783045
   VONHOLST E, 1950, NATURWISSENSCHAFTEN, V37, P464, DOI 10.1007/BF00622503
   Wang W, 2019, 2019 IEEE MTT-S INTERNATIONAL MICROWAVE BIOMEDICAL CONFERENCE (IMBIOC 2019), DOI 10.1109/imbioc.2019.8777805
   Wen W, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00329
   Wolpaw JR, 2001, ANNU REV NEUROSCI, V24, P807, DOI 10.1146/annurev.neuro.24.1.807
   Yger F, 2017, IEEE T NEUR SYS REH, V25, P1753, DOI 10.1109/TNSRE.2016.2627016
   Zeng H, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00060
   Zhang WB, 2022, IEEE T NEUR SYS REH, V30, P2732, DOI 10.1109/TNSRE.2022.3208312
   Zhao XQ, 2019, IEEE T NEUR SYS REH, V27, P2164, DOI 10.1109/TNSRE.2019.2938295
NR 56
TC 0
Z9 0
U1 6
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7086
EP 7096
DI 10.1109/TVCG.2024.3456154
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300039
PM 39250394
DA 2024-11-06
ER

PT J
AU Fan, RZ
   Shi, XH
   Wang, KY
   Ma, QX
   Wang, LL
AF Fan, Runze
   Shi, Xuehuai
   Wang, Kangyu
   Ma, Qixiang
   Wang, Lili
TI Scene-aware Foveated Rendering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Rendering (computer graphics); Size control; Pipelines;
   Image resolution; Three-dimensional displays; Virtual reality; Foveated
   rendering; Visual importance; Pixel size control
ID PERCEPTION
AB We propose a new scene-aware foveated rendering method, which incorporates the scene awareness and characteristics of the human visual system into the mapping-based foveated rendering framework. First, we generate the conservative visual importance map that encodes the visual features of the scene, visual acuity, and gaze motion. Second, we construct the pixel size control map using a convolution kernel method. Third, we utilize the pixel size control map to guide the foveated rendering. At last, a temporal coherent refinement strategy is used to maintain the smooth foveated rendering for the adjacent frames. Compared to the state-of-the-art mapping-based foveated rendering methods using the same compression ratio, our method achieves smaller MSE, higher PSNR, and SSIM in the fovea, periphery, salient regions, and the whole image. We also conducted user studies, and the results proved that the perceptual quality of our method has a high visual similarity with the around truth rendered with the full resolution.
C1 [Wang, Lili] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Shengzhen, Peoples R China.
   [Fan, Runze; Wang, Kangyu; Ma, Qixiang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Shi, Xuehuai] Nanjing Univ Posts & Telecommun, Nanjing, Peoples R China.
C3 Beihang University; Beihang University; Nanjing University of Posts &
   Telecommunications
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Shengzhen, Peoples R China.
EM taoistze@163.com; xuehuai@njupt.edu.cn; sy2306314@buaa.edu.cn;
   sycamore_ma@outlook.com; wanglily@buaa.edu.cn
RI Ma, Qixiang/IZQ-5562-2023; Shi, Xuehuai/ISB-5757-2023
OI Ma, Qixiang/0009-0004-0424-0308
FU National Natural Science Foundation of China [61932003, 62372026];
   Science and Technology Plan Project [Z221100007722004]; National Key RD
   plan [2019YFC1521102]
FX This work is supported by the National Natural Science Foundation of
   China through Projects 61932003 and 62372026, Beijing he applicable
   license agreement with IEEE. Restrictions apply. Science and Technology
   Plan Project Z221100007722004, and the National Key R&D plan
   2019YFC1521102.
CR Amazon Lumberyard, 2017, Amazon lumberyard bistro, open research content archive (orca)
   Araujo H, 1997, II WORKSHOP ON CYBERNETIC VISION, PROCEEDINGS, P139
   Bauer D, 2023, IEEE T VIS COMPUT GR, V29, P515, DOI 10.1109/TVCG.2022.3209498
   Cooling Robert J, 1990, The British Journal of Ophthalmology, V74, P511
   Denes G, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392411
   Deng NC, 2022, IEEE T VIS COMPUT GR, V28, P3854, DOI 10.1109/TVCG.2022.3203102
   Epic Games, 2017, Unreal engine sun temple, open research content archive (orca)
   Fujita Masahiro, 2014, Light Transport Entertainment Research, V1, P2
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Hull Kate Anderson Nicholas, 2017, Nvidia emerald square, open research content archive (orca)
   Jindal A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480514
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Kim Y, 2021, INT SYM MIX AUGMENT, P413, DOI 10.1109/ISMAR52148.2021.00058
   Koskela Matias K., 2018, Computational Visual Media, V4, P267, DOI 10.1007/s41095-018-0113-0
   Krajancich B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592406
   Li D, 2021, IEEE T VIS COMPUT GR, V27, P2638, DOI 10.1109/TVCG.2021.3067762
   Ling S, 2009, VISION RES, V49, P1194, DOI 10.1016/j.visres.2008.05.025
   Lisboa T, 2023, INT SYM MIX AUGMENT, P1104, DOI 10.1109/ISMAR59233.2023.00127
   Marzecová A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18347-1
   Meng XX, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203199
   NVIDIA, 2017, Multi-res shading
   NVIDIA, 2017, Lens matched shading
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Policarpo F., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P55
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Ren DY, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3387-7
   SCHWARTZ EL, 1984, IEEE T SYST MAN CYB, V14, P257, DOI 10.1109/TSMC.1984.6313208
   Shi XH, 2022, IEEE T VIS COMPUT GR, V28, P3684, DOI 10.1109/TVCG.2022.3203089
   Shi XH, 2021, IEEE T VIS COMPUT GR, V27, P4183, DOI 10.1109/TVCG.2021.3106488
   Stengel M, 2016, COMPUT GRAPH FORUM, V35, P129, DOI 10.1111/cgf.12956
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Swafford Nicholas T, 2016, P ACM S APPL PERC, P7, DOI DOI 10.1145/2931002.2931011
   Tariq T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530101
   Traver VJ, 2010, ROBOT AUTON SYST, V58, P378, DOI 10.1016/j.robot.2009.10.002
   Turner E, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P711, DOI 10.1109/VR.2018.8446142
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   Vaidyanathan Karthik, 2014, P HIGH PERF GRAPH, P9
   Vater C, 2022, PSYCHON B REV, V29, P1531, DOI 10.3758/s13423-022-02117-w
   Walton DR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459943
   Wang LL, 2023, COMPUT VIS MEDIA, V9, P195, DOI 10.1007/s41095-022-0306-4
   Wang YX, 2015, J VISION, V15, DOI 10.1167/15.5.15
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Wu TT, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3427-2
   Yang L, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3320287
   Ye JN, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P756, DOI 10.1109/VR51125.2022.00097
NR 46
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7097
EP 7106
DI 10.1109/TVCG.2024.3456195
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300042
PM 39255108
DA 2024-11-06
ER

PT J
AU Wei, YS
   Xu, KM
   Li, Y
   Yu, LY
   Liang, HN
AF Wei, Yushi
   Xu, Kemu
   Li, Yue
   Yu, Lingyun
   Liang, Hai-Ning
TI Exploring and Modeling Directional Effects on Steering Behavior in
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Predictive models;
   Computational modeling; Mathematical models; Performance evaluation;
   Muscles; Virtual reality; human performance modeling; steering law;
   barehand interaction; head-mounted display
ID FITTS LAW; INFORMATION CAPACITY; MOVEMENTS; UNDERLIE; MOTION; MOUSE
AB Steering is a fundamental task in interactive Virtual Reality (VR) systems. Prior work has demonstrated that movement direction can significantly influence user behavior in the steering task, and different interactive environments (VEs) can lead to various behavioral patterns, such as tablets and PCs. However, its impact on VR environments remains unexplored. Given the widespread use of steering tasks in VEs, including menu adjustment and object manipulation, this work seeks to understand and model the directional effect with a focus on barehand interaction, which is typical in VEs. This paper presents the results of two studies. The first study was conducted to collect behavioral data with four categories: movement time, average movement speed, success rate, and reenter times. According to the results, we examined the effect of movement direction and built the S theta Model. We then empirically evaluated the model through the data collected from the first study. The results proved that our proposed model achieved the best performance across all the metrics (r(2) > 0.95), with more than 15% improvement over the original Steering Law in terms of prediction accuracy. Next, we further validated the S theta Model by another study with the change of device and steering direction. Consistent with previous assessments, the model continues to exhibit optimal performance in both predicting movement time and speed. Finally, based on the results, we formulated design recommendations for steering tasks in VEs to enhance user experience and interaction efficiency.
C1 [Wei, Yushi; Liang, Hai-Ning] Hong Kong Univ Sci & Technol Guangzhou, Computat Media & Arts Thrust, Guangzhou, Peoples R China.
   [Xu, Kemu; Li, Yue; Yu, Lingyun] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou, Peoples R China.
C3 Hong Kong University of Science & Technology (Guangzhou); Xi'an
   Jiaotong-Liverpool University
RP Liang, HN (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Computat Media & Arts Thrust, Guangzhou, Peoples R China.
EM hainingliang@hkust-gz.edu.cn
OI Li, Yue/0000-0003-3728-218X; Wei, Yushi/0000-0002-6003-0557; Liang,
   Hai-Ning/0000-0003-3600-8955
FU National Science Foundation of China [62272396, 62207022]
FX This work was funded in part by the National Science Foundation of China
   (#62272396; #62207022)
CR Accot J., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P1, DOI 10.1145/365024.365027
   Accot Johnny, 1997, P ACM SIGCHI C HUM F, P295, DOI [10.1145/258549.258760, DOI 10.1145/258549.258760]
   Adkins A., 2021, Evaluating grasping visualizations and control modes in a vr game, V18, DOI [10.1145/3486582, DOI 10.1145/3486582]
   Batmaz AU, 2022, PROCEEDINGS OF THE 2022 ACM SYMPOSIUM ON SPATIAL USER INTERACTION, SUI 2022, DOI 10.1145/3565970.3567702
   Benko H., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1263
   Bi Xiaojun, 2013, P SIGCHI C HUM FACT, P1363, DOI DOI 10.1145/2470654.2466180
   Bringoux L, 2012, J NEUROPHYSIOL, V108, P1685, DOI 10.1152/jn.00407.2012
   Buckingham G, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.728461
   CARD SK, 1978, ERGONOMICS, V21, P601, DOI 10.1080/00140137808931762
   Cha Y, 2013, INT J IND ERGONOM, V43, P350, DOI 10.1016/j.ergon.2013.05.005
   Chen HL, 2019, ICBBE 2019: 2019 6TH INTERNATIONAL CONFERENCE ON BIOMEDICAL AND BIOINFORMATICS ENGINEERING, P182, DOI 10.1145/3375923.3375940
   Crevecoeur F, 2014, J NEUROPHYSIOL, V112, P384, DOI 10.1152/jn.00061.2014
   Esenther A., 2006, Proceedings of the Working Conference on Advanced Visual Interfaces, P112, DOI [DOI 10.1145/1133265.11332892, 10.1145/1133265.1133289, DOI 10.1145/1133265.1133289]
   FITTS PM, 1964, J EXP PSYCHOL, V67, P103, DOI 10.1037/h0045689
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Georgiadis A., 2017, Vr gaming-hands on: The use and effects of bare hand gestures as an interaction method in multiplayer virtual reality games, P3
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P159, DOI 10.1109/VR.2018.8446059
   Hidaka T, 2024, INT J HUM-COMPUT INT, V40, P3622, DOI 10.1080/10447318.2023.2192586
   Hincapié-Ramos JD, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1063, DOI 10.1145/2556288.2557130
   HOFFMANN ER, 1995, ERGONOMICS, V38, P828, DOI 10.1080/00140139508925153
   Hoffmann ER, 2009, INT J IND ERGONOM, V39, P578, DOI 10.1016/j.ergon.2008.02.007
   Holz C, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2501
   Hu X., 2024, P 23 IEEE INT S MIX, P1
   Jang HJ, 2019, J INFORM DISPLAY, V20, P1, DOI 10.1080/15980316.2019.1572662
   Kangas J, 2022, MULTIMODAL TECHNOLOG, V6, DOI 10.3390/mti6010006
   KERR R, 1973, J MOTOR BEHAV, V5, P175, DOI 10.1080/00222895.1973.10734962
   Ko Yu-Jung, 2020, Proc ACM Symp User Interface Softw Tech, V2020, P858, DOI 10.1145/3379337.3415871
   LANGOLF GD, 1976, J MOTOR BEHAV, V8, P113, DOI 10.1080/00222895.1976.10735061
   Lee S, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P309
   Levin O, 2001, EXP BRAIN RES, V141, P471, DOI 10.1007/s002210100874
   Lin L, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P510, DOI 10.1109/vr.2019.8797787
   Liu L, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P39, DOI 10.1109/3DUI.2010.5444724
   Luong T., 2023, IEEE Transactions on Visualization and Computer Graphics, V1, P3
   Machuca MDB, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300437
   MacKenzie I. S., 1992, Human-Computer Interaction, V7, P91, DOI 10.1207/s15327051hci0701_3
   Masurovsky A, 2020, MULTIMODAL TECHNOLOG, V4, DOI 10.3390/mti4040091
   Murata A, 2001, HUM MOVEMENT SCI, V20, P791, DOI 10.1016/S0167-9457(01)00058-6
   Nancel M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P244, DOI 10.1145/3025453.3025951
   Oulasvirta A., 2018, Computational interaction, P2
   Papaxanthis C, 1998, NEUROSCI LETT, V253, P103, DOI 10.1016/S0304-3940(98)00604-1
   Pei SY, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501898
   Renaux A, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0275876
   Sainburg RL, 2000, J NEUROPHYSIOL, V83, P2661, DOI 10.1152/jn.2000.83.5.2661
   Shi R., 2023, Proc. ACM Hum.- Comput. Interact., V7, DOI DOI 10.1145/3591129
   Shi Rongkai, 2023, 2023 CHI C HUM FACT, DOI DOI 10.1145/3544549.3585615
   Su GE, 2020, INT J HUM-COMPUT ST, V141, DOI 10.1016/j.ijhcs.2020.102433
   Thibbotuwawa N, 2012, HUM FACTORS, V54, P138, DOI 10.1177/0018720811424743
   Ustinova KI, 2006, J NEUROPHYSIOL, V96, P1124, DOI 10.1152/jn.01152.2005
   Vogel D, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P657
   Von Hardenberg C., 2001, P 2001 WORKSH PERC U, P1
   Vosinakis S, 2018, VIRTUAL REAL-LONDON, V22, P47, DOI 10.1007/s10055-017-0313-4
   Wei YS, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581042
   Whisenand TG, 1999, COMPUT HUM BEHAV, V15, P85, DOI 10.1016/S0747-5632(98)00036-3
   Xiaolei Zhou, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P1098, DOI 10.1109/CSSE.2008.1310
   Xu WG, 2022, INT SYM MIX AUGMENT, P131, DOI 10.1109/ISMAR55827.2022.00027
   Yamanaka S, 2024, INT J HUM-COMPUT INT, V40, P4300, DOI 10.1080/10447318.2023.2212221
   Yamanaka S, 2019, PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS '19), P65, DOI 10.1145/3343055.3359697
   Yamanaka S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300800
   Yamanaka S, 2016, PROCEEDINGS OF THE NORDICHI '16: THE 9TH NORDIC CONFERENCE ON HUMAN-COMPUTER INTERACTION - GAME CHANGING DESIGN, DOI 10.1145/2971485.2971486
   Yu D., 2023, P 2023 CHI C HUM FAC, DOI [10.1145/3544548.35810111, DOI 10.1145/3544548.35810111]
   Yu DF, 2022, INT SYM MIX AUGMENT, P637, DOI 10.1109/ISMAR55827.2022.00081
   Yu DF, 2022, IEEE INT SYMP M AU R, P905, DOI 10.1109/ISMAR-Adjunct57072.2022.00198
   Yu DF, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P500, DOI [10.1109/VR46266.2020.1581291122479, 10.1109/VR46266.2020.00-33]
   Yu LY, 2010, IEEE T VIS COMPUT GR, V16, P1613, DOI 10.1109/TVCG.2010.157
   Zhang X., 2023, ISS '23
   Zhang Xinyong., 2012, Extending Fitts law to account for the effects of movement direction on 2d pointing, P3185, DOI [DOI 10.1145/2207676.22087371,2,3,4,8, 10.1145/2207676.2208737, DOI 10.1145/2207676.2208737]
   Zhu FY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376233
NR 67
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7107
EP 7117
DI 10.1109/TVCG.2024.3456147
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300005
PM 39255122
DA 2024-11-06
ER

PT J
AU Shen, JX
   Khaldi, K
   Zhou, EM
   Surale, HB
   Karlson, A
AF Shen, Junxiao
   Khaldi, Khadija
   Zhou, Enmin
   Surale, Hemant Bhaskar
   Karlson, Amy
TI Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR
   Through Trajectory Coarse Discretization and Pre-Training
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Decoding; Trajectory; Keyboards; Accuracy; Sharks; Extended reality;
   Training; Pre-trained models; text entry; word-gesture keyboard;
   discretization
AB Text entry with word-gesture keyboards (WGK) is emerging as a popular method and becoming a key interaction for Extended Reality (XR). However, the diversity of interaction modes, keyboard sizes, and visual feedback in these environments introduces divergent word-gesture trajectory data patterns, thus leading to complexity in decoding trajectories into text. Template-matching decoding methods, such as SHARK(2) [32], are commonly used for these WGK systems because they are easy to implement and configure. However, these methods are susceptible to decoding inaccuracies for noisy trajectories. While conventional neural-network-based decoders (neural decoders) trained on word-gesture trajectory data have been proposed to improve accuracy, they have their own limitations: they require extensive data for training and deep-learning expertise for implementation. To address these challenges, we propose a novel solution that combines ease of implementation with high decoding accuracy: a generalizable neural decoder enabled by pre-training on large-scale coarsely discretized word-gesture trajectories. This approach produces a ready-to-use WGK decoder that is generalizable across mid-air and on-surface WGK systems in augmented reality (AR) and virtual reality (VR), which is evident by a robust average Top-4 accuracy of 90.4% on four diverse datasets. It significantly outperforms SHARK(2) with a 37.2% enhancement and surpasses the conventional neural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only 4 MB after quantization, without sacrificing accuracy, and it can operate in real-time, executing in just 97 milliseconds on Quest 3.
C1 [Shen, Junxiao] Univ Bristol, Bristol, England.
   [Khaldi, Khadija; Zhou, Enmin; Surale, Hemant Bhaskar; Karlson, Amy] Meta, Real Labs Res, Burlingame, CA USA.
C3 University of Bristol
RP Shen, JX (corresponding author), Univ Bristol, Bristol, England.
EM shawn.shen.trinity@icloud.com
RI khaldi, khadija/IWM-3388-2023; Surale, Hemant/AAG-9672-2020
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Agarwal Aman, 2018, arXiv
   Alsharif O, 2015, INT CONF ACOUST SPEE, P2076, DOI 10.1109/ICASSP.2015.7178336
   [Anonymous], Apple vision pro
   [Anonymous], 1999, P SIGCHI C HUM FACT
   [Anonymous], 2023, Use your mac with apple vision pro
   Ba J.L., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bergstra J., 2011, Adv. Neural Inform. Process. Syst., V24
   Brown T.B., 2020, Advances in neural information processing systems, V33, P1877
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen SB, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312762
   DAVID PA, 1985, AM ECON REV, V75, P332
   Dube TJ, 2019, IN SY AP IN WE HC, V11567, P419, DOI 10.1007/978-3-030-22643-5_33
   Dudley J. J., 2023, IEEE Transactions on Visualization and Computer Graphics, V1
   Dudley JJ, 2018, ACM T COMPUT-HUM INT, V25, DOI 10.1145/3232163
   Dvorak A. C., 1936, Typewriter keyboard
   Freitag M., 2017, NMT ACL
   Gordon M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3817, DOI 10.1145/2858036.2858242
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Graves A, 2012, STUD COMPUT INTELL, V385, P61
   Gu YZ, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3432204
   Gupta A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300244
   Han Song, 2016, ICLR
   Henninger F, 2022, BEHAV RES METHODS, V54, P556, DOI 10.3758/s13428-019-01283-5
   Kern Florian, 2023, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2023.3247098
   Kim T., 2023, P 36 ANN ACM S US IN, P1
   Kingma D.P., 2014, P INT C LEARNING REP
   Klimt B, 2004, LECT NOTES COMPUT SC, V3201, P217
   Kojima T, 2022, Arxiv, DOI [arXiv:2205.11916, 10.48550/arXiv.2205.11916]
   Kristensson P., 2004, P 17 ANN ACM S USER, P43, DOI DOI 10.1145/1029632.1029640
   Kristensson P. O., 2007, PhD thesis, P2
   Lee LH, 2019, INT CONF PERVAS COMP, DOI 10.1109/percom.2019.8767420
   Leiva LA, 2021, PROCEEDINGS OF 23RD ACM INTERNATIONAL CONFERENCE ON MOBILE HUMAN-COMPUTER INTERACTION (MOBILEHCI 2021): MOBILE APART, MOBILE TOGETHER, DOI 10.1145/3447526.3472059
   Liang C, 2022, PROC ACM INTERACT MO, V6, DOI 10.1145/3569463
   Liang H, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P743, DOI 10.1145/2733373.2807972
   MacKenzie I.S., 2003, CHI 03 EXTENDED ABST, P754, DOI DOI 10.1145/765891.765971
   Markussen A, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1073, DOI 10.1145/2556288.2556964
   Meta, 2023, Quest v56 software update: Hand tracking improvements, live captions, facebook livestreaming, P4
   Meta AI, 2018, Pytext: A natural language modeling framework based on pytorch
   Meta Platforms Inc, Oculus Quest Series
   microsoft, Microsoft hololens
   Mifsud DM, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P581, DOI 10.1109/VRW55335.2022.00146
   Ott M, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P48
   Paszke A., 2019, Proceedings of Advances in Neural Information Processing Systems, P8024, DOI DOI 10.48550/ARXIV.1912.01703
   Peshock Anna., 2014, Proceedings of the 2014 ACM International Symposium on Wearable Computers: Adjunct Program, ISWC '14 Adjunct, P87, DOI DOI 10.1145/2641248.2641266
   Reyal S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P679, DOI 10.1145/2702123.2702597
   sensel, Haptic capacitive
   Shen J., 2023, IEEE Transactions on Visualization and Computer Graphics, V1
   Shen JX, 2021, IEEE INT CONF AUTOMA
   Shen JX, 2022, INT SYM MIX AUGMENT, P702, DOI 10.1109/ISMAR55827.2022.00088
   Shen JX, 2021, INT SYM MIX AUGMENT, P393, DOI 10.1109/ISMAR52148.2021.00056
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194
   Vaswani A, 2017, ADV NEUR IN, V30
   Vertanen K, 2021, NAT LANG ENG, V27, P1, DOI 10.1017/S1351324919000548
   Wang AL, 2019, Arxiv, DOI arXiv:1804.07461
   Wang YH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051582
   Wolf T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P38
   Xu WG, 2019, INT SYM MIX AUGMENT, P279, DOI 10.1109/ISMAR.2019.00026
   Xu ZE, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P883, DOI 10.1145/3332165.3347865
   Yanagihara N, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3365026
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
   Zhai S., 2003, Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI 2003), P97, DOI DOI 10.1145/642611.642630
   Zhai S., 2000, P ACM S USER INTERFA, P119, DOI DOI 10.1145/354401.354424
   Zhai SM, 2012, COMMUN ACM, V55, P91, DOI 10.1145/2330667.2330689
   Zhao MZ, 2023, PROCEEDINGS OF 2023 28TH ANNUAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2023, P595, DOI 10.1145/3581641.3584072
NR 66
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7118
EP 7128
DI 10.1109/TVCG.2024.3456157
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300041
PM 39283795
DA 2024-11-06
ER

PT J
AU Zhai, HJ
   Huang, G
   Hu, QR
   Li, GL
   Bao, HJ
   Zhang, GF
AF Zhai, Hongjia
   Huang, Gan
   Hu, Qirui
   Li, Guanglin
   Bao, Hujun
   Zhang, Guofeng
TI NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene
   Understanding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Implicit representation; Neural dense SLAM; Semantic segmentation;
   Semantic segmentation; Scene understanding; Scene understanding;
   Semantic segmentation; Scene understanding
AB In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications.
C1 [Zhai, Hongjia; Huang, Gan; Hu, Qirui; Li, Guanglin; Bao, Hujun; Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
C3 Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM zhjsmileeveryday@gmail.com; huanggan@zju.edu.cn; 3200105912@zju.edu.cn;
   liguanglin@zju.edu.cn; baohujun@zju.edu.cn; zhangguofeng@zju.edu.cn
RI zhang, Guofeng/H-4991-2011; Hu, Qi-rui/JGD-3692-2023
OI Zhang, Guofeng/0000-0001-5661-8430; Bao, Hujun/0000-0002-2662-0334;
   Huang, Gan/0000-0001-8515-2721
FU NSF of China [61932003]
FX This work was partially supported by NSF of China (No.61932003).
CR ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Azinovic D, 2022, PROC CVPR IEEE, P6280, DOI 10.1109/CVPR52688.2022.00619
   Bloesch M, 2018, PROC CVPR IEEE, P2560, DOI 10.1109/CVPR.2018.00271
   Blomqvist K, 2022, Arxiv, DOI arXiv:2209.12744
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen D., 2024, Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction, P2
   Chen Y, 2023, PROC CVPR IEEE, P8264, DOI 10.1109/CVPR52729.2023.00799
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Chung CM, 2023, Arxiv, DOI arXiv:2209.13274
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Fu X, 2022, INT CONF 3D VISION, P301, DOI 10.1109/3DV57658.2022.00042
   Gao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5692, DOI 10.1109/ICCV48922.2021.00566
   Grinvald M, 2019, IEEE ROBOT AUTOM LET, V4, P3037, DOI 10.1109/LRA.2019.2923960
   Haghighi Y, 2023, Arxiv, DOI arXiv:2304.14560
   Han MZ, 2021, IEEE INT CONF ROBOT, P12199, DOI 10.1109/ICRA48506.2021.9561546
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   Huang JH, 2021, PROC CVPR IEEE, P8928, DOI 10.1109/CVPR46437.2021.00882
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang CX, 2023, Arxiv, DOI arXiv:2306.03207
   Johari MM, 2023, PROC CVPR IEEE, P17408, DOI 10.1109/CVPR52729.2023.01670
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Kong X, 2023, PROC CVPR IEEE, P952, DOI 10.1109/CVPR52729.2023.00098
   Kulhanek J, 2023, IEEE I CONF COMP VIS, P18412, DOI 10.1109/ICCV51070.2023.01692
   Kundu A, 2022, PROC CVPR IEEE, P12861, DOI 10.1109/CVPR52688.2022.01253
   Li H, 2023, IEEE T VIS COMPUT GR, V29, P2837, DOI 10.1109/TVCG.2023.3247459
   Li H, 2024, IEEE T VIS COMPUT GR, V30, P1743, DOI 10.1109/TVCG.2022.3225844
   Li H, 2021, INT SYM MIX AUGMENT, P80, DOI 10.1109/ISMAR52148.2021.00022
   Liu L., 2020, ANN C NEUR INF PROC, P2
   Lorensen W. E., 1998, SEMINAL GRAPHICS PIO, P7
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Munkberg J, 2022, PROC CVPR IEEE, P8270, DOI 10.1109/CVPR52688.2022.00810
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Narita G, 2019, IEEE INT C INT ROBOT, P4205, DOI [10.1109/iros40897.2019.8967890, 10.1109/IROS40897.2019.8967890]
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Park K, 2021, Arxiv, DOI arXiv:2106.13228
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Rosinol A., 2022, arXiv
   Rosu RA, 2023, PROC CVPR IEEE, P8466, DOI 10.1109/CVPR52729.2023.00818
   Sandström E, 2023, IEEE I CONF COMP VIS, P18387, DOI 10.1109/ICCV51070.2023.01690
   Savva M, 2019, IEEE I CONF COMP VIS, P9338, DOI 10.1109/ICCV.2019.00943
   Schöps T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022
   Shan ZY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102251
   SHEN TIANCHANG, 2021, Advances in Neural Information Processing Systems, V34, P6087
   Siddiqui Y, 2023, PROC CVPR IEEE, P9043, DOI 10.1109/CVPR52729.2023.00873
   Straub J, 2019, Arxiv, DOI arXiv:1906.05797
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sucar E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6209, DOI 10.1109/ICCV48922.2021.00617
   Sun JM, 2021, PROC CVPR IEEE, P15593, DOI 10.1109/CVPR46437.2021.01534
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Teed Z, 2021, ADV NEUR IN, V34
   Turki H, 2022, PROC CVPR IEEE, P12912, DOI 10.1109/CVPR52688.2022.01258
   Wang HY, 2023, PROC CVPR IEEE, P13293, DOI 10.1109/CVPR52729.2023.01277
   Wang P., 2021, P 35 C NEUR INF PROC, P27171
   Weder S, 2021, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR46437.2021.00318
   Wei Y., 2020, P EUR C COMP VIS
   Whelan T., 2012, P RSS 12 WORKSH RGB
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Xu QG, 2022, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR52688.2022.00536
   Yang XR, 2022, INT SYM MIX AUGMENT, P499, DOI 10.1109/ISMAR55827.2022.00066
   Zhai HJ, 2024, Arxiv, DOI arXiv:2403.12536
   Zhang YM, 2023, IEEE I CONF COMP VIS, P3704, DOI 10.1109/ICCV51070.2023.00345
   Zhi SF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15818, DOI 10.1109/ICCV48922.2021.01554
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu Siting, 2024, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P21167, DOI 10.1109/CVPR52733.2024.02000
   Zhu Z., 2023, arXiv
   Zhu ZX, 2023, IEEE INT CONF ROBOT, P8326, DOI 10.1109/ICRA48891.2023.10161570
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
NR 78
TC 0
Z9 0
U1 7
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7129
EP 7139
DI 10.1109/TVCG.2024.3456166
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300020
PM 39255118
DA 2024-11-06
ER

PT J
AU Yang, FC
   Duque, K
   Mousas, C
AF Yang, Fu-Chia
   Duque, Kevin
   Mousas, Christos
TI The Effects of Depth of Knowledge of a Virtual Agent
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; virtual agent; conversational AI; depth of knowledge;
   knowledge bank; prompt engineering; Virtual reality; virtual agent;
   conversational AI; depth of knowledge; knowledge bank; prompt
   engineering
ID ANTHROPOMORPHISM; INTELLIGENCE; INFORMATION
AB We explored the impact of depth of knowledge on conversational agents and human perceptions in a virtual reality (VR) environment. We designed experimental conditions with low, medium, and high depths of knowledge in the domain of game development and tested them among 27 game development students. We aimed to understand how the agent's predefined knowledge levels affected the participants' perceptions of the agent and its knowledge. Our findings showed that participants could distinguish between different knowledge levels of the virtual agent. Moreover, the agent's depth of knowledge significantly impacted participants' perceptions of intelligence, rapport, factuality, the uncanny valley effect, anthropomorphism, and willingness for future interaction. We also found strong correlations between perceived knowledge, perceived intelligence, factuality, and willingness for future interactions. We developed design guidelines for creating conversational agents from our data and observations. This study contributes to the human-agent interaction field in VR settings by providing empirical evidence on the importance of tailoring virtual agents' depth of knowledge to improve user experience, offering insights into designing more engaging and effective conversational agents.
C1 [Yang, Fu-Chia; Mousas, Christos] Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN 47907 USA.
   [Duque, Kevin] Tecnol Monterrey, Engn & Sci, Monterrey 64849, NL, Mexico.
C3 Purdue University System; Purdue University; Tecnologico de Monterrey
RP Yang, FC (corresponding author), Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN 47907 USA.
EM yang1684@purdue.edu; A01174501@exatec.tec.mx; cmousas@purdue.edu
RI Mousas, Christos/AGV-3533-2022
OI Mousas, Christos/0000-0003-0955-7959
CR Agho AO, 2001, CANCER NURS, V24, P165, DOI 10.1097/00002820-200106000-00001
   Amershi S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300233
   Ball G., 2000, Emotion and personality in a conversational agent, V3
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Bickmore T., 1999, Narrative Intelligence. Papers from the 1999 AAAI Fall Symposium, P87
   Biocca F., 2001, 4 ANN INT WORKSH PRE, P1
   Bradshaw J. M., 2017, The handbook of human-machine interaction, P2
   Car LT, 2020, J MED INTERNET RES, V22, DOI 10.2196/17158
   Cerekovic A, 2017, IEEE T AFFECT COMPUT, V8, P382, DOI 10.1109/TAFFC.2016.2545650
   Chang Y., 2023, ACM Transactions on Intelligent Systems and Technology, P3
   Chen BH, 2024, Arxiv, DOI arXiv:2310.14735
   Chen JYC, 2014, IEEE T HUM-MACH SYST, V44, P13, DOI 10.1109/THMS.2013.2293535
   Choi Minsoo, 2023, 2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), P555, DOI 10.1109/ISMAR-Adjunct60411.2023.00118
   Clark L, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300705
   Cook FL, 2010, J POLIT, V72, P397, DOI 10.1017/S0022381610000034
   DRASS JA, 1989, DIABETES CARE, V12, P351, DOI 10.2337/diacare.12.5.351
   Ekin S., 2023, TechRxiv, DOI DOI 10.36227/TECHRXIV.22683919.V2
   El Saddik A., 2023, IEEE Consumer Electronics Magazine, V1
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Goh O. S., 2005, 3 INT C COMP INT ROB, P3
   Gratch J., 2007, INTELLIGENT VIRTUAL, V7, P5
   Guo SQ, 2024, P ACM COMPUT GRAPH, V7, DOI 10.1145/3651288
   Hadi M. U., 2023, Authorea, P3
   Ho CC, 2017, INT J SOC ROBOT, V9, P129, DOI 10.1007/s12369-016-0380-9
   Hou XY, 2024, Arxiv, DOI arXiv:2308.10620
   Hu Q, 2021, INT J INFORM MANAGE, V56, DOI 10.1016/j.ijinfomgt.2020.102250
   Iqbal S. M., 2023, A gpt-based practical architecture for conversational human digital twins
   Jaccard J., 2005, Health Psychology, V24, P3
   Jadhav Komal P., 2020, Computing in Engineering and Technology. Proceedings of ICCET 2019. Advances in Intelligent Systems and Computing (AISC 1025), P533, DOI 10.1007/978-981-32-9515-5_51
   Jiang XM, 2016, J EXP PSYCHOL HUMAN, V42, P1412, DOI 10.1037/xhp0000240
   Kao Dominic, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474661
   Kasneci E, 2023, LEARN INDIVID DIFFER, V103, DOI 10.1016/j.lindif.2023.102274
   KAY A, 1984, SCI AM, V251, P52, DOI 10.1038/scientificamerican0984-52
   Khan S., 2020, P 2020 4 INT S COMP, P1
   Lester J., 2004, The practical handbook of internet computing, P2
   Lewis M, 1998, AI MAG, V19, P67
   Liew A., 2013, Business Management Dynamics, V2, P3
   Lin CL, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.579993
   Liu Y., 2023, Meta-Radiology, V1, DOI [DOI 10.1016/J.METRAD.2023.100017, DOI 10.1016/J.METRAD.2023.1000172]
   Lo LS, 2023, J ACAD LIBR, V49, DOI 10.1016/j.acalib.2023.102720
   Lugrin B., 2022, Interactivity, Platforms, Application, V2, P561
   Luo ZH, 2023, Arxiv, DOI arXiv:2303.15621
   McTear M., 2022, Conversational ai: Dialogue systems, conversational agents, and chatbots, P2
   Moore F., 2011, Journal of Evolutionary Psychology, V9, P3
   Moussawi S., 2019, Perceived intelligence and perceived anthropomorphism of personal intelligent agents: Scale development and validation, P5
   Murdock JW, 2008, J EXP THEOR ARTIF IN, V20, P1, DOI 10.1080/09528130701472416
   NORMAN DA, 1994, COMMUN ACM, V37, P68, DOI 10.1145/176789.176796
   Norman DA, 1986, User Centered System Design, DOI [10.1201/b15703, DOI 10.1201/B15703-3]
   Norouzi* N., 2019, Artificial intelligence in IoT, P1
   Nourani M., 2020, P AAAI C HUM COMP CR, V8, P112, DOI DOI 10.1609/HCOMP.V8I1.7469
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Nwana HS, 1996, KNOWL ENG REV, V11, P205, DOI 10.1017/S026988890000789X
   Paranjape B, 2023, Arxiv, DOI arXiv:2303.09014
   Park C. W., 1988, The American Journal of Psychology, P3
   Pinchin C., 2005, Issues in Philoso- phy: An Introduction, P3
   Qian Yang, 2020, CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3313831.3376301
   RADECKI CM, 1995, J EXP SOC PSYCHOL, V31, P107, DOI 10.1006/jesp.1995.1006
   Reeves B., 1996, Cambridge, UK, V10, P9
   Rheu M, 2021, INT J HUM-COMPUT INT, V37, P81, DOI 10.1080/10447318.2020.1807710
   Rolfhus EL, 1999, J EDUC PSYCHOL, V91, P511, DOI 10.1037/0022-0663.91.3.511
   Rosenfeld A, 2017, ARTIF INTELL, V252, P211, DOI 10.1016/j.artint.2017.08.005
   Rothmann S., 2003, SA Journal of industrial psychology, V29, P3
   Sahoo P, 2024, Arxiv, DOI arXiv:2402.07927
   Schwind V., 2018, interactions, V25, P9
   Seaborn K, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3386867
   Soliman M., 2022, Learning with Technologies and Technologies in Learning: Experience, Trends and Challenges in Higher Education, P3
   Sonlu S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3439795
   Sparks JR, 2018, LECT NOTES ARTIF INT, V10948, P469, DOI 10.1007/978-3-319-93846-2_88
   Suhel Sasha Fathima, 2020, 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), P611, DOI 10.1109/ICRITO48877.2020.9197825
   Tarau P., 2004, P 2004 ACM S APPL CO, P39
   Taylor M., 2023, The rise of the ai scientist: Unleashing the potential of chat-gpt powered avatars in virtual reality digital-twin laboratories, P1
   Thirunavukarasu AJ, 2023, NAT MED, V29, P1930, DOI 10.1038/s41591-023-02448-8
   Thorndike EL, 1927, AM J PSYCHOL, V39, P212, DOI 10.2307/1415413
   Tullis JG, 2018, MEM COGNITION, V46, P1360, DOI 10.3758/s13421-018-0842-4
   Volonte M, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P39, DOI 10.1109/VRW55335.2022.00015
   Wang CX, 2023, Arxiv, DOI arXiv:2310.07521
   Wanner L, 2017, LECT NOTES ARTIF INT, V10349, P284, DOI 10.1007/978-3-319-59930-4_23
   White J, 2023, Arxiv, DOI [arXiv:2302.11382, 10.48550/arXiv.2302.11382, DOI 10.48550/ARXIV.2302.11382]
   WILLIAMS EJ, 1949, AUST J SCI RES SER A, V2, P149, DOI 10.1071/CH9490149
   Zhang Qinyu, 2022, HCI International 2022 - Late Breaking Posters: 24th International Conference on Human-Computer Interaction, HCII 2022, Virtual Event, Proceedings. Communications in Computer and Information Science (1655), P713, DOI 10.1007/978-3-031-19682-9_91
   Zhu JR, 2023, INT SYM MIX AUGMENT, P751, DOI 10.1109/ISMAR59233.2023.00090
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 82
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7140
EP 7151
DI 10.1109/TVCG.2024.3456198
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300037
PM 39255105
DA 2024-11-06
ER

PT J
AU Ang, S
   Quarles, J
AF Ang, Samuel
   Quarles, John
TI SmoothRide: A Versatile Solution to Combat Cybersickness in
   Elevation-Altering Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cybersickness; Navigation; Virtual environments; Meters; Frequency
   modulation; Solid modeling; Skin; Human-centered computing-Human
   computer interaction (HCI)-Empirical studies in HCI; Human-centered
   computing-Human computer interaction (HCI)-Interaction paradigms-Virtual
   reality
ID TESTS
AB Cybersickness continues to bar many individuals from taking full advantage of virtual reality (VR) technology. Previous work has established that navigating virtual terrain with elevation changes poses a significant risk in this regard. In this paper, we investigate the effectiveness of three cybersickness reduction strategies on users performing a navigation task across virtual elevation-altering terrain. These strategies include static field of view (FOV) reduction, a flat surface approach that disables terrain collision and maintains constant elevation for users, and SmoothRide, a novel technique designed to dampen a user's perception of vertical motion as they travel. To assess the impact of these strategies, we conducted a within-subjects study involving 61 participants. Each strategy was compared against a control condition, where users navigated across terrain without any cybersickness reduction measures in place. Cybersickness data were collected using the Fast Motion Sickness Scale (FMS) and Simulator Sickness Questionnaire (SSQ), along with galvanic skin response (GSR) data. We measured user presence using the IGroup Presence questionnaire (IPQ) and a Single-Item Presence Scale (SIP). Our findings reveal that users experienced significantly lower levels of cybersickness using SmoothRide or FOV reduction. Presence scores reported on the IPQ were statistically similar between SmoothRide and the control condition. Conversely, terrain flattening had adverse effects on user presence scores, and we could not identify a significant effect on cybersickness compared to the control. We demonstrate that SmoothRide is an effective, lightweight, configurable, and easy-to-integrate tool for reducing cybersickness in simulations featuring elevation-altering terrain.
C1 [Ang, Samuel; Quarles, John] Univ Texas San Antonio, San Antonio, TX 78249 USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Ang, S (corresponding author), Univ Texas San Antonio, San Antonio, TX 78249 USA.
EM samuel.ang@utsa.edu; john.quarles@utsa.edu
FU National Science Foundation [IIS 2007041, IIS 2211785]
FX This work was funded through grants from the National Science Foundation
   (IIS 2007041, IIS 2211785). This organization had no input regarding the
   planning or details of the study.
CR Aaron R., 2022, PsyArXiv, DOI [10.31234/osf.io/ty8de6, DOI 10.31234/OSF.IO/TY8DE6]
   Agic A., 2020, Journal of Graphic Engineering and Design, V11, P5
   Al Zayer M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300584
   Ang S, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1027552
   Ang S, 2023, Symposium Virtual Re, P561, DOI 10.1109/VR55154.2023.00071
   Ang S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P428, DOI 10.1109/VR51125.2022.00062
   Bala P, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P244, DOI 10.1109/ISMAR-Adjunct.2018.00077
   Barhorst-Cates EM, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163785
   Bouchard S, 2004, 3RD IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2004, P59, DOI 10.1109/HAVE.2004.1391882
   Cummings JJ, 2016, MEDIA PSYCHOL, V19, P272, DOI 10.1080/15213269.2015.1015740
   Davis S, 2015, P 11 AUSTR C INT ENT
   Davis Simon, 2014, P 2014 C INT ENT, P1
   Dong X, 2011, J EXP PSYCHOL-APPL, V17, P128, DOI 10.1037/a0024097
   Dorado JL, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P47, DOI 10.1109/3DUI.2014.6798841
   Farmani Y., 2018, P 44 GRAPH INT C CAN, P2
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Freeman D, 2017, PSYCHOL MED, V47, P2393, DOI 10.1017/S003329171700040X
   Grassini S, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01743
   Habgood MPJ, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P371
   Heidicker P, 2017, IEEE SYMP 3D USER, P233, DOI 10.1109/3DUI.2017.7893357
   Ihemedu-Steinke QC, 2017, LECT NOTES COMPUT SC, V10280, P521, DOI 10.1007/978-3-319-57987-0_42
   Kala Nupur, 2017, SID Symposium Digest of Technical Papers, V48, P1645, DOI 10.1002/sdtp.11956
   Kapikiran G, 2022, CLIN SIMUL NURS, V62, P99, DOI 10.1016/j.ecns.2021.09.001
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Kim NG, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9091919
   Kim YY, 2005, PSYCHOPHYSIOLOGY, V42, P616, DOI 10.1111/j.1469-8986.2005.00349.x
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lin J, 2020, ADV ENG INFORM, V43, DOI 10.1016/j.aei.2020.101040
   Lin YX, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419984
   Lou RD, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1058, DOI [10.1109/VR.2019.8798164, 10.1109/vr.2019.8798164]
   Loup G, 2019, INT J HUM-COMPUT INT, V35, P1270, DOI 10.1080/10447318.2018.1519164
   Luks R, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P280, DOI 10.1145/3316782.3321535
   Mann HB, 1945, ECONOMETRICA, V13, P245, DOI 10.2307/1907187
   Newman M, 2022, J ENVIRON PSYCHOL, V79, DOI 10.1016/j.jenvp.2021.101733
   Nie GY, 2020, IEEE T VIS COMPUT GR, V26, P2535, DOI 10.1109/TVCG.2019.2893668
   Nie TY, 2023, Symposium Virtual Re, P658, DOI 10.1109/VR55154.2023.00081
   Onuki Y, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1864, DOI 10.1109/VR.2019.8797722
   Pouke M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P665, DOI 10.1109/VR.2018.8446078
   Qionghua W, 2019, Arxiv, DOI arXiv:1903.12617
   Sargunam SP, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON INTERACTIVE AND SPATIAL COMPUTING (IWISC 18), P74, DOI 10.1145/3191801.3191815
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   SCHUIRMANN DJ, 1987, J PHARMACOKINET BIOP, V15, P657, DOI 10.1007/BF01068419
   Seibert J, 2018, VIRTUAL REAL-LONDON, V22, P79, DOI 10.1007/s10055-017-0316-1
   Shafer DM, 2019, GAMES HEALTH J, V8, P15, DOI 10.1089/g4h.2017.0190
   Sharples S, 2008, DISPLAYS, V29, P58, DOI 10.1016/j.displa.2007.09.005
   Freitas JRS, 2021, PSYCHIAT QUART, V92, P1685, DOI 10.1007/s11126-021-09935-6
   SLATER M, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P90, DOI 10.1109/VRAIS.1993.380793
   Slater M., 1994, Presence: Teleoperators Virtual Environ, V3, P130, DOI DOI 10.1162/PRES.1994.3.2.130
   Sylaiou S, 2010, INT J HUM-COMPUT ST, V68, P243, DOI 10.1016/j.ijhcs.2009.11.002
   Theil H., 1950, Indagationes mathematicae, V12, P6
   Venkatakrishnan R., 2023, IEEE T VISUALIZATION
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P682, DOI [10.1109/VR46266.2020.00-13, 10.1109/VR46266.2020.1581195115265]
   Weech S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00158
   Whittinghill DavidMatthew., 2015, Games Developers Conference (GDC), page, P74
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Ziegler P, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P743, DOI 10.1109/VR.2018.8446221
NR 57
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7152
EP 7161
DI 10.1109/TVCG.2024.3456201
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300025
PM 39255132
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Schmidt, S
   Köysürenbars, I
   Steinicke, F
AF Schmidt, Susanne
   Koeysuerenbars, Ipek
   Steinicke, Frank
TI Frankenstein's Monster in the Metaverse: User Interaction With
   Customized Virtual Agents
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Cultural differences; Games; Robots; Metaverse; Medical
   services; Artificial intelligence; Intelligent virtual agents;
   personalization; user study
ID DISCREPANCY; PERSONALITY
AB Enabled by the latest achievements in artificial intelligence (AI), computer graphics as well as virtual, augmented, and mixed reality (VR/AR/MR), virtual agents are increasingly resembling humans in both their appearance and intelligent behavior. This results in enormous potential for agents to support users in their daily lives, for example in customer service, healthcare, education or the envisioned all-encompassing metaverse. Today's technology would allow users to customize their conversation partners in the metaverse - as opposed to reality - according to their preferences, potentially improving the user experience. On the other hand, there is little research on how reshaping the head of a communication partner might affect the immediate interaction with them. In this paper, we investigate the user requirements for and the effects of agent customization. In a two-stage user study ($N=30$), we collected both self-reported evaluations (e.g., intrinsic motivation) and interaction metrics (e.g., interaction duration and number of tried out items) for the process of agent customization itself as well as data on how users perceived the subsequent human-agent interaction in VR. Our results indicate that users only wish to have full customization for agents in their personal social circle, while for general services, a selection or even a definite assignment of pre-configured agents is sufficient. When customization is offered, attributes such as gender, clothing or hair are subjectively more relevant to users than facial features such as skin or eye color. Although the customization of human interaction partners is beyond our control, customization of virtual agents significantly increases perceived social presence as well as rapport and trust. Further findings on user motivation and agent diversity are discussed in the paper.
C1 [Schmidt, Susanne; Steinicke, Frank] Univ Hamburg, Hamburg, Germany.
C3 University of Hamburg
RP Schmidt, S (corresponding author), Univ Hamburg, Hamburg, Germany.
EM susanne.schmidt@uni-hamburg.de;
   ipek.koeysuerenbars@studium.uni-hamburg.de;
   frank.steinicke@uni-hamburg.de
RI ; Steinicke, Frank/AAC-2976-2020
OI Koysurenbars, Ipek/0009-0009-9295-7910; Steinicke,
   Frank/0000-0001-9879-7414
FU German Federal Ministry of Education and Research (BMBF)
FX This work was supported by the German Federal Ministry of Education and
   Research (BMBF).
CR Adinolf Sonam, 2020, CHI PLAY '20: Proceedings of the Annual Symposium on Computer-Human Interaction in Play, P461, DOI 10.1145/3410404.3414241
   Aparicio-Martinez P, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16214177
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Baylor A.L., 2003, E LEARN WORLD C E LE, P1507
   Baylor AL, 2005, FR ART INT, V125, P65
   Bessière K, 2007, CYBERPSYCHOL BEHAV, V10, P530, DOI 10.1089/cpb.2007.9994
   Birk MV, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2982, DOI 10.1145/2858036.2858062
   Cerekovic A, 2017, IEEE T AFFECT COMPUT, V8, P382, DOI 10.1109/TAFFC.2016.2545650
   Dolgov I, 2014, COMPUT HUM BEHAV, V33, P49, DOI 10.1016/j.chb.2013.12.028
   Ducheneaut N, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1151
   Dunn RA, 2012, COMPUT HUM BEHAV, V28, P97, DOI 10.1016/j.chb.2011.08.015
   Fox J, 2015, HUM-COMPUT INTERACT, V30, P401, DOI 10.1080/07370024.2014.921494
   Freeman Guo, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3432938
   Google, 2024, Text-to-Speech AI: Lifelike Speech Synthesis
   HIGGINS ET, 1987, PSYCHOL REV, V94, P319, DOI 10.1037/0033-295X.94.3.319
   Jian J.-Y., 2000, Int. J. Cognitive Ergonomics, V4, P53, DOI [10.1207/s15327566ijce040104, DOI 10.1207/S15327566IJCE040104, 10.1207/S15327566IJCE0401_04, DOI 10.1207/S15327566IJCE0401_04, 10.1207/S15327566IJCE040104]
   Jordbugg, 2024, Character Customizer | Game Toolkits | Unity Asset Store
   Kang H, 2020, COMPUT HUM BEHAV, V112, DOI 10.1016/j.chb.2020.106446
   Kelly R., 2004, MASSIVELY MULTIPLAYE
   Kim K, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3139151
   Knabe T., 2023, Realistic Eye Movements | Animation Tools | Unity Asset Store
   Kruse L, 2023, INT SYM MIX AUGMENT, P672, DOI 10.1109/ISMAR59233.2023.00082
   Kyrlitsias C, 2022, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.786665
   Loewen MGH, 2021, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01902
   Mancini T, 2017, COMPUT HUM BEHAV, V69, P275, DOI 10.1016/j.chb.2016.12.031
   marlenaklein msft, 2023, Mixed Reality Toolkit 3 Developer Documentation -MRTK3
   Meta, 2024, Oculus Lipsync for Unity Development: Unity
   Myers I. B., 1985, A guide to the development and use of the Myers-Briggs Type Indicator: Manual
   Nag P, 2020, PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (ACM IVA 2020), DOI 10.1145/3383652.3423876
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Rosenberg-Kima RB, 2008, COMPUT HUM BEHAV, V24, P2741, DOI 10.1016/j.chb.2008.03.017
   Ross J.M., 2008, Moderators of trust and reliance across multiple decision aids
   RYAN RM, 1983, J PERS SOC PSYCHOL, V45, P736, DOI 10.1037/0022-3514.45.4.736
   Schmidt S, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357591
   Schroeder R., 2001, The social life of avatars: Presence and interaction in shared virtual environments
   Soliman M., 2010, 2010 33rd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), P827
   Sung Y., 2011, Journal of Virtual Worlds Research, V4, DOI DOI 10.4101/JVWR.V4I1.1927
   Tsitseklis K, 2023, 2023 ADJUNCT PROCEEDINGS OF THE 31ST ACM CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION, UMAP 2023, P388, DOI 10.1145/3563359.3596661
   Wald Rebecca, 2021, UMAP '21: Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization, P382, DOI 10.1145/3450614.3463600
   Wu SX, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12102286
   Xiao J, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1293
   Yee N., 2006, Proceedings of PRESENCE, P24
NR 43
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7162
EP 7171
DI 10.1109/TVCG.2024.3456148
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300017
PM 39255104
DA 2024-11-06
ER

PT J
AU You, C
   Venkatakrishnan, R
   Venkatakrishnan, R
   Han, ZM
   Lok, B
   Peck, T
AF You, Christopher
   Venkatakrishnan, Roshan
   Venkatakrishnan, Rohith
   Han, Zhuoming
   Lok, Benjamin
   Peck, Tabitha
TI A Sense of Urgency on the Sense of Agency: Challenges in Evaluating
   Agency and Embodiment in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Virtual environments; Torso; Particle measurements; Motor
   drives; Current measurement; Atmospheric measurements
ID SELF-AVATAR; BODY-OWNERSHIP; IMPACT; INTERFERENCE; LOCOMOTION; BIAS
AB Control over an avatar in virtual reality can improve one's perceived sense of agency and embodiment towards their avatar. Yet, the relationship between control on agency and embodiment remains unclear. This work aims to investigate two main ideas: (1) the effectiveness of currently used metrics in measuring agency and embodiment and (2) the relationship between different levels of control on agency, embodiment, and cognitive performance. To do this, we conducted a between-participants user study with three conditions on agency (n = 57). Participants embodied an avatar with one of three types of control (i.e., Low - control over head only, Medium - control over head and torso, or High - control over head, torso, and arms) and completed a Stroop test. Our results indicate that the degree of control afforded to participants impacted their embodiment and cognitive performance but, as expected, could not be detected in the self-reported agency scores. Furthermore, our results elucidated further insights into the relationship between control and embodiment, suggesting potential uncanny valley-like effects. Future work should aim to refine agency measures to better capture the effect of differing levels of control and consider other methodologies to measure agency.
C1 [You, Christopher; Venkatakrishnan, Roshan; Venkatakrishnan, Rohith; Han, Zhuoming; Lok, Benjamin] Univ Florida, Gainesville, FL 32611 USA.
   [Peck, Tabitha] Davidson Coll, Davidson, NC USA.
C3 State University System of Florida; University of Florida; Davidson
   College
RP You, C (corresponding author), Univ Florida, Gainesville, FL 32611 USA.
EM christopheryou@ufl.edu; rvenkatakrishnan@ufl.edu;
   rohith.venkatakr@ufl.edu; zhuominghan@ufl.edu; lok@cise.ufl.edu;
   tapeck@davidson.edu
RI Venkatakrishnan, Roshan/JDC-3508-2023; Venkatakrishnan,
   Rohith/JCE-8736-2023
OI Venkatakrishnan, Roshan/0000-0002-6538-627X; Lok,
   Benjamin/0000-0002-1190-3729; Venkatakrishnan,
   Rohith/0000-0002-8484-3915
FU National Science Foundation [1942146]
FX This material is based upon work supported by the National Science
   Foundation under Grant 1942146. Any opinions, findings, and conclusions
   or recommendations expressed in this material are those of the authors
   and do not necessarily reflect the views of the National Science
   Foundation. We'd also like to thank Eileen Kelley for her contribution
   to the article and demo video.
CR Argelaguet F, 2016, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2016.7504682
   Ash E, 2016, GAMES CULT, V11, P422, DOI 10.1177/1555412014568870
   Banakou D, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00917
   Banakou D, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00601
   Banakou D, 2014, P NATL ACAD SCI USA, V111, P17678, DOI 10.1073/pnas.1414936111
   Banakou D, 2013, P NATL ACAD SCI USA, V110, P12846, DOI 10.1073/pnas.1306779110
   Berger CC, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aar7010
   Bhargava A, 2023, IEEE T VIS COMPUT GR, V29, P2348, DOI 10.1109/TVCG.2023.3247067
   Bhargava A, 2023, Symposium Virtual Re, P308, DOI 10.1109/VR55154.2023.00046
   Blanke O, 2009, TRENDS COGN SCI, V13, P7, DOI 10.1016/j.tics.2008.10.003
   Caspar EA, 2015, CONSCIOUS COGN, V33, P226, DOI 10.1016/j.concog.2015.01.007
   D'Alonzo M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-55478-z
   de Vignemont F, 2011, CONSCIOUS COGN, V20, P82, DOI 10.1016/j.concog.2010.09.004
   Dewez D, 2020, INT SYM MIX AUGMENT, P452, DOI 10.1109/ISMAR50242.2020.00070
   Ebrahimi E, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1, DOI 10.1109/VR.2018.8446539
   Eubanks JC, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.647896
   Eubanks JC, 2020, INT SYM MIX AUGMENT, P54, DOI 10.1109/ISMAR50242.2020.00025
   Falconer CJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111933
   Fox J., 2013, Handbook of research on technoself: identity in a technological society, P255, DOI DOI 10.4018/978-1-4666-2211-1.CH014
   Franck N, 2001, AM J PSYCHIAT, V158, P454, DOI 10.1176/appi.ajp.158.3.454
   Fribourg R, 2021, IEEE T VIS COMPUT GR, V27, P4023, DOI 10.1109/TVCG.2020.2999197
   Fribourg R, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P273, DOI 10.1109/VR.2018.8448293
   Frith CD, 2000, PHILOS T R SOC B, V355, P1771, DOI 10.1098/rstb.2000.0734
   Goldin-Meadow S, 2001, PSYCHOL SCI, V12, P516, DOI 10.1111/1467-9280.00395
   Gonzalez-Franco M, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00074
   González-Franco M, 2010, P IEEE VIRT REAL ANN, P111, DOI 10.1109/VR.2010.5444805
   Guy M., 2022, ICAT EGVE2022, P1
   Hefner D, 2007, LECT NOTES COMPUT SC, V4740, P39
   Jiang Y, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3581027
   Jostmann NB, 2007, J EXP PSYCHOL GEN, V136, P593, DOI 10.1037/0096-3445.136.4.593
   Kilteni K, 2013, IEEE T VIS COMPUT GR, V19, P597, DOI 10.1109/TVCG.2013.29
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kobayashi D, 2018, LECT NOTES COMPUT SC, V10904, P381, DOI 10.1007/978-3-319-92043-6_32
   Koch S, 2009, PSYCHOL SCI, V20, P549, DOI 10.1111/j.1467-9280.2009.02342.x
   Kokkinara E, 2015, ACM T APPL PERCEPT, V13, DOI 10.1145/2818998
   Kondo R, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-25951-2
   Kong GQ, 2017, CONSCIOUS COGN, V52, P115, DOI 10.1016/j.concog.2017.04.018
   La Rocca S, 2020, ANN REV CYBERTHERAPY, V18, P127
   Lee KM, 2004, COMMUN THEOR, V14, P27, DOI 10.1093/ct/14.1.27
   Lee YH, 2018, CYBERPSYCH BEH SOC N, V21, P173, DOI 10.1089/cyber.2017.0451
   Lopez S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300787
   Lugrin JL, 2015, P IEEE VIRT REAL ANN, P225, DOI 10.1109/VR.2015.7223377
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Ogawa N, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P647, DOI 10.1109/VR.2018.8446318
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Peck TC, 2021, IEEE T VIS COMPUT GR, V27, P2502, DOI 10.1109/TVCG.2021.3067767
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1964, DOI 10.1109/TVCG.2020.2973061
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Peña J, 2014, COMPUT HUM BEHAV, V41, P262, DOI 10.1016/j.chb.2014.09.038
   Peña J, 2009, COMMUN RES, V36, P838, DOI 10.1177/0093650209346802
   Petkova VI, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00035
   Piryankova IV, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103428
   Rosa N, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1138, DOI [10.1109/VR.2019.8798055, 10.1109/vr.2019.8798055]
   Sato A, 2005, COGNITION, V94, P241, DOI 10.1016/j.cognition.2004.04.003
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Slater M, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.006.2008
   Spanlang B, 2014, FRONT ROBOT AI, DOI 10.3389/frobt.2014.00009
   Spiel K, 2021, PROCEEDINGS OF THE FIFTEENTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION, TEI 2021, DOI 10.1145/3430524.3440651
   Steed A, 2016, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2016.7504689
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Szolin K, 2023, HUM-COMPUT INTER-US, V38, P374, DOI 10.1080/07370024.2022.2103419
   Taylor TL, 1999, AM BEHAV SCI, V43, P436, DOI 10.1177/00027649921955362
   Tsakiris M, 2007, CONSCIOUS COGN, V16, P645, DOI 10.1016/j.concog.2007.05.012
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   Venkatakrishnan R, 2023, IEEE T VIS COMPUT GR, V29, P2258, DOI 10.1109/TVCG.2023.3247041
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
   You C, 2024, FRONT VIRTUAL REAL, V5, DOI 10.3389/frvir.2024.1251564
   Yuan Y, 2010, P IEEE VIRT REAL ANN, P95, DOI 10.1109/VR.2010.5444807
NR 69
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7172
EP 7182
DI 10.1109/TVCG.2024.3456194
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300033
PM 39255098
DA 2024-11-06
ER

PT J
AU Wang, ZJ
   Wu, J
   Fan, RZ
   Ke, W
   Wang, LL
AF Wang, Zijun
   Wu, Jian
   Fan, Runze
   Ke, Wei
   Wang, Lili
TI VPRF: Visual Perceptual Radiance Fields for Foveated Image Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Neural radiance field; Visualization;
   Sensitivity; Training; Three-dimensional displays; Image reconstruction;
   Virtual reality; Foveated rendering; Visual perceptual; Contrast
   sensitivity
AB Neural radiance fields (NeRF) has achieved revolutionary breakthrough in the novel view synthesis task for complex 3D scenes. However, this new paradigm struggles to meet the requirements for real-time rendering and high perceptual quality in virtual reality. In this paper, we propose VPRF, a novel visual perceptual based radiance fields representation method, which for the first time integrates the visual acuity and contrast sensitivity models of human visual system (HVS) into the radiance field rendering framework. Initially, we encode both the appearance and visual sensitivity information of the scene into our radiance field representation. Then, we propose a visual perceptual sampling strategy, allocating computational resources according to the HVS sensitivity of different regions. Finally, we propose a sampling weight-constrained training scheme to ensure the effectiveness of our sampling strategy and improve the representation of the radiance field based on the scene content. Experimental results demonstrate that our method renders more efficiently, with higher PSNR and SSIM in the foveal and salient regions compared to the state-of-the-art FoV-NeRF. The results of the user study confirm that our rendering results exhibit high-fidelity visual perception.
C1 [Wang, Lili] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Shengzhen, Peoples R China.
   [Wang, Lili] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing, Peoples R China.
   [Wang, Zijun; Wu, Jian; Fan, Runze] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Ke, Wei] Macau Polytech Univ, Fac Appl Sci, Macau, Peoples R China.
C3 Beihang University; Beihang University; Beihang University; Macao
   Polytechnic University
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Shengzhen, Peoples R China.; Wang, LL (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing, Peoples R China.
EM 892710638@qq.com; lanayawj@buaa.edu.cn; by2106131@buaa.edu.cn;
   wke@mpu.edu.mo; wanglily@buaa.edu.cn
RI Ke, Wei/LOS-3255-2024
OI Ke, Wei/0000-0003-0952-0961
FU National Natural Science Foundation of China [61932003, 62372026];
   Beijing Science and Technology Plan [Z221100007722004]; National Key RD
   plan [2019YFC1521102]
FX This work is supported by the National Natural Science Foundation of
   China through Projects 61932003 and 62372026, Beijing Science and
   Technology Plan Project Z221100007722004, and the National Key R&D plan
   2019YFC1521102.
CR Bauer D, 2023, IEEE T VIS COMPUT GR, V29, P515, DOI 10.1109/TVCG.2022.3209498
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Deng NC, 2022, IEEE T VIS COMPUT GR, V28, P3854, DOI 10.1109/TVCG.2022.3203102
   Franke L, 2021, COMPUT GRAPH FORUM, V40, P110, DOI 10.1111/cgf.14176
   Fridovich-Keil S, 2023, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR52729.2023.01201
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Friston S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323033
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Gu J., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Hu T, 2022, PROC CVPR IEEE, P12892, DOI 10.1109/CVPR52688.2022.01256
   Jindal A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480514
   Kaplanyan AS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356557
   Kim KJ, 2013, PROC SPIE, V8651, DOI 10.1117/12.2002178
   Koskela Matias, 2020, Foveated path tracing with fast reconstruction and efficient sample distribution
   Koskela Matias, 2019, P 30 EUR S REND EUR
   Kurz A, 2022, LECT NOTES COMPUT SC, V13677, P254, DOI 10.1007/978-3-031-19790-1_16
   Meng XX, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203199
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Mohanto B, 2022, COMPUT GRAPH-UK, V102, P474, DOI 10.1016/j.cag.2021.10.010
   Molenaar Erik N, 2018, Master's thesis
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Murphy HA, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462053
   Neff T., 2021, COMPUT GRAPH FORUM, V40, P45, DOI [10.1111/cgf.14340, DOI 10.1111/CGF.14340]
   Piala M, 2021, INT CONF 3D VISION, P1106, DOI 10.1109/3DV53792.2021.00118
   Shi XH, 2023, Symposium Virtual Re, P471, DOI 10.1109/VR55154.2023.00062
   Stengel M, 2016, COMPUT GRAPH FORUM, V35, P129, DOI 10.1111/cgf.12956
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   Vater C, 2022, PSYCHON B REV, V29, P1531, DOI 10.3758/s13423-022-02117-w
   Wang LL, 2023, COMPUT VIS MEDIA, V9, P195, DOI 10.1007/s41095-022-0306-4
   Watson AB, 2014, J VISION, V14, DOI 10.1167/14.7.15
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   WEYMOUTH FW, 1958, AM J OPHTHALMOL, V46, P102, DOI 10.1016/0002-9394(58)90042-4
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yue YH, 2023, SCI CHINA INFORM SCI, V66, DOI 10.1007/s11432-022-3686-1
   Zhou WJ, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-020-3337-9
NR 39
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7183
EP 7192
DI 10.1109/TVCG.2024.3456205
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300019
PM 39259633
DA 2024-11-06
ER

PT J
AU Mahmud, MR
   Cordova, A
   Quarles, J
AF Mahmud, M. Rasel
   Cordova, Alberto
   Quarles, John
TI Multimodal Feedback Methods for Advancing the Accessibility of Immersive
   Virtual Reality for People With Balance Impairments Due to Multiple
   Sclerosis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Virtual environments; Stability analysis; Multiple
   sclerosis; Motors; Usability; Tactile sensors; Multimodal feedback;
   immersive VR; HMDs; Multiple Sclerosis
AB Maintaining balance in immersive virtual reality (VR) environments poses a significant challenge for users, particularly affecting those with pre-existing balance disorders. This study investigates the efficacy of multimodal feedback-comprising auditory, vibrotactile, and visual stimuli-in mitigating balance issues within VR. A sample of 68 participants, divided equally between individuals with balance deficits related to multiple sclerosis and those without, was evaluated. The research explored the impact of various feedback conditions on balance performance. The results demonstrated that the multimodal feedback condition significantly enhanced balance control compared to other conditions, with statistical analysis confirming this improvement (p <. 001). These findings underscore the potential of integrated sensory feedback in addressing balance-related difficulties in VR, thereby improving the overall accessibility and user experience for individuals affected by balance impairments. This research contributes valuable insights into optimizing VR environments for enhanced stability and user comfort.
C1 [Mahmud, M. Rasel] Kennesaw State Univ, Dept Comp Sci, Kennesaw, GA 30144 USA.
   [Cordova, Alberto] Univ Texas San Antonio, Dept Hlth & Kinesiol, San Antonio, TX USA.
   [Quarles, John] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX USA.
C3 University System of Georgia; Kennesaw State University; University of
   Texas System; University of Texas at San Antonio (UTSA); University of
   Texas System; University of Texas at San Antonio (UTSA)
RP Mahmud, MR (corresponding author), Kennesaw State Univ, Dept Comp Sci, Kennesaw, GA 30144 USA.
EM mmahmud2@kennesaw.edu; Alberto.Cordova@utsa.edu; John.Quarles@utsa.edu
CR Bouchard S, 2004, 3RD IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2004, P59, DOI 10.1109/HAVE.2004.1391882
   Chiari L, 2005, IEEE T BIO-MED ENG, V52, P2108, DOI 10.1109/TBME.2005.857673
   Epure P., 2014, The 10th International Conference on Disability Virtual Reality Associated Technologies: Proceedings, P119
   Ferdous S. M. S., 2018, P 24 ACM S VIRT REAL, P1
   Ferdous SMS, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.582169
   Gandemer L, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00357
   Gerber Stephan M, 2019, JMIR Perioper Med, V2, pe15579, DOI 10.2196/15579
   Ghai S, 2018, AGING DIS, V9, P901, DOI 10.14336/AD.2017.1031
   Helps SK, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112768
   Hombeck J, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P514, DOI 10.1109/VR51125.2022.00071
   Kelly JW, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3313902
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Kingma H, 2019, J NEUROL, V266, P19, DOI 10.1007/s00415-018-9133-z
   Mahmud MR, 2022, 28TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2022, DOI 10.1145/3562939.3565638
   Mahmud MR, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P782, DOI 10.1109/VR51125.2022.00100
   Martinez A., 2018, Journal on Interactive Systems, V9
   Mikropoulos TA, 2020, PROCEEDINGS OF 2020 6TH INTERNATIONAL CONFERENCE OF THE IMMERSIVE LEARNING RESEARCH NETWORK (ILRN 2020), P86, DOI [10.23919/ilrn47897.2020.9155113, 10.23919/iLRN47897.2020.9155113]
   Mohebbi A, 2022, J NEUROPHYSIOL, V127, P1159, DOI 10.1152/jn.00283.2021
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Murata A, 2004, INT J HUM-COMPUT INT, V17, P463, DOI 10.1207/s15327590ijhc1704_2
   Naef AC, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-21575-9
   Naef M., 2002, VIRTUAL REALITY SOFT, P65
   Rasel Mahmud M., 2022, Vibrotactile feedback to make real walking in virtual reality more accessible
   Salavati M, 2009, GAIT POSTURE, V29, P460, DOI 10.1016/j.gaitpost.2008.11.016
   Schepens S, 2010, ARCH GERONTOL GERIAT, V51, P9, DOI 10.1016/j.archger.2009.06.003
   Soffel F, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P351, DOI 10.1145/2993369.2996341
   Soltani P, 2021, FRONT SPORTS ACT LIV, V2, DOI 10.3389/fspor.2020.531535
   Sondell B, 2005, PRESENCE-TELEOP VIRT, V14, P191, DOI 10.1162/1054746053967003
   Wenzel E. M., 2017, Immersive sound, P8
NR 29
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7193
EP 7202
DI 10.1109/TVCG.2024.3456139
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300023
DA 2024-11-06
ER

PT J
AU Ebner, C
   Plopski, A
   Schmalstieg, D
   Kalkofen, D
AF Ebner, Christoph
   Plopski, Alexander
   Schmalstieg, Dieter
   Kalkofen, Denis
TI Gaze-Contingent Layered Optical See-Through Displays with a
   Confidence-Driven View Volume
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Gaze tracking; Estimation; Lenses; Optical imaging; Light fields;
   Three-dimensional displays; Focusing; Gaze-Contingent Layered Display;
   Optical See-Through Mixed Reality; Vergence-Accommodation Conflict
ID MIXED REALITY
AB The vergence-accommodation conflict (VAC) presents a major perceptual challenge for head-mounted displays with a fixed image plane. Varifocal and layered display designs can mitigate the VAC. However, the image quality of varifocal displays is affected by imprecise eye tracking, whereas layered displays suffer from reduced image contrast as the distance between layers increases. Combined designs support a larger workspace and tolerate some eye-tracking error. However, any layered design with a fixed layer spacing restricts the amount of error compensation and limits the in-focus contrast. We extend previous hybrid designs by introducing confidence-driven volume control, which adjusts the size of the view volume at runtime. We use the eye tracker's confidence to control the spacing of display layers and optimize the trade-off between the display's view volume and the amount of eye tracking error the display can compensate. In the case of high-quality focus point estimation, our approach provides high in-focus contrast, whereas low-quality eye tracking increases the view volume to tolerate the error. We describe our design, present its implementation as an optical-see head-mounted display using a multiplicative layer combination, and present an evaluation comparing our design with previous approaches.
C1 [Ebner, Christoph; Plopski, Alexander; Schmalstieg, Dieter; Kalkofen, Denis] Graz Univ Technol, Graz, Austria.
   [Schmalstieg, Dieter] Univ Stuttgart, Stuttgart, Germany.
   [Kalkofen, Denis] Flinders Univ S Australia, Adelaide, Australia.
C3 Graz University of Technology; University of Stuttgart; Flinders
   University South Australia
RP Ebner, C (corresponding author), Graz Univ Technol, Graz, Austria.
EM christoph.ebner@tugraz.at; alexander.plopski@tugraz.at;
   dieter.schmalstieg@visus.uni-stuttgart.de;
   denis.kalkofen@flinders.edu.au
FU Snap, Inc.; Alexander von Humboldt Foundation-German Federal Ministry of
   Education and Research
FX This work was supported by Snap, Inc., and the Alexander von Humboldt
   Foundation funded by the German Federal Ministry of Education and
   Research.
CR Aksit K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130892
   Arefin MS, 2022, IEEE T VIS COMPUT GR, V28, P2014, DOI 10.1109/TVCG.2022.3150503
   Batmaz AU, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502067
   Chakravarthula P, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356539
   Chang JHR, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392424
   Chang JHR, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275015
   Duchowski A. T., 2014, P S EYE TRACK RES AP, P103, DOI DOI 10.1145/2578153.2578168
   Dunn D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1838, DOI [10.1109/VR.2019.8798273, 10.1109/vr.2019.8798273]
   Dunn D, 2017, IEEE T VIS COMPUT GR, V23, P1275, DOI 10.1109/TVCG.2017.2657058
   Ebner C, 2023, IEEE T VIS COMPUT GR, V29, P2816, DOI 10.1109/TVCG.2023.3247077
   Ebner C, 2022, IEEE T VIS COMPUT GR, V28, P2256, DOI 10.1109/TVCG.2022.3150504
   Hensley J, 2005, COMPUT GRAPH FORUM, V24, P547, DOI 10.1111/j.1467-8659.2005.00880.x
   Hirzle T, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300855
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Itoh Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3453157
   Kim J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322987
   Kress B., 2020, Press Monographs, DOI [10.1117/3.25593041,2, DOI 10.1117/3.25593041,2]
   Lanman D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508366
   Lee S, 2015, J DISP TECHNOL, V11, P845, DOI 10.1109/JDT.2014.2386216
   Liu S, 2008, INT SYM MIX AUGMENT, P33, DOI 10.1109/ISMAR.2008.4637321
   Machuca MDB, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300437
   MacKenzie KJ, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.1.011002
   Maimone A, 2013, INT SYM MIX AUGMENT, P29, DOI 10.1109/ISMAR.2013.6671761
   Maruyama K, 2020, IEEE ACCESS, V8, P38767, DOI 10.1109/ACCESS.2020.2975209
   Matsumura I., 1983, Advances in Diagnostic Visual Optics. Proceedings of the Second International Symposium, P36
   Mercier O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130846
   Narain R, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766909
   Padmanaban N, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aav6187
   Padmanaban N, 2017, P NATL ACAD SCI USA, V114, P2183, DOI 10.1073/pnas.1617251114
   Peng YF, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417802
   Pi DP, 2022, LIGHT-SCI APPL, V11, DOI 10.1038/s41377-022-00916-3
   Rathinavel K, 2019, IEEE T VIS COMPUT GR, V25, P3125, DOI 10.1109/TVCG.2019.2933120
   Rathinavel K, 2018, IEEE T VIS COMPUT GR, V24, P2857, DOI 10.1109/TVCG.2018.2868570
   Schmalstieg D., 2016, Augmented Reality-Principles and Practice, P1
   Schwerdtfeger B, 2009, INT SYM MIX AUGMENT, P115, DOI 10.1109/ISMAR.2009.5336484
   Takahashi K, 2018, IEEE T IMAGE PROCESS, V27, P4571, DOI 10.1109/TIP.2018.2839263
   Toyama T, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, AVI 2014, P25, DOI 10.1145/2598153.2598154
   Weier M, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204547
   Wetzstein G, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185576
   Wetzstein G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964990
   Wilson A, 2018, PROC SPIE, V10676, DOI 10.1117/12.2315771
   Wu WM, 2016, IEEE INT CON MULTI
   Youngho Lee, 2017, 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR). Proceedings, P26, DOI 10.1109/ISUVR.2017.13
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 45
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7203
EP 7213
DI 10.1109/TVCG.2024.3456184
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300022
PM 39255112
DA 2024-11-06
ER

PT J
AU Lammert, A
   Rendle, G
   Immohr, F
   Neidhardt, A
   Brandenburg, K
   Raake, A
   Froehlich, B
AF Lammert, Anton
   Rendle, Gareth
   Immohr, Felix
   Neidhardt, Annika
   Brandenburg, Karlheinz
   Raake, Alexander
   Froehlich, Bernd
TI Immersive Study Analyzer: Collaborative Immersive Analysis of Recorded
   Social VR Studies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaboration; Recording; Annotations; Reviews; Encoding; Navigation;
   Manuals; Collaborative Immersive Analytics; Social VR Studies; Behavior
   Analysis
ID BEHAVIOR
AB Virtual Reality (VR) has become an important tool for conducting behavioral studies in realistic, reproducible environments. In this paper, we present ISA, an Immersive Study Analyzer system designed for the comprehensive analysis of social VR studies. For in-depth analysis of participant behavior, ISA records all user actions, speech, and the contextual environment of social VR studies. A key feature is the ability to review and analyze such immersive recordings collaboratively in VR, through support of behavioral coding and user-defined analysis queries for efficient identification of complex behavior. Respatialization of the recorded audio streams enables analysts to follow study participants' conversations in a natural and intuitive way. To support phases of close and loosely coupled collaboration, ISA allows joint and individual temporal navigation, and provides tools to facilitate collaboration among users at different temporal positions. An expert review confirms that ISA effectively supports collaborative immersive analysis, providing a novel and effective tool for nuanced understanding of user behavior in social VR studies.
C1 [Lammert, Anton; Rendle, Gareth; Froehlich, Bernd] Bauhaus Univ Weimar, Weimar, Germany.
   [Immohr, Felix; Brandenburg, Karlheinz; Raake, Alexander] TU Ilmenau, Ilmenau, Germany.
   [Neidhardt, Annika] Univ Surrey, Surrey, England.
C3 Bauhaus-Universitat Weimar; Technische Universitat Ilmenau; University
   of Surrey
RP Lammert, A (corresponding author), Bauhaus Univ Weimar, Weimar, Germany.
EM anton.benjamin.lammert@uni-weimar.de; gareth.rendle@uni-weimar.de;
   felix.immohr@tu-ilmenau.de; a.neidhardt@surrey.ac.uk;
   karlheinz.brandenburg@tu-ilmenau.de; alexander.raake@tu-ilmenau.de;
   bernd.froehlich@uni-weimar.de
RI ; Raake, Alexander/R-7050-2017
OI Rendle, Gareth/0000-0003-0399-8294; Froehlich,
   Bernd/0000-0002-9439-1959; Brandenburg, Karlheinz/0000-0002-8089-9508;
   Immohr, Felix/0009-0000-4777-7560; Raake, Alexander/0000-0002-9357-1763;
   Neidhardt, Annika/0000-0002-4243-5737; Lammert,
   Anton/0009-0003-7254-1690
FU Deutsche Forschungsgemeinschaft (DFG) [444831328, SPP2236-AUDICTIVE];
   German Federal Ministry of Education and Research (BMBF) [16SV8716];
   Thuringian Ministry for Economic Affairs, Science and Digital Society
   [5575/10-5]
FX This work was mainly funded by the Deutsche Forschungsgemeinschaft (DFG)
   under the project "Audiovisual Plausibility and Experience in
   Multi-Party Mixed Reality"(APlausE-MR) (ID: 444831328) as part of the
   DFG Priority Program SPP2236-AUDICTIVE. This work was also funded by the
   German Federal Ministry of Education and Research (BMBF) under grant
   16SV8716 (project Goethe-Live-3D) and the Thuringian Ministry for
   Economic Affairs, Science and Digital Society under grant 5575/10-5
   (MetaReal). We thank the members of the Virtual Reality and
   Visualization group at Bauhaus-Universitat Weimar (
   https://uni-weimar.de/vr ) for their support.
CR Apaolaza A., 2016, P 2016 CHI C HUM FAC, P2766
   Arora M., 2020, ACM SIGGRAPH 2020, P1, DOI [10.1145/3388770.34074122, DOI 10.1145/3388770.34074122]
   Barto D, 2017, BEHAV RES METHODS, V49, P1563, DOI 10.3758/s13428-016-0787-0
   Brandstätter K, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P979, DOI 10.1109/VRW58643.2023.00332
   Brüning B, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P301
   Brugman H., 2009, INT C LANG RES EV, P6
   Buschel Wolfgang, 2021, P 2021 CHI C HUM FAC, P1, DOI [DOI 10.1145/3411764.3445651, 10.1145/3411764.3445651]
   Butcher PWS, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312798
   Cavallo M, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364242
   Chandler T, 2015, 2015 BIG DATA VISUAL ANALYTICS (BDVA)
   Chow Kevin, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359142
   Coan J., 2007, Handbook of Emotion Elicitation and Assessment, P4
   Coan J., 2007, Handbook of Emotion Elicitation and Assessment, P2
   Cockburn A., 1997, GROUP '97. Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work. The Integration Challenge, P47, DOI 10.1145/266838.266857
   Congdon EL, 2018, ORGAN RES METHODS, V21, P489, DOI 10.1177/1094428116654548
   Craven M., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P30, DOI 10.1145/365024.365032
   Nguyen C, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P267, DOI 10.1145/3126594.3126659
   DUMAS J, 1995, HUM FAC ERG SOC P, P228
   Dwyer T, 2018, LECT NOTES COMPUT SC, V11190, P1, DOI 10.1007/978-3-030-01388-2_1
   Ens B., 2021, P 2021 CHI C HUM FAC, P1, DOI [10. 1145/3411764.3446866 8, DOI 10.1145/3411764.34468668]
   Fominykh M., 2015, Intelligent Systems Reference Library, V84, P137, DOI 10.1007/
   Fonnet A, 2021, IEEE T VIS COMPUT GR, V27, P2101, DOI 10.1109/TVCG.2019.2929033
   Gutwin C., 2002, Computer Supported Cooperative Work: The Journal of Collaborative Computing, V11, P411, DOI 10.1023/A:1021271517844
   Hayes AT, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.773448
   Hubenschmid S, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517550
   Nguyen H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1098, DOI [10.1109/vr.2019.8797845, 10.1109/VR.2019.8797845]
   Imai T., 2000, P INET, P2
   Immohr F, 2024, Symposium Virtual Re, P849, DOI 10.1109/VR58804.2024.00104
   Jansen P, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580760
   Kloiber S, 2022, IEEE INT SYMP M AU R, P391, DOI 10.1109/ISMAR-Adjunct57072.2022.00085
   Kloiber S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P421, DOI 10.1109/VRW55335.2022.00094
   Kloiber S, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P93, DOI 10.1109/CW52790.2021.00021
   Kloiber S, 2020, VISUAL COMPUT, V36, P1937, DOI 10.1007/s00371-020-01942-1
   Kunert A., 2014, P 17 ACM C COMPUTER, P1388, DOI [10.1145/2531602.25317274, DOI 10.1145/2531602.25317274]
   Lee J, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545679
   Lilija K, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376604
   Lin TC, 2023, Arxiv, DOI arXiv:2307.12539
   Luo WZ, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580715
   Mahadevan K., P 2023 CHI C HUM FAC, P1, DOI [10.1145/3544548.3580876 2, 8, DOI 10.1145/3544548.35808762,8]
   Marquardt N, 2015, LECT NOTES COMPUT SC, V9297, P89, DOI 10.1007/978-3-319-22668-2_8
   Nebeling M., 2020, P 2020 CHI C HUM FAC, P1, DOI [10.1145/3313831.3376330 2, 5, 8, DOI 10.1145/3313831.33763302,5,8]
   Nebeling M, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445323
   Neidhardt A., 2017, AUDIO ENG SOC CONVEN
   Pick S., 2019, PhD thesis, DOI [10.18154/RWTH-2019-11071, DOI 10.18154/RWTH-2019-11071]
   Reipschläger P, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517676
   Rinaldi V, 2022, LECT NOTES COMPUT SC, V13445, P154, DOI 10.1007/978-3-031-15546-8_14
   Schoenenberg K, 2014, SPEECH COMMUN, V63-64, P1, DOI 10.1016/j.specom.2014.04.005
   Stahl O., 1999, VRST'99. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, P164, DOI 10.1145/323663.323691
   Tang HH, 2011, DESIGN STUD, V32, P1, DOI 10.1016/j.destud.2010.06.004
   Thanyadit Santawat, 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3512983
   Van Acker BB, 2021, ERGONOMICS, V64, P78, DOI 10.1080/00140139.2020.1811400
   Wang CY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376642
   Weingart L.R., 2004, International Negotiation, V9, P441, DOI [DOI 10.1163/1571806053498805, 10.1163/15718060534988052, DOI 10.1163/15718060534988052]
   Williamson J. R., CHI C HUM FACT COMP, P1
   Xia HJ, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P853, DOI 10.1145/3242587.3242597
   Yuan L., 2023, Echoes in the gallery: A collaborative immersive analytics system for analyzing audience reactions in virtual reality exhibitions, DOI [10.31219/osf.io/zmyx92, DOI 10.31219/OSF.IO/ZMYX92]
   Zandbelt LC, 2005, SOC SCI MED, V61, P661, DOI 10.1016/j.socscimed.2004.12.006
NR 57
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7214
EP 7224
DI 10.1109/TVCG.2024.3456189
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300018
PM 39255175
DA 2024-11-06
ER

PT J
AU Pöhlmann, KMT
   Li, G
   Wilson, G
   McGill, M
   Pollick, F
   Brewster, S
AF Pohlmann, Katharina M. T.
   Li, Gang
   Wilson, Graham
   McGill, Mark
   Pollick, Frank
   Brewster, Stephen
TI Is Video Gaming a Cure for Cybersickness? Gamers Experience Less
   Cybersickness Than Non-Gamers in a VR Self-Motion Task
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cybersickness; Visualization; Virtual Reality; Video Gamers;
   Habituation; Virtual Reality; Video Gamers; Habituation
ID SEX-DIFFERENCES; VIRTUAL ENVIRONMENTS; SIMULATOR SICKNESS; HABITUATION;
   GENDER; PERFORMANCE; ADAPTATION; CAPTURE
AB Cybersickness remains a major drawback of Virtual Reality (VR) headsets, as a breadth of stationary experiences with visual self-motion can result in visually-induced motion sickness. However, not everybody experiences the same intensity or type of adverse symptoms. Here we propose that prior experience with virtual environments can predict ones degree of cybersickness. Video gaming can enhance visuospatial abilities, which in-turn relate negatively to cybersickness - meaning that consistently engaging in virtual environments can result in protective habituation effects. In a controlled stationary VR experiment, we found that 'VR-naive' video gamers experienced significantly less cybersickness in a virtual tunnel-travel task and outperformed 'VR-naive' non-video gamers on a visual attention task. These findings strongly motivate the use of non-VR games for training VR cybersickness resilience, with future research needed to further understand the mechanism(s) by which gamers become cybersickness resilient - potentially expanding access to VR for even the most susceptible participants.
C1 [Pohlmann, Katharina M. T.] KITE Toronto Res Inst, Toronto, ON, Canada.
   [Li, Gang; Wilson, Graham; McGill, Mark; Pollick, Frank; Brewster, Stephen] Univ Glasgow, Glasgow, Scotland.
C3 University of Glasgow
RP Pöhlmann, KMT (corresponding author), KITE Toronto Res Inst, Toronto, ON, Canada.
EM Pohlmannkat.poehlmann@gmail.com; gang.li@glasgow.ac.uk;
   graham.wilson@glasgow.ac.uk; mark.mcgill@glasgow.ac.uk;
   frank.pollick@glasgow.ac.uk; stephen.brewster@glasgow.ac.uk
RI Li, Gang/AEI-7865-2022; brewster, stephen/J-9003-2017; Pollick,
   Frank/F-3186-2011
OI Wilson, Graham/0000-0003-2664-1634; brewster,
   stephen/0000-0001-9720-3899; Pollick, Frank/0000-0002-7212-4622;
   Pohlmann, Katharina/0000-0003-1989-3411; Li, Gang/0000-0002-5251-7445
CR Activision, 2023, About us
   Adhanom I, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.848001
   Adhanom IB, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P645, DOI [10.1109/VR46266.2020.00-17, 10.1109/VR46266.2020.1581314696458]
   Al Zayer M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300584
   Anguera JA, 2013, NATURE, V501, P97, DOI 10.1038/nature12486
   Apple, 2024, Introducing apple vision pro
   Bailey K, 2010, PSYCHOPHYSIOLOGY, V47, P34, DOI 10.1111/j.1469-8986.2009.00925.x
   Bakker NH, 2003, HUM FACTORS, V45, P160, DOI 10.1518/hfes.45.1.160.27234
   Bavelier D, 2012, VISION RES, V61, P132, DOI 10.1016/j.visres.2011.08.007
   Bavelier D, 2019, NEURON, V104, P147, DOI 10.1016/j.neuron.2019.09.031
   Bethesda, 2023, The elderscrolls: Skyrim
   Bhagat KK, 2016, VIRTUAL REAL-LONDON, V20, P127, DOI 10.1007/s10055-016-0284-x
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Budhiraja P, 2017, Arxiv, DOI arXiv:1710.02599
   C. D. Group, 2023, Timb raider
   Caniard F, 2015, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.01058
   Chisholm JD, 2012, ATTEN PERCEPT PSYCHO, V74, P257, DOI 10.3758/s13414-011-0253-0
   Chisholm JD, 2010, ATTEN PERCEPT PSYCHO, V72, P667, DOI 10.3758/APP.72.3.667
   Chopin A, 2019, CURR OPIN PSYCHOL, V29, P168, DOI 10.1016/j.copsyc.2019.03.004
   D'Amour S, 2017, EXP BRAIN RES, V235, P2811, DOI 10.1007/s00221-017-5009-1
   Dichgans J, 1978, Handbook of Sensory Physiology, V8, P755, DOI [DOI 10.1007/978-3-642-46354-9_25, 10.1007/978-3-642-46354-925, DOI 10.1007/978-3-642-46354-925, 10.1007/978-3-642-46354-9_25]
   Domeyer JE, 2013, ACCIDENT ANAL PREV, V53, P127, DOI 10.1016/j.aap.2012.12.039
   Farmani Y, 2020, VIRTUAL REAL-LONDON, V24, P645, DOI 10.1007/s10055-020-00425-x
   Feng J, 2007, PSYCHOL SCI, V18, P850, DOI 10.1111/j.1467-9280.2007.01990.x
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Flanagan MB, 2005, AVIAT SPACE ENVIR MD, V76, P642
   Föcker J, 2018, BRAIN BEHAV, V8, DOI 10.1002/brb3.1019
   Föcker J, 2019, J COGNITIVE NEUROSCI, V31, P377, DOI 10.1162/jocn_a_01230
   Freitag S, 2016, IEEE T VIS COMPUT GR, V22, P1462, DOI 10.1109/TVCG.2016.2518298
   Gadelha R., 2018, CHILDHOOD EDUC, V94, P40, DOI [DOI 10.1080/00094056.2018.1420362, 10.1080/00094056.2018, DOI 10.1080/00094056.2018]
   Gamito P., 2008, Annual Review of CyberTherapy and Telemedicine: Changing the Face of Healthcare, P83
   Gamito P, 2010, STUD HEALTH TECHNOL, V154, P128, DOI 10.3233/978-1-60750-561-7-128
   Golding JF, 1998, BRAIN RES BULL, V47, P507, DOI 10.1016/S0361-9230(98)00091-4
   Green CS, 2007, PSYCHOL SCI, V18, P88, DOI 10.1111/j.1467-9280.2007.01853.x
   Green CS, 2006, J EXP PSYCHOL HUMAN, V32, P1465, DOI 10.1037/0096-1523.32.6.1465
   Green CS, 2010, CURR BIOL, V20, P1573, DOI 10.1016/j.cub.2010.07.040
   Green CS, 2003, NATURE, V423, P534, DOI 10.1038/nature01647
   Green D. M., 1966, Signal detection theory and psychophysics, V1
   Heutink J, 2019, ERGONOMICS, V62, P65, DOI 10.1080/00140139.2018.1518543
   Hill KJ, 2000, DISPLAYS, V21, P25, DOI 10.1016/S0141-9382(00)00029-9
   Howard MC, 2021, VIRTUAL REAL-LONDON, V25, P1221, DOI 10.1007/s10055-021-00524-3
   Howarth PA, 2008, DISPLAYS, V29, P117, DOI 10.1016/j.displa.2007.09.009
   Hussain R, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21124006
   Hutchinson CV, 2013, PERCEPTION, V42, P675, DOI 10.1068/p7411
   Jasper A, 2023, COMPUT HUM BEHAV, V146, DOI 10.1016/j.chb.2023.107800
   Jasper A, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.582108
   Johnson David M, 2005, Technical Report
   Kennedy RS, 2000, PRESENCE-TELEOP VIRT, V9, P463, DOI 10.1162/105474600566952
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Keshavarz Behrang, 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P147, DOI 10.1007/978-3-319-39907-2_14
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Kourtesis P., 2024, Virtual Worlds, V3, P62, DOI [10.3390/virtualworlds3010004, DOI 10.3390/VIRTUALWORLDS3010004]
   Kourtesis P, 2023, IEEE T VIS COMPUT GR, V29, P2326, DOI 10.1109/TVCG.2023.3247062
   Large AM, 2019, MEDIA COMMUN-LISBON, V7, P198, DOI 10.17645/mac.v7i4.2314
   Lee H. A., 2022, The real reason vr makes some people sick
   Levine ME, 2002, PERCEPT MOTOR SKILL, V95, P425, DOI 10.2466/PMS.95.6.425-431
   Li G, 2022, IEEE J BIOMED HEALTH, V26, P2469, DOI 10.1109/JBHI.2021.3134024
   Li Jin-rang, 2012, Zhonghua Er Bi Yan Hou Tou Jing Wai Ke Za Zhi, V47, P642
   Lin JJW, 2002, P IEEE VIRT REAL ANN, P164, DOI 10.1109/VR.2002.996519
   Lin YX, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419984
   LINN MC, 1985, CHILD DEV, V56, P1479, DOI 10.2307/1130467
   Mcgill M, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3380959
   McGill M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5655, DOI 10.1145/3025453.3026046
   Meta, 2024, Meta quest 3
   Meta, 2024, Introducing project aria, from meta
   Meyerhoff HS, 2017, ATTEN PERCEPT PSYCHO, V79, P1255, DOI 10.3758/s13414-017-1338-1
   Mittelstaedt J, 2018, DISPLAYS, V51, P43, DOI 10.1016/j.displa.2018.01.002
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Nintendo, Nintendo switchtm family-nintendo
   Pavan A, 2016, PERCEPTION, V45, P1193, DOI 10.1177/0301006616663215
   Pöhlmann KMT, 2022, J COGN ENHANCE, V6, P3, DOI 10.1007/s41465-021-00215-6
   Pratticò FG, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167527
   R. Interactive, 2023, Grand theft auto v
   Rankin CH, 2009, NEUROBIOL LEARN MEM, V92, P135, DOI 10.1016/j.nlm.2008.09.012
   REASON JT, 1978, J ROY SOC MED, V71, P819, DOI 10.1177/014107687807101109
   Rebenitsch L., 2014, Proceedings of the 27th annual ACM symposium on User interface software and technology, P309, DOI [DOI 10.1145/2642918.2647394, 10.1145/2642918.2647394]
   Regan C., 1995, VIRTUAL REAL-LONDON, V1, P17, DOI DOI 10.1007/BF02009710
   Riesenhuber M, 2004, TRENDS NEUROSCI, V27, P72, DOI 10.1016/j.tins.2003.11.004
   Shapiro KL, 1997, TRENDS COGN SCI, V1, P291, DOI 10.1016/S1364-6613(97)01094-2
   SIGNORELLA ML, 1989, DEV PSYCHOL, V25, P89, DOI 10.1037/0012-1649.25.1.89
   Smyth J, 2021, APPL ERGON, V90, DOI 10.1016/j.apergo.2020.103264
   Spence I, 2010, REV GEN PSYCHOL, V14, P92, DOI 10.1037/a0019491
   Stanney K. M., 2014, Handbook of virtual environments, P677
   Stoffregen TA, 1998, BRAIN RES BULL, V47, P437, DOI 10.1016/S0361-9230(98)00102-6
   Tarita-Nistor L, 2006, J VESTIBUL RES-EQUIL, V16, P265
   Teixeira J, 2021, VIRTUAL REAL-LONDON, V25, P433, DOI 10.1007/s10055-020-00466-2
   Thompson RF, 2009, NEUROBIOL LEARN MEM, V92, P127, DOI 10.1016/j.nlm.2008.07.011
   Ujike H, 2008, DISPLAYS, V29, P81, DOI 10.1016/j.displa.2007.09.003
   Valve, 2023, Counter strike: Global offensive
   Woon APN, 2021, NURS EDUC TODAY, V98, DOI 10.1016/j.nedt.2020.104655
   Wu F, 2022, 28TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2022, DOI 10.1145/3562939.3565611
NR 91
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7225
EP 7233
DI 10.1109/TVCG.2024.3456204
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300013
PM 39255120
DA 2024-11-06
ER

PT J
AU Manakhov, P
   Sidenmark, L
   Pfeuffer, K
   Gellersen, H
AF Manakhov, Pavel
   Sidenmark, Ludwig
   Pfeuffer, Ken
   Gellersen, Hans
TI Filtering on the Go: Effect of Filters on Gaze Pointing Accuracy During
   Physical Locomotion in Extended Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Filters; Accuracy; Target tracking; Noise; Head; Gaze tracking; Extended
   reality; eye tracking; gaze filters; gaze-based pointing; extended
   reality; spatial reference frames; physical locomotion
ID TASK-PERFORMANCE; HEAD; INFORMATION; WALKING
AB Eye tracking filters have been shown to improve accuracy of gaze estimation and input for stationary settings. However, their effectiveness during physical movement remains underexplored. In this work, we compare common online filters in the context of physical locomotion in extended reality and propose alterations to improve them for on-the-go settings. We conducted a computational experiment where we simulate performance of the online filters using data on participants attending visual targets located in world-, path-, and two head-based reference frames while standing, walking, and jogging. Our results provide insights into the filters' effectiveness and factors that affect it, such as the amount of noise caused by locomotion and differences in compensatory eye movements, and demonstrate that filters with saccade detection prove most useful for on-the-go settings. We discuss the implications of our findings and conclude with guidance on gaze data filtering for interaction in extended reality.
C1 [Manakhov, Pavel; Pfeuffer, Ken; Gellersen, Hans] Aarhus Univ, Aarhus, Denmark.
   [Sidenmark, Ludwig] Univ Toronto, Toronto, ON, Canada.
   [Gellersen, Hans] Univ Lancaster, Lancaster, England.
C3 Aarhus University; University of Toronto; Lancaster University
RP Manakhov, P (corresponding author), Aarhus Univ, Aarhus, Denmark.
EM pavel.manakhov@gmail.com; lsidenmark@dgp.toronto.edu; ken@cs.au.dk;
   h.gellersen@lancaster.ac.uk
OI Gellersen, Hans/0000-0003-2233-2121; Pfeuffer, Ken/0000-0002-5870-1120;
   Sidenmark, Ludwig/0000-0002-7965-0107
FU European Research Council (ERC) [101021229]; Pioneer Centre for
   Artificial Intelligence (P1); European Research Council (ERC)
   [101021229] Funding Source: European Research Council (ERC)
FX The current work was partially supported by the European Research
   Council (ERC) under the European Union's Horizon 2020 research and
   innovation programme (grant no. 101021229 GEMINI) as well as the Pioneer
   Centre for Artificial Intelligence (P1). We would like to thank Stasia
   Manakhova for her help with illustrations for the paper.
CR Alinaghi N, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529624
   Binaee K., 2022, Journal of Vision, V22, P4469, DOI [10.1167/jov.22.14.4469, DOI 10.1167/JOV.22.14.4469]
   Blignaut Pieter., 2012, Proceedings of the symposium on eye tracking research and applications, P289, DOI DOI 10.1145/2168556.2168618
   Borg O, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129902
   Bulling A, 2010, IEEE PERVAS COMPUT, V9, P8, DOI 10.1109/MPRV.2010.86
   Casiez G., 2012, P SIGCHI C HUM FACT, P2527, DOI [10.1145/2207676.22086391,2,5, DOI 10.1145/2207676.22086391,2,5]
   Chartier S, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P153, DOI 10.1145/1344471.1344511
   Davari S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P436, DOI 10.1109/VR51125.2022.00063
   Diaz G, 2013, J VISION, V13, DOI 10.1167/13.12.5
   Drewes J., 2012, P S EYE TRACK RES AP, P209, DOI [DOI 10.1145/2168556.2168596, 10.1145/2168556.2168596]
   Duchowski A. T., 2017, Eye Tracking Methodology: Theory and Practice, DOI [10.1007/978-3-319-57883-52,3,6, DOI 10.1007/978-3-319-57883-52,3,6]
   Elkin Lisa A., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P754, DOI 10.1145/3472749.3474784
   Erickson A, 2020, IEEE T VIS COMPUT GR, V26, P1934, DOI 10.1109/TVCG.2020.2973054
   Esteves A, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P457, DOI 10.1145/2807442.2807499
   Fares Ribel., 2013, P SIGCHI C HUMAN FAC, P1387
   Feit AM, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1118, DOI 10.1145/3025453.3025599
   Fernandes AS, 2023, IEEE T VIS COMPUT GR, V29, P2269, DOI 10.1109/TVCG.2023.3247058
   Flatla D., 2011, Proc. of ACM UIST 2011, P403, DOI [10.1145/2047196.2047248, DOI 10.1145/2047196.2047248, 10.1145/2047196.20472482, DOI 10.1145/2047196.20472482]
   Gomez AR, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204585
   Graupner ST, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P91, DOI 10.1145/1344471.1344495
   Grubert J, 2017, IEEE T VIS COMPUT GR, V23, P1706, DOI 10.1109/TVCG.2016.2543720
   Han YH, 2005, EXP BRAIN RES, V165, P294, DOI 10.1007/s00221-005-2305-y
   Imai T, 2001, EXP BRAIN RES, V136, P1, DOI 10.1007/s002210000533
   J. C. for Guides in Metrology, 2021, International vocabulary of metrology, fourth edition-committee draft (vim4 cd), V1, P2
   Jimenez J, 2008, J UNIVERS COMPUT SCI, V14, P3085
   Kumar M, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P65, DOI 10.1145/1344471.1344488
   Kytö M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173655
   Lamb M, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.864653
   Lappi O, 2015, J EYE MOVEMENT RES, V8
   LaViola Jr J. J., 2017, 3D User Interfaces: Theory and Practice, P1
   Lu FY, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P930, DOI [10.1109/VR46266.2020.00118, 10.1109/VR46266.2020.1581100361198]
   Lu YJ, 2023, VIRTUAL REAL-LONDON, V27, P603, DOI 10.1007/s10055-022-00677-9
   Lystbaek M. N., 2022, P ACM HUM COMP INT, V6, P2
   Manakhov P, 2024, PROCEEDINGS OF THE 2024 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYTEMS (CHI 2024), DOI 10.1145/3613904.3642915
   Marteniuk RG, 2000, MOTOR CONTROL, V4, P165, DOI 10.1123/mcj.4.2.165
   Moore ST, 2001, ANN NY ACAD SCI, V942, P139
   Mughrabi MH, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P301, DOI 10.1109/VRW55335.2022.00070
   Mustonen T, 2013, J EXP PSYCHOL-APPL, V19, P333, DOI 10.1037/a0034635
   Niehorster DC, 2020, BEHAV RES METHODS, V52, P1140, DOI 10.3758/s13428-019-01307-0
   Norouzi N, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357587
   Olsen A., 2012, Tobii Technol., V21, P4
   Pastel S, 2021, VIRTUAL REAL-LONDON, V25, P175, DOI 10.1007/s10055-020-00449-3
   Pfeuffer K., 2013, P 26 ANN ACM S US IN, P261, DOI [DOI 10.1145/2501988.2501998, 10.1145/2501988.2501998]
   Pfeuffer K, 2023, ACM SYMPOSIUM ON SPATIAL USER INTERACTION, SUI 2023, DOI 10.1145/3607822.3614523
   Pfeuffer K, 2021, COMPUT GRAPH-UK, V95, P1, DOI 10.1016/j.cag.2021.01.001
   Pfeuffer K, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P99, DOI 10.1145/3131277.3132180
   Piening R, 2021, LECT NOTES COMPUT SC, V12932, P544, DOI 10.1007/978-3-030-85623-6_32
   Purves D., 2001, Neuroscience, V2nd, P2
   Reiter K, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529233
   Salvucci Dario D, 2000, P 2000 S EYE TRACK R, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Sidenmark L, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580685
   Sidenmark L, 2022, IEEE T VIS COMPUT GR, V28, P3585, DOI 10.1109/TVCG.2022.3203096
   Sidenmark L, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376438
   Sidenmark L, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319815
   Spakov O., 2012, P S EYE TRACK RES AP, P281, DOI [DOI 10.1145/2168556.2168616, 10.1145/2168556.21686162,5, DOI 10.1145/2168556.21686162,5]
   Spakov O, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P287, DOI 10.1145/2857491.2857526
   Stuart S, 2014, IEEE ENG MED BIO, P5739, DOI 10.1109/EMBC.2014.6944931
   Taranta E. M., 2019, P 45 GRAPH INT C P G, DOI [10.20380/GI2019.272, DOI 10.20380/GI2019.272]
   Tomasi M, 2016, J VISION, V16, DOI 10.1167/16.3.27
   Velloso E, 2016, DIS 2016: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON DESIGNING INTERACTIVE SYSTEMS, P812, DOI 10.1145/2901790.2901867
   Vidal M, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P439, DOI 10.1145/2493432.2493477
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Zhai S., 1999, P SIGCHI C HUM FACT, P246, DOI [10.1145/302979.303053, DOI 10.1145/302979.303053, DOI 10.1145/302979.3030532]
NR 64
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7234
EP 7244
DI 10.1109/TVCG.2024.3456146
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300026
PM 39255110
DA 2024-11-06
ER

PT J
AU Hmaiti, Y
   Maslych, M
   Ghasemaghaei, A
   Ghamandi, RK
   LaViola, JJ Jr
AF Hmaiti, Yahya
   Maslych, Mykola
   Ghasemaghaei, Amirpouya
   Ghamandi, Ryan K.
   LaViola, Joseph J.
TI Visual Perceptual Confidence: Exploring Discrepancies Between
   Self-reported and Actual Distance Perception In Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Complexity theory; Legged locomotion; Accuracy; Resists;
   Electronic mail; Estimation; Virtual reality; spatial perception;
   distance perception; visual complexity; understanding people
ID JUDGMENTS; ENVIRONMENTS; INTERVALS; GRAPHICS; QUALITY; SCALE
AB Virtual Reality (VR) systems are widely used, and it is essential to know if spatial perception in virtual environments (VEs) is similar to reality. Research indicates that users tend to underestimate distances in VR. Prior work suggests that actual distance judgments in VR may not always match the users self-reported preference of where they think they most accurately estimated distances. However, no explicit investigation evaluated whether user preferences match actual performance in a spatial judgment task. We used blind walking to explore potential dissimilarities between actual distance estimates and user-selected preferences of visual complexities, VE conditions, and targets. Our findings show a gap between user preferences and actual performance when visual complexities were varied, which has implications for better visual perception understanding, VR applications design, and research in spatial perception, indicating the need to calibrate and align user preferences and true spatial perception abilities in VR.
C1 [Hmaiti, Yahya; Maslych, Mykola; Ghasemaghaei, Amirpouya; Ghamandi, Ryan K.; LaViola, Joseph J.] Univ Cent Florida, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Hmaiti, Y (corresponding author), Univ Cent Florida, Orlando, FL 32816 USA.
EM Yohan.Hmaiti@ucf.edu; mykola.maslych@ucf.edu; Aghaei.ap@ucf.edu;
   ryanghamandi1@gmail.com; jlaviola@ucf.edu
OI LaViola Jr., Joseph J./0000-0003-1186-4130; Ghasemaghaei,
   Amirpouya/0009-0002-8463-1207; Hmaiti, Yahya/0000-0003-1052-1152;
   Maslych, Mykola/0000-0001-7037-3513
CR Allen C. G., 2011, PhD thesis,, P8
   Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   Bernhard C, 2021, HUM FACTORS, V63, P415, DOI 10.1177/0018720819895866
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Choudhary Z, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P788, DOI 10.1109/VR50410.2021.00106
   Cousineau D, 2005, TUTOR QUANT METHODS, V1, P42, DOI 10.20982/tqmp.01.1.p042
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Creem-Regehr SH, 2023, PHILOS T R SOC B, V378, DOI 10.1098/rstb.2021.0456
   Cutting J. E., 1995, Perception of space and motion, P69, DOI DOI 10.1016/B978-012240530-3/50005-5
   Deng ZH, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343135
   Diaz C, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P111, DOI 10.1109/ISMAR.2017.28
   Dong B, 2023, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.1061917
   El Jamiy F, 2019, IET IMAGE PROCESS, V13, P707, DOI 10.1049/iet-ipr.2018.5920
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   Feldstein IT, 2020, PERCEPTION, V49, P940, DOI 10.1177/0301006620951997
   Finnegan DJ, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P200, DOI 10.1145/2858036.2858065
   Gaines D, 2020, PROCEEDINGS OF THE 2020 ACM SYMPOSIUM ON SPATIAL USER INTERACTION, SUI 2020, DOI 10.1145/3385959.3418447
   Geuss Michael., 2010, P 7 S APPL PERCEPTIO, P61, DOI 10.1145/1836248.1836259
   Ghasemaghaei A., 2024, Graphics Interface 2024 Second Deadline
   Gonzalez-Franco M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P941, DOI [10.1109/VR.2019.8798348, 10.1109/vr.2019.8798348]
   Guo Q, 2023, BEHAV SCI-BASEL, V13, DOI 10.3390/bs13100827
   Hmaiti Y., 2023, Honors Undergraduate Theses
   Hmaiti Y, 2023, INT SYM MIX AUGMENT, P263, DOI 10.1109/ISMAR59233.2023.00041
   Hornsey RL, 2021, VIRTUAL REAL-LONDON, V25, P1087, DOI 10.1007/s10055-021-00500-x
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Ishihara Shinobu., 1960, TESTS COLOUR BLINDNE, V15th
   Jones JA, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2983631
   Jones JA, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P221, DOI 10.1109/3DUI.2016.7460055
   Jones JA, 2013, IEEE T VIS COMPUT GR, V19, P701, DOI 10.1109/TVCG.2013.37
   Jones JA, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P9
   Kelly JW, 2023, IEEE T VIS COMPUT GR, V29, P4978, DOI 10.1109/TVCG.2022.3196606
   Kelly JW, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.850471
   Kelly JW, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3106155
   Korshunova-Fucci V, 2024, INT J HUM-COMPUT INT, V40, P5431, DOI 10.1080/10447318.2023.2234117
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   LaViola Jr J. J., 2017, 3D user interfaces: theory and practice
   Lee C, 2013, IEEE T VIS COMPUT GR, V19, P547, DOI 10.1109/TVCG.2013.41
   Leyrer M., 2011, P ACM SIGGRAPH S APP, P67, DOI [10.1145/2077451.20774642, DOI 10.1145/2077451.20774642, 10.1145/2077451.2077464]
   Leyrer M, 2015, ACM T APPL PERCEPT, V12, P3, DOI 10.1145/2699254
   Liao BJ, 2022, CITIES, V125, DOI 10.1016/j.cities.2022.103650
   Loomis J. M., 2008, CARN S COGN 2006 PIT
   Loomis JM, 1999, BEHAV RES METH INS C, V31, P557, DOI 10.3758/BF03200735
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Loyola M, 2018, VIRTUAL REAL-LONDON, V22, P235, DOI 10.1007/s10055-017-0331-2
   Lucaci AI, 2022, VISIGRAPP, P244, DOI 10.5220/0010890100003124
   Maruhn P, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0224651
   Masnadi S, 2023, Arxiv, DOI arXiv:2304.08604
   Masnadi S, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517548
   Masnadi S, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P542, DOI 10.1109/VRW52623.2021.00153
   Mayer A., 2023, New Digital Work: Digital Sovereignty at the Workplace, P87
   McManus ErinA., 2011, Proceedings of the ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization, P37
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Morey RD, 2008, TUTOR QUANT METHODS, V4, P61, DOI 10.20982/tqmp.04.2.p061
   Murgia Alessio, 2009, International Journal of Virtual Reality, V8, P67
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Park H, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app112311510
   Pfeil K, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445223
   Philbeck JW, 2008, PERCEPT PSYCHOPHYS, V70, P1459, DOI 10.3758/PP.70.8.1459
   Phillips Lane., 2009, P 6 S APPL PERCEPTIO, P11, DOI [10.1145/1620993.1620996, DOI 10.1145/1620993.1620996]
   Ping JM, 2020, J SOC INF DISPLAY, V28, P164, DOI 10.1002/jsid.840
   Proffitt DennisR., 2003, Handbook of psychology, P213
   Proffitt DR, 2003, PSYCHOL SCI, V14, P106, DOI 10.1111/1467-9280.t01-1-01427
   Ragan ED, 2015, IEEE T VIS COMPUT GR, V21, P794, DOI 10.1109/TVCG.2015.2403312
   Rand KM, 2011, PERCEPTION, V40, P143, DOI 10.1068/p6843
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Sahm C. S., 2005, ACM Transactions on Applied Perception, V2, P35, DOI [https://doi.org/10.1145/1048687.1048690, DOI 10.1145/1048687.1048690, 10.1145/1048687.1048690]
   Sepahyar S, 2022, PROCEEDINGS OF THE ACM SYMPOSIUM ON APPLIED PERCEPTION, SAP 2022, DOI 10.1145/3548814.3551463
   Sinai MJ, 1998, NATURE, V395, P497, DOI 10.1038/26747
   Steinicke F, 2010, COMPUT GRAPH-UK, V34, P26, DOI 10.1016/j.cag.2009.12.003
   Tarr MJ, 2002, NAT NEUROSCI, V5, P1089, DOI 10.1038/nn948
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Vaziri K, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P336, DOI 10.1109/VR50410.2021.00056
   Vaziri K, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119892
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI [DOI 10.1145/1012551.1012558, 10.1145/1012551.1012558]
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Witt JK, 2004, PERCEPTION, V33, P577, DOI 10.1068/p5090
   Wraga M, 1999, PERCEPT PSYCHOPHYS, V61, P490, DOI 10.3758/BF03211968
   Yamamoto N, 2019, EARLY CHILD DEV CARE, V189, P56, DOI 10.1080/03004430.2017.1299148
NR 81
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7245
EP 7254
DI 10.1109/TVCG.2024.3456176
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300034
PM 39255097
DA 2024-11-06
ER

PT J
AU Achberger, A
   Gebhardt, P
   Sedlmair, M
AF Achberger, Alexander
   Gebhardt, Patrick
   Sedlmair, Michael
TI An Exploratory Expert-Study for Multi-Type Haptic Feedback for
   Automotive Virtual Reality Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic interfaces; Automotive engineering; Actuators; Thumb;
   Automobiles; Reservoirs; Force; Haptics; Virtual Reality; Human Computer
   Interaction
AB Previous research has shown that integrating haptic feedback can improve immersion and realism in automotive VR applications. However, current haptic feedback approaches primarily focus on a single feedback type. This means users must switch between devices to experience haptic stimuli for different feedback types, such as grabbing, collision, or weight simulation. This restriction limits the ability to simulate haptics realistically for complex tasks such as maintenance. To address this issue, we evaluated existing feedback devices based on our requirements analysis to determine which devices are most suitable for simulating these three feedback types. Since no suitable haptic feedback system can simulate all three feedback types simultaneously, we evaluated which devices can be combined. Based on that, we devised a new multi-type haptic feedback system combining three haptic feedback devices. We evaluated the system with different feedback-type combinations through a qualitative expert study involving twelve automotive VR experts. The results showed that combining weight and collision feedback yielded the best and most realistic experience. The study also highlighted technical limitations in current grabbing devices. Our findings provide insights into the effectiveness of haptic device combinations and practical boundaries for automotive virtual reality tasks.
C1 [Achberger, Alexander; Gebhardt, Patrick; Sedlmair, Michael] Univ Stuttgart, Stuttgart, Germany.
C3 University of Stuttgart
RP Achberger, A (corresponding author), Univ Stuttgart, Stuttgart, Germany.
EM alexander.achberger@hotmail.com; pattigebhardt@gmx.de;
   michael.sedlmair@visus.uni-stuttgart.de
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy [EXC 2075 -390740016]; Stuttgart Center
   for Simulation Science (SimTech)
FX Funded by Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) under Germany's Excellence Strategy - EXC 2075 -390740016.
   We acknowledge the support by the Stuttgart Center for Simulation
   Science (SimTech) and Katherine J. Kuchenbecker for her valuable
   support.
CR A. P. Association, 2010, Publication manual of the american psychological association, V6
   Abtahi P, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300589
   Achberger Alexander, 2021, VINCI 2021: The 14th International Symposium on Visual Information Communication and Interaction, DOI 10.1145/3481549.3481563
   Achberger Alexander, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P841, DOI 10.1145/3472749.3474790
   Achberger A, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P112, DOI 10.1109/VR51125.2022.00029
   Adilkhanov A, 2022, IEEE ACCESS, V10, P91923, DOI 10.1109/ACCESS.2022.3202986
   Al-Sada M, 2020, VIRTUAL REAL-LONDON, V24, P191, DOI 10.1007/s10055-019-00404-x
   Barrow AL, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P295
   Berg LP, 2017, VIRTUAL REAL-LONDON, V21, P1, DOI 10.1007/s10055-016-0293-9
   Bowman D. A., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P35, DOI 10.1145/253284.253301
   Caeiro-Rodríguez M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082667
   Chen WY, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041074
   Cheng CH, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281569
   Choi I, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P119, DOI 10.1145/3126594.3126599
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Cumming G, 2005, AM PSYCHOL, V60, P170, DOI 10.1037/0003-066X.60.2.170
   Cumming G., 2013, Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis, P7
   Dragicevic P, 2016, HUM-COMPUT INT-SPRIN, P291, DOI 10.1007/978-3-319-26633-6_13
   Endo T, 2011, IEEE T HAPTICS, V4, P14, DOI [10.1109/TOH.2010.62, 10.1109/ToH.2010.62]
   Fallows Emma, 2022, Academic Mindtrek 2022: 25th International Academic Mindtrek conference, P242, DOI 10.1145/3569219.3569382
   G. R. C. for Artificial Intelligence GmbH, 2021, Exoskeleton active (vibot)
   Garrec P., 2004, P S ROB OSR, V3, P8
   Gonzalez Eric J., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P732, DOI 10.1145/3472749.3474782
   Greenberg S, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P111
   Heo S, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3186544
   Huang Y, 2022, MATER TODAY PHYS, V22, DOI 10.1016/j.mtphys.2021.100602
   Isenberg T, 2013, IEEE T VIS COMPUT GR, V19, P2818, DOI 10.1109/TVCG.2013.126
   Je S, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 EMERGING TECHNOLOGIES, DOI 10.1145/3214907.3214915
   Kalus A., 2024, ACM C HUM FACT COMP, P1
   Kim E, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376280
   Lazar J, 2017, RESEARCH METHODS IN HUMAN-COMPUTER INTERACTION, 2ND EDITION, P1
   Lopes P, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1471, DOI 10.1145/3025453.3025600
   McGrath J. E., 1995, Readings in Human-Computer Interaction, V7, P152, DOI 10.1016/B978-0-08-051574-8.50019-4
   Nagai K, 2015, SIGGRAPH ASIA 2015 HAPTIC MEDIA AND CONTENTS DESIGN (SA'15), DOI 10.1145/2818384.2818403
   Pacchierotti C, 2017, IEEE T HAPTICS, V10, P580, DOI 10.1109/TOH.2017.2689006
   Park C, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P449, DOI [10.1109/whc.2019.8816116, 10.1109/WHC.2019.8816116]
   Perret J., 2009, SPRINGER TRACTS ADV
   Polygerinos P, 2015, ROBOT AUTON SYST, V73, P135, DOI 10.1016/j.robot.2014.08.014
   Seifi H, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300788
   Shigeyama J, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 EMERGING TECHNOLOGIES, DOI 10.1145/3214907.3214923
   Tory M, 2005, IEEE COMPUT GRAPH, V25, P8, DOI 10.1109/MCG.2005.102
   Wang DX, 2020, IEEE T IND ELECTRON, V67, P610, DOI 10.1109/TIE.2019.2920602
   Weise Matthias, 2020, i-com: Journal of Interactive Media, V19, P67, DOI 10.1515/icom-2020-0011
   Wong R. Y., 2018, Speculative design in HCI: From corporate imaginations to critical orientations, P7
   Zenner A, 2017, IEEE T VIS COMPUT GR, V23, P1312, DOI 10.1109/TVCG.2017.2656978
   Zhang Y., 2021, Journal of Healthcare Engineering, V2021, P8
   Zimmermann P., 2008, Product Engineering, P1
NR 47
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7255
EP 7265
DI 10.1109/TVCG.2024.3456153
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300028
PM 39255123
OA Green Accepted, Green Published
DA 2024-11-06
ER

PT J
AU Liu, CC
   Ma, SN
   Liu, Y
   Wang, YT
   Song, WT
AF Liu, Chaochao
   Ma, Shining
   Liu, Yue
   Wang, Yongtian
   Song, Weitao
TI Depth Perception in Optical See-Through Augmented Reality: Investigating
   the Impact of Texture Density, Luminance Contrast, and Color Contrast
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE depth perception; Augmented reality; texture contrast; OSTAR; texture
   density; texture density; texture contrast; texture density
ID PERCEIVED DEPTH; DISTANCE; BRIGHTNESS; JUDGMENTS; SHAPE; SIZE; ACCURACY;
   SURFACE
AB The immersive augmented reality (AR) system necessitates precise depth registration between virtual objects and the real scene. Prior studies have emphasized the efficacy of surface texture in providing depth cues to enhance depth perception across various media, including the real scene, virtual reality, and AR. However, these studies predominantly focus on black-and-white textures, leaving a gap in understanding the effectiveness of colored textures. To address this gap and further explore texture-related factors in AR, a series of experiments were conducted to investigate the effects of different texture cues on depth perception using the perceptual matching method. Findings indicate that the absolute depth error increases with decreasing contrast under black-and-white texture. Moreover, textures with higher color contrast also contribute to enhanced accuracy of depth judgments in AR. However, no significant effect of texture density on depth perception was observed. The findings serve as a theoretical reference for texture design in AR, aiding in the optimization of virtual-real registration processes.
C1 [Liu, Chaochao; Ma, Shining; Liu, Yue; Wang, Yongtian; Song, Weitao] Beijing Inst Technol, Zhengzhou Res Inst, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, Zhengzhou, Peoples R China.
C3 Beijing Institute of Technology
RP Liu, CC (corresponding author), Beijing Inst Technol, Zhengzhou Res Inst, Beijing Engn Res Ctr Mixed Real & Adv Display, Sch Opt & Photon, Zhengzhou, Peoples R China.
EM chaocaho.liu@bit.edu.cn; shining.ma@bit.edu.cn; liuyue@bit.edu.cn;
   wyt@bit.edu.cn; swt@bit.edu.cn
RI Li, Yumeng/JBS-1868-2023
OI Ma, Shining/0000-0003-2551-4027; Wang, Yongtian/0000-0001-9422-0888
FU National Key Research and Development Program of China [2023YFB3611503];
   National Natural Science Foundation of China [62205018, 62332003,
   61960206007]
FX This work was supported in part by the National Key Research and
   Development Program of China under grant 2023YFB3611503, National
   Natural Science Foundation of China under grant 62205018, 62332003,
   61960206007.
CR Adams H, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P792, DOI 10.1109/VR51125.2022.00101
   Adams WJ, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003576
   Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   Berning M, 2014, INT SYM MIX AUGMENT, P93, DOI 10.1109/ISMAR.2014.6948413
   Bodenheimer B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P35
   Bradshaw MF, 1996, VISION RES, V36, P1255, DOI 10.1016/0042-6989(95)00190-5
   Brenner E., 2018, Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience, P1, DOI [10.1002/9781119170174.epcn209, DOI 10.1002/9781119170174.EPCN209]
   BULTHOFF HH, 1988, J OPT SOC AM A, V5, P1749, DOI 10.1364/JOSAA.5.001749
   Carmigniani J, 2011, HANDBOOK OF AUGMENTED REALITY, P3, DOI 10.1007/978-1-4614-0064-6_1
   COULES J, 1955, J EXP PSYCHOL, V50, P19, DOI 10.1037/h0044343
   CUTTING JE, 1984, J EXP PSYCHOL GEN, V113, P198, DOI 10.1037/0096-3445.113.2.198
   Cutting JE, 1997, BEHAV RES METH INS C, V29, P27, DOI 10.3758/BF03200563
   DASILVA JA, 1985, AM J PSYCHOL, V98, P119
   De Paolis LT, 2019, MED BIOL ENG COMPUT, V57, P995, DOI 10.1007/s11517-018-1929-6
   Dey A., Evaluating depth perception of photorealistic mixed reality visualizations for occluded objects in outdoor environments
   Diaz C, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P111, DOI 10.1109/ISMAR.2017.28
   Díaz J, 2017, VISUAL COMPUT, V33, P47, DOI 10.1007/s00371-015-1151-6
   Drascic D, 1996, P SOC PHOTO-OPT INS, V2653, P123, DOI 10.1117/12.237425
   Eggleston R. G., 1996, Virtual Reality System Effects on Size-Distance Judgments in a Virtual Environment
   EGUSA H, 1982, PERCEPTION, V11, P671, DOI 10.1068/p110671
   EGUSA H, 1983, PERCEPTION, V12, P167, DOI 10.1068/p120167
   El Jamiy F, 2020, INT CONF ELECTRO INF, P281, DOI [10.1109/eit48999.2020.9208300, 10.1109/EIT48999.2020.9208300]
   Evans G, 2017, PROC SPIE, V10197, DOI 10.1117/12.2262626
   FARNE M, 1977, PERCEPTION, V6, P287, DOI 10.1068/p060287
   FLOCK HR, 1964, PSYCHOL REV, V71, P380, DOI 10.1037/h0042387
   Gagnon HC, 2021, ACM T APPL PERCEPT, V18, DOI 10.1145/3449067
   Gao Y, 2020, J SOC INF DISPLAY, V28, P117, DOI 10.1002/jsid.832
   Gibson J. J., 1950, The perception of the visual world, P242
   Hertel J, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P122, DOI 10.1109/VR50410.2021.00033
   Hoover M, 2020, J COMPUT INF SCI ENG, V20, DOI 10.1115/1.4046006
   Hou M., User Experience with Alignment of Real and Virtual Objects in a Stereoscopic Augmented Reality Interface
   Ichihara S, 2007, PERCEPTION, V36, P686, DOI 10.1068/p5696
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Jones J. A., 2011, P ACM SIGGRAPH S APP, P29, DOI [10.1145/2077451.2077457, DOI 10.1145/2077451.2077457]
   Kingdom F. A. A., 2016, Elsevier science & technology books, VSecond
   KUKKONEN H, 1993, VISION RES, V33, P1431, DOI 10.1016/0042-6989(93)90049-3
   Lappin JS, 2006, PERCEPT PSYCHOPHYS, V68, P571, DOI 10.3758/BF03208759
   Livingston MA, 2011, HANDBOOK OF AUGMENTED REALITY, P671, DOI 10.1007/978-1-4614-0064-6_31
   Livingston MA, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P55, DOI 10.1109/VR.2009.4810999
   Loomis J.M., 2008, EMBODIMENT EGO SPACE, P1, DOI DOI 10.1145/1498700.1498702
   Lucaci AI, 2022, VISIGRAPP, P244, DOI 10.5220/0010890100003124
   Mikkola M., 2010, P 3 WORKSH MOB VID D, P63, DOI [DOI 10.1145/1878022, 10.1145/1878022.1878038, DOI 10.1145/1878022.1878038]
   Norman JF, 2004, PSYCHOL SCI, V15, P565, DOI 10.1111/j.0956-7976.2004.00720.x
   Norman JF, 2000, PERCEPTION, V29, P1335, DOI 10.1068/p3111
   OShea RP, 1997, PERCEPTION, V26, P599, DOI 10.1068/p260599
   Ozkan K, 2010, VIS COGN, V18, P229, DOI 10.1080/13506280802674101
   Phillips Lane., 2009, P 6 S APPL PERCEPTIO, P11, DOI [10.1145/1620993.1620996, DOI 10.1145/1620993.1620996]
   Ping JM, 2020, J SOC INF DISPLAY, V28, P892, DOI 10.1002/jsid.947
   Rand KM, 2012, SEEING PERCEIVING, V25, P425, DOI 10.1163/187847611X620946
   Rohaly AM, 1999, VISION RES, V39, P9, DOI 10.1016/S0042-6989(98)00034-0
   ROLLAND JP, 1995, PRESENCE-TELEOP VIRT, V4, P24, DOI 10.1162/pres.1995.4.1.24
   Rosales CS, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P237, DOI [10.1109/VR.2019.8798095, 10.1109/vr.2019.8798095]
   Singh G., 2013, NEAR FIELD DEPTH PER
   Singh G, 2020, IEEE T VIS COMPUT GR, V26, P1385, DOI 10.1109/TVCG.2018.2869729
   Stone Maureen, 2014, FIN PROGR P IS T SID, P253, DOI [10.2352/CIC.2014.22.1.ART000453,9, DOI 10.2352/CIC.2014.22.1.ART000453,9]
   SUNDET JM, 1978, SCAND J PSYCHOL, V19, P133, DOI 10.1111/j.1467-9450.1978.tb00313.x
   Surdick RT, 1997, PRESENCE-TELEOP VIRT, V6, P513, DOI 10.1162/pres.1997.6.5.513
   Swan JE, 2007, IEEE T VIS COMPUT GR, V13, P429, DOI 10.1109/TVCG.2007.1035
   Swan JE, 2017, INT J HUM-COMPUT INT, V33, P576, DOI 10.1080/10447318.2016.1265783
   Swan JE, 2015, IEEE T VIS COMPUT GR, V21, P1289, DOI 10.1109/TVCG.2015.2459895
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Todd JT, 1997, PERCEPTION, V26, P807, DOI 10.1068/p260807
   Wang JY, 2024, LEUKOS, V20, P194, DOI 10.1080/15502724.2023.2227345
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI [DOI 10.1145/1012551.1012558, 10.1145/1012551.1012558]
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
   Yount ZF, 2022, TRANSPORT RES F-TRAF, V88, P132, DOI 10.1016/j.trf.2022.05.019
   Yuen SCY., 2011, J ED TECHNOLOGY DEV, V4, P11, DOI DOI 10.18785/JETDE.0401.10
NR 67
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7266
EP 7276
DI 10.1109/TVCG.2024.3456165
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300014
PM 39255102
DA 2024-11-06
ER

PT J
AU Wang, ZM
   Lu, F
AF Wang, Zhimin
   Lu, Feng
TI Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type
   Recognition in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Switches; Extended reality; Measurement; Tracking;
   Annotations; Gaze tracking; Virtual reality; eye tracking; visual task
   type recognition; deep learning; intelligent application
ID AUGMENTED REALITY; MOVEMENTS
AB With eye tracking finding widespread utility in augmented reality and virtual reality headsets, eye gaze has the potential to recognize users' visual tasks and adaptively adjust virtual content displays, thereby enhancing the intelligence of these headsets. However, current studies on visual task recognition often focus on scene-specific tasks, like copying tasks for office environments, which lack applicability to new scenarios, e.g., museums. In this paper, we propose four scene-agnostic task types for facilitating task type recognition across a broader range of scenarios. We present a new dataset that includes eye and head movement data recorded from 20 participants while they engaged in four task types across 15 360-degree VR videos. Using this dataset, we propose an egocentric gaze-aware task type recognition method, TRCLP, which achieves promising results. Additionally, we illustrate the practical applications of task type recognition with three examples. Our work offers valuable insights for content developers in designing task-aware intelligent applications. Our dataset and source code are available at zhimin-wang.github.io/TaskTypeRecognition.html.
C1 [Wang, Zhimin; Lu, Feng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Lu, F (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zm.wang@buaa.edu.cn; lufeng@buaa.edu.cn
OI Lu, Feng/0000-0001-9064-7964
FU National Natural Science Foundation of China (NSFC) [62372019]
FX This work was supported by the National Natural Science Foundation of
   China (NSFC) under Grant 62372019. We would like to thank Zhiming Hu for
   providing some data format conversion codes.
CR Agnoli S, 2023, PSYCHOL RES-PSYCH FO, V87, P17, DOI 10.1007/s00426-022-01658-y
   Aizenberg I, 2016, NEUROCOMPUTING, V175, P980, DOI 10.1016/j.neucom.2015.06.092
   BALLARD DH, 1995, J COGNITIVE NEUROSCI, V7, P66, DOI 10.1162/jocn.1995.7.1.66
   Barteit S, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29080
   Bektas K, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3588402
   Bird JM, 2023, J TRAVEL RES, V62, P1427, DOI 10.1177/00472875221134031
   Borji A, 2014, J VISION, V14, DOI 10.1167/14.3.29
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bulling A, 2011, IEEE T PATTERN ANAL, V33, P741, DOI 10.1109/TPAMI.2010.86
   Coutrot A, 2018, BEHAV RES METHODS, V50, P362, DOI 10.3758/s13428-017-0876-8
   David-John B., 2021, ACM S EYE TRACK RES, V2, DOI [10.1145/3448018.34580081,2, DOI 10.1145/3448018.34580081,2]
   De Paolis LT, 2020, VIRTUAL REAL-LONDON, V24, P483, DOI 10.1007/s10055-019-00409-6
   Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355
   Gardner MW, 1998, ATMOS ENVIRON, V32, P2627, DOI 10.1016/S1352-2310(97)00447-0
   Hadnett-Hunter J, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3352763
   Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009
   Henderson J.M., 1998, EYE GUIDANCE READING, P269, DOI [10.1016/B978-008043361-5/50013-4, DOI 10.1016/B978-008043361-5/50013-4]
   Henderson JM, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0064937
   Hessels RS, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.180502
   Hild J, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204575
   Hu ZM, 2023, IEEE T VIS COMPUT GR, V29, P1992, DOI 10.1109/TVCG.2021.3138902
   Hu ZM, 2020, IEEE T VIS COMPUT GR, V26, P1902, DOI 10.1109/TVCG.2020.2973473
   Hutt S, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445269
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Ishimaru S, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC '17 ADJUNCT), P704, DOI 10.1145/3123024.3129271
   Jonker T. R., 2020, CHI2020 AI4HCI WORKS
   Keshava A, 2020, ETRA 2020 SHORT PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3379156.3391338
   Khosla P., 2021, arXiv
   Kiefer P., 2013, Using eye movements to recognize activities on cartographic maps, DOI [10.1145/2525314.25254672, DOI 10.1145/2525314.25254672]
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Lan GH, 2022, 2022 21ST ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2022), P233, DOI 10.1109/IPSN54338.2022.00026
   Land M., 2009, Looking and Acting: Vision and eye movements in natural behaviour, DOI [10.1093/acprof:oso/9780198570943.001.00012,8, DOI 10.1093/ACPROF:OSO/9780198570943.001.00012,8]
   Larsson L, 2013, IEEE T BIO-MED ENG, V60, P2484, DOI 10.1109/TBME.2013.2258918
   Liao H, 2019, INT J GEOGR INF SCI, V33, P739, DOI 10.1080/13658816.2018.1482554
   Lu FY, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517723
   Malpica S., 2023, IEEE Transactions on Visualization and Computer Graphics
   Martinez H., 2014, Journal of Multimedia Theory and Applications, V2, P27, DOI DOI 10.11159/JMTA.2014.0041
   Martinez-Conde S, 2004, NAT REV NEUROSCI, V5, P229, DOI 10.1038/nrn1348
   Min K, 2021, IEEE WINT CONF APPL, P1068, DOI 10.1109/WACV48630.2021.00111
   Oguiza I., 2022, tsai-a state-of-the-art deep learning library for time series and sequential data
   Pfeuffer K, 2021, COMPUT GRAPH-UK, V95, P1, DOI 10.1016/j.cag.2021.01.001
   Prasse P, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3588410
   Qin Y, 2017, Arxiv, DOI arXiv:1704.02971
   Rayner K, 2007, VISION RES, V47, P2714, DOI 10.1016/j.visres.2007.05.007
   Rook K, 2019, INT CONF PERVAS COMP, P227, DOI [10.1109/percomw.2019.8730692, 10.1109/PERCOMW.2019.8730692]
   Seeliger A, 2024, INT J HUM-COMPUT INT, V40, P761, DOI 10.1080/10447318.2022.2122114
   Soomro Khurram, 2012, CORR
   Srivastava Namrata, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3287067
   Um T. T., 2017, P 19 ACM INT C MULT, P216, DOI [10.1145/3136755.3136817, DOI 10.1145/3136755.3136817]
   Vortmann Lisa-Marie, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, P1, DOI 10.1145/3334480.3382889
   Wang Zhimin, 2024, 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), P839, DOI 10.1109/VRW62533.2024.00216
   Wang ZM, 2023, INT SYM MIX AUGMENT, P632, DOI 10.1109/ISMAR59233.2023.00078
   Wang ZM, 2022, IEEE T VIS COMPUT GR, V28, P3843, DOI 10.1109/TVCG.2022.3203110
   Wang ZM, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P539, DOI 10.1109/VRW55335.2022.00125
   Wang ZM, 2021, IEEE T HUM-MACH SYST, V51, P524, DOI 10.1109/THMS.2021.3097973
   Wang ZM, 2020, ADJUNCT PROCEEDINGS OF THE 2020 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2020), P165, DOI [10.1109/ISMAR-Adjunct51615.2020.00052, 10.1109/BDEIM52318.2020.00045]
   Widiasari IR, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIVE AND CREATIVE INFORMATION TECHNOLOGY (ICITECH)
   Yarbus A.L., 1973, Eye movements and vision
   Zhou B, 2020, IEEE T VIS COMPUT GR, V26, P3514, DOI 10.1109/TVCG.2020.3023635
NR 59
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7277
EP 7287
DI 10.1109/TVCG.2024.3456203
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300006
PM 39255152
DA 2024-11-06
ER

PT J
AU Nugegoda, N
   Jannat, ME
   Hasan, K
   Lasserre, P
AF Nugegoda, Nelusha
   Jannat, Marium-E
   Hasan, Khalad
   Lasserre, Patricia
TI Exploring the Effect of Viewing Attributes of Mobile AR Interfaces on
   Remote Collaborative and Competitive Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaboration; Mars; Mobile handsets; Augmented reality; Resists;
   Usability; Synchronization; Mobile Augmented Reality; Remote
   Collaboration; Collaborative Task; Competitive Task
ID AUGMENTED REALITY; MIXED REALITY; SYSTEM; DESIGN
AB Mobile devices have the potential to facilitate remote tasks through Augmented Reality (AR) solutions by integrating digital information into the real world. Although prior studies have explored Mobile Augmented Reality (MAR) for co-located collaboration, none have investigated the impact of various viewing attributes that can influence remote task performance, such as target object viewing angles, synchronization styles, or having a secondary small screen showing other users current view in the MAR environment. In this paper, we explore five techniques considering these attributes, specifically designed for two modes of remote tasks: collaborative and competitive. We conducted a user study employing various combinations of those attributes for both tasks. In both instances, results indicate users' optimal performance and preference for the technique that allows asynchronous viewing of object manipulations on the small screen. Overall, this paper contributes novel techniques for remote tasks in MAR, addressing aspects such as viewing angle and synchronization in object manipulation alongside secondary small-screen interfaces. Additionally, it presents the results of a user study evaluating the effectiveness, usability, and user preference of these techniques in remote settings and offers a set of recommendations for designing and implementing MAR solutions to enhance remote activities.
C1 [Nugegoda, Nelusha; Jannat, Marium-E; Hasan, Khalad; Lasserre, Patricia] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
C3 University of British Columbia
RP Nugegoda, N (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
EM nelusha.nugegodawattaranthenne@ubc.ca; marium.jannat@ubc.ca;
   khalad.hasan@ubc.ca; patricia.lasserre@ubc.ca
OI Hasan, Mohammad Khalad/0000-0002-4815-5461; Lasserre,
   Patricia/0000-0001-7080-2437; Nugegoda, Nelusha/0009-0007-8095-542X;
   Jannat, Marium-E-/0000-0002-4815-4741
FU NSERC CREATE [F20-05186]
FX This research was funded by an NSERC CREATE grant # F20-05186.
CR Antunes P., 2010, Collaboration and Technology,, P2
   Bai Z, 2012, INTERACT COMPUT, V24, P450, DOI 10.1016/j.intcom.2012.07.004
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Billinghurst M., 1998, Virtual Reality, V3, P25, DOI 10.1007/BF01409795
   Billinghurst M., 1999, Proc. of CHI, P194, DOI [10.1145/632716.6328382, DOI 10.1145/632716.6328382]
   Bowles C. T., 2010, P 4 S COMP HUM INT M, DOI [10.1145/1873561.18735682, DOI 10.1145/1873561.18735682]
   Bruno F, 2019, INT J ADV MANUF TECH, V105, P875, DOI 10.1007/s00170-019-04254-4
   Criollo-C S, 2024, SUSTAINABILITY-BASEL, V16, DOI 10.3390/su16031192
   Dagan E., Proc. ACM Hum.-Comput. Interact., V6, DOI [10.1145/35129091, DOI 10.1145/35129091]
   de Belen R. A. J., 2019, AIMS Electronics and Electrical Engineering, V3, P2
   Elvezio Carmine, 2017, ACM SIGGRAPH 2017 VR, DOI [10.1145/3089269.3089281, DOI 10.1145/3089269.3089281]
   Ens B, 2019, INT J HUM-COMPUT ST, V131, P81, DOI 10.1016/j.ijhcs.2019.05.011
   Exit Games, 2023, Photon Unity Networking (PUN) 2
   Fages Arthur, 2022, Proceedings of the ACM on Human-Computer Interaction, DOI 10.1145/3555607
   Fink Daniel Immanuel, 2022, Proceedings of the ACM on Human-Computer Interaction, DOI 10.1145/3567709
   Fischer JE, 2018, HUM-COMPUT INTERACT, V33, P305, DOI 10.1080/07370024.2018.1440556
   G. LLC, 2023, Ar core
   GAINES BR, 1991, INFORM SCIENCES, V57-8, P3, DOI 10.1016/0020-0255(91)90066-4
   Gauglitz S., 2014, P 27 ANN ACM S US IN, P449
   Guo WZZ, 2024, COMPUT METH PROG BIO, V244, DOI 10.1016/j.cmpb.2023.107970
   Hall M., 2018, DS 92, P347
   HART S G, 1988, P139
   Henrysson A, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P80
   Isenberg P, 2012, IEEE T VIS COMPUT GR, V18, P689, DOI 10.1109/TVCG.2011.287
   Johnson S, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P1825, DOI 10.1145/2675133.2675176
   Juanes J. A., 2014, P 2 INT C TECHN EC E, P7, DOI [10.1145/2669711.2669870, DOI 10.1145/2669711.2669870]
   Kim B., 2024, How many people have smartphones worldwide
   Kim K, 2018, IEEE T VIS COMPUT GR, V24, P2947, DOI 10.1109/TVCG.2018.2868591
   Kim S., 2020, Multimodal interfaces and communication cues for remote collaboration, P2
   Kim S, 2020, J MULTIMODAL USER IN, V14, P321, DOI 10.1007/s12193-020-00335-x
   Kim S, 2018, COMPUT SUPP COOP W J, V27, P569, DOI 10.1007/s10606-018-9324-2
   Knoll T., 2023, 2023 CHI C HUM FACT, DOI DOI 10.1145/3544549.3585841
   Koceski S., 2012, ICT Innovations 2011, P2
   Lah N. H. C., 2024, International Journal of Evaluation and Research in Education, V13, P1007, DOI [10.11591/ijere.v13i2.251981, DOI 10.11591/IJERE.V13I2.251981]
   Lukosch S., 2015, Computer Supported Cooperative Work (CSCW), V24, P2
   Lumivero, 2023, Nvivo-qualitative data analysis software
   Lundgren S, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P496, DOI 10.1145/2675133.2675171
   Marques B, 2022, COMPUT GRAPH-UK, V102, P619, DOI 10.1016/j.cag.2021.08.006
   Milgram P, 1999, HUM FAC ERG SOC P, P1177
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Nilsson S, 2009, INT SYM MIX AUGMENT, P3, DOI 10.1109/ISMAR.2009.5336522
   Oda O, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P405, DOI 10.1145/2807442.2807497
   Perdue T., 2020, Applications of augmented reality-ar is evolv- ing as computing power increases
   Plank T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4548, DOI 10.1145/3025453.3025537
   Romalee W, 2024, ARCH GERONTOL GERIAT, V117, DOI 10.1016/j.archger.2023.105277
   Seo J., 2022, Proc. ACM Hum.-Comput. Interact., V7, DOI [10.1145/35675612, DOI 10.1145/35675612]
   Stoakley R., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P265
   Takao N, 2003, INT J COMPUT VISION, V53, P115, DOI 10.1023/A:1023084706295
   Thomas P. J., 1996, CSCW Requirements and Evaluation, P2
   van Lopik K, 2020, COMPUT IND, V117, DOI 10.1016/j.compind.2020.103208
   Villanueva A., Proc. ACM Hum.-Comput. Interact., V6, DOI [10.1145/35129289, DOI 10.1145/35129289]
   Vuforia, 2024, Vuforia developer portal
   Wang HW, 2010, ROBOT CIM-INT MANUF, V26, P778, DOI 10.1016/j.rcim.2010.05.005
   Wang P, 2019, INT J ADV MANUF TECH, V105, P3031, DOI 10.1007/s00170-019-04434-2
   Wang X, 2016, ADV MANUF, V4, P1, DOI 10.1007/s40436-015-0131-4
   Wells Thomas, 2022, Proceedings of the ACM on Human-Computer Interaction, DOI 10.1145/3546735
   Wells T, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376541
   Yuill N, 2012, ACM T COMPUT-HUM INT, V19, DOI 10.1145/2147783.2147784
   Zagermann J, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5470, DOI 10.1145/2858036.2858224
   zoom, 2023, Zoom video communications
NR 60
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7288
EP 7298
DI 10.1109/TVCG.2024.3456162
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300029
PM 39250404
DA 2024-11-06
ER

PT J
AU Chen, J
   Yuan, ZX
   Xi, JQ
   Gao, ZQ
   Li, Y
   Zhu, XQ
   Shi, YS
   Guan, F
   Wang, YM
AF Chen, Jie
   Yuan, Zexin
   Xi, Jiaqi
   Gao, Ziqin
   Li, Ying
   Zhu, Xiaoqiang
   Shi, Yun Stone
   Guan, Frank
   Wang, Yimin
TI Efficient and Accurate Semi-Automatic Neuron Tracing with Extended
   Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neurons; Image reconstruction; Accuracy; Manuals; Visualization;
   Extended reality; Three-dimensional displays; Extended Reality; Neuron
   Tracing; Human-centered Computing; Eye Tracking
ID FAST ITERATIVE METHOD; VISUALIZATION; MORPHOLOGY; SEGMENTATION; BRAIN
AB Neuron tracing, alternately referred to as neuron reconstruction, is the procedure for extracting the digital representation of the three-dimensional neuronal morphology from stacks of microscopic images. Achieving accurate neuron tracing is critical for profiling the neuroanatomical structure at single-cell level and analyzing the neuronal circuits and projections at whole-brain scale. However, the process often demands substantial human involvement and represents a nontrivial task. Conventional solutions towards neuron tracing often contend with challenges such as non-intuitive user interactions, suboptimal data generation throughput, and ambiguous visualization. In this paper, we introduce a novel method that leverages the power of extended reality (XR) for intuitive and progressive semi-automatic neuron tracing in real time. In our method, we have defined a set of interactors for controllable and efficient interactions for neuron tracing in an immersive environment. We have also developed a GPU-accelerated automatic tracing algorithm that can generate updated neuron reconstruction in real time. In addition, we have built a visualizer for fast and improved visual experience, particularly when working with both volumetric images and 3D objects. Our method has been successfully implemented with one virtual reality (VR) headset and one augmented reality (AR) headset with satisfying results achieved. We also conducted two user studies and proved the effectiveness of the interactors and the efficiency of our method in comparison with other approaches for neuron tracing.
C1 [Chen, Jie; Yuan, Zexin; Xi, Jiaqi; Gao, Ziqin; Shi, Yun Stone; Wang, Yimin] Guangdong Inst Intelligent Sci & Technol, Guangzhou, Peoples R China.
   [Chen, Jie; Yuan, Zexin; Gao, Ziqin; Li, Ying; Zhu, Xiaoqiang] Shanghai Univ, Shanghai, Peoples R China.
   [Guan, Frank] Singapore Inst Technol, Singapore, Singapore.
C3 Shanghai University; Singapore Institute of Technology
RP Wang, YM (corresponding author), Guangdong Inst Intelligent Sci & Technol, Guangzhou, Peoples R China.
EM chenjie@gdiist.cn; yuan.zx@outlook.com; xijiaqi@gdiist.cn;
   gaoziqin@gdiist.cn; yinglotus@shu.edu.cn; xqzhu@shu.edu.cn;
   shiyun@gdiist.cn; Frank.guan@singaporetech.edu.sg; ywang@gdiist.cn
FU National Natural Science Foundation of China [32071367]; Guangdong High
   Level Innovation Research Institute [2021B0909050004]; National Key R&D
   Program of China [2019YFA0801603]; Key Area Research and Development
   Program of Guangdong Province [2021B0909060002]
FX This study was supported by the National Natural Science Foundation of
   China (32071367), the Guangdong High Level Innovation Research Institute
   (2021B0909050004), the National Key R&D Program of China
   (2019YFA0801603), and the Key Area Research and Development Program of
   Guangdong Province (2021B0909060002)
CR Acciai L, 2016, NEUROINFORMATICS, V14, P353, DOI 10.1007/s12021-016-9310-0
   Arshadi C, 2021, NAT METHODS, V18, P374, DOI 10.1038/s41592-021-01105-7
   Bao YW, 2023, Symposium Virtual Re, P22, DOI 10.1109/VR55154.2023.00018
   Basu S, 2016, IEEE T MED IMAGING, V35, P1443, DOI 10.1109/TMI.2016.2515068
   Basu S, 2014, IEEE IMAGE PROC, P3597, DOI 10.1109/ICIP.2014.7025730
   Bear M.F., 2016, NEUROSCIENCE EXPLORI, VFourth, P1
   Beyer J, 2022, COMPUT GRAPH FORUM, V41, P573, DOI 10.1111/cgf.14574
   BREU H, 1995, IEEE T PATTERN ANAL, V17, P529, DOI 10.1109/34.391389
   Bria A, 2016, NAT METHODS, V13, P192, DOI 10.1038/nmeth.3767
   Brooke J., 1996, Usability Eval. Ind./Taylor Fr, V189, P4, DOI DOI 10.1201/9781498710411-35
   Brunyé TT, 2019, COGN RES, V4, DOI 10.1186/s41235-019-0159-2
   Chengoden R, 2023, IEEE ACCESS, V11, P12764, DOI 10.1109/ACCESS.2023.3241628
   Deakin L, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P25, DOI 10.1145/3355088.3365164
   Dickstein Dara L, 2016, Curr Protoc Neurosci, V77, DOI 10.1002/cpns.16
   Dorkenwald S., 2023, CAVE: Connectome Annotation Versioning Engine, DOI [10.1101/2023.07.26.550598, DOI 10.1101/2023.07.26.550598]
   Dorkenwald S., 2023, Neuronal wiring diagram of an adult brain, DOI [10.1101/2023.06.27.546656, DOI 10.1101/2023.06.27.546656]
   Eckstein MK, 2017, DEV COGN NEUROS-NETH, V25, P69, DOI 10.1016/j.dcn.2016.11.001
   El Beheiry M, 2020, J MOL BIOL, V432, P4745, DOI 10.1016/j.jmb.2020.05.026
   Engel K., 2004, ACM SIGGRAPH 2004 CO, P29, DOI [10.1145/1103900.1103929, DOI 10.1145/1103900.1103929]
   Feit AM, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1118, DOI 10.1145/3025453.3025599
   Fu ZS, 2011, SIAM J SCI COMPUT, V33, P2468, DOI 10.1137/100788951
   Fulmer W, 2019, PROCEEDINGS OF IUI 2019, P367, DOI 10.1145/3301275.3302319
   Gao L, 2022, NAT NEUROSCI, V25, P515, DOI 10.1038/s41593-022-01041-5
   Gong H, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12142
   Günther U, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P166, DOI [10.1109/visual.2019.8933605, 10.1109/VISUAL.2019.8933605]
   HART S G, 1988, P139
   Jeong WK, 2008, SIAM J SCI COMPUT, V30, P2512, DOI 10.1137/060670298
   Kytö M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173655
   Li QF, 2022, BIOINFORMATICS, V38, P809, DOI 10.1093/bioinformatics/btab716
   Li RJ, 2017, IEEE T MED IMAGING, V36, P1533, DOI 10.1109/TMI.2017.2679713
   Li Y.X., 2019, VRIH, V56, P84, DOI DOI 10.3724/SP.J.2096-5796.2018.0006
   Li ZY, 2018, NEUROINFORMATICS, V16, P339, DOI 10.1007/s12021-018-9361-5
   Liu SQ, 2018, IEEE T MED IMAGING, V37, P2441, DOI 10.1109/TMI.2018.2833420
   Liu YF, 2022, BIOINFORMATICS, V38, P5329, DOI 10.1093/bioinformatics/btac712
   Manubens-Gil L, 2023, NAT METHODS, V20, P824, DOI 10.1038/s41592-023-01848-5
   McDonald T, 2021, IEEE T VIS COMPUT GR, V27, P744, DOI 10.1109/TVCG.2020.3030363
   Meijering E., 2003, Proceedings of the Fifth IASTED International Conference on Signal and Image Processing, P491
   Mutasim A. K., 2021, ACM S EYE TRACK RES, P1, DOI [10.1145/3448018.34579986, DOI 10.1145/3448018.34579986]
   Parekh R, 2013, NEURON, V77, P1017, DOI 10.1016/j.neuron.2013.03.008
   Peng H., 2023, PREPRINT, DOI [10.21203/rs.3.rs-3371435/v12,7, DOI 10.21203/RS.3.RS-3371435/V12,7]
   Peng HC, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5342
   Peng HC, 2011, BIOINFORMATICS, V27, pI239, DOI 10.1093/bioinformatics/btr237
   Peng HC, 2010, BIOINFORMATICS, V26, pi38, DOI 10.1093/bioinformatics/btq212
   Peng HC, 2010, NAT BIOTECHNOL, V28, P348, DOI 10.1038/nbt.1612
   Pfeuffer K, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P99, DOI 10.1145/3131277.3132180
   Qin XF, 2021, BIOCHEM BIOPH RES CO, V557, P8, DOI 10.1016/j.bbrc.2021.03.160
   Rakkolainen I, 2021, MULTIMODAL TECHNOLOG, V5, DOI 10.3390/mti5120081
   Schindelin J, 2012, NAT METHODS, V9, P676, DOI [10.1038/NMETH.2019, 10.1038/nmeth.2019]
   Schneider-Mizell CM, 2016, ELIFE, V5, DOI 10.7554/eLife.12059
   Steiniger BS, 2021, HISTOCHEM CELL BIOL, V155, P341, DOI 10.1007/s00418-020-01924-3
   Usher W, 2018, IEEE T VIS COMPUT GR, V24, P994, DOI 10.1109/TVCG.2017.2744079
   Venkatesan M, 2021, CELL REP MED, V2, DOI 10.1016/j.xcrm.2021.100348
   Wang H, 2019, IEEE COMPUT SOC CONF, P1105, DOI 10.1109/CVPRW.2019.00144
   Wang S, 2022, IEEE T MED IMAGING, V41, P1688, DOI 10.1109/TMI.2022.3146973
   Wang SY, 2018, Arxiv, DOI arXiv:1805.04997
   Wang XJ, 2021, CELL REP, V34, DOI 10.1016/j.celrep.2021.108709
   Wang YM, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11443-y
   Winnubst J, 2019, CELL, V179, P268, DOI 10.1016/j.cell.2019.07.042
   Wu JP, 2014, NEUROIMAGE, V87, P199, DOI 10.1016/j.neuroimage.2013.10.036
   Xiao H, 2013, BIOINFORMATICS, V29, P1448, DOI 10.1093/bioinformatics/btt170
   Yang B, 2022, IEEE T MED IMAGING, V41, P903, DOI 10.1109/TMI.2021.3125777
   Yang J, 2019, NEUROINFORMATICS, V17, P185, DOI 10.1007/s12021-018-9392-y
   Zhou Zhi, 2018, Brain Inform, V5, P3, DOI 10.1186/s40708-018-0081-2
   Zhou Z, 2015, NEUROINFORMATICS, V13, P153, DOI 10.1007/s12021-014-9249-y
NR 65
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7299
EP 7309
DI 10.1109/TVCG.2024.3456164
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300045
PM 39255163
DA 2024-11-06
ER

PT J
AU Cen, YC
   Deng, HC
   Ma, Y
   Liang, XH
AF Cen, Yunchi
   Deng, Hanchen
   Ma, Yue
   Liang, Xiaohui
TI A Real-Time and Interactive Fluid Modeling System for Mixed Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE fluid modeling; mixed reality; differentiable rendering; differentiable
   rendering; mixed reality; differentiable rendering
ID FLOW; DENSITY
AB -Within the realm of mixed reality, the capability to dynamically render environmental effects with high realism plays a crucial role in amplifying user engagement and interaction. Fluid dynamics, in particular, stand out as essential elements for crafting immersive virtual settings. This includes the simulation of phenomena like smoke, fire, and clouds, which are instrumental in enriching the virtual experience. This work showcases a cutting-edge system developed to produce dynamic and interactive fluid effects that mirror real captured data in real-time for mixed reality applications. This innovative system seamlessly incorporates fluid reconstruction alongside velocity estimation processes within the Unity engine environment. Our approach leverages a novel physics-based differentiable rendering technique, grounded in the principles of light transport in participating media, to simulate the intricate behaviors of fluid while ensuring high fidelity in visual appearance. To further enhance realism, we have expanded our framework to include the estimation of velocity fields, addressing the critical need for fluid motion simulation. The practical application of these techniques demonstrates the system's capacity to offer a robust platform for fluid modeling in mixed reality environments. Through extensive evaluations, we illustrate the effectiveness of our approach in various scenes, underscoring its potential to transform mixed reality content creation by providing developers with the tools to incorporate highly realistic and interactive fluid seamlessly.
C1 [Cen, Yunchi; Deng, Hanchen; Ma, Yue; Liang, Xiaohui] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Liang, Xiaohui] Zhongguancun Lab, Beijing, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory
RP Liang, XH (corresponding author), Zhongguancun Lab, Beijing, Peoples R China.
EM cenyc@buaa.edu.cn; liang_xiaohui@buaa.edu.cn
RI Cen, Yunchi/ITT-5154-2023
OI Cen, Yunchi/0000-0003-4930-9386; liang, xiaohui/0000-0001-6351-2538
FU National Natural Science Foundation of China [62272019]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62272019).
CR Aguirre-Pablo AA, 2019, EXP FLUIDS, V60, DOI 10.1007/s00348-018-2660-7
   Atcheson B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409085
   Atcheson B, 2009, EXP FLUIDS, V46, P467, DOI 10.1007/s00348-008-0572-7
   Bai K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480492
   Belden J, 2010, MEAS SCI TECHNOL, V21, DOI 10.1088/0957-0233/21/12/125403
   Boettcher KER, 2020, IEEE GLOB ENG EDUC C, P1563, DOI [10.1109/educon45650.2020.9125348, 10.1109/EDUCON45650.2020.9125348]
   Bridgeman R., 2007, ACM SIGGRAPH 2007, P1, DOI DOI 10.1145/1281500.1281681
   Cen Y., 2023, Computer Graphics Forum, V42
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chen W., 2019, Advances in Neural Information Processing Systems, V32, P9609
   Chu MY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530169
   Cong Huang, 2020, Frontier Computing. Theory, Technologies and Applications (FC 2019). Lecture Notes in Electrical Engineering (LNEE 551), P760, DOI 10.1007/978-981-15-3250-4_97
   Corpetti T, 2002, IEEE T PATTERN ANAL, V24, P365, DOI 10.1109/34.990137
   Eckert ML, 2018, COMPUT GRAPH FORUM, V37, P47, DOI 10.1111/cgf.13511
   Eckert ML, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356545
   Elsinga GE, 2006, EXP FLUIDS, V41, P933, DOI 10.1007/s00348-006-0212-z
   Fahringer TW, 2015, MEAS SCI TECHNOL, V26, DOI 10.1088/0957-0233/26/11/115201
   Feng YT, 2024, Arxiv, DOI arXiv:2401.15318
   Franz E, 2021, PROC CVPR IEEE, P1632, DOI 10.1109/CVPR46437.2021.00168
   Gattullo M, 2015, IEEE COMPUT GRAPH, V35, P52, DOI 10.1109/MCG.2015.36
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Gkioulekas I, 2016, LECT NOTES COMPUT SC, V9907, P685, DOI 10.1007/978-3-319-46487-9_42
   Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508377
   Goldhahn E, 2007, EXP FLUIDS, V43, P241, DOI 10.1007/s00348-007-0331-1
   Gregson J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185548
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   Han PH, 2023, IEEE T VIS COMPUT GR, V29, P2670, DOI 10.1109/TVCG.2023.3247073
   Henderson P, 2018, Arxiv, DOI arXiv:1807.09259
   Herlin I, 2012, LECT NOTES COMPUT SC, V7575, P15, DOI 10.1007/978-3-642-33765-9_2
   Höhl W, 2023, EUR J FUTURES RES, V11, DOI 10.1186/s40309-023-00218-w
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu YM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459671
   Ihrke I., 2004, P 2004 ACM SIGGRAPH, P365, DOI DOI 10.1145/1028523.1028572
   Ihrke I, 2006, GRAPH MODELS, V68, P484, DOI 10.1016/j.gmod.2006.08.001
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Kak A. C., 2001, SIAM, P2
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kingma D.P., 2014, P INT C LEARNING REP
   Lee C, 2013, IEEE T VIS COMPUT GR, V19, P547, DOI 10.1109/TVCG.2013.41
   Liu SC, 2019, Arxiv, DOI arXiv:1901.05567
   Liu TS, 2008, J FLUID MECH, V614, P253, DOI 10.1017/S0022112008003273
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Martin G, 2020, J MED INTERNET RES, V22, DOI 10.2196/21486
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mourtzis Dimitris, 2022, Procedia CIRP, P1144, DOI 10.1016/j.procir.2022.05.122
   Nimier-David M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392406
   Okabe M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766958
   Psomathianos G, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P117, DOI 10.1109/CW52790.2021.00025
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Raptis GE, 2018, INT J HUM-COMPUT ST, V114, P69, DOI 10.1016/j.ijhcs.2018.02.003
   Rokhsaritalemi S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020636
   Ruhnau P, 2007, MEAS SCI TECHNOL, V18, P755, DOI 10.1088/0957-0233/18/3/027
   Simondon G., 1980, On the mode of existence of technical objects, P2
   Slater M, 2018, BRIT J PSYCHOL, V109, P431, DOI 10.1111/bjop.12305
   Solmaz S, 2022, COMPUT CHEM ENG, V156, DOI 10.1016/j.compchemeng.2021.107570
   Song P., 2008, P 7 ACM SIGGRAPH INT
   Tan Z. P., 2019, AIAA SCIT 2019 FOR, P0267
   Tang YM, 2020, VIRTUAL REAL-LONDON, V24, P797, DOI 10.1007/s10055-020-00427-9
   Thapa S, 2020, PROC CVPR IEEE, P21, DOI 10.1109/CVPR42600.2020.00010
   Um K, 2018, COMPUT GRAPH FORUM, V37, P171, DOI 10.1111/cgf.13522
   Wang Y, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P987, DOI 10.1109/VRW58643.2023.00336
   Xiong J., 2018, 2018 IEEE INT C COMP, P1
   Xiong JH, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073662
   Yuan J, 2007, SIAM J SCI COMPUT, V29, P2283, DOI 10.1137/060660709
   Zhang C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356522
   Zhang FQ, 2020, MULTIMED TOOLS APPL, V79, P16683, DOI 10.1007/s11042-019-08002-4
   Zhang QF, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P823, DOI 10.1109/VRW58643.2023.00254
   Zhou KL, 2023, INT SYM MIX AUGMENT, P167, DOI 10.1109/ISMAR59233.2023.00031
   Zhou KL, 2023, IEEE T VIS COMPUT GR, V29, P2456, DOI 10.1109/TVCG.2023.3247092
NR 69
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7310
EP 7320
DI 10.1109/TVCG.2024.3456177
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300011
PM 39250403
DA 2024-11-06
ER

PT J
AU Mishra, A
   Singh, H
   Parnami, A
   Shukla, J
AF Mishra, Abhijeet
   Singh, Harshvardhan
   Parnami, Aman
   Shukla, Jainendra
TI MobiTangibles: Enabling Physical Manipulation Experiences of Virtual
   Precision Hand-Held Tools' Miniature Control in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Smart phones; Sensors; Magnetic field measurement; Training;
   Magnetometers; Complexity theory; Aerospace electronics; Physical
   manipulation; Miniature control; Skill training; Hand-held tool; Virtual
   reality
ID REALITY
AB Realistic simulation for miniature control interactions, typically identified by precise and confined motions, commonly found in precision hand-held tools, like calipers, powered engravers, retractable knives, etc., are beneficial for skill training associated with these kinds of tools in virtual reality (VR) environments. However, existing approaches aiming to simulate hand-held tools' miniature control manipulation experiences in VR entail prototyping complexity and require expertise, posing challenges for novice users and individuals with limited resources. Addressing this challenge, we introduce MobiTangibles-proxies for precision hand-held tools' miniature control interactions utilizing smartphone-based magnetic field sensing. MobiTangibles passively replicate fundamental miniature control experiences associated with hand-held tools, such as single-axis translation and rotation, enabling quick and easy use for diverse VR scenarios without requiring extensive technical knowledge. We conducted a comprehensive technical evaluation to validate the functionality of MobiTangibles across diverse settings, including evaluations for electromagnetic interference within indoor environments. In a user-centric evaluation involving 15 participants across bare hands, VR controllers, and MobiTangibles conditions, we further assessed the quality of miniaturized manipulation experiences in VR. Our findings indicate that MobiTangibles outperformed conventional methods in realism and fatigue, receiving positive feedback.
EM abhijeet@iiitd.ac.in; jainendra@iiitd.ac.in
RI Shukla, Jainendra/AAE-2245-2019
OI Singh, Harsh Vardhan/0009-0003-4514-5502; Parnami,
   Aman/0000-0003-3845-6305; Mishra, Abhijeet/0000-0001-6662-3946
FU TCS - Centre for Design and New Media; Infosys Centre for Artificial
   Intelligence at IIIT Delhi
FX This research was supported by the TCS - Centre for Design and New Media
   and Infosys Centre for Artificial Intelligence at IIIT Delhi.
CR Abe T, 2021, PROCEEDINGS OF 23RD ACM INTERNATIONAL CONFERENCE ON MOBILE HUMAN-COMPUTER INTERACTION (MOBILEHCI 2021): MOBILE APART, MOBILE TOGETHER, DOI 10.1145/3447526.3472052
   [Anonymous], 2016, HTC Vive
   [Anonymous], 2017, STATE WORLDS CHILDRE
   Arisandi R, 2012, ADJUNCT PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P17
   Arora J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300286
   Ashbrook D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2043
   Bai HD, 2021, COMPUT GRAPH-UK, V97, P42, DOI 10.1016/j.cag.2021.04.004
   Barnard L, 2005, INT J HUM-COMPUT ST, V62, P487, DOI 10.1016/j.ijhcs.2004.12.002
   Besancon L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4727, DOI 10.1145/3025453.3025863
   Besançon L, 2017, IEEE T VIS COMPUT GR, V23, P881, DOI 10.1109/TVCG.2016.2599217
   Bossavit B, 2015, PRESENCE-TELEOP VIRT, V23, P377, DOI 10.1162/PRES_a_00207
   Cabaret P.-A., 2023, IEEE Transactions on Visualization and Computer Graphics, V2
   Chulliat A., 2015, The us/uk world magnetic model for 2015-2020, V3, P6
   Cockburn A., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P203, DOI 10.1145/503376.503413
   Dickrell P. L., 2018, 2018 ASEE ANN C EXP, P1
   Dourish P., 2001, Where the action is: the foundations of embodied interaction, P2
   Ens B, 2021, IEEE T VIS COMPUT GR, V27, P1193, DOI 10.1109/TVCG.2020.3030334
   Ens B, 2019, INT J HUM-COMPUT ST, V131, P81, DOI 10.1016/j.ijhcs.2019.05.011
   Fang C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376470
   Farago F. T., 1994, Handbook of dimensional measurement, P3
   Feick M., 2023, P 36 ANN ACM S US IN, P1
   Feick M, 2020, INT SYM MIX AUGMENT, P195, DOI 10.1109/ISMAR50242.2020.00042
   fishersci, Precision movable syringes
   Greenberg S., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P209, DOI 10.1145/502348.502388
   Harrison C, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P121
   Henderson S, 2010, IEEE T VIS COMPUT GR, V16, P4, DOI 10.1109/TVCG.2009.91
   Heo S, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3186544
   Hettiarachchi A, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1957, DOI 10.1145/2858036.2858134
   HINCKLEY K, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P452, DOI 10.1145/191666.191821
   Hwang S., 2013, Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, UIST '13, P411
   Ishii Hiroshi, 1997, Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, DOI DOI 10.1145/258549.258715
   Kari M, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580677
   Kashiwagi T, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1411, DOI 10.1109/vr.2019.8798370
   Ketabdar H, 2010, IUI 2010, P413
   Ketabdar Hamed., 2010, Proceedings of the 12th international conference on Human computer interaction with mobile devices and services, P443
   Kovacs Robert, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P1046, DOI 10.1145/3379337.3415854
   Laput G, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2161
   Lee J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300301
   Liang RH, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P319
   Lipari Nicholas G., 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P129, DOI 10.1109/3DUI.2015.7131737
   Loftin R. B., 1994, INT IND TRAIN SYST E, P1
   Lyons K, 2016, IEEE INT SYM WRBL CO, P176, DOI 10.1145/2071763.2971787
   Manches A, 2012, PERS UBIQUIT COMPUT, V16, P405, DOI 10.1007/s00779-011-0406-0
   mathworks, Matlab mobile app
   mathworks, Matlab to unity game engine
   Matulic Fabrice., 2021, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, P1, DOI [10.1145/3411764.3445583, DOI 10.1145/3411764.3445583]
   meta, Meta quest
   Muender T, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300903
   Nascimbeni F., 2019, Scoping Paper, V1, P1
   Nianlong Li, 2020, UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, P1261, DOI 10.1145/3379337.3415812
   oppo, Oppo a5s
   Osiurak F, 2016, PSYCHOL REV, V123, P534, DOI 10.1037/rev0000027
   phyphox, phyphox
   pond5, Precise syringe dropping
   Ricca A, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P103, DOI 10.1109/VR50410.2021.00031
   Salemi Parizi Farshid, 2022, GetMobile: Mobile Computing and Communications, V25, P34, DOI 10.1145/3511285.3511295
   Simeone AL, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3307, DOI 10.1145/2702123.2702389
   Smus B, 2015, ISWC 2015: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P43, DOI 10.1145/2802083.2808395
   Spittle B., 2022, IEEE Transactions on Visualization and Computer Graphics, P2
   Strandholt PL, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376303
   Strasnick E, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174218
   Strasnick E, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5075, DOI 10.1145/3025453.3025988
   Tsuchida Taichi, 2022, Proceedings of the ACM on Human-Computer Interaction, DOI 10.1145/3567717
   ultimaker, about us
   Unity3d, ABOUT US
   Wang ZL, 2023, INT J HUM FACT ERGON, V10, DOI 10.1504/IJHFE.2023.128584
   White M., 2019, P 12 ACM SIGGRAPH C, P1
   Wilkes C. B., 2012, 3d user interfaces using tracked multi-touch mobile devices, P2
   Xiao C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322943
   Yang J, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P889, DOI 10.1145/3242587.3242643
   youtube, Jewelry polishing
   youtube, Precise desoldering smd components
   youtube, Precise soldering/desoldering
   youtube, Precise carving
   Zhang L, 2023, VIRTUAL REAL-LONDON, V27, P1273, DOI 10.1007/s10055-022-00735-2
   Zheng C, 2018, PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION (TEI'18), P57, DOI 10.1145/3173225.3173268
   Zhu KN, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300923
NR 77
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7321
EP 7331
DI 10.1109/TVCG.2024.3456197
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300027
PM 39269808
DA 2024-11-06
ER

PT J
AU Asish, SM
   Kulshreshth, AK
   Borst, CW
   Sutradhar, S
AF Asish, Sarker M.
   Kulshreshth, Arun K.
   Borst, Christoph W.
   Sutradhar, Shaon
TI Classification of Internal and External Distractions in an Educational
   VR Environment Using Multimodal Features
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Electroencephalography; Feature extraction; Gaze tracking; Brain
   modeling; Physiology; Avatars; Convolutional neural networks; Machine
   Learning; Human-centered computing; EEG; Eye-tracking
ID ATTENTION; MEMORY; MIND; AGE
AB Virtual reality (VR) can potentially enhance student engagement and memory retention in the classroom. However, distraction among participants in a VR-based classroom is a significant concern. Several factors, including mind wandering, external noise, stress, etc., can cause students to become internally and/or externally distracted while learning. To detect distractions, single or multi-modal features can be used. A single modality is found to be insufficient to detect both internal and external distractions, mainly because of individual variability. In this work, we investigated multi-modal features: eye tracking and EEG data, to classify the internal and external distractions in an educational VR environment. We set up our educational VR environment and equipped it for multi-modal data collection. We implemented different machine learning (ML) methods, including k-nearest-neighbors (kNN), Random Forest (RF), one-dimensional convolutional neural network - long short-term memory (1 D-CNN-LSTM), and two-dimensional convolutional neural networks (2D-CNN) to classify participants' internal and external distraction states using the multi-modal features. We performed cross-subject, cross-session, and gender-based grouping tests to evaluate our models. We found that the RF classifier achieves the highest accuracy over 83% in the cross-subject test, around 68% to 78% in the cross-session test, and around 90% in the gender-based grouping test compared to other models. SHAP analysis of the extracted features illustrated greater contributions from the occipital and prefrontal regions of the brain, as well as gaze angle, gaze origin, and head rotation features from the eye tracking data.
C1 [Asish, Sarker M.] Florida Polytech Univ, Lakeland, FL 33805 USA.
   [Kulshreshth, Arun K.; Borst, Christoph W.] Univ Louisiana, Lafayette, LA 70504 USA.
   [Sutradhar, Shaon] AIMEN, AI & Data Analyt Lab, O Porrino, Spain.
C3 Florida Polytechnical University; University of Louisiana Lafayette
RP Asish, SM (corresponding author), Florida Polytech Univ, Lakeland, FL 33805 USA.
EM asish.sust@gmail.com; arunkul@louisiana.edu; cwborst@gmail.com;
   shaon.cuet@gmail.com
RI Asish, Sarker Monojit/AAE-1811-2022
FU National Science Foundation [1815976]; Louisiana Board of Regents
   [LEQSF(2022-25)-RD-A-24]
FX This work was supported by the National Science Foundation under Grant
   No. 1815976 and the Louisiana Board of Regents under contract No.
   LEQSF(2022-25)-RD-A-24.
CR Urigüen JA, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/3/031001
   Asish SM, 2023, PROCEEDINGS OF THE ACM SYMPOSIUM ON APPLIED PERCEPTION, SAP 2023, DOI 10.1145/3605495.3605790
   Asish SM, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P703, DOI 10.1109/VRW58643.2023.00194
   Asish SM, 2022, COMPUT GRAPH-UK, V109, P75, DOI 10.1016/j.cag.2022.10.007
   Asish Sarker Monojit., 2021, ICAT-EGVE2021-International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments, DOI DOI 10.2312/EGVE.20211326
   Baceviciute S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376872
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Barmaki Roghayeh Leila, 2024, 2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR), P205, DOI 10.1109/AIxVR59861.2024.00033
   Bhat SS, 2023, INT SYM MIX AUGMENT, P503, DOI 10.1109/ISMAR59233.2023.00065
   Borst CW, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P467, DOI 10.1109/VR.2018.8448286
   Borst CW, 2016, P IEEE VIRT REAL ANN, P157, DOI 10.1109/VR.2016.7504701
   Bozkir E, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P597, DOI 10.1109/VR50410.2021.00085
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Dunnoulin S, 2020, J PAIN RES, V13, P2213, DOI 10.2147/JPR.S238766
   Gardony AL, 2013, SPAT COGN COMPUT, V13, P319, DOI 10.1080/13875868.2013.792821
   Glass AL, 2019, EDUC PSYCHOL-UK, V39, P395, DOI 10.1080/01443410.2018.1489046
   Goodfellow I., 2016, Deep learning, P5
   Han Y, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519627
   Homan R. W., 1988, American Journal of EEG Technology, V28, P269
   Hutt S, 2019, USER MODEL USER-ADAP, V29, P821, DOI 10.1007/s11257-019-09228-5
   Jeong D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P827, DOI [10.1109/vr.2019.8798334, 10.1109/VR.2019.8798334]
   Kingma D.P., 2014, P INT C LEARNING REP
   Klimesch W, 1997, INT J PSYCHOPHYSIOL, V26, P319, DOI 10.1016/S0167-8760(97)00773-3
   Kosmyna N, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19235200
   Lahat D, 2015, P IEEE, V103, P1449, DOI 10.1109/JPROC.2015.2460697
   Lamb R, 2020, J SCI EDUC TECHNOL, V29, P573, DOI 10.1007/s10956-020-09837-5
   Liang-Yi Chung, 2011, Proceedings 2011 16th North-East Asia Symposium on Nano, Information Technology and Reliability (NASNIT 2011), P29, DOI 10.1109/NASNIT.2011.6111116
   Liebling DJ, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1169, DOI 10.1145/2638728.2641688
   Lundberg SM, 2019, Arxiv, DOI [arXiv:1802.03888, 10.48550/arXiv.1802.03888]
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   MALACH R, 1995, P NATL ACAD SCI USA, V92, P8135, DOI 10.1073/pnas.92.18.8135
   Mendoza JS, 2018, COMPUT HUM BEHAV, V86, P52, DOI 10.1016/j.chb.2018.04.027
   Menze BH, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-213
   Miller EK, 2001, ANNU REV NEUROSCI, V24, P167, DOI 10.1146/annurev.neuro.24.1.167
   Neiterman E, 2019, CAN J SCHOLARSH TEA, V10, DOI 10.5206/cjsotl-rcacea.2019.1.8002
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1945, DOI 10.1109/TVCG.2020.2973498
   RABBITT P, 1995, PSYCHOL AGING, V10, P307, DOI 10.1037/0882-7974.10.3.307
   Radianti Jaziar, 2020, Computers & Education, V147, P18, DOI 10.1016/j.compedu.2019.103778
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Rodrigue M., 2015, P 20 INT C INT US IN, P121, DOI [DOI 10.1145/2678025.2701382, 10.1145/2678025.2701382]
   Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838
   SCHAIE KW, 1993, PSYCHOL AGING, V8, P44, DOI 10.1037/0882-7974.8.1.44
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Tao W, 2023, IEEE T AFFECT COMPUT, V14, P382, DOI 10.1109/TAFFC.2020.3025777
   Vortmann LM, 2022, FRONT COMP SCI-SWITZ, V4, DOI 10.3389/fcomp.2022.780580
   Wallace JC, 2002, J GEN PSYCHOL, V129, P238, DOI 10.1080/00221300209602098
   Wang ZG, 2015, Arxiv, DOI [arXiv:1506.00327, 10.48550/ARXIV.1506.00327]
   Wikipedia, 2023, 21 electrodes of international 10-20 system for eeg
   Zheng WL, 2014, IEEE ENG MED BIO, P5040, DOI 10.1109/EMBC.2014.6944757
   Zulman DM, 2021, JAMA-J AM MED ASSOC, V325, P437, DOI 10.1001/jama.2020.27304
NR 50
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7332
EP 7342
DI 10.1109/TVCG.2024.3456140
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300021
PM 39255100
DA 2024-11-06
ER

PT J
AU Jiang, WW
   Windl, M
   Tag, B
   Sarsenbayeva, Z
   Mayer, S
AF Jiang, Weiwei
   Windl, Maximiliane
   Tag, Benjamin
   Sarsenbayeva, Zhanna
   Mayer, Sven
TI An Immersive and Interactive VR Dataset to Elicit Emotions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual Reality; Emotions; Dataset
   (https//github.com/HighTemplar-wjiang/VR-Dataset-Emotions); Virtual
   Reality; Emotions; Dataset
   (https//github.com/HighTemplar-wjiang/VR-Dataset-Emotions)
ID VIRTUAL-REALITY; INDIVIDUAL-DIFFERENCES; NEGATIVE AFFECT; MEMORY; PANAS
AB Images and videos are widely used to elicit emotions; however, their visual appeal differs from real-world experiences. With virtual reality becoming more realistic, immersive, and interactive, we envision virtual environments to elicit emotions effectively, rapidly, and with high ecological validity. This work presents the first interactive virtual reality dataset to elicit emotions. We created five interactive virtual environments based on corresponding validated 360 degrees videos and validated their effectiveness with 160 participants. Our results show that our virtual environments successfully elicit targeted emotions. Compared with the existing methods using images or videos, our dataset allows virtual reality researchers and practitioners to integrate their designs effectively with emotion elicitation settings in an immersive and interactive way.
C1 [Jiang, Weiwei] Nanjing Univ Informat Sci Technol, Nanjing, Peoples R China.
   [Windl, Maximiliane; Mayer, Sven] Ludwig Maximilians Univ Munchen, Munich, Germany.
   [Windl, Maximiliane] Munich Ctr Machine Learning MCML, Munich, Germany.
   [Tag, Benjamin] Monash Univ, Melbourne, Australia.
   [Sarsenbayeva, Zhanna] Univ Sydney, Sydney, Australia.
C3 Nanjing University of Information Science & Technology; University of
   Munich; Monash University; University of Sydney
RP Jiang, WW (corresponding author), Nanjing Univ Informat Sci Technol, Nanjing, Peoples R China.
EM weiwei.jiang@nuist.edu.cn; maximiliane.windl@ifi.lmu.de;
   benjamin.tag@monash.edu; zhanna.sarsenbayeva@sydney.edu.au;
   info@sven-mayer.com
RI Windl, Maximiliane/KRP-2843-2024; Mayer, Sven/A-5174-2019; Tag,
   Benjamin/S-6178-2019
OI Mayer, Sven/0000-0001-5462-8782; Sarsenbayeva,
   Zhanna/0000-0002-1247-6036; Windl, Maximiliane/0000-0002-9743-3819; Tag,
   Benjamin/0000-0002-7831-2632; Jiang, Weiwei/0000-0003-4413-2497
FU NSFC [62072004]; Startup Foundation for Introducing Talent of NUIST;
   Federal Ministry of Education and Research (BMBF); Australia-Germany
   Joint Research Cooperation Scheme (DAAD) [57600233]
FX This work was supported in part by the NSFC under grants No. 62072004,
   the Startup Foundation for Introducing Talent of NUIST, and funded by
   the Federal Ministry of Education and Research (BMBF) and the
   Australia-Germany Joint Research Cooperation Scheme (DAAD grant number:
   57600233).
CR ADAIR JG, 1984, J APPL PSYCHOL, V69, P334, DOI 10.1037/0021-9010.69.2.334
   Annamary K, 2016, J CLIN DIAGN RES, V10, pZC26, DOI 10.7860/JCDR/2016/18506.8128
   Babaei E, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445370
   Bailenson JN, 2007, HUM-COMPUT INTERACT, V22, P325
   Bartram L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1364, DOI 10.1145/3025453.3026041
   Betella A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148037
   Bisby JA, 2017, CURR OPIN BEHAV SCI, V17, P124, DOI 10.1016/j.cobeha.2017.07.012
   Bonanno GA, 2013, PERSPECT PSYCHOL SCI, V8, P591, DOI 10.1177/1745691613504116
   Bradley MM, 2000, PSYCHOPHYSIOLOGY, V37, P204, DOI 10.1017/S0048577200990012
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Butler EA, 2007, EMOTION, V7, P30, DOI 10.1037/1528-3542.7.1.30
   Chander H, 2021, WORKPLACE HEALTH SAF, V69, P32, DOI 10.1177/2165079920934000
   Chanel G, 2009, INT J HUM-COMPUT ST, V67, P607, DOI 10.1016/j.ijhcs.2009.03.005
   Chirico A, 2018, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02351
   Chirico A, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-01242-0
   Coan JA, 2007, Handbook of emotion elicitation and assessment
   Crawford JR, 2004, BRIT J CLIN PSYCHOL, V43, P245, DOI 10.1348/0144665031752934
   Dozio N, 2022, INT J HUM-COMPUT ST, V162, DOI 10.1016/j.ijhcs.2022.102791
   Dozio N, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-03380-y
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   FakhrHosseini M, 2015, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS (AUTOMOTIVE UI '15), P38, DOI 10.1145/2809730.2809751
   Fakhrhosseini SM, 2017, EMOTIONS AND AFFECT IN HUMAN FACTORS AND HUMAN-COMPUTER INTERACTION, P235, DOI 10.1016/B978-0-12-801851-4.00010-0
   Felnhofer A, 2015, INT J HUM-COMPUT ST, V82, P48, DOI 10.1016/j.ijhcs.2015.05.004
   Goto T, 2018, ACM INT CONF PR SER, DOI 10.1145/3174910.3174939
   Gross JJ, 2003, J PERS SOC PSYCHOL, V85, P348, DOI 10.1037/0022-3514.85.2.348
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   Gupta K., 2024, IEEE Transactions on Visualization and Computer Graphics, P1
   Halvey Martin, 2012, Haptic and Audio Interaction Design. Proceedings 7th International Conference, HAID 2012, P91, DOI 10.1007/978-3-642-32796-4_10
   Hassib M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P6133, DOI 10.1145/3025453.3025953
   Healey J., 2014, The Oxford Handbook of A ective Computing, P204
   Howell N, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174005
   Jun J, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174175
   Juslin PN, 2008, BEHAV BRAIN SCI, V31, P559, DOI 10.1017/S0140525X08005293
   Kemeny ME, 2012, EMOTION, V12, P338, DOI 10.1037/a0026118
   Kern Angelika C., 2020, AM '20: Proceedings of the 15th International Conference on Audio Mostly, P233, DOI 10.1145/3411109.3411129
   Kim J, 2022, VIRTUAL REAL-LONDON, V26, P425, DOI 10.1007/s10055-021-00570-x
   Koval P, 2023, EMOTION, V23, P357, DOI 10.1037/emo0001097
   LANG PJ, 1995, AM PSYCHOL, V50, P372, DOI 10.1037/0003-066X.50.5.372
   LeDoux J, 2012, NEURON, V73, P653, DOI 10.1016/j.neuron.2012.02.004
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Li M, 2022, IEEE T VIS COMPUT GR, V28, P3832, DOI 10.1109/TVCG.2022.3203099
   Lieberoth A, 2021, ROY SOC OPEN SCI, V8, DOI 10.1098/rsos.200589
   Ma Y, 2023, LECT NOTES COMPUT SC, V14144, P511, DOI 10.1007/978-3-031-42286-7_29
   MacArthur C., 2021, P 2021 CHI C HUM FAC
   Marcolin F, 2021, IEEE COMPUT GRAPH, V41, P171, DOI 10.1109/MCG.2021.3115015
   Mavridou I, 2023, PLOS ONE, V18, DOI 10.1371/journal.pone.0278065
   Morina N, 2015, BEHAV RES THER, V74, P18, DOI 10.1016/j.brat.2015.08.010
   Park Y.-W., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P1679, DOI DOI 10.1145/2470654.2466222
   Pollak JP, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P725
   Riva G, 2012, CYBERPSYCH BEH SOC N, V15, P69, DOI 10.1089/cyber.2011.0139
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Sarsenbayeva Z., 2021, P 32 AUSTR C HUM COM, P755
   Sarsenbayeva Z, 2023, LECT NOTES COMPUT SC, V14143, P385, DOI 10.1007/978-3-031-42283-6_22
   Sassatelli L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2208, DOI 10.1145/3343031.3350601
   Sato W, 2007, SOC BEHAV PERSONAL, V35, P863, DOI 10.2224/sbp.2007.35.7.863
   Schmidt SR, 2016, MEMORY, V24, P916, DOI 10.1080/09658211.2015.1059859
   Schöne B, 2023, CURR PSYCHOL, V42, P5366, DOI 10.1007/s12144-021-01841-1
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Shi YX, 2023, INT J HUM-COMPUT ST, V170, DOI 10.1016/j.ijhcs.2022.102958
   Siedlecka E, 2019, EMOT REV, V11, P87, DOI 10.1177/1754073917749016
   Smith W., 2022, P 2022 CHI C HUM FAC, P2
   Somarathna R, 2023, IEEE T AFFECT COMPUT, V14, P2626, DOI 10.1109/TAFFC.2022.3181053
   Spering M, 2005, COGNITION EMOTION, V19, P1252, DOI 10.1080/02699930500304886
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Tag B, 2022, COMPUT HUM BEHAV REP, V6, DOI 10.1016/j.chbr.2022.100192
   Tag B, 2022, INT J HUM-COMPUT ST, V166, DOI 10.1016/j.ijhcs.2022.102872
   Tag B, 2022, IEEE PERVAS COMPUT, V21, P28, DOI 10.1109/MPRV.2021.3106272
   TERWOGT MM, 1995, J GEN PSYCHOL, V122, P5, DOI 10.1080/00221309.1995.9921217
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Watson D, 1999, J PERS SOC PSYCHOL, V76, P820, DOI 10.1037/0022-3514.76.5.820
   WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269
   Wilson G, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P4838, DOI 10.1145/2858036.2858205
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Yang K., 2022, IEEE Transactions on Affective Computing
   Yang KN, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P562, DOI 10.1145/3512527.3531385
   Yang KN, 2023, IEEE T AFFECT COMPUT, V14, P1082, DOI 10.1109/TAFFC.2021.3100868
NR 76
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7343
EP 7353
DI 10.1109/TVCG.2024.3456191
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300031
PM 39255143
DA 2024-11-06
ER

PT J
AU Pan, XK
   Huang, G
   Zhang, ZY
   Li, JY
   Bao, HJ
   Zhang, GF
AF Pan, Xiaokun
   Huang, Gan
   Zhang, Ziyang
   Li, Jinyu
   Bao, Hujun
   Zhang, Guofeng
TI Robust Collaborative Visual-Inertial SLAM for Mobile Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaboration; Simultaneous localization and mapping; Real-time systems;
   Servers; Accuracy; Location awareness; Mobile handsets; SLAM; VIO;
   Collaborative; Tightly coupled; Map fusion
ID LOCALIZATION; FILTER
AB Achieving precise real-time localization and ensuring robustness are critical challenges in multi-user mobile AR applications. Leveraging collaborative information to augment tracking accuracy on lightweight devices and fortify overall system robustness emerges as a crucial necessity. In this paper, we propose a robust centralized collaborative rnulti-agent VI-SLAM system for mobile AR interaction and server-side efficient consistent mapping. The system deploys a lightweight VIO frontend on mobile devices for real-time tracking, and a backend running on a remote server to update multiple submaps. When overlapping areas between submaps across agents are detected, the system performs submap fusion to establish a globally consistent map. Additionally, we propose a map registration and fusion strategy based on covisibility areas for online registration and fusion in multi-agent scenarios. To improve the tracking accuracy of the frontend on agent, we introduce a strategy for updating the global map to the local map at a moderate frequency between the camera-rate pose estimation of the frontend VIO and the low-frequency global map optimization, using a tightly coupled strategy to achieve consistency of the multi-agent frontend poses estimation in the global map. The effectiveness of the proposed method is further confirmed by executing backend mapping on the server and deploying VIO frontends on multiple mobile devices for AR demostration. Additionally, we discuss the scalability of the proposed system by analyzing network traffic, synchronization frequency, and other factors at both the agent and server ends.
C1 [Pan, Xiaokun; Huang, Gan; Zhang, Ziyang; Bao, Hujun; Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Li, Jinyu] Dreame Technol, Suzhou, Peoples R China.
C3 Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM xkpan@zju.edu.cn; huanggan@zju.edu.cn; zhangzion@zju.edu.cn;
   mail@jinyu.li; baohujun@zju.edu.cn; zhangguofeng@zju.edu.cn
RI zhang, Guofeng/H-4991-2011; Zhang, Ziyang/AAF-4011-2019; ,
   panxkun/JCE-7904-2023
OI Li, Jinyu/0000-0002-5206-8600; Bao, Hujun/0000-0002-2662-0334; Huang,
   Gan/0000-0001-8515-2721; Zhang, Guofeng/0000-0001-5661-8430; Zhang,
   Ziyang/0009-0004-4169-2282; Pan, Xiaokun/0000-0002-7438-1665
FU NSF of China [61932003]
FX This work was partially supported by NSF of China (No. 61932003). The
   authors would like to thank Tianxing Fan for his kind help in demo
   recording and advice on paper formatting.
CR Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bao HJ, 2022, IEEE T VIS COMPUT GR, V28, P2212, DOI 10.1109/TVCG.2022.3150495
   Bloesch M, 2017, INT J ROBOT RES, V36, P1053, DOI 10.1177/0278364917728574
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Chang Y, 2021, IEEE INT CONF ROBOT, P11210, DOI 10.1109/ICRA48506.2021.9561090
   Cioffi G, 2020, IEEE INT C INT ROBOT, P5089, DOI 10.1109/IROS45743.2020.9341697
   Civera J, 2008, IEEE T ROBOT, V24, P932, DOI 10.1109/TRO.2008.2003276
   Dong B, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22093391
   Elvira R, 2019, IEEE INT C INT ROBOT, P6253, DOI [10.1109/IROS40897.2019.8967572, 10.1109/iros40897.2019.8967572]
   Gálvez-López D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158
   Geneva P, 2020, IEEE INT CONF ROBOT, P4666, DOI [10.1109/ICRA40945.2020.9196524, 10.1109/icra40945.2020.9196524]
   Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266
   Huai Z, 2018, IEEE INT C INT ROBOT, P6319, DOI 10.1109/IROS.2018.8593643
   Huang HY, 2020, Arxiv, DOI arXiv:2011.04173
   Huang HY, 2020, IEEE ROBOT AUTOM LET, V5, P5043, DOI 10.1109/LRA.2020.3005130
   Karrer M, 2018, IEEE ROBOT AUTOM LET, V3, P2762, DOI 10.1109/LRA.2018.2837226
   Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813
   Li J., 2024, IEEE Transactions on Visualization and Computer Graphics, P2
   Li PL, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P11, DOI 10.1109/ISMAR.2017.18
   Liu XY, 2021, IEEE INT C INT ROBOT, P8722, DOI 10.1109/IROS51168.2021.9636645
   Lucas B., 1981, P 7 INT JOINT C ART, V81, P1
   Lynen S, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   M. Grupp. evo, 2017, Python package for the evaluation of odometry and slam
   Matsuki H, 2024, Arxiv, DOI arXiv:2312.06741
   Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024
   Ouyang M, 2021, IEEE INT C INT ROBOT, P8679, DOI 10.1109/IROS51168.2021.9636798
   Patel M, 2023, Arxiv, DOI arXiv:2301.07147
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Riazuelo L, 2014, ROBOT AUTON SYST, V62, P401, DOI 10.1016/j.robot.2013.11.007
   Schmuck P, 2021, INT SYM MIX AUGMENT, P171, DOI 10.1109/ISMAR-Adjunct54149.2021.00043
   Schmuck P, 2019, J FIELD ROBOT, V36, P763, DOI 10.1002/rob.21854
   Sibley G, 2010, J FIELD ROBOT, V27, P587, DOI 10.1002/rob.20360
   Tang Y., 2023, ACM Transactions on Graphics (TOG), V42, P1
   Thien-Minh N, 2021, Arxiv, DOI arXiv:2105.03296
   Wang HY, 2023, PROC CVPR IEEE, P13293, DOI 10.1109/CVPR52729.2023.01277
   X. Contributors, 2022, Openxrlab visual-inertial SLAM toolbox and benchmark
   Xu H, 2024, Arxiv, DOI arXiv:2211.01538
   Xu H, 2022, IEEE T INTELL TRANSP, V23, P19760, DOI 10.1109/TITS.2021.3137253
   Yang XR, 2022, INT SYM MIX AUGMENT, P499, DOI 10.1109/ISMAR55827.2022.00066
   Yousif K., 2015, INTELL IND SYST, V1, P289, DOI [10.1007/s40903-015-0032-7, DOI 10.1007/S40903-015-0032-7]
   Zhu PX, 2021, IEEE INT CONF ROBOT, P13135, DOI 10.1109/ICRA48506.2021.9561674
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
   Zou DP, 2013, IEEE T PATTERN ANAL, V35, P354, DOI 10.1109/TPAMI.2012.104
   Zuo XX, 2019, IEEE ROBOT AUTOM LET, V4, P3394, DOI 10.1109/LRA.2019.2927123
NR 45
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7354
EP 7363
DI 10.1109/TVCG.2024.3456207
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300012
PM 39250393
DA 2024-11-06
ER

PT J
AU Peng, X
   Zhang, YX
   Jiménez-Navarro, D
   Serrano, A
   Myszkowski, K
   Sun, Q
AF Peng, Xi
   Zhang, Yunxiang
   Jimenez-Navarro, Daniel
   Serrano, Ana
   Myszkowski, Karol
   Sun, Qi
TI Measuring and Predicting Multisensory Reaction Latency: A Probabilistic
   Model for Visual-Auditory Integration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Computational modeling; Probabilistic logic; Predictive
   models; Headphones; Data models; Statistical analysis; Virtual reality;
   augmented reality; human perception; visual-audio; reaction latency
ID SACCADIC REACTION-TIME; DRIFT-DIFFUSION MODEL; EYE-MOVEMENTS;
   INFORMATION; ACCURACY; SIGNALS; ACCOUNT; TARGET; WINDOW; SPEED
AB Virtual/augmented reality (VR/AR) devices offer both immersive imagery and sound. With those wide-field cues, we can simultaneously acquire and process visual and auditory signals to quickly identify objects, make decisions, and take action. While vision often takes precedence in perception, our visual sensitivity degrades in the periphery. In contrast, auditory sensitivity can exhibit an opposite trend due to the elevated interaural time difference. What occurs when these senses are simultaneously integrated, as is common in VR applications such as 360 degrees video watching and immersive gaming? We present a computational and probabilistic model to predict VR users' reaction latency to visual-auditory multisensory targets. To this aim, we first conducted a psychophysical experiment in VR to measure the reaction latency by tracking the onset of eye movements. Experiments with numerical metrics and user studies with naturalistic scenarios showcase the model's accuracy and generalizability. Lastly, we discuss the potential applications, such as measuring the sufficiency of target appearance duration in immersive video playback, and suggesting the optimal spatial layouts for AR interface design.
C1 [Peng, Xi; Zhang, Yunxiang; Sun, Qi] NYU, New York, NY 10012 USA.
   [Jimenez-Navarro, Daniel; Myszkowski, Karol] Max Planck Inst Informat, Saarbrucken, Germany.
   [Serrano, Ana] Univ Zaragoza, Zaragoza, Spain.
C3 New York University; Max Planck Society; University of Zaragoza
RP Peng, X (corresponding author), NYU, New York, NY 10012 USA.
EM xp2011@nyu.edu; yunxiang.zhang@nyu.edu; djimenez@mpi-inf.mpg.de;
   anase@unizar.es; karol@mpi-inf.mpg.de; qisun@nyu.edu
RI Sun, Qi/AGY-5791-2022; Zhang, Yunxiang/KTI-6624-2024; Jiménez Navarro,
   Daniel/LGY-1607-2024; Serrano Pacheu, Ana Belen/ABC-3358-2021
OI Serrano Pacheu, Ana Belen/0000-0002-7796-3177
FU MICIU/AEI [PID2022-141539NB-I00]; ERDF, EU; National Science Foundation
   (NSF) [2225861, 2232817]
FX This work has been partially supported by grant PID2022-141539NB-I00,
   funded by MICIU/AEI/10.13039/501100011033 and by ERDF, EU, an academic
   gift from Meta, and the National Science Foundation (NSF) grants
   #2225861 and #2232817.
CR Altosaar R, 2019, TEI'19: PROCEEDINGS OF THE THIRTEENTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION, P553, DOI 10.1145/3294109.3301256
   Arabadzhiyska E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073642
   Arnold DH, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-37888-7
   Battaglia PW, 2003, J OPT SOC AM A, V20, P1391, DOI 10.1364/JOSAA.20.001391
   Beierholm UR, 2009, J VISION, V9, DOI 10.1167/9.5.23
   Bitzer S, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00102
   Bobu A, 2020, ACMIEEE INT CONF HUM, P429, DOI 10.1145/3319502.3374811
   Burr D, 2006, PROG BRAIN RES, V155, P243, DOI 10.1016/S0079-6123(06)55014-9
   Chao Fang-Yi, 2020, IEEE INT CONF MULTI, P1, DOI [DOI 10.1109/icmew46912.2020.9105956, 10.1109/ICMEW46912.2020.9105956]
   Charbonneau G, 2013, J VISION, V13, DOI 10.1167/13.12.20
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen SY, 2022, IEEE T VIS COMPUT GR, V28, P2157, DOI 10.1109/TVCG.2022.3150522
   Colonius H, 2004, J COGNITIVE NEUROSCI, V16, P1000, DOI 10.1162/0898929041502733
   Colonius H, 2010, FRONT INTEGR NEUROSC, V4, DOI 10.3389/fnint.2010.00011
   Drugowitsch J, 2014, ELIFE, V3, DOI 10.7554/eLife.03005
   Duinkharjav B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3618334
   Duinkharjav B, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530055
   Engbert R, 2006, P NATL ACAD SCI USA, V103, P7192, DOI 10.1073/pnas.0509557103
   Engelken E. J., 1991, Technical report, P9
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Farrell J., 2021, The Journal of Science and Medicine
   Fetsch CR, 2012, NAT NEUROSCI, V15, P146, DOI 10.1038/nn.2983
   FOLKS JL, 1978, J ROY STAT SOC B MET, V40, P263
   Fudenberg D, 2020, P NATL ACAD SCI USA, V117, P33141, DOI 10.1073/pnas.2011446117
   Fujiwara K, 2006, INT J SPORTS MED, V27, P792, DOI 10.1055/s-2005-872959
   Gabriel DN, 2010, HEARING RES, V262, P19, DOI 10.1016/j.heares.2010.01.016
   Gomez P, 2007, J EXP PSYCHOL GEN, V136, P389, DOI 10.1037/0096-3445.136.3.389
   GREGG LW, 1950, J COMP PHYSIOL PSYCH, V43, P389, DOI 10.1037/h0054211
   Gruen R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P791, DOI [10.1109/VR46266.2020.1580498468656, 10.1109/VR46266.2020.000-1]
   HENRY FM, 1961, RES QUART, V32, P353, DOI 10.1080/10671188.1961.10613157
   Hespanhol L., 2013, P 25 AUSTR COMP HUM, P569
   Hidaka S, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0008188
   Hirway A, 2022, PROCEEDINGS OF THE 13TH ACM MULTIMEDIA SYSTEMS CONFERENCE, MMSYS 2022, P39, DOI 10.1145/3524273.3528179
   Hsiao L, 2022, ACM SIGCOMM COMP COM, V52, P11, DOI 10.1145/3523230.3523233
   Jain Aditya, 2015, Int J Appl Basic Med Res, V5, P124, DOI 10.4103/2229-516X.157168
   KANTOROVICH LV, 1960, MANAGE SCI, V6, P366, DOI 10.1287/mnsc.6.4.366
   Kaplanyan AS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356557
   Kasahara S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300873
   Kosinski R.J., 2008, A Literature Review on Reaction Time, V10, P337
   Krajancich B, 2023, Arxiv, DOI arXiv:2302.01368
   Krajancich B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459784
   Krajancich B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417820
   Krajbich I, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00193
   Krajbich I, 2011, P NATL ACAD SCI USA, V108, P13852, DOI 10.1073/pnas.1101328108
   Lisi M, 2019, P NATL ACAD SCI USA, V116, P16137, DOI 10.1073/pnas.1901963116
   Ma WJ, 2008, BRAIN RES, V1242, P4, DOI 10.1016/j.brainres.2008.04.082
   Malpica S, 2022, PROCEEDINGS OF SIGGRAPH 2022 POSTERS, SIGGRAPH 2022, DOI 10.1145/3532719.3543220
   Mantiuk RK, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530115
   Mantiuk RK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459831
   Martin D, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3508361
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Milosavljevic M, 2010, JUDGM DECIS MAK, V5, P437
   Murakami EAY, 2010, MEASUREMENT, V43, P675, DOI 10.1016/j.measurement.2010.01.006
   Newsweek, 2023, Teen killed by train while wearing noise-canceling headphones
   Nie BB, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-82331-z
   Nosek BA, 2001, SOC COGNITION, V19, P625, DOI 10.1521/soco.19.6.625.20886
   Ohshiro T, 2011, NAT NEUROSCI, V14, P775, DOI 10.1038/nn.2815
   Ophir E, 2009, P NATL ACAD SCI USA, V106, P15583, DOI 10.1073/pnas.0903620106
   Padmanaban N, 2018, IEEE T VIS COMPUT GR, V24, P1594, DOI 10.1109/TVCG.2018.2793560
   Palmer EM, 2011, J EXP PSYCHOL HUMAN, V37, P58, DOI 10.1037/a0020747
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   POSNER MI, 1976, PSYCHOL REV, V83, P157, DOI 10.1037/0033-295X.83.2.157
   Ratcliff R, 2008, NEURAL COMPUT, V20, P873, DOI 10.1162/neco.2008.12-06-420
   Reddi BAJ, 2003, J NEUROPHYSIOL, V90, P3538, DOI 10.1152/jn.00689.2002
   RITTER W, 1979, SCIENCE, V203, P1358, DOI 10.1126/science.424760
   Roberts JA, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00797
   Schmiedek F, 2007, J EXP PSYCHOL GEN, V136, P414, DOI 10.1037/0096-3445.136.3.414
   Seideman JA, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05319-w
   Shadlen MN, 2013, NEURON, V80, P791, DOI 10.1016/j.neuron.2013.10.047
   Shahar N, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006803
   Shelton J, 2010, Neurosci Med, V1, P30, DOI DOI 10.4236/NM.2010.11004
   Shinn M, 2020, ELIFE, V9, DOI 10.7554/eLife.56938
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Song K, 2018, WEST J EMERG MED, V19, P101, DOI 10.5811/westjem.2017.10.36027
   Spjut J, 2022, Arxiv, DOI arXiv:2208.11774
   Summala H., 2000, TRANSPORTATION HUMAN, V2, P217, DOI DOI 10.1207/STHF0203_2
   Sun Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130807
   Tabry V, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00932
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Thaler L, 2013, VISION RES, V76, P31, DOI 10.1016/j.visres.2012.10.012
   Turner BM, 2017, PSYCHON B REV, V24, P1819, DOI 10.3758/s13423-017-1255-2
   Tursun C, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3564241
   Ursino M, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00089
   van Beers RJ, 2007, J NEUROSCI, V27, P8757, DOI 10.1523/JNEUROSCI.2311-07.2007
   Van der Burg E, 2008, J EXP PSYCHOL HUMAN, V34, P1053, DOI 10.1037/0096-1523.34.5.1053
   Walton DR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459943
   Wierzbicki J, 2013, ACTA PHYS POL A, V123, P1114, DOI 10.12693/APhysPolA.123.1114
   Wu YB, 2021, TRANSPORT RES F-TRAF, V81, P355, DOI 10.1016/j.trf.2021.06.017
   Yao LJ, 1997, EXP BRAIN RES, V115, P25, DOI 10.1007/PL00005682
NR 89
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7364
EP 7374
DI 10.1109/TVCG.2024.3456202
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300030
PM 39250397
DA 2024-11-06
ER

PT J
AU Hu, ZM
   Yin, ZM
   Haeufle, D
   Schmitt, S
   Bulling, A
AF Hu, Zhiming
   Yin, Zheming
   Haeufle, Daniel
   Schmitt, Syn
   Bulling, Andreas
TI HOIMotion: Forecasting Human Motion During Human-Object Interactions
   Using Egocentric 3D Object Bounding Boxes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Forecasting; Feature extraction; Three-dimensional displays; Solid
   modeling; Brain modeling; Predictive models; Augmented reality; Human
   motion forecasting; human-object interaction; graph convolutional
   network; augmented reality
AB We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.
C1 [Hu, Zhiming; Yin, Zheming; Schmitt, Syn] Univ Stuttgart, Stuttgart, Germany.
   [Haeufle, Daniel] Heidelberg Univ, Heidelberg, Germany.
   [Haeufle, Daniel] Univ Tubingen, Tubingen, Germany.
   [Haeufle, Daniel] Ctr Bion Intelligence Tuebingen Stuttgart BITS, Stuttgart, Germany.
C3 University of Stuttgart; Ruprecht Karls University Heidelberg; Eberhard
   Karls University of Tubingen
RP Hu, ZM (corresponding author), Univ Stuttgart, Stuttgart, Germany.
EM zhiming.hu@vis.uni-stuttgart.de; daniel.haeufle@ziti.uni-heidelberg.de;
   st178328@stud.uni-stuttgart.de; schmitt@simtech.uni-stuttgart.de;
   andreas.bulling@vis.uni-stuttgart.de
OI Hu, Zhiming/0000-0002-5105-9753
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy [EXC 2075 - 390740016]
FX This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation) under Germany's Excellence Strategy - EXC 2075 -
   390740016.
CR Adebayo S, 2022, IFAC PAPERSONLINE, V55, P174, DOI 10.1016/j.ifacol.2022.07.627
   Aksan E, 2021, INT CONF 3D VISION, P565, DOI 10.1109/3DV53792.2021.00066
   Bektas K, 2023, ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, ETRA 2023, DOI 10.1145/3588015.3588402
   Butaslac I, 2020, LECT NOTES COMPUT SC, V12243, P165, DOI 10.1007/978-3-030-58468-9_13
   Cho YH, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P527, DOI 10.1109/VR.2018.8446442
   Crivellaro A, 2018, IEEE T PATTERN ANAL, V40, P1465, DOI 10.1109/TPAMI.2017.2708711
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   David-John B., 2021, P 2021 ACM S EYE TRA, P1
   Dell'Agnola Fabio, 2020, Virtual, Augmented and Mixed Reality. Design and Interaction. 12th International Conference, VAMR 2020 Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12190), P397, DOI 10.1007/978-3-030-49695-1_26
   Emery K. J., 2021, P 2021 ACM S EYE TRA, P1
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Guo W, 2023, IEEE WINT CONF APPL, P4798, DOI 10.1109/WACV56688.2023.00479
   Hadnett-Hunter J, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3352763
   Hale A., Holoyolo: Understand the real world by running object detection on the hololens 2 and projecting the detection results in space
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   Hu Z., 2024, IEEE Transactions on Visualization and Computer Graphics, P2
   Hu Z., 2022, IEEE Transac- tions on Visualization and Computer Graphics, V2, P3
   Hu ZM, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P543, DOI [10.1109/VRW50115.2020.0-153, 10.1109/VRW50115.2020.00123]
   Hu ZM, 2021, IEEE T VIS COMPUT GR, V27, P2681, DOI 10.1109/TVCG.2021.3067779
   Hu ZM, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P707, DOI 10.1109/VRW52623.2021.00236
   Hu ZM, 2020, IEEE T VIS COMPUT GR, V26, P1902, DOI 10.1109/TVCG.2020.2973473
   Hu ZM, 2019, IEEE T VIS COMPUT GR, V25, P2002, DOI 10.1109/TVCG.2019.2899187
   Jiao C., 2023, P 2023 ACM S US INT, P1
   Kim J, 2019, IEEE I CONF COMP VIS, P10579, DOI 10.1109/ICCV.2019.01068
   Koulieris GA, 2016, P IEEE VIRT REAL ANN, P113, DOI 10.1109/VR.2016.7504694
   Kratzer P, 2021, IEEE ROBOT AUTOM LET, V6, P367, DOI 10.1109/LRA.2020.3043167
   Le AT, 2021, IEEE ROMAN, P7, DOI 10.1109/RO-MAN50785.2021.9515539
   Lehrmann AM, 2014, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2014.171
   Li C.-L., 2018, Scientific Reports, V8, P2
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Mao W., 2022, Advances in Neural Information Processing Systems, V35, P2
   Mao W, 2021, INT J COMPUT VISION, V129, P2513, DOI 10.1007/s11263-021-01483-7
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Ohkawa T, 2023, PROC CVPR IEEE, P12999, DOI 10.1109/CVPR52729.2023.01249
   Pan X., 2023, P 2023 IEEE INT C CO, P20133
   Park Y, 2008, INT SYM MIX AUGMENT, P117, DOI 10.1109/ISMAR.2008.4637336
   Razali H, 2022, IEEE INT CONF ROBOT, P8497, DOI 10.1109/ICRA46639.2022.9812079
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Taylor G. W., 2006, Advances in Neural Information Processing Systems, V19, P2
   Tremmel C, 2019, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00401
   Valkov D., 2023, 2023 CHI C HUM FACT, P1
   Wang J., 2005, Advances in Neural Information Processing Systems, V18, P2
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Zheng Y, 2022, LECT NOTES COMPUT SC, V13673, P676, DOI 10.1007/978-3-031-19778-9_39
NR 46
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7375
EP 7385
DI 10.1109/TVCG.2024.3456152
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300038
PM 39255111
DA 2024-11-06
ER

PT J
AU Fielder, ML
   Wolf, E
   Dollinger, N
   Mal, D
   Botsch, M
   Latoschik, ME
   Wienrich, C
AF Fielder, Marie Luisa
   Wolf, Erik
   Dollinger, Nina
   Mal, David
   Botsch, Mario
   Latoschik, Marc Erich
   Wienrich, Carolin
TI From Avatars to Agents: Self-Related Cues Through Embodiment and
   Personalization Affect Body Perception in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual human; virtual body ownership; agency; self-location; body
   image; body weight perception; Virtual human; virtual body ownership;
   agency; self-location; body image; body weight perception
ID LONGITUDINAL DATA; OWNERSHIP; PACKAGE; SCALE; SENSE; BMI
AB Our work investigates the influence of self-related cues in the design of virtual humans on body perception in virtual reality. In a $2\times 2$ mixed design, 64 participants faced photorealistic virtual humans either as a motion-synchronized embodied avatar or as an autonomous moving agent, appearing subsequently with a personalized and generic texture. Our results unveil that self-related cues through embodiment and personalization yield an individual and complemented increase in participants' sense of embodiment and self-identification towards the virtual human. Different body weight modification and estimation tasks further showed an impact of both factors on participants' body weight perception. Additional analyses revealed that the participant's body mass index predicted body weight estimations in all conditions and that participants' self-esteem and body shape concerns correlated with different body weight perception results. Hence, we have demonstrated the occurrence of double standards through induced self-related cues in virtual human perception, especially through embodiment.
C1 [Fielder, Marie Luisa; Dollinger, Nina; Wienrich, Carolin] Univ Wurzburg, Psychol Intelligent Interact Syst PIIS, Wurzburg, Germany.
   [Wolf, Erik; Mal, David; Latoschik, Marc Erich] Univ Wurzburg, Human Comp Interact HCI Grp, Wurzburg, Germany.
   [Wolf, Erik] Univ Hamburg, Human Comp Interact HCI, Hamburg, Germany.
   [Botsch, Mario] TU Dortmund Univ, Comp Graph Grp, Dortmund, Germany.
C3 University of Wurzburg; University of Wurzburg; University of Hamburg;
   Dortmund University of Technology
RP Fielder, ML (corresponding author), Univ Wurzburg, Psychol Intelligent Interact Syst PIIS, Wurzburg, Germany.
EM marie.fiedler@uni-wuerzburg.de; erik.wolf@uni-wuerzburg.de;
   nina.doellinger@uni-wuerzburg.de; david.mal@uni-wuerzburg.de;
   mario.botsch@tu-dortmund.de; marc.latoschik@uni-wuerzburg.de;
   carolin.wienrich@uni-wuerzburg.de
OI Fiedler, Marie Luisa/0000-0002-3472-3798; Mal,
   David/0000-0002-0254-7275; Wolf, Erik/0000-0003-4881-9982; Wienrich,
   Carolin/0000-0003-3052-7172; Dollinger, Nina/0000-0002-0609-8841
FU German Federal Ministry of Education and Research in the project ViTraS
   [16SV8219, 16SV8225]; German Federal Ministry of Labor and Social
   Affairs in the project AIL AT WORK [DKI.00.00030.21]; Bavarian State
   Ministry For Digital Affairs in the project XR Hub [A5-3822-2-16]
FX This research has been funded by the German Federal Ministry of
   Education and Research in the project ViTraS (16SV8219 and 16SV8225), by
   the German Federal Ministry of Labor and Social Affairs in the project
   AIL AT WORK (DKI.00.00030.21), and by the Bavarian State Ministry For
   Digital Affairs in the project XR Hub (A5-3822-2-16). Erik Wolf
   gratefully acknowledges a Meta Research PhD Fellowship.
CR Achenbach J, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3139154
   Aristidou A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13310
   Bartl A, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.694617
   Bimberg P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P464, DOI [10.1109/VRW50115.2020.0-178, 10.1109/VRW50115.2020.00098]
   Bouchard S, 2004, 3RD IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2004, P59, DOI 10.1109/HAVE.2004.1391882
   Braun K., 2006, Qualitative. Psychol. Res., V3, P77, DOI DOI 10.1191/1478088706QP063OA
   Brunner E, 2002, Nonparametric analysis of longitudinal data in Factorial Experiments
   Cash TF, 1997, INT J EAT DISORDER, V22, P107, DOI 10.1002/(SICI)1098-108X(199709)22:2<107::AID-EAT1>3.0.CO;2-J
   Cornelissen KK, 2016, BRIT J HEALTH PSYCH, V21, P555, DOI 10.1111/bjhp.12185
   Cornelissen KK, 2015, BODY IMAGE, V13, P75, DOI 10.1016/j.bodyim.2015.01.001
   Cornelissen PL, 2018, BODY IMAGE, V24, P116, DOI 10.1016/j.bodyim.2017.12.007
   De Coster L, 2021, FRONT HUM NEUROSCI, V15, DOI 10.3389/fnhum.2021.701872
   De Coster L, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-93865-7
   de Vignemont F, 2011, CONSCIOUS COGN, V20, P82, DOI 10.1016/j.concog.2010.09.004
   Do TD, 2024, IEEE T VIS COMPUT GR, V30, P2434, DOI 10.1109/TVCG.2024.3372067
   Döllinger N, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2023, DOI 10.1145/3544548.3580918
   Döllinger N, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.935449
   Dollinger N., 2019, P MUC, P1, DOI DOI 10.18420/MUC2019-WS-633
   Eubanks JC, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.647896
   Ferring D, 1996, DIAGNOSTICA, V42, P284
   Fiedler ML, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P799, DOI 10.1109/VRW58643.2023.00242
   Fildes A, 2015, AM J PUBLIC HEALTH, V105, pE54, DOI 10.2105/AJPH.2015.302773
   Foschi M, 1996, SOC PSYCHOL QUART, V59, P237, DOI 10.2307/2787021
   Friedrich S, 2019, R J, V11, P380
   Friedrich S, 2018, J MULTIVARIATE ANAL, V165, P166, DOI 10.1016/j.jmva.2017.12.008
   Friedrich S, 2017, J MULTIVARIATE ANAL, V153, P255, DOI 10.1016/j.jmva.2016.10.004
   Genay A, 2022, IEEE T VIS COMPUT GR, V28, P5071, DOI 10.1109/TVCG.2021.3099290
   Gescheider GA, 1997, Psychophysics: the fundamentals, Vthird
   Gonzalez-Franco M, 2020, IEEE T VIS COMPUT GR, V26, P2023, DOI 10.1109/TVCG.2020.2973075
   Gorisse G, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00008
   Grewe CM, 2023, BEHAV RES METHODS, V55, P867, DOI 10.3758/s13428-021-01761-9
   Hamamoto Y, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0262513
   Hayes A.F., 2012, PROCESS: a versatile computational tool for observed variable mediation, moderation, and conditional process modeling
   Hayes A.F., 2022, Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach, V3rd
   Horne M, 2020, INTERNET INTERV, V19, DOI 10.1016/j.invent.2019.100295
   Hudson GM, 2020, EUR J INVEST HEALTH, V10, P579, DOI 10.3390/ejihpe10020043
   Inoue Y, 2021, J ROBOT MECHATRON, V33, P1004, DOI 10.20965/jrm.2021.p1004
   Kamaria K, 2016, J TEKNOL, V78, P37
   Kaplan RA, 2013, COGN NEUROPSYCHIATRY, V18, P594, DOI 10.1080/13546805.2012.758878
   Kennedy RS., 1993, INT J AVIAT PSYCHOL, V3, P203, DOI [10.1207/s15327108ijap03033, DOI 10.1207/S15327108IJAP0303_3]
   Khalsa SS, 2017, J EAT DISORD, V5, DOI 10.1186/s40337-017-0145-3
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Maalin N, 2021, BEHAV RES METHODS, V53, P1308, DOI 10.3758/s13428-020-01494-1
   Matamala-Gomez M, 2021, J CLIN MED, V10, DOI 10.3390/jcm10010139
   Mölbert SC, 2018, PSYCHOL MED, V48, P642, DOI 10.1017/S0033291717002008
   Muller M., 2016, Proceedings of the 9th International Conference on Motion in Games, MIG'16, P55, DOI DOI 10.1145/2994258.2994269
   Neyret S, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00031
   Noguchi K, 2012, J STAT SOFTW, V50, P1, DOI 10.18637/jss.v050.i12
   O'Dea J., 2012, Encyclopedia of body image and human appearance, V12, P141, DOI 10.1016/B978-0-12-384925-0.00021-3
   Park J, 2018, CYBERPSYCHOLOGY, V12, DOI 10.5817/CP2018-1-3
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Piryankova IV, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2641568
   Pook M, 2002, VERHALTENSTHERAPIE, V12, P116, DOI 10.1159/000064375
   Portingale J, 2023, medRxiv, DOI [10.1101/2023.10.21.23297282, 10.1101/2023.10.21.23297282, DOI 10.1101/2023.10.21.23297282]
   Poulton E., 1989, Bias in Quantifying Judgments
   Robinette KM., Civilian American and European Surface Anthropometry Resource (CAESAR), V1, DOI DOI 10.21236/ADA406704
   Roth D, 2020, IEEE T VIS COMPUT GR, V26, P3546, DOI 10.1109/TVCG.2020.3023603
   Roth M, 2008, EUR J PSYCHOL ASSESS, V24, P190, DOI 10.1027/1015-5759.24.3.190
   Salagean A, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581182
   Sforza A, 2010, SOC NEUROSCI-UK, V5, P148, DOI 10.1080/17470910903205503
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Stauffert JP, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.582204
   Thaler A, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343134
   Thaler A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0192152
   Thaler Anne, 2018, Frontiers in ICT, V5, P18, DOI DOI 10.3389/FICT.2018.00018
   Thompson JK, 1998, OBES RES, V6, P375, DOI 10.1002/j.1550-8528.1998.tb00366.x
   Tsakiris M, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0004040
   Turbyne C, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.657638
   Voges MM, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.853398
   Voges MM, 2019, EAT WEIGHT DISORD-ST, V24, P1173, DOI 10.1007/s40519-017-0450-5
   von Spreckelsen P, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0198532
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Waltemate T, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P27, DOI 10.1145/2993369.2993381
   Wang I, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300511
   Wiederhold BK, 2016, CYBERPSYCH BEH SOC N, V19, P67, DOI 10.1089/cyber.2016.0012
   Wolf E, 2024, 2024 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW 2024, P859, DOI 10.1109/VRW62533.2024.00225
   Wolf E, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.1031093
   Wolf E, 2022, INT SYM MIX AUGMENT, P489, DOI 10.1109/ISMAR55827.2022.00065
   Wolf E, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P350, DOI 10.1109/VR51125.2022.00054
   Wolf E, 2020, INT SYM MIX AUGMENT, P462, DOI 10.1109/ISMAR50242.2020.00071
   Wolf E, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P65, DOI 10.1109/VR50410.2021.00027
NR 82
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7386
EP 7396
DI 10.1109/TVCG.2024.3456185
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300024
PM 39269805
DA 2024-11-06
ER

PT J
AU Song, WF
   Wang, X
   Jiang, YM
   Li, S
   Hao, AM
   Hou, X
   Qin, H
AF Song, Wenfeng
   Wang, Xuan
   Jiang, Yiming
   Li, Shuai
   Hao, Aimin
   Hou, Xia
   Qin, Hong
TI Expressive 3D Facial Animation Generation Based on Local-to-Global
   Latent Diffusion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Facial animation; Diffusion models; Three-dimensional displays; Lips;
   Synchronization; Frequency division multiplexing; Faces; 3D facial
   animation; speech-driven; diffusion model
AB 3D Facial animations, crucial to augmented and mixed reality digital media, have evolved from mere aesthetic elements to potent storytelling media. Despite considerable progress in facial animation of neutral emotions, existing methods still struggle to capture the authenticity of emotions. This paper introduces a novel approach to capture fine facial expressions and generate facial animations using audio synchronization. Our method consists of two key components: First, the Local-to-global Latent Diffusion Model (LG-LDM) tailored for authentic facial expressions, which can integrate audio, time step, facial expressions, and other conditions towards possible encoding of emotionally rich yet latent features in response to possibly noisy raw audio signals. The core of LG-LDM is our carefully designed Facial Denoiser Model (FDM) for aligning the local-to-global animation feature with audio. Second, we redesign an Emotion-centric Vector Quantized-Variational AutoEncoder framework (EVQ-VAE) to finely decode the subtle differences under different emotions and reconstruct the final 3D facial geometry. Our work significantly contributes to the key challenges of emotionally realistic 3D facial animation for audio synchronization and enhances the immersive experience and emotional depth in augmented and mixed reality applications. We provide a reproducibility kit including our code, dataset, and detailed instructions for running the experiments. This kit is available at https://github.com/wangxuanx/Face-Diffusion-Model.
C1 [Song, Wenfeng; Wang, Xuan; Hou, Xia] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing, Peoples R China.
   [Jiang, Yiming; Li, Shuai; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Li, Shuai] Zhongguancun Lab, Beijing, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, New York, NY 11794 USA.
C3 Beijing Information Science & Technology University; Beihang University;
   Zhongguancun Laboratory; State University of New York (SUNY) System;
   Stony Brook University
RP Song, WF (corresponding author), Beijing Informat Sci & Technol Univ, Comp Sch, Beijing, Peoples R China.; Li, S (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM songwenfenga@163.com; lishuai@buaa.edu.cn
OI HOU, Xia/0000-0001-6006-4215; Wang, Xuan/0009-0003-2777-0179; QIN,
   HONG/0000-0001-7699-1355
FU National Natural Science Foundation of China [62441201, 62102036,
   62272021]; Beijing Natural Science Foundation [L232102, 4222024]; R&D
   Program of Beijing Municipal Education Commission [KM202211232003];
   Beijing Science and Technology Plan Project [Z231100005923039]; National
   Key R&D Program of China [2023YFF1203803]; USA NSF [IIS-1715985,
   IIS-1812606]
FX This paper is supported by National Natural Science Foundation of China
   (62441201, 62102036, 62272021), Beijing Natural Science Foundation
   (L232102, 4222024), R&D Program of Beijing Municipal Education
   Commission (KM202211232003), Beijing Science and Technology Plan Project
   Z231100005923039, National Key R&D Program of China (No.
   2023YFF1203803), USA NSF IIS-1715985 and USA NSF IIS-1812606 (awarded to
   Hong QIN).
CR Adiban M., 2022, BRIT MACH VIS C, P636
   Amit T, 2022, Arxiv, DOI arXiv:2112.00390
   Ao TL, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592097
   Arshad MS, 2022, IEEE INT SYMP M AU R, P216, DOI 10.1109/ISMAR-Adjunct57072.2022.00048
   Cao HW, 2014, IEEE T AFFECT COMPUT, V5, P377, DOI 10.1109/TAFFC.2014.2336244
   Chang H., 2022, ACM SIGGRAPH C P, P1
   Chen Junming, 2024, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P7352, DOI 10.1109/CVPR52733.2024.00702
   Chen LY, 2024, Arxiv, DOI arXiv:2310.07236
   Cheng SY, 2018, PROC CVPR IEEE, P5117, DOI 10.1109/CVPR.2018.00537
   Chowdary GJ, 2023, LECT NOTES COMPUT SC, V14223, P622, DOI 10.1007/978-3-031-43901-8_59
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Danecek R., 2023, ACM Transactions on Graphics, P1
   Danecek R, 2022, PROC CVPR IEEE, P20279, DOI 10.1109/CVPR52688.2022.01967
   Dhariwal P, 2021, ADV NEUR IN, V34
   Elad M., 2006, P IEEE CVF C COMP VI, V1, P895, DOI DOI 10.1109/CVPR.2006.142
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fan YR, 2022, PROC CVPR IEEE, P18749, DOI 10.1109/CVPR52688.2022.01821
   Fanelli G, 2010, IEEE T MULTIMEDIA, V12, P591, DOI 10.1109/TMM.2010.2052239
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   Haque KI, 2023, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2023, P282, DOI 10.1145/3577190.3614157
   Hebborn AK, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P38, DOI [10.1109/ISMAR-Adjunct.2016.0034, 10.1109/ISMAR-Adjunct.2016.27]
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Hsu WN, 2021, IEEE-ACM T AUDIO SPE, V29, P3451, DOI 10.1109/TASLP.2021.3122291
   Huang RJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2595, DOI 10.1145/3503161.3547855
   Jo Y, 2021, PROC CVPR IEEE, P691, DOI 10.1109/CVPR46437.2021.00075
   Kaisiyuan Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P700, DOI 10.1007/978-3-030-58589-1_42
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Khryashchev V., 2018, P INT C MACH VIS APP, P66
   Kim J, 2023, AAAI CONF ARTIF INTE, P8255
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu ZX, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P200, DOI 10.1109/ISMAR-Adjunct.2019.00-47
   Livingstone SR, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196391
   Loshchilov I., 2019, P 7 INT C LEARN REPR
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Ni HM, 2023, PROC CVPR IEEE, P18444, DOI 10.1109/CVPR52729.2023.01769
   Nichol A, 2021, PR MACH LEARN RES, V139
   Nichol A, 2022, PR MACH LEARN RES
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Parida Sibam, 2023, 2023 2nd International Conference on Paradigm Shifts in Communications Embedded Systems, Machine Learning and Signal Processing (PCEMS), P1, DOI 10.1109/PCEMS58491.2023.10136091
   Peng ZQ, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P5292, DOI 10.1145/3581783.3611734
   Peng ZQ, 2023, IEEE I CONF COMP VIS, P20630, DOI 10.1109/ICCV51070.2023.01891
   Popov Vadim, 2021, INT C MACH LEARN, P8599
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Razavi A, 2019, ADV NEUR IN, V32
   Reddy M. D. M., 2021, UGC CARE GROUP J, V8, P71
   Richard A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1153, DOI 10.1109/ICCV48922.2021.00121
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C., 2022, NeurIPS, V35, P36479, DOI 10.1145/3528233.3530757
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Singer U., 2023, INT C LEARN REPR, P1
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   SONG J., 2021, INT C LEARN REPR
   Song Wenfeng, 2024, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P811, DOI 10.1109/CVPR52733.2024.00083
   Song Wenfeng, 2024, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P821, DOI 10.1109/CVPR52733.2024.00084
   SONG W., 2023, IEEE Transactions on Visualization and Computer Graphics, V3, P3
   Song W., 2024, IEEE Transactions on Visualization and Computer Graphics, P1
   Song WF, 2022, IEEE INT SYMP M AU R, P429, DOI 10.1109/ISMAR-Adjunct57072.2022.00092
   Song YD, 2024, Arxiv, DOI arXiv:2403.16627
   Stan S, 2023, 15TH ANNUAL ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2023, DOI 10.1145/3623264.3624447
   Takegawa Y, 2020, INT SYM MIX AUGMENT, P101, DOI 10.1109/ISMAR50242.2020.00030
   Tevet G., 2023, INT C LEARN REPR, P1
   Thambiraja B, 2023, IEEE I CONF COMP VIS, P20564, DOI 10.1109/ICCV51070.2023.01885
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tseng J, 2023, PROC CVPR IEEE, P448, DOI 10.1109/CVPR52729.2023.00051
   van den Oord A, 2017, ADV NEUR IN, V30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu CY, 2022, PROC CVPR IEEE, P10442, DOI 10.1109/CVPR52688.2022.01020
   Wu JZ, 2023, IEEE I CONF COMP VIS, P7589, DOI 10.1109/ICCV51070.2023.00701
   Xing JB, 2023, PROC CVPR IEEE, P12780, DOI 10.1109/CVPR52729.2023.01229
   Xu JR, 2023, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR52729.2023.00289
   Yang DC, 2023, IEEE-ACM T AUDIO SPE, V31, P1720, DOI 10.1109/TASLP.2023.3268730
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Zhang JR, 2023, PROC CVPR IEEE, P14730, DOI 10.1109/CVPR52729.2023.01415
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhang MY, 2024, IEEE T PATTERN ANAL, V46, P4115, DOI 10.1109/TPAMI.2024.3355414
   Zhang SQ, 2019, IEEE ACCESS, V7, P32297, DOI 10.1109/ACCESS.2019.2901521
   Zhang Y., 2023, Medical Imaging with Deep Learning
NR 78
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7397
EP 7407
DI 10.1109/TVCG.2024.3456161
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300010
PM 39255115
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Do, TD
   Benjamin, J
   Protko, CI
   McMahan, RP
AF Do, Tiffany D.
   Benjamin, Juanita
   Protko, Camille Isabella
   McMahan, Ryan P.
TI Cultural Reflections in Virtual Reality: The Effects of User Ethnicity
   in Avatar Matching Experiences on Sense of Embodiment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Cybersickness; Interviews; Virtual environments; Skin; Cultural
   differences; Social groups; Virtual reality; sense of embodiment;
   avatars; diversity
ID GENDER; IMPACT; OWNERSHIP; IDENTITY; MOTION; MEMORY; AGE
AB Matching avatar characteristics to a user can impact sense of embodiment (SoE) in YR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups.
C1 [Do, Tiffany D.] Drexel Univ, Philadelphia, PA 19104 USA.
   [Benjamin, Juanita; Protko, Camille Isabella] Univ Cent Florida, Orlando, FL 32816 USA.
   [McMahan, Ryan P.] Virginia Tech, Blacksburg, VA 24061 USA.
C3 Drexel University; State University System of Florida; University of
   Central Florida; Virginia Polytechnic Institute & State University
RP Do, TD (corresponding author), Drexel Univ, Philadelphia, PA 19104 USA.
EM tiffany.do@drexel.edu; juanita.benjamin@ucf.edu; ca916041@ucf.edu;
   rpm@vt.edu
OI McMahan, Ryan/0000-0001-9357-9696; Do, Tiffany D./0000-0003-3323-4586;
   Benjamin, Juanita/0000-0003-1308-6678
FU Doctoral Research Support Award from the College of Graduate Studies at
   The University of Central Florida
FX This work was supported in part by the Doctoral Research Support Award
   from the College of Graduate Studies at The University of Central
   Florida.
CR Almog I, 2009, 2009 VIRTUAL REHABILITATION INTERNATIONAL CONFERENCE, P78, DOI 10.1109/ICVR.2009.5174209
   Ambron E, 2022, FRONT VIRTUAL REAL, V3, DOI 10.3389/frvir.2022.884189
   Banakou D, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00601
   Chen VHH, 2021, MULTIMODAL TECHNOLOG, V5, DOI 10.3390/mti5080042
   Cheymol A, 2023, IEEE T VIS COMPUT GR, V29, P4426, DOI 10.1109/TVCG.2023.3320209
   de Vignemont F, 2011, CONSCIOUS COGN, V20, P82, DOI 10.1016/j.concog.2010.09.004
   Dirin Amir, 2019, International Journal of Interactive Mobile Technologies, V13, P93, DOI 10.3991/ijim.v13i06.10487
   Dluhopolskyi Oleksandr, 2021, 2021 11th International Conference on Advanced Computer Information Technologies (ACIT), P360, DOI 10.1109/ACIT52158.2021.9548495
   Do T. D., 2024, IEEE Transactions on Visualization and Computer Graphics, P1, DOI [10.1109/TVCG.2024.33720671,2,3,4,8, DOI 10.1109/TVCG.2024.33720671,2,3,4,8]
   Do TD, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1248915
   Döllinger N, 2023, INT SYM MIX AUGMENT, P483, DOI 10.1109/ISMAR59233.2023.00063
   Elkin Lisa A., 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P754, DOI 10.1145/3472749.3474784
   Erikson E. H., 1968, Identity: Youth in Crisis
   Eubanks JC, 2020, INT SYM MIX AUGMENT, P54, DOI 10.1109/ISMAR50242.2020.00025
   Felnhofer A., 2012, P INT SOC PRES RES A, P103
   Fiedler ML, 2023, 2023 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW, P799, DOI 10.1109/VRW58643.2023.00242
   Freeman G., 2021, Proc. ACM Hum.-Comput. Interact, V4, DOI [10.1145/34329381,2,3,7, DOI 10.1145/34329381,2,3,7]
   Fribourg R, 2020, IEEE T VIS COMPUT GR, V26, P2062, DOI 10.1109/TVCG.2020.2973077
   Gonzalez-Franco M, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.561558
   Gonzalez-Franco M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P941, DOI [10.1109/VR.2019.8798348, 10.1109/vr.2019.8798348]
   Grassini S, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01604
   Islam R, 2021, INT SYM MIX AUGMENT, P31, DOI 10.1109/ISMAR52148.2021.00017
   Islam R, 2020, INT SYM MIX AUGMENT, P400, DOI 10.1109/ISMAR50242.2020.00066
   Jin WN, 2018, 2018 IEEE GAMES, ENTERTAINMENT, MEDIA CONFERENCE (GEM), P382, DOI 10.1109/GEM.2018.8516469
   Jo D, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3141214
   Jung S, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P60, DOI 10.1145/3267782.3267920
   Kilteni K, 2013, IEEE T VIS COMPUT GR, V19, P597, DOI 10.1109/TVCG.2013.29
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kim H, 2023, IEEE T VIS COMPUT GR, V29, P4794, DOI 10.1109/TVCG.2023.3320240
   Kitson A, 2016, FRONT BEHAV NEUROSCI, V10, DOI 10.3389/fnbeh.2016.00022
   Krell F, 2023, SYMB INTERACT, V46, P159, DOI 10.1002/symb.629
   Lopez S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300787
   Marini M, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.989582
   Martinez-Fuentes S, 2020, IDENTITY, V20, P208, DOI 10.1080/15283488.2020.1784177
   Martingano AJ, 2022, J MED INTERNET RES, V24, DOI 10.2196/36843
   Miller MR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74486-y
   Miller R, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P409, DOI 10.1109/VR51125.2022.00060
   Moore AG, 2023, INT SYM MIX AUGMENT, P396, DOI 10.1109/ISMAR59233.2023.00054
   Moore AG, 2021, INT SYM MIX AUGMENT, P221, DOI 10.1109/ISMAR52148.2021.00037
   Moore AG, 2021, BIG DATA COGN COMPUT, V5, DOI 10.3390/bdcc5030029
   Moore AG, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P556, DOI 10.1109/VRW52623.2021.00160
   Moore AG, 2020, INT SYM MIX AUGMENT, P694, DOI 10.1109/ISMAR50242.2020.00099
   Mottelson A, 2023, ACM T COMPUT-HUM INT, V30, DOI 10.1145/3590767
   Munafo J, 2017, EXP BRAIN RES, V235, P889, DOI 10.1007/s00221-016-4846-7
   Nair V., 2023, P 32 USENIX C SEC S, P7
   Olson D. M., 2019, PhD thesis, P2
   Peck TC, 2021, IEEE COMPUT GRAPH, V41, P133, DOI 10.1109/MCG.2021.3113455
   Peck TC, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376419
   Peck TC, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.575943
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1945, DOI 10.1109/TVCG.2020.2973498
   Peck TC, 2018, IEEE T VIS COMPUT GR, V24, P1604, DOI 10.1109/TVCG.2018.2793598
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Pfeuffer K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300340
   Phosaard S., 2010, Effects of demographics and computer usage characteristics as moderators on importance of quality factors for virtual reality commerce interface, P2
   Plechatá A, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.01330
   Rack C, 2023, FRONT VIRTUAL REAL, V4, DOI 10.3389/frvir.2023.1272234
   Reinhardt D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P802, DOI [10.1109/VR.2019.8797977, 10.1109/vr.2019.8797977]
   Rhodes G, 2010, J EXP PSYCHOL LEARN, V36, P217, DOI 10.1037/a0017680
   Rogers-Sirin L, 2012, J COUNS PSYCHOL, V59, P555, DOI 10.1037/a0029329
   Roth D, 2020, IEEE T VIS COMPUT GR, V26, P3546, DOI 10.1109/TVCG.2020.3023603
   Sagnier C, 2020, ADV INTELL SYST, V972, P305, DOI 10.1007/978-3-030-19135-1_30
   Salagean A, 2023, PROCEEDINGS OF THE 2023 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2023), DOI 10.1145/3544548.3581182
   Salmanowitz N, 2018, J LAW BIOSCI, V5, P174, DOI 10.1093/jlb/lsy005
   Scheibler C, 2019, SAC '19: PROCEEDINGS OF THE 34TH ACM/SIGAPP SYMPOSIUM ON APPLIED COMPUTING, P2340, DOI 10.1145/3297280.3299749
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Sears DO, 2003, SOC PSYCHOL QUART, V66, P419, DOI 10.2307/1519838
   Serino S, 2018, CYBERPSYCH BEH SOC N, V21, P304, DOI 10.1089/cyber.2017.0674
   Shafer DM, 2019, GAMES HEALTH J, V8, P15, DOI 10.1089/g4h.2017.0190
   Spanlang B, 2014, FRONT ROBOT AI, DOI 10.3389/frobt.2014.00009
   Stanney K, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00004
   Steed A, 2016, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2016.7504689
   Taillade M, 2013, AGING NEUROPSYCHOL C, V20, P298, DOI 10.1080/13825585.2012.706247
   Teoh KK, 2008, INNOVATION AND KNOWLEDGE MANAGEMENT IN BUSINESS GLOBALIZATION: THEORY & PRACTICE, VOLS 1 AND 2, P357
   Vargas N, 2016, AM BEHAV SCI, V60, P442, DOI 10.1177/0002764215613396
   Vasalou A, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P445
   Vila J., 2003, 36 ANN HAW INT C SYS, DOI [10.1109/HICSS.2003.11742392,3, DOI 10.1109/HICSS.2003.11742392,3]
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Xia G., 2021, 14 C INT COL ASS, P909
   Zibrek K, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419985
NR 80
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7408
EP 7418
DI 10.1109/TVCG.2024.3456211
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300036
PM 39250390
DA 2024-11-06
ER

PT J
AU Schütz, L
   Matinfar, S
   Schafroth, G
   Navab, N
   Fairhurst, M
   Wagner, A
   Wiestler, B
   Eck, U
   Navab, N
AF Schuetz, Laura
   Matinfar, Sasan
   Schafroth, Gideon
   Navab, Navid
   Fairhurst, Merle
   Wagner, Arthur
   Wiestler, Benedikt
   Eck, Ulrich
   Navab, Nassir
TI A Framework for Multimodal Medical Image Interaction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Anatomy; Surgery; Three-dimensional displays; Sonification;
   Visualization; Anatomical structure; Navigation; Multimodal interaction;
   Audiovisual feedback; Physical modeling synthesis; Virtual reality;
   Augmented reality; Human-centered design; Human-computer interaction;
   HCI; Medical images; Medical image interaction; Surgical navigation;
   Brain surgery; Brain tumor; Tumor localization
ID AUGMENTED REALITY; SONIFICATION; GUIDANCE
AB Medical doctors rely on images of the human anatomy, such as magnetic resonance imaging (MRI), to localize regions of interest in the patient during diagnosis and treatment. Despite advances in medical imaging technology, the information conveyance remains unimodal. This visual representation fails to capture the complexity of the real, multisensory interaction with human tissue. However, perceiving multimodal information about the patient's anatomy and disease in real-time is critical for the success of medical procedures and patient outcome. We introduce a Multimodal Medical Image Interaction (MMII) framework to allow medical experts a dynamic, audiovisual interaction with human tissue in three-dimensional space. In a virtual reality environment, the user receives physically informed audiovisual feedback to improve the spatial perception of anatomical structures. MMII uses a model-based sonification approach to generate sounds derived from the geometry and physical properties of tissue, thereby eliminating the need for hand-crafted sound design. Two user studies involving 34 general and nine clinical experts were conducted to evaluate the proposed interaction framework's learnability, usability, and accuracy. Our results showed excellent learnability of audiovisual correspondence as the rate of correct associations significantly improved (p < 0.001) over the course of the study. MMII resulted in superior brain tumor localization accuracy (p < 0.05) compared to conventional medical image interaction. Our findings substantiate the potential of this novel framework to enhance interaction with medical images, for example, during surgical procedures where immediate and precise feedback is needed.
C1 [Schuetz, Laura; Matinfar, Sasan; Schafroth, Gideon; Wagner, Arthur; Wiestler, Benedikt; Eck, Ulrich; Navab, Nassir] Tech Univ Munich TUM, Munich, Germany.
   [Navab, Navid] Concordia Univ Montreal, Montreal, PQ, Canada.
   [Fairhurst, Merle] Tech Univ Dresden, Garmany, Dresden, Germany.
C3 Technical University of Munich; Technische Universitat Dresden
RP Schütz, L (corresponding author), Tech Univ Munich TUM, Munich, Germany.
EM laura.schuetz@tum.de; sasan.matinfar@tum.de; gideon.schafroth@tum.de;
   navid.nav@gmail.com; merle.fairhurst@tu-dresden.de;
   arthur.wagner@tum.de; b.wiestler@tum.de; ulrich.eck@tum.de;
   nassir.navab@tum.de
CR Ahmad A, 2010, OPT EXPRESS, V18, P9934, DOI 10.1364/OE.18.009934
   Andress S, 2018, J MED IMAGING, V5, DOI 10.1117/1.JMI.5.2.021209
   Bartlett RD, 2020, BIOMATERIALS, V258, DOI 10.1016/j.biomaterials.2020.120303
   Bartlett RD, 2016, REGEN MED, V11, P659, DOI 10.2217/rme-2016-0065
   Ben Awadh A, 2022, ANAT SCI EDUC, V15, P127, DOI 10.1002/ase.2045
   Blum T, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P115, DOI 10.1109/VR.2012.6180909
   Bogomolova K, 2020, ANAT SCI EDUC, V13, P558, DOI 10.1002/ase.1941
   Bork F, 2019, ANAT SCI EDUC, V12, P585, DOI 10.1002/ase.1864
   Bork F, 2017, P IEEE VIRT REAL ANN, P373, DOI 10.1109/VR.2017.7892332
   Bork F, 2015, 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P7, DOI 10.1109/ISMAR.2015.16
   Bovermann T., 2006, P 12 INT C AUD DISPL
   Chen C, 2022, INT SYM MIX AUGMENT, P64, DOI 10.1109/ISMAR55827.2022.00020
   Cook P.R., 1996, Proceedings of the 1996 International Computer Music Conference, P228
   Dennler C, 2020, J ORTHOP SURG RES, V15, DOI 10.1186/s13018-020-01690-x
   Dixon BJ, 2014, AM J RHINOL ALLERGY, V28, P433, DOI 10.2500/ajra.2014.28.4067
   Dubus G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082491
   Eckel G., 1995, P ISMA 95, P478
   Franinovic K, 2013, SONIC INTERACTION DESIGN, P39
   Hansen C, 2013, INT J MED ROBOT COMP, V9, P36, DOI 10.1002/rcs.1466
   HART S G, 1988, P139
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Hermann T., 1999, Advances in Intelligent Computing and Multimedia Systems, P189
   Hermann Thomas, 2011, The sonification handbook, V1
   Illanes A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30641-0
   Jang S, 2017, COMPUT EDUC, V106, P150, DOI 10.1016/j.compedu.2016.12.009
   Joeres F., 2021, Graphics Interface 2021
   Kang W, 2024, J BIOMECH, V162, DOI 10.1016/j.jbiomech.2023.111888
   Kantan P. R., 2022, NORD HUM COMP INT C, P1
   Keenan I. D., 2020, Interdimensional Travel: Visualisation of 3D- 2D Transitions in Anatomy Learning, P103
   Madden M. E., 2008, Introduction to sectional anatomy
   Marquardt A, 2020, IEEE T VIS COMPUT GR, V26, P3389, DOI 10.1109/TVCG.2020.3023605
   Matinfar Sasan, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P673, DOI 10.1007/978-3-319-66185-8_76
   Matinfar S., 2019, P NORD SOUND MUS COM
   Matinfar S, 2023, LECT NOTES COMPUT SC, V14228, P207, DOI 10.1007/978-3-031-43996-4_20
   Matinfar S, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-32778-z
   Navab N, 2023, J IMAGING, V9, DOI 10.3390/jimaging9010004
   Ngo MK, 2010, ATTEN PERCEPT PSYCHO, V72, P1654, DOI 10.3758/APP.72.6.1654
   Ostler D, 2020, INT J COMPUT ASS RAD, V15, P771, DOI 10.1007/s11548-020-02146-7
   Parseihian G, 2016, IEEE T MULTIMEDIA, V18, P674, DOI 10.1109/TMM.2016.2531978
   Porter Roy., 2001, The Cambridge Illustrated History of Medicine
   Roodaki H, 2017, IEEE T VIS COMPUT GR, V23, P2366, DOI 10.1109/TVCG.2017.2734327
   Schütz L, 2024, PROCEEDINGS OF THE 2024 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYTEMS, CHI 2024, DOI 10.1145/3613904.3642257
   Schütz L, 2023, COMP M BIO BIO E-IV, V11, P1158, DOI 10.1080/21681163.2022.2154277
   Shams L, 2008, TRENDS COGN SCI, V12, P411, DOI 10.1016/j.tics.2008.07.006
   Taha AA, 2015, BMC MED IMAGING, V15, DOI 10.1186/s12880-015-0068-x
   Valjamae A., 2013, INT C AUD DISPL 2013
   Van der Burg E, 2008, J EXP PSYCHOL HUMAN, V34, P1053, DOI 10.1037/0096-1523.34.5.1053
   Van Hese L, 2022, DISCOV ONCOL, V13, DOI 10.1007/s12672-022-00585-z
   Wegner C M, 1997, Stud Health Technol Inform, V39, P450
   Wright M., 2005, Organised Sound, V10, P193, DOI 10.1017/S1355771805000932
   Yang J, 2022, J AUDIO ENG SOC, V70, P788, DOI 10.17743/jaes.2022.0048
   Ziemer T., 2019, arXiv, DOI [10.48550/arXiv.1912.00766, DOI 10.48550/ARXIV.1912.00766]
   Ziemer T., 2017, P M AC, V30
   Ziemer T, 2023, J MULTIMODAL USER IN, V17, P253, DOI 10.1007/s12193-023-00422-9
   Ziemer T, 2018, ACTA ACUST UNITED AC, V104, P1075, DOI 10.3813/AAA.919273
   Zwicker E., 2013, Psychoacoustics: Facts and Models
NR 56
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7419
EP 7429
DI 10.1109/TVCG.2024.3456213
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300003
DA 2024-11-06
ER

PT J
AU Zhang, JJ
   Huang, MJ
   Chen, YL
   Liao, KL
   Shi, JJ
   Liang, HN
   Yang, R
AF Zhang, Jingjing
   Huang, Mengjie
   Chen, Yonglin
   Liao, Kai-Lun
   Shi, Jiajia
   Liang, Hai-Ning
   Yang, Rui
TI <i>TouchMark</i>: Partial Tactile Feedback Design for Upper Limb
   Rehabilitation in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tactile sensors; Visualization; Training; Grasping; Thumb; Shape;
   Motors; Virtual rehabilitation; embodiment; body ownership; agency;
   self-location; tactile sensation
ID HAPTIC VOLUME PERCEPTION; SENSE; SHAPE
AB The use of Virtual Reality (VR) technology, especially in medical rehabilitation, has expanded to include tactile cues along with visual stimuli. For patients with upper limb hemiplegia, tangible handles with haptic stimuli could improve their ability to perform daily activities. Traditional VR controllers are unsuitable for patient rehabilitation in VR, necessitating the design of specialized tangible handles with integrated tracking devices. Besides, matching tactile stimulation with corresponding virtual visuals could strengthen users' embodiment (i.e., owning and controlling virtual bodies) in VR, which is crucial for patients' training with virtual hands. Haptic stimuli have been shown to amplify the embodiment in VR, whereas the effect of partial tactile stimulation from tangible handles on embodiment remains to be clarified. This research, including three experiments, aims to investigate how partial tactile feedback of tangible handles impacts users' embodiment, and we proposed a design concept called TouchMark for partial tactile stimuli that could help users quickly connect the physical and virtual worlds. To evaluate users' tactile and comfort perceptions when grasping tangible handles in a non-VR setting, various handles with three partial tactile factors were manipulated in Study 1. In Study 2, we explored the effects of partial feedback using three forms of TouchMark on the embodiment of healthy users in VR, with various tangible handles, while Study 3 focused on similar investigations with patients. These handles were utilized to complete virtual food preparation tasks. The tactile and comfort perceptions of tangible handles and users' embodiment were evaluated in this research using questionnaires and interviews. The results indicate that TouchMark with haptic line and ring forms over no stimulation would significantly enhance users' embodiment, especially for patients. The low-cost and innovative TouchMark approach may assist users, particularly those with limited VR experience, in achieving the embodiment and enhancing their virtual interactive experience.
C1 [Zhang, Jingjing; Huang, Mengjie] Xian Jiaotong Liverpool Univ, Design Sch, Suzhou, Peoples R China.
   [Zhang, Jingjing] Univ Liverpool, Sch Engn, Liverpool, England.
   [Liao, Kai-Lun; Yang, Rui] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou, Peoples R China.
   [Chen, Yonglin] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Liang, Hai-Ning] Hong Kong Univ Sci & Technol Guangzhou, Computat Media & Arts Thrust, Guangzhou, Peoples R China.
   [Shi, Jiajia] Kunshan Rehabil Hosp, Phys Med & Rehabil Med Dept, Suzhou, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; University of Liverpool; Xi'an
   Jiaotong-Liverpool University; City University of Hong Kong; Hong Kong
   University of Science & Technology (Guangzhou)
RP Huang, MJ (corresponding author), Xian Jiaotong Liverpool Univ, Design Sch, Suzhou, Peoples R China.
EM Jingjing.Zhang3@liverpool.ac.uk; mengjie.huang@xjtlu.edu.cn;
   yonglin.chen@cityu.edu.hk; kailun.liao18@student.xjtlu.edu.cn;
   jiajiashi8711@gmail.com; hainingliang@hkust-gz.edu.cn;
   r.yang@xjtlu.edu.cn
RI Yang, Rui/AFN-1679-2022
OI Liang, Hai-Ning/0000-0003-3600-8955; YANG, RUI/0000-0002-5634-5476;
   Chen, Yonglin/0009-0004-3081-1813; Huang, Mengjie/0000-0001-8163-8679;
   Zhang, Jingjing/0009-0007-5405-9138
FU Jiangsu Provincial Qinglan Project; XJTLU Research Enhancement Fund
   [REF-23-01-008]; Natural Science Foundation of the Jiangsu Higher
   Educational Institution of China [23KJB520038]
FX This work is supported by the Jiangsu Provincial Qinglan Project, XJTLU
   Research Enhancement Fund (REF-23-01-008) and The Natural Science
   Foundation of the Jiangsu Higher Educational Institution of China
   (23KJB520038). Thanks to Xiaoyi Xue, who helped in creating the
   illustrations of cartoon figures for Fig. 2 in this paper. We are
   grateful to the patients with upper limb mobility impairments at Kunshan
   Rehabilitation Hospital for participating in our study and to the
   hospital staffs for their essential support.
CR Bouzbib E., 2021, P 32 C INT HOMM MACH, P1
   Bouzbib E, 2024, IEEE T VIS COMPUT GR, V30, P3973, DOI 10.1109/TVCG.2023.3244076
   Bozgeyikli L, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P812, DOI 10.1109/VR51125.2022.00103
   Browne A, 2012, APPL ERGON, V43, P493, DOI 10.1016/j.apergo.2011.08.002
   Chiang VCL, 2017, DISABIL REHABIL-ASSI, V12, P672, DOI 10.1080/17483107.2016.1218554
   Cuesta-Gómez A, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00718-x
   Cui D., 2023, P 18 ACM SIGGRAPH IN, P1
   Cui DX, 2022, MULTIMODAL TECHNOLOG, V6, DOI 10.3390/mti6090076
   Demeco A, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23031712
   Dewez D, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445379
   Feick M, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517671
   Feick M, 2020, INT SYM MIX AUGMENT, P195, DOI 10.1109/ISMAR50242.2020.00042
   Feix T, 2014, IEEE T HAPTICS, V7, P311, DOI 10.1109/TOH.2014.2326871
   Fröhner J, 2019, IEEE T HAPTICS, V12, P339, DOI 10.1109/TOH.2018.2889497
   Ganzer PD, 2020, CELL, V181, P763, DOI 10.1016/j.cell.2020.03.054
   Gonzalez-Franco M, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00074
   Gunther Sebastian, 2022, CHI '22: CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3491102.3517454
   Heinrich C, 2021, VIRTUAL REAL-LONDON, V25, P313, DOI 10.1007/s10055-020-00456-4
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kim M, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17051141
   Krogmeier C, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1883
   Kuijt-Evers LFM, 2007, INT J IND ERGONOM, V37, P73, DOI 10.1016/j.ergon.2006.09.019
   Lam P, 2008, J NEUROENG REHABIL, V5, DOI 10.1186/1743-0003-5-15
   Matamala-Gomez M, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01962
   Melo M, 2022, IEEE T VIS COMPUT GR, V28, P1428, DOI 10.1109/TVCG.2020.3010088
   Moore CH, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.662397
   Morimoto K, 2023, 29TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, VRST 2023, DOI 10.1145/3611659.3615689
   Ozgur AG, 2018, ACMIEEE INT CONF HUM, P241, DOI 10.1145/3171221.3171262
   Pereira F., 2018, Exploring Materials and Object Properties in an Interactive Tangible System for Upper Limb Rehabilitation
   Phelan I., 2023, Virtual Reality, V27
   Rebelo A. R., 2022, 2022 INT C GRAPH INT, P1
   Richard G, 2021, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.573167
   Rizzo A, 2005, PRESENCE-TELEOP VIRT, V14, P119, DOI 10.1162/1054746053967094
   Schwind V, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225158
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Sheng B, 2023, HELIYON, V9, DOI 10.1016/j.heliyon.2023.e13588
   Teng SY, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P5, DOI 10.1145/3242587.3242628
   Tieri G, 2018, EXPERT REV MED DEVIC, V15, P107, DOI 10.1080/17434440.2018.1425613
   Tony BJAR, 2019, P I MECH ENG L-J MAT, V233, P1564, DOI 10.1177/1464420718766961
   Tsai HR, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517489
   Veerbeek JM, 2011, STROKE, V42, P1482, DOI 10.1161/STROKEAHA.110.604090
   Wang L, 2023, IEEE T VIS COMPUT GR, V29, P2184, DOI 10.1109/TVCG.2022.3142198
   Whitmire E, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173660
   Yoshida S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376358
   Zhang J, 2019, PERCEPTION, V48, P1252, DOI 10.1177/0301006619878560
   Zhang JJ, 2024, 2024 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS, VRW 2024, P943, DOI 10.1109/VRW62533.2024.00268
   Zhang JJ, 2023, AI EDAM, V37, DOI 10.1017/S0890060423000045
   Zhang JJ, 2020, INT SYM COMPUT INTEL, P364, DOI 10.1109/ISCID51228.2020.00088
   Zhang ZL, 2021, IEEE T HAPTICS, V14, P816, DOI 10.1109/TOH.2021.3077882
   Zhang ZL, 2018, ATTEN PERCEPT PSYCHO, V80, P576, DOI 10.3758/s13414-017-1453-z
   Zhu KN, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300923
   Zhu LF, 2022, IEEE T VIS COMPUT GR, V28, P3928, DOI 10.1109/TVCG.2022.3203087
NR 52
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7430
EP 7440
DI 10.1109/TVCG.2024.3456196
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300002
PM 39255139
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Shen, JX
   Boldu, R
   Kalla, A
   Glueck, M
   Surale, HB
   Karlson, A
AF Shen, Junxiao
   Boldu, Roger
   Kalla, Arpit
   Glueck, Michael
   Surale, Hemant Bhaskar
   Karlson, Amy
TI RingGesture: A Ring-Based Mid-Air Gesture Typing System Powered by a
   Deep-Learning Word Prediction Framework
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tracking; Keyboards; Context modeling; Wrist; Predictive models;
   Decoding; Trajectory; Text entry; augmented reality; word prediction;
   language models
ID INPUT
AB Text entry is a critical capability for any modern computing experience, with lightweight augmented reality (AR) glasses being no exception. Designed for all-day wearability, a limitation of lightweight AR glass is the restriction to the inclusion of multiple cameras for extensive field of view in hand tracking. This constraint underscores the need for an additional input device. We propose a system to address this gap: a ring-based mid-air gesture typing technique, RingGesture, utilizing electrodes to mark the start and end of gesture trajectories and inertial measurement units (IMU) sensors for hand tracking. This method offers an intuitive experience similar to raycast-based mid-air gesture typing found in VR headsets, allowing for a seamless translation of hand movements into cursor navigation. To enhance both accuracy and input speed, we propose a novel deep-learning word prediction framework, Score Fusion, comprised of three key components: a) a word-gesture decoding model, b) a spatial spelling correction model, and c) a lightweight contextual language model. In contrast, this framework fuses the scores from the three models to predict the most likely words with higher precision. We conduct comparative and longitudinal studies to demonstrate two key findings: firstly, the overall effectiveness of RingGesture, which achieves an average text entry speed of 27.3 words per minute (WPM) and a peak performance of 47.9 WPM. Secondly, we highlight the superior performance of the Score Fusion framework, which offers a 28.2% improvement in uncorrected Character Error Rate over a conventional word prediction framework, Naive Correction, leading to a 55.2% improvement in text entry speed for RingGesture. Additionally, RingGesture received a System Usability Score of 83 signifying its excellent usability.
C1 [Shen, Junxiao; Boldu, Roger; Kalla, Arpit; Glueck, Michael; Surale, Hemant Bhaskar; Karlson, Amy] Meta, Real Labs Res, Menlo Pk, CA 94025 USA.
   [Shen, Junxiao] Univ Bristol, Bristol, England.
C3 University of Bristol
RP Shen, JX (corresponding author), Meta, Real Labs Res, Menlo Pk, CA 94025 USA.; Shen, JX (corresponding author), Univ Bristol, Bristol, England.
EM shawn.shen.trinity@icloud.com
RI Surale, Hemant/AAG-9672-2020; Glueck, Michael/AAK-3015-2020
CR Alsharif O., 2015, Long short term memory neural network for keyboard gesture decoding, P2076
   [Anonymous], ENRON EMAIL DATASET
   Apple Inc, 2023, Apple Vision Pro
   Baba Y., 2020, P 50 ANN M ASS COMP, P373
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Brooke J., 1995, USABILITY EVAL IND, P189
   Burtsev M, 2018, SPRING SER CHALLENGE, P25, DOI 10.1007/978-3-319-94042-7_2
   Cai F, 2016, FOUND TRENDS INF RET, V10, P274, DOI 10.1561/1500000055
   contributors W., 2019, Microsoft hololens 2-wikipedia
   contributors W., 2023, Apple vision pro-wikipedia
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dudley J., 2023, IEEE Transactions on Visualization and Computer Graphics
   Dudley JJ, 2019, INT SYM MIX AUGMENT, P289, DOI 10.1109/ISMAR.2019.00027
   Dudley JJ, 2018, ACM T COMPUT-HUM INT, V25, DOI 10.1145/3232163
   Furtado Joshua S., 2019, Advances in Motion Sensing and Control for Robotic Applications. Selected Papers from the Symposium on Mechatronics, Robotics, and Control (SMRC18)CSME International Congress 2018. Lecture Notes in Mechanical Engineering (LNME), P15, DOI 10.1007/978-3-030-17369-2_2
   Goldberg Y., 2017, Neural network methods for natural language processing
   Gong J, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173755
   Graves A, 2005, LECT NOTES COMPUT SC, V3697, P799
   Grossman T., 2015, P 17 INT C HUMAN COM, P144, DOI DOI 10.1145/2785830.2785867
   Gu YZ, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3432204
   Gupta A, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300244
   Henderson J., 2020, 22 INT C HUM COMP IN
   io, Reddit Data via pushshift.io
   Jiang HY, 2024, INT J HUM-COMPUT INT, V40, P278, DOI 10.1080/10447318.2022.2115333
   Jiang HY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19143063
   Jurafsky D., 2019, A comprehensive guide to the field of speech and language processing, covering both traditional and modern approaches including N-gram models and deep learning techniques, V3
   Kienzle W, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445094
   Kim T., 2023, P 36 ANN ACM S US IN, P1
   Klimt B, 2004, LECT NOTES COMPUT SC, V3201, P217
   Kristensson P., 2004, P 17 ANN ACM S USER, P43, DOI DOI 10.1145/1029632.1029640
   KUKICH K, 1992, COMPUT SURV, V24, P377
   Lee LH, 2019, INT CONF PERVAS COMP, DOI 10.1109/percom.2019.8767420
   Lee Y, 2017, LECT NOTES COMPUT SC, V10280, P111, DOI 10.1007/978-3-319-57987-0_9
   Leiva LA, 2021, PROCEEDINGS OF 23RD ACM INTERNATIONAL CONFERENCE ON MOBILE HUMAN-COMPUTER INTERACTION (MOBILEHCI 2021): MOBILE APART, MOBILE TOGETHER, DOI 10.1145/3447526.3472059
   Lepouras G, 2018, VIRTUAL REAL-LONDON, V22, P63, DOI 10.1007/s10055-017-0312-5
   Liang C, 2022, PROC ACM INTERACT MO, V6, DOI 10.1145/3569463
   Liu C.-W., 2016, How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation, P13, DOI [10.18653/v1/D16-12305, DOI 10.18653/V1/D16-12305]
   MacKenzie I.S., 2003, CHI 03 EXTENDED ABST, P754, DOI DOI 10.1145/765891.765971
   Markussen A, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1073, DOI 10.1145/2556288.2556964
   Maxwell S. E., 2004, Designing Experiments and Analyzing Data: A Model Comparison Perspective, V2, P8
   Meta AI, 2018, Pytext: A natural language modeling framework based on pytorch
   Meta Platforms Inc, Oculus Quest Series
   Microsoft Corporation, 2019, Microsoft HoloLens 2
   Mitton R., 1996, English Spelling and the Computer, P3
   Ogitani T, 2018, INT CON ADV INFO NET, P342, DOI 10.1109/AINA.2018.00059
   Pauls A., 2011, ANN M ASS COMP LING, P3
   Peshock Anna., 2014, Proceedings of the 2014 ACM International Symposium on Wearable Computers: Adjunct Program, ISWC '14 Adjunct, P87, DOI DOI 10.1145/2641248.2641266
   Pirinen TA, 2014, LECT NOTES COMPUT SC, V8404, P519, DOI 10.1007/978-3-642-54903-8_43
   Radford Alec, 2019, OpenAI blog, V1, P9
   Rakhmetulla G., 2020, Graphics Interface 2021
   Reyal S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P679, DOI 10.1145/2702123.2702597
   Salehinejad H., 2017, Recent advances in recurrent neural networks, V6
   Salimans T, 2016, Arxiv, DOI [arXiv:1602.07868, 10.48550/ARXIV.1602.07868]
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   sensel, Haptic capacitive
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Shen JX, 2023, IEEE T VIS COMPUT GR, V29, P4622, DOI 10.1109/TVCG.2023.3320218
   Shen JX, 2022, INT SYM MIX AUGMENT, P702, DOI 10.1109/ISMAR55827.2022.00088
   Shen JX, 2022, IUI'22: 27TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P853, DOI 10.1145/3490099.3511145
   Shen JX, 2021, INT SYM MIX AUGMENT, P393, DOI 10.1109/ISMAR52148.2021.00056
   Snoddy GS, 1926, J APPL PSYCHOL, V10, P1, DOI 10.1037/h0075814
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkatesan R., 2017, Convolutional Neural Networks in Visual Computing: A Concise Guide
   Wang Cheng-Yao, 2015, P 17 INT C HUM COMP, P153, DOI [DOI 10.1145/2785830.2785886, DOI 10.1145/2785830.27858862]
   wikimedia, Wikipedia Talk Page Data
   Wolf D, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376876
   Xiang, 2014, Proceedings of the 27th annual ACM symposium on User interface software and technology - UIST, V14, P615, DOI [10.1145/2642918.2647354, DOI 10.1145/2642918.2647354]
   Xu WG, 2019, INT SYM MIX AUGMENT, P279, DOI 10.1109/ISMAR.2019.00026
   Xu Z, 2022, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, UIST 2022, DOI 10.1145/3526113.3545683
   Xu ZE, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P883, DOI 10.1145/3332165.3347865
   yelp, Yelp Dataset
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
   Yu DF, 2018, IEEE T VIS COMPUT GR, V24, P2927, DOI 10.1109/TVCG.2018.2868581
   Zhao MZ, 2023, PROCEEDINGS OF 2023 28TH ANNUAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2023, P595, DOI 10.1145/3581641.3584072
NR 76
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7441
EP 7451
DI 10.1109/TVCG.2024.3456163
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300040
PM 39250409
OA Green Submitted
DA 2024-11-06
ER

PT J
AU Pooryousef, V
   Cordeil, M
   Besançon, L
   Bassed, R
   Dwyer, T
AF Pooryousef, Vahid
   Cordeil, Maxime
   Besancon, Lonni
   Bassed, Richard
   Dwyer, Tim
TI Collaborative Forensic Autopsy Documentation and Supervised Report
   Generation Using a Hybrid Mixed-Reality Environment and Generative AI
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Forensic autopsy; report generation; documentation; documentation; mixed
   reality; mixed reality; generative AI; generative AI; mixed reality;
   generative AI
ID AUGMENTED REALITY; HOLOLENS; IMPACT; TOOL
AB Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities-including mixed-reality (MR) tools to support 3D visualisation of such data-these tools do not integrate seamlessly into their existing collaborative workflow and report authoring process, requiring extra steps, e.g. to extract imagery from the MR tool and combine with physical autopsy findings for inclusion in the report. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our preliminary findings indicate that this approach has the potential to improve data management, aid reviewability, and thus, achieve more robust standards. Further, it potentially streamlines report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education.
C1 [Pooryousef, Vahid; Bassed, Richard; Dwyer, Tim] Monash Univ, Melbourne, Australia.
   [Cordeil, Maxime] Univ Queensland, St Lucia, Australia.
   [Besancon, Lonni] Linkoping Univ, Linkoping, Sweden.
   [Bassed, Richard] Victorian Inst Forens Med, Southbank, Australia.
C3 Monash University; University of Queensland; Linkoping University
RP Pooryousef, V (corresponding author), Monash Univ, Melbourne, Australia.
EM vahid.pooryousef@monash.edu; m.cordeil@uq.edu.au;
   lonni.besancon@gmail.com; richard.bassed@monash.edu;
   tim.dwyer@monash.edu
OI Bassed, Richard/0000-0001-5473-055X; Pooryousef,
   Vahid/0000-0003-4258-4502; Cordeil, Maxime/0000-0002-9732-4874; Dwyer,
   Tim/0000-0002-9076-9571
FU VIFM; Knut and Alice Wallenberg Foundation [KAW 2019.0024]
FX The authors wish to thank all participants from VIFM, and University of
   Toronto. This research was supported by the VIFM funding as well as the
   Knut and Alice Wallenberg Foundation (grant KAW 2019.0024).
CR Affolter R, 2019, J FORENSIC RADIOL IM, V16, P5, DOI 10.1016/j.jofri.2018.11.003
   Ahmad A, 2023, 27TH INTERNATIONAL CONFERENCE ON EVALUATION AND ASSESSMENT IN SOFTWARE ENGINEERING, EASE 2023, P279, DOI 10.1145/3593434.3593468
   Alawad M, 2020, J AM MED INFORM ASSN, V27, P89, DOI 10.1093/jamia/ocz153
   Alsharid M, 2019, LECT NOTES COMPUT SC, V11767, P338, DOI 10.1007/978-3-030-32251-9_37
   Bacinger F, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11244124
   Batch A, 2020, IEEE T VIS COMPUT GR, V26, P536, DOI 10.1109/TVCG.2019.2934803
   Bergström H, 2018, BMC SURG, V18, DOI 10.1186/s12893-018-0428-x
   Besançon L, 2021, BMC MED RES METHODOL, V21, DOI 10.1186/s12874-021-01304-y
   Besancon L, 2020, COMPUT GRAPH FORUM, V39, P462, DOI 10.1111/cgf.13886
   Bhave DP, 2020, J MANAGE, V46, P127, DOI 10.1177/0149206319878254
   Bulliard J, 2020, FOREN IMAG, V23, DOI 10.1016/j.fri.2020.200417
   Buschel Wolfgang, 2021, P 2021 CHI C HUM FAC, P1, DOI [DOI 10.1145/3411764.3445651, 10.1145/3411764.3445651]
   Caruccio L, 2024, EXPERT SYST APPL, V235, DOI 10.1016/j.eswa.2023.121186
   Caswell D, 2018, JOURNAL PRACT, V12, P477, DOI 10.1080/17512786.2017.1320773
   Charbonneau É, 2020, PUBLIC ADMIN REV, V80, P780, DOI 10.1111/puar.13278
   Chen ZH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1439
   Co M.., 2023, Surgery, DOI [10.1016/j.surg.2023.07.0152, DOI 10.1016/J.SURG.2023.07.0152]
   Coulentianos M. J.., 2020, Frontiers in Biomedical Devices, V83549, DOI [10.1115/DMD2020-90204, DOI 10.1115/DMD2020-90204]
   Doberstein C, 2022, PUBLIC PERFORM MANAG, V45, P198, DOI 10.1080/15309576.2021.1931374
   Ens B, 2021, IEEE T VIS COMPUT GR, V27, P1193, DOI 10.1109/TVCG.2020.3030334
   Feiner S., 1991, UIST Fourth Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P9, DOI 10.1145/120782.120783
   Feng YY, 2021, SURG INNOV, V28, P33, DOI 10.1177/1553350620947206
   Gabrielli M, 2021, J MED ETHICS, V47, P798, DOI 10.1136/medethics-2019-106056
   Gasques D  ..., 2021, P CHI, DOI [DOI 10.1145/3411764.3445576, 10.1145/3411764.3445576]
   Golomingi R, 2023, SCI JUSTICE, V63, P451, DOI 10.1016/j.scijus.2023.04.009
   Grinschgl S, 2021, Q J EXP PSYCHOL, V74, P1477, DOI 10.1177/17470218211008060
   Han ZY, 2018, LECT NOTES COMPUT SC, V11073, P185, DOI 10.1007/978-3-030-00937-3_22
   Hanna MG, 2018, ARCH PATHOL LAB MED, V142, P638, DOI 10.5858/arpa.2017-0189-OA
   Headley AM, 2017, J CRIM JUST, V53, P102, DOI 10.1016/j.jcrimjus.2017.10.003
   Hubenschmid S, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445298
   Javaid M., 2023, BenchCouncil Transactions on Benchmarks, Standards and Evaluations, V3, DOI DOI 10.1016/J.TBENCH.2023.100105
   Jean WC, 2022, WORLD NEUROSURG, V161, P459, DOI 10.1016/j.wneu.2021.08.150
   Ji ZW, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3571730
   Katz C. M  ..., 2014, Evaluating the impact of officer worn body cameras in the phoenix police department, P9
   Kilgus T, 2015, INT J COMPUT ASS RAD, V10, P573, DOI 10.1007/s11548-014-1106-9
   Kim Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-77258-w
   Kolecki Radek., 2022, Translational Research in Anatomy, V28, P100214, DOI DOI 10.1016/J.TRIA.2022.100214
   Koller S, 2019, FORENSIC SCI INT, V295, P30, DOI 10.1016/j.forsciint.2018.11.006
   Kormilitzin A, 2021, ARTIF INTELL MED, V118, DOI 10.1016/j.artmed.2021.102086
   Kumaravel BT, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P161, DOI 10.1145/3332165.3347872
   Kurul R, 2020, ANAT SCI EDUC, V13, P648, DOI 10.1002/ase.1959
   Li W  ..., 2023, Gastrointest. Endosc, V1, P152, DOI [10.1016/j.gande.2023.06.001, DOI 10.1016/J.GANDE.2023.06.001]
   López-Ubeda P, 2022, AM J PATHOL, V192, P1486, DOI 10.1016/j.ajpath.2022.07.012
   Messina P, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3522747
   Morrow WJ, 2016, POLICE Q, V19, P303, DOI 10.1177/1098611116652850
   Noain-Sánchez A, 2022, COMMUN SOC-SPAIN, V35, P105, DOI 10.15581/003.35.3.105-121
   O'Hare J, 2020, CODESIGN, V16, P111, DOI 10.1080/15710882.2018.1546319
   Poelman R., 2012, P ACM 2012 C COMP SU, P1267, DOI [10.1145/2145204.2145394, DOI 10.1145/2145204.2145394]
   Pooryousef Vahid, 2023, CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3544548.3580768
   Pooryousef V, 2020, PROCEEDINGS OF THE 31ST AUSTRALIAN CONFERENCE ON HUMAN-COMPUTER-INTERACTION (OZCHI'19), P433, DOI 10.1145/3369457.3369508
   Quesada-Olarte J  ..., 2022, The Journal of Sexual Medicine, V19, P1580, DOI [10.1016/j.jsxm.2022.07.0102,8, DOI 10.1016/J.JSXM.2022.07.0102,8]
   Raith A, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10040672
   Raj R., 2023, BenchCouncil Transac. Benchmarks, Standards and Evaluations, V3, DOI [DOI 10.1016/J.TBENCH.2023.100140, 10.1016/j.benchc.2023.100140, DOI 10.1016/J.BENCHC.2023.100140]
   Reichherzer Carolin, 2021, CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3411764.3445464
   Reinschluessel AV, 2022, FRONT SURG, V9, DOI 10.3389/fsurg.2022.821060
   Reipschläger P, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3383138
   Reipschlager P, 2021, IEEE T VIS COMPUT GR, V27, P1182, DOI 10.1109/TVCG.2020.3030460
   Rogers J, 2021, IEEE T VIS COMPUT GR, V27, P1106, DOI 10.1109/TVCG.2020.3030405
   Roy M., 2021, R0-HTC, P1, DOI [10.1109/R10-HTC53172.2021.9641627, DOI 10.1109/R10-HTC53172.2021.9641627]
   Saad A, 2023, SURG-J R COLL SURG E, V21, P263, DOI 10.1016/j.surge.2023.07.001
   Saghafian M, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.634352
   Salomon G  ..., 2005, Intelligence and technology, V4, P8
   Sarkar A  ..., 2023, CHI Extended Abstracts, P1, DOI [10.1145/3544549.3582741, DOI 10.1145/3544549.3582741]
   Sereno M, 2022, IEEE T VIS COMPUT GR, V28, P2530, DOI 10.1109/TVCG.2020.3032761
   Sezgin E, 2023, JMIR FORM RES, V7, DOI 10.2196/43014
   Sobania D, 2023, 2023 IEEE/ACM INTERNATIONAL WORKSHOP ON AUTOMATED PROGRAM REPAIR, APR, P23, DOI 10.1109/APR59189.2023.00012
   Sousa M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4057, DOI 10.1145/3025453.3025566
   Su YF, 2023, ASSESS WRIT, V57, DOI 10.1016/j.asw.2023.100752
   Suncksen M., 2019, P MUC, P755, DOI [10.1145/3340764.3344903, DOI 10.1145/3340764.3344903]
   Takatsu A., 2007, Rechtsmedizin, V17, P13, DOI [10.1007/s00194-006-0422-y, DOI 10.1007/S00194-006-0422-Y]
   Thia BC, 2019, SURV OPHTHALMOL, V64, P570, DOI 10.1016/j.survophthal.2019.01.005
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, J MED INTERNET RES, V25, DOI 10.2196/48009
   Wang XY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376657
   Wen Y, 2020, IEEE INT C BIOINFORM, P349, DOI 10.1109/BIBM49941.2020.9313481
   Yilmaz R., 2023, Comput. Hum. Behav.: Artificial Humans, V1, P100005, DOI DOI 10.1016/J.CHBAH.2023.100005
   Zaiti IA, 2015, PERS UBIQUIT COMPUT, V19, P821, DOI 10.1007/s00779-015-0863-y
   Zhai Y., 2013, P UBICOMP ADJ, P175, DOI [10.1145/2494091.2494148, DOI 10.1145/2494091.2494148]
   Zhang J, 2017, MULTIMED TOOLS APPL, V76, P12005, DOI 10.1007/s11042-016-3936-7
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P12910
   Zorzal ER, 2019, COMPUT GRAPH-UK, V85, P74, DOI 10.1016/j.cag.2019.09.006
NR 82
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7452
EP 7462
DI 10.1109/TVCG.2024.3456173
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300043
PM 39250385
DA 2024-11-06
ER

PT J
AU Nagano, R
   Kinoshita, T
   Hattori, S
   Hiroi, Y
   Itoh, Y
   Hiraki, T
AF Nagano, Rina
   Kinoshita, Takahiro
   Hattori, Shingo
   Hiroi, Yuichi
   Itoh, Yuta
   Hiraki, Takefumi
TI HaptoFloater: Visuo-Haptic Augmented Reality by Embedding Imperceptible
   Color Vibration Signals for Tactile Display Control in a Mid-Air Image
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic interfaces; Vibrations; Visualization; Image color analysis;
   Actuators; Delays; Tactile sensors; Visuo-haptic displays; mid-air
   images; LCD displays; imperceptible color vibration
AB We propose HaptoFloater, a low-latency mid-air visuo-haptic augmented reality (VHAR) system that utilizes imperceptible color vibrations. When adding tactile stimuli to the visual information of a mid-air image, the user should not perceive the latency between the tactile and visual information. However, conventional tactile presentation methods for mid-air images, based on camera-detected fingertip positioning, introduce latency due to image processing and communication. To mitigate this latency, we use a color vibration technique; humans cannot perceive the vibration when the display alternates between two different color stimuli at a frequency of 25 Hz or higher. In our system, we embed this imperceptible color vibration into the mid-air image formed by a micromirror array plate, and a photodiode on the fingertip device directly detects this color vibration to provide tactile stimulation. Thus, our system allows for the tactile perception of multiple patterns on a mid-air image in 59.5 ms. In addition, we evaluate the visual-haptic delay tolerance on a mid-air display using our VHAR system and a tactile actuator with a single pattern and faster response time. The results of our user study indicate a visual-haptic delay tolerance of 110.6 ms, which is considerably larger than the latency associated with systems using multiple tactile patterns.
C1 [Nagano, Rina; Kinoshita, Takahiro; Hattori, Shingo; Hiraki, Takefumi] Univ Tsukuba, Tsukuba, Japan.
   [Kinoshita, Takahiro; Hattori, Shingo; Hiroi, Yuichi; Hiraki, Takefumi] Cluster Metaverse Lab, Tsukuba, Japan.
   [Itoh, Yuta] Univ Tokyo, Tokyo, Japan.
C3 University of Tsukuba; University of Tokyo
RP Nagano, R (corresponding author), Univ Tsukuba, Tsukuba, Japan.
EM nagano@pml.slis.tsukuba.ac.jp; kinoshita@hpcs.cs.tsukuba.ac.jp;
   s.hattori@cluster.mu; y.hiroi@cluster.mu; yuta.itoh@iii.u-tokyo.ac.jp;
   t.hiraki@cluster.mu
RI Hiroi, Yuichi/KGM-7451-2024
OI Hiraki, Takefumi/0000-0002-5767-3607; Nagano, Rina/0009-0007-8338-8563;
   Kinoshita, Takahiro/0009-0000-9832-0296; Hiroi,
   Yuichi/0000-0001-8567-6947; Itoh, Yuta/0000-0002-5901-797X
FU JST FOREST [JP- MJFR206E]; JST ACT-X [JPMJAX190O]; JST ASPIRE
   [JPMJAP2327]; JSPS KAKENHI, Japan [JP20H04222]
FX This study was supported by JST FOREST Grant Number JP- MJFR206E, JST
   ACT-X Grant Number JPMJAX190O, JST ASPIRE Grant Number JPMJAP2327, and
   JSPS KAKENHI Grant Number JP20H04222, Japan.
CR Abe S., 2017, P 2017 CHI C HUM FAC, P1464, DOI [10.1145/3027063.3053074, DOI 10.1145/3027063.3053074]
   Basdogan C, 2020, IEEE T HAPTICS, V13, P450, DOI 10.1109/TOH.2020.2990712
   Bau O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185585
   Cosco F, 2013, IEEE T VIS COMPUT GR, V19, P159, DOI 10.1109/TVCG.2012.107
   Costes A, 2020, IEEE T HAPTICS, V13, P530, DOI 10.1109/TOH.2020.2984754
   de Tinguy X, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P81, DOI 10.1109/VR.2018.8446280
   Eck U, 2015, IEEE T VIS COMPUT GR, V21, P1427, DOI 10.1109/TVCG.2015.2480087
   Freeman E., 2017, P 19 ACM INT C MULT, P491, DOI [10.1145/3136755.31430203, DOI 10.1145/3136755.31430203, 10.1145/3136755.3143020, DOI 10.1145/3136755.3143020]
   Harders M, 2009, IEEE T VIS COMPUT GR, V15, P138, DOI 10.1109/TVCG.2008.63
   Hattori S., 2024, Measurement of the imperceptible threshold for color vibration pairs selected by using macadam ellipse
   Hattori S., 2022, SIGGRAPH ASIA 2022 P, DOI [10.1145/3550082.35641904,8, DOI 10.1145/3550082.35641904,8]
   He L, 2015, ISWC 2015: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P47, DOI 10.1145/2802083.2802091
   Hiraki T, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC'18 ADJUNCT), P584, DOI 10.1145/3267305.3267307
   Inami M., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P233, DOI 10.1109/VR.2000.840503
   Jiang Y, 2007, NAT NEUROSCI, V10, P657, DOI 10.1038/nn1879
   Kikuchi T, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2022), P713, DOI 10.1109/VRW55335.2022.00212
   Kimura S., 2008, P ACM SIGGRAPH
   Kiuchi S, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P606, DOI 10.1109/VR51125.2022.00081
   Kyung KU, 2009, IEEE COMPUT GRAPH, V29, P56, DOI 10.1109/MCG.2009.17
   Lee J.C., 2004, P 17 ANN ACM S USER, P291, DOI [10.1145/1029632, DOI 10.1145/1029632, DOI 10.1145/1029632.1029682]
   Lee Y, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11156932
   Leigh S.-W., 2015, ACM C TANGIBLE EMBED, P89, DOI DOI 10.1145/2677199.2680584
   Li-Wei Chan, 2008, 2008 IEEE International Workshop on Horizontal Interactive Human Computer Systems (TABLETOP), P169, DOI 10.1109/TABLETOP.2008.4660201
   Maeda Y, 2014, 2014 IEEE 3RD GLOBAL CONFERENCE ON CONSUMER ELECTRONICS (GCCE), P211, DOI 10.1109/GCCE.2014.7031217
   Maekawa S, 2006, PROC SPIE, V6392, DOI 10.1117/12.690574
   Maisto M, 2017, IEEE T HAPTICS, V10, P511, DOI 10.1109/TOH.2017.2691328
   Makino Y, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1980, DOI 10.1145/2858036.2858481
   Miyasato T., 1995, The Journal of the Institute of Television Engineers of Japan, V49, P1353
   Miyatake Y, 2023, IEEE T VIS COMPUT GR, V29, P2005, DOI 10.1109/TVCG.2021.3136214
   Monnai Y., 2014, P 27 ANN ACM S US IN, P663, DOI [DOI 10.1145/2642918.2647407, 10.1145/2642918.2647407]
   Morisaki T, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 EMERGING TECHNOLOGIES, DOI 10.1145/3476122.3484849
   Pacchierotti C, 2017, IEEE T HAPTICS, V10, P580, DOI 10.1109/TOH.2017.2689006
   Rahal L, 2009, 2009 IEEE INTERNATIONAL WORKSHOP ON ROBOTIC AND SENSORS ENVIRONMENTS (ROSE 2009), P86, DOI 10.1109/ROSE.2009.5355986
   Rekimoto Jun., 2009, Proceedings of the 27th international conference extended abstracts on Human factors in computing systems - CHI EA'09, page, P2519
   Sandor C., 2007, Technical Report 106, P1
   Sandor C, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P292
   Silva JM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457451
   Strese Matti, 2014, 2014 IEEE International Symposium on Haptic, Audio and Visual Environments and Games (HAVE). Proceedings, P118, DOI 10.1109/HAVE.2014.6954342
   Systems D., "Touch"[Sold as Phantom Omni at by SensAbleTechnologies]
   Tsuchiya K, 2021, IEEE ACCESS, V9, P25110, DOI 10.1109/ACCESS.2021.3057056
   Ueda Y., 2015, 22 INT DISPL WORKSH, P1137
   Uematsu H., 2016, ACM SIGGRAPH 2016 EM, P2
   Wang D, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P303
   Wang DX, 2011, IEEE T HAPTICS, V4, P321, DOI [10.1109/TOH.2011.17, 10.1109/ToH.2011.17]
   Woo G, 2012, INT SYM MIX AUGMENT, P59, DOI 10.1109/ISMAR.2012.6402539
   Xiao R, 2016, PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES, (ISS 2016), P169, DOI 10.1145/2992154.2992182
   Yamamoto G, 2017, IEICE T INF SYST, VE100D, P2027, DOI 10.1587/transinf.2016PCP0015
   Yoshida K., 2017, ACM SIGGRAPH 2017 EM, P3
   Zhang L, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P372, DOI 10.1145/2789168.2790106
NR 49
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7463
EP 7472
DI 10.1109/TVCG.2024.3456179
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300046
PM 39283800
DA 2024-11-06
ER

PT J
AU Shen, HW
   Kiyokawa, K
AF Shen, Han-Wei
   Kiyokawa, Kiyoshi
TI Message from the Editor-in-Chief and from the Associate Editor-in-Chief
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Editorial Material
C1 [Shen, Han-Wei] Ohio State Univ, Columbus, OH 43210 USA.
   [Kiyokawa, Kiyoshi] Nara Inst Sci & Technol, Ikoma, Japan.
C3 University System of Ohio; Ohio State University; Nara Institute of
   Science & Technology
RP Shen, HW (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM shen.94@osu.edu
RI Shen, Han-wei/A-4710-2012
NR 0
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
DI 10.1109/TVCG.2024.3456212
PG 2
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300009
OA Green Published
DA 2024-11-06
ER

EF